[
    {
        "details": [
            {
                "question verbose": "What is to arm ",
                "b": "arm",
                "expected answer": [
                    "armless"
                ],
                "predictions": [
                    {
                        "score": 0.15687521349538583,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.1548483150321482,
                        "answer": "smaller",
                        "hit": false
                    },
                    {
                        "score": 0.15209512309147913,
                        "answer": "ultrabooks",
                        "hit": false
                    },
                    {
                        "score": 0.15056692708363675,
                        "answer": "frame",
                        "hit": false
                    },
                    {
                        "score": 0.14712644733226105,
                        "answer": "osx",
                        "hit": false
                    },
                    {
                        "score": 0.1469357410043456,
                        "answer": "impactwith",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "arm"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5782459080219269
            },
            {
                "question verbose": "What is to art ",
                "b": "art",
                "expected answer": [
                    "artless"
                ],
                "predictions": [
                    {
                        "score": 0.15646712000505267,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.1383749538515926,
                        "answer": "latest",
                        "hit": false
                    },
                    {
                        "score": 0.13599694938720314,
                        "answer": "educational",
                        "hit": false
                    },
                    {
                        "score": 0.12936464182631702,
                        "answer": "curricular",
                        "hit": false
                    },
                    {
                        "score": 0.1254986132138054,
                        "answer": "computerland",
                        "hit": false
                    },
                    {
                        "score": 0.12372732581868005,
                        "answer": "martial",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "art"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5198490042239428
            },
            {
                "question verbose": "What is to bone ",
                "b": "bone",
                "expected answer": [
                    "boneless"
                ],
                "predictions": [
                    {
                        "score": 0.18870481683496046,
                        "answer": "infuse",
                        "hit": false
                    },
                    {
                        "score": 0.15970308196552346,
                        "answer": "joining",
                        "hit": false
                    },
                    {
                        "score": 0.15075066129624176,
                        "answer": "statistical",
                        "hit": false
                    },
                    {
                        "score": 0.14872562064410735,
                        "answer": "kindle",
                        "hit": false
                    },
                    {
                        "score": 0.14542372113792995,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.14220625867131878,
                        "answer": "enjoying",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bone"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.47080487199127674
            },
            {
                "question verbose": "What is to breath ",
                "b": "breath",
                "expected answer": [
                    "breathless"
                ],
                "predictions": [
                    {
                        "score": 0.21292385726835536,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.19299555831012186,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.18930059768806962,
                        "answer": "lebowski",
                        "hit": false
                    },
                    {
                        "score": 0.18639333444999853,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.1861358251238546,
                        "answer": "monitoring",
                        "hit": false
                    },
                    {
                        "score": 0.18318298046548315,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "breath"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6124536246061325
            },
            {
                "question verbose": "What is to carbon ",
                "b": "carbon",
                "expected answer": [
                    "carbonless"
                ],
                "predictions": [
                    {
                        "score": 0.21856647177009383,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19255656532962254,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.18022587981220492,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.1721897127599143,
                        "answer": "reagan",
                        "hit": false
                    },
                    {
                        "score": 0.1721185866114888,
                        "answer": "surprising",
                        "hit": false
                    },
                    {
                        "score": 0.1621206059493505,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "carbon"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6272585541009903
            },
            {
                "question verbose": "What is to child ",
                "b": "child",
                "expected answer": [
                    "childless"
                ],
                "predictions": [
                    {
                        "score": 0.13310965960242982,
                        "answer": "academic",
                        "hit": false
                    },
                    {
                        "score": 0.12939879723586029,
                        "answer": "smugler",
                        "hit": false
                    },
                    {
                        "score": 0.12847844470516637,
                        "answer": "convicted",
                        "hit": false
                    },
                    {
                        "score": 0.12570190282910923,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.12544811686121635,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.1246165887246587,
                        "answer": "recommend",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "child"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5691130384802818
            },
            {
                "question verbose": "What is to collar ",
                "b": "collar",
                "expected answer": [
                    "collarless"
                ],
                "predictions": [
                    {
                        "score": 0.27059477575802915,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.24726944196825398,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.24319986375679334,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.24004651515897948,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.235851089432729,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.23272405908513666,
                        "answer": "exagerate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "collar"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6223726570606232
            },
            {
                "question verbose": "What is to death ",
                "b": "death",
                "expected answer": [
                    "deathless"
                ],
                "predictions": [
                    {
                        "score": 0.1652409496284994,
                        "answer": "therein",
                        "hit": false
                    },
                    {
                        "score": 0.1564706730738877,
                        "answer": "ethical",
                        "hit": false
                    },
                    {
                        "score": 0.15524727126971213,
                        "answer": "mooncakes",
                        "hit": false
                    },
                    {
                        "score": 0.14829653645097934,
                        "answer": "sculpture",
                        "hit": false
                    },
                    {
                        "score": 0.14711064359649811,
                        "answer": "commits",
                        "hit": false
                    },
                    {
                        "score": 0.14279563225152478,
                        "answer": "rigid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "death"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.533506378531456
            },
            {
                "question verbose": "What is to defence ",
                "b": "defence",
                "expected answer": [
                    "defenceless",
                    "defenseless"
                ],
                "predictions": [
                    {
                        "score": 0.8715077346846847,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21587104689548053,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.19265878271273254,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.18129792840890582,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.18030642429653712,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.17838416185594272,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "defence"
                ],
                "rank": 1832,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to effort ",
                "b": "effort",
                "expected answer": [
                    "effortless"
                ],
                "predictions": [
                    {
                        "score": 0.18505149364882595,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1674144238757221,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1228780068040972,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.1209178127235128,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.11610497567820757,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.11449864539423814,
                        "answer": "prefer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "effort"
                ],
                "rank": 2299,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6753151416778564
            },
            {
                "question verbose": "What is to ego ",
                "b": "ego",
                "expected answer": [
                    "egoless"
                ],
                "predictions": [
                    {
                        "score": 0.22925626190632534,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21189380196306312,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.20821862183867276,
                        "answer": "dose",
                        "hit": false
                    },
                    {
                        "score": 0.2040404636365347,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.2002829739109613,
                        "answer": "purely",
                        "hit": false
                    },
                    {
                        "score": 0.19699444107365233,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ego"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.633568525314331
            },
            {
                "question verbose": "What is to emotion ",
                "b": "emotion",
                "expected answer": [
                    "emotionless"
                ],
                "predictions": [
                    {
                        "score": 0.20433691686864852,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.19158833520977034,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.1707174990848292,
                        "answer": "authentic",
                        "hit": false
                    },
                    {
                        "score": 0.1672560066716345,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.1647014375722591,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.1641747134432138,
                        "answer": "backed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "emotion"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5472006350755692
            },
            {
                "question verbose": "What is to error ",
                "b": "error",
                "expected answer": [
                    "errorless"
                ],
                "predictions": [
                    {
                        "score": 0.24775924790813533,
                        "answer": "publicly",
                        "hit": false
                    },
                    {
                        "score": 0.17408342151149056,
                        "answer": "irreparable",
                        "hit": false
                    },
                    {
                        "score": 0.1738583259482074,
                        "answer": "purchased",
                        "hit": false
                    },
                    {
                        "score": 0.1718514312592035,
                        "answer": "expansiveness",
                        "hit": false
                    },
                    {
                        "score": 0.17074877188442086,
                        "answer": "furthermore",
                        "hit": false
                    },
                    {
                        "score": 0.16971884571760434,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "error"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5797998085618019
            },
            {
                "question verbose": "What is to expression ",
                "b": "expression",
                "expected answer": [
                    "expressionless"
                ],
                "predictions": [
                    {
                        "score": 0.21571081380732135,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17978197549872352,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.1787307629889399,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.17817762366365963,
                        "answer": "publicly",
                        "hit": false
                    },
                    {
                        "score": 0.16658873031007881,
                        "answer": "constitutes",
                        "hit": false
                    },
                    {
                        "score": 0.1661960886495382,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expression"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.625684380531311
            },
            {
                "question verbose": "What is to faith ",
                "b": "faith",
                "expected answer": [
                    "faithless"
                ],
                "predictions": [
                    {
                        "score": 0.1492139853984397,
                        "answer": "blaiming",
                        "hit": false
                    },
                    {
                        "score": 0.14453958233484546,
                        "answer": "argued",
                        "hit": false
                    },
                    {
                        "score": 0.13565848393950142,
                        "answer": "testimony",
                        "hit": false
                    },
                    {
                        "score": 0.1347519619515681,
                        "answer": "kindle",
                        "hit": false
                    },
                    {
                        "score": 0.12989965281069704,
                        "answer": "singing",
                        "hit": false
                    },
                    {
                        "score": 0.12975680498995287,
                        "answer": "static",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "faith"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5532718636095524
            },
            {
                "question verbose": "What is to friction ",
                "b": "friction",
                "expected answer": [
                    "frictionless"
                ],
                "predictions": [
                    {
                        "score": 0.26483750775118564,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25457836716222665,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.2378817095683959,
                        "answer": "monitoring",
                        "hit": false
                    },
                    {
                        "score": 0.23403663917916387,
                        "answer": "meditates",
                        "hit": false
                    },
                    {
                        "score": 0.229381099916912,
                        "answer": "omari",
                        "hit": false
                    },
                    {
                        "score": 0.22394035132926343,
                        "answer": "cilip",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "friction"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6543683707714081
            },
            {
                "question verbose": "What is to friend ",
                "b": "friend",
                "expected answer": [
                    "friendless"
                ],
                "predictions": [
                    {
                        "score": 0.15817610158269482,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.13419853616787725,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1232756153242096,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.12161828110857163,
                        "answer": "heckler",
                        "hit": false
                    },
                    {
                        "score": 0.11944560127358932,
                        "answer": "disciple",
                        "hit": false
                    },
                    {
                        "score": 0.11450172048857048,
                        "answer": "puncture",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "friend"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5782244801521301
            },
            {
                "question verbose": "What is to gender ",
                "b": "gender",
                "expected answer": [
                    "genderless"
                ],
                "predictions": [
                    {
                        "score": 0.24909682269364003,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.22724136160717118,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.22336542976839965,
                        "answer": "publicly",
                        "hit": false
                    },
                    {
                        "score": 0.21912317330305708,
                        "answer": "expansiveness",
                        "hit": false
                    },
                    {
                        "score": 0.21668193114986362,
                        "answer": "hanh",
                        "hit": false
                    },
                    {
                        "score": 0.21472262606954032,
                        "answer": "slick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gender"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5634922161698341
            },
            {
                "question verbose": "What is to goal ",
                "b": "goal",
                "expected answer": [
                    "goalless"
                ],
                "predictions": [
                    {
                        "score": 0.30458246570698433,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2060275156447379,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.19237554093906808,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.17022341987541498,
                        "answer": "underage",
                        "hit": false
                    },
                    {
                        "score": 0.16985607328924962,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.16764302307900772,
                        "answer": "sociopath",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goal"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6773351728916168
            },
            {
                "question verbose": "What is to god ",
                "b": "god",
                "expected answer": [
                    "godless"
                ],
                "predictions": [
                    {
                        "score": 0.15502146808201972,
                        "answer": "blessing",
                        "hit": false
                    },
                    {
                        "score": 0.15264881892433452,
                        "answer": "lord",
                        "hit": false
                    },
                    {
                        "score": 0.14956733276297596,
                        "answer": "mormon",
                        "hit": false
                    },
                    {
                        "score": 0.14359260145961525,
                        "answer": "slowed",
                        "hit": false
                    },
                    {
                        "score": 0.1432197247925352,
                        "answer": "established",
                        "hit": false
                    },
                    {
                        "score": 0.14212416732656216,
                        "answer": "thr",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "god"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5560401193797588
            },
            {
                "question verbose": "What is to guile ",
                "b": "guile",
                "expected answer": [
                    "guileless"
                ],
                "predictions": [
                    {
                        "score": 0.8716316338619291,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21522377297309003,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1912489156672105,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1854247825952298,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.17872998985584476,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.17754157665979203,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guile"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to guilt ",
                "b": "guilt",
                "expected answer": [
                    "guiltless"
                ],
                "predictions": [
                    {
                        "score": 0.20669010073459065,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.20358986434006335,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.19458367935807003,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.1928266008043027,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.18917891683620716,
                        "answer": "sentinel",
                        "hit": false
                    },
                    {
                        "score": 0.18654051893495147,
                        "answer": "heartless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guilt"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5981351733207703
            },
            {
                "question verbose": "What is to hair ",
                "b": "hair",
                "expected answer": [
                    "hairless"
                ],
                "predictions": [
                    {
                        "score": 0.1493366344055675,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.1444450255154353,
                        "answer": "appointment",
                        "hit": false
                    },
                    {
                        "score": 0.1443298357917916,
                        "answer": "omari",
                        "hit": false
                    },
                    {
                        "score": 0.14100912601596427,
                        "answer": "wrangle",
                        "hit": false
                    },
                    {
                        "score": 0.13869640318685164,
                        "answer": "academic",
                        "hit": false
                    },
                    {
                        "score": 0.13790829232122578,
                        "answer": "adapting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hair"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5536688081920147
            },
            {
                "question verbose": "What is to heart ",
                "b": "heart",
                "expected answer": [
                    "heartless"
                ],
                "predictions": [
                    {
                        "score": 0.19200777235647515,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.13295917905129326,
                        "answer": "seek",
                        "hit": false
                    },
                    {
                        "score": 0.12784809111182868,
                        "answer": "pissed",
                        "hit": false
                    },
                    {
                        "score": 0.12748979399213167,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.12380658445756577,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.12179363251287366,
                        "answer": "exponentially",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "heart"
                ],
                "rank": 8077,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6147216707468033
            },
            {
                "question verbose": "What is to heir ",
                "b": "heir",
                "expected answer": [
                    "heirless"
                ],
                "predictions": [
                    {
                        "score": 0.8711313981798057,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2153522152808363,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.19851328121601378,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.18189102059321663,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.18087705384439595,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.17441571045450455,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "heir"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to home ",
                "b": "home",
                "expected answer": [
                    "homeless"
                ],
                "predictions": [
                    {
                        "score": 0.14541511215515535,
                        "answer": "adt",
                        "hit": false
                    },
                    {
                        "score": 0.13656657519298246,
                        "answer": "operation",
                        "hit": false
                    },
                    {
                        "score": 0.1357792338297272,
                        "answer": "depot",
                        "hit": false
                    },
                    {
                        "score": 0.13477029059692153,
                        "answer": "mainstream",
                        "hit": false
                    },
                    {
                        "score": 0.12625303951494057,
                        "answer": "primed",
                        "hit": false
                    },
                    {
                        "score": 0.12490690880670159,
                        "answer": "magneto",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "home"
                ],
                "rank": 10272,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.589729331433773
            },
            {
                "question verbose": "What is to law ",
                "b": "law",
                "expected answer": [
                    "lawless"
                ],
                "predictions": [
                    {
                        "score": 0.26754744085221915,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.15980464787485088,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.1493750433004813,
                        "answer": "involve",
                        "hit": false
                    },
                    {
                        "score": 0.14714264296651267,
                        "answer": "somehow",
                        "hit": false
                    },
                    {
                        "score": 0.14383283058743296,
                        "answer": "request",
                        "hit": false
                    },
                    {
                        "score": 0.14373798247098551,
                        "answer": "rush",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "law"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6556597054004669
            },
            {
                "question verbose": "What is to leg ",
                "b": "leg",
                "expected answer": [
                    "legless"
                ],
                "predictions": [
                    {
                        "score": 0.18110886050501754,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.17806308740043625,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.16979872529679835,
                        "answer": "computerland",
                        "hit": false
                    },
                    {
                        "score": 0.165835483718992,
                        "answer": "soaked",
                        "hit": false
                    },
                    {
                        "score": 0.16498469033012095,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.15882421460247556,
                        "answer": "velocity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leg"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5961104482412338
            },
            {
                "question verbose": "What is to life ",
                "b": "life",
                "expected answer": [
                    "lifeless",
                    "liveless"
                ],
                "predictions": [
                    {
                        "score": 0.1651869075138576,
                        "answer": "teenager",
                        "hit": false
                    },
                    {
                        "score": 0.14578793515937627,
                        "answer": "sport",
                        "hit": false
                    },
                    {
                        "score": 0.1347930068254395,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.13400621947125238,
                        "answer": "rigid",
                        "hit": false
                    },
                    {
                        "score": 0.1264912554718304,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.1251510533683729,
                        "answer": "description",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "life"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5431033745408058
            },
            {
                "question verbose": "What is to luck ",
                "b": "luck",
                "expected answer": [
                    "luckless"
                ],
                "predictions": [
                    {
                        "score": 0.22849281629119797,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21720246496982965,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.2134225257785406,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21316026186170323,
                        "answer": "nincompoop",
                        "hit": false
                    },
                    {
                        "score": 0.2127125454481853,
                        "answer": "exagerate",
                        "hit": false
                    },
                    {
                        "score": 0.21095228041764935,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "luck"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.633120983839035
            },
            {
                "question verbose": "What is to meat ",
                "b": "meat",
                "expected answer": [
                    "meatless"
                ],
                "predictions": [
                    {
                        "score": 0.3051383737068819,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28484972931300573,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.2846421856371103,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.25382894390140853,
                        "answer": "freaking",
                        "hit": false
                    },
                    {
                        "score": 0.25357350559136854,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.2534905186403923,
                        "answer": "computerland",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "meat"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6777160316705704
            },
            {
                "question verbose": "What is to mirth ",
                "b": "mirth",
                "expected answer": [
                    "mirthless"
                ],
                "predictions": [
                    {
                        "score": 0.8705194900068888,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2249334302169254,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.18989091172390585,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1839876767802646,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.18166443006553892,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.171194040426496,
                        "answer": "slick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mirth"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to money ",
                "b": "money",
                "expected answer": [
                    "moneyless"
                ],
                "predictions": [
                    {
                        "score": 0.1749221463275683,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1486196427978076,
                        "answer": "underage",
                        "hit": false
                    },
                    {
                        "score": 0.14769752742023512,
                        "answer": "scheme",
                        "hit": false
                    },
                    {
                        "score": 0.14349646982672767,
                        "answer": "madoff",
                        "hit": false
                    },
                    {
                        "score": 0.1417680068816167,
                        "answer": "freaking",
                        "hit": false
                    },
                    {
                        "score": 0.1396636762524212,
                        "answer": "tripled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "money"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6019043177366257
            },
            {
                "question verbose": "What is to odor ",
                "b": "odor",
                "expected answer": [
                    "odorless",
                    "odourless"
                ],
                "predictions": [
                    {
                        "score": 0.8718153515638057,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21880697234983443,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2016695910539555,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1855709646804274,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.18322000516884565,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.18078785387326146,
                        "answer": "slick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "odor"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to passion ",
                "b": "passion",
                "expected answer": [
                    "passionless"
                ],
                "predictions": [
                    {
                        "score": 0.1886308686103329,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.17565624545494873,
                        "answer": "teenager",
                        "hit": false
                    },
                    {
                        "score": 0.17183802769114664,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.1663001693651093,
                        "answer": "robinson",
                        "hit": false
                    },
                    {
                        "score": 0.16576128372608187,
                        "answer": "restrained",
                        "hit": false
                    },
                    {
                        "score": 0.16025772516431983,
                        "answer": "insanely",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "passion"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5575968809425831
            },
            {
                "question verbose": "What is to path ",
                "b": "path",
                "expected answer": [
                    "pathless"
                ],
                "predictions": [
                    {
                        "score": 0.16666327803462833,
                        "answer": "teenager",
                        "hit": false
                    },
                    {
                        "score": 0.15469853721151314,
                        "answer": "ore",
                        "hit": false
                    },
                    {
                        "score": 0.1522328473194458,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.15091191094655665,
                        "answer": "screening",
                        "hit": false
                    },
                    {
                        "score": 0.15083969351967128,
                        "answer": "willow",
                        "hit": false
                    },
                    {
                        "score": 0.14493857367412993,
                        "answer": "specie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "path"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.516118872910738
            },
            {
                "question verbose": "What is to penny ",
                "b": "penny",
                "expected answer": [
                    "penniless"
                ],
                "predictions": [
                    {
                        "score": 0.2684791732537546,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23174987248374682,
                        "answer": "homeless",
                        "hit": false
                    },
                    {
                        "score": 0.21905731478183343,
                        "answer": "monitoring",
                        "hit": false
                    },
                    {
                        "score": 0.21734941656923498,
                        "answer": "increment",
                        "hit": false
                    },
                    {
                        "score": 0.21693235834966765,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.21515481000832182,
                        "answer": "carol",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "penny"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6564870923757553
            },
            {
                "question verbose": "What is to remorse ",
                "b": "remorse",
                "expected answer": [
                    "remorseless"
                ],
                "predictions": [
                    {
                        "score": 0.23680070832642,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.2175679282201948,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.20816025756553494,
                        "answer": "recreational",
                        "hit": false
                    },
                    {
                        "score": 0.20027389579334837,
                        "answer": "electromagnetic",
                        "hit": false
                    },
                    {
                        "score": 0.19825929538847373,
                        "answer": "cilip",
                        "hit": false
                    },
                    {
                        "score": 0.1958566029261693,
                        "answer": "sentinel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remorse"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5893637910485268
            },
            {
                "question verbose": "What is to ruth ",
                "b": "ruth",
                "expected answer": [
                    "ruthless"
                ],
                "predictions": [
                    {
                        "score": 0.20860840762370667,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.20409387600460915,
                        "answer": "injunction",
                        "hit": false
                    },
                    {
                        "score": 0.20394042179751504,
                        "answer": "irreparable",
                        "hit": false
                    },
                    {
                        "score": 0.20232914059342574,
                        "answer": "counsel",
                        "hit": false
                    },
                    {
                        "score": 0.20213820625752776,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.1977881401719266,
                        "answer": "sacrificed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ruth"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5687837973237038
            },
            {
                "question verbose": "What is to sensor ",
                "b": "sensor",
                "expected answer": [
                    "sensorless"
                ],
                "predictions": [
                    {
                        "score": 0.282076130961334,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24601174733398504,
                        "answer": "computerland",
                        "hit": false
                    },
                    {
                        "score": 0.23393695664431038,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.2285598285465484,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.2272507634059522,
                        "answer": "carol",
                        "hit": false
                    },
                    {
                        "score": 0.22335907596952734,
                        "answer": "slick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sensor"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6643070578575134
            },
            {
                "question verbose": "What is to sleeve ",
                "b": "sleeve",
                "expected answer": [
                    "sleeveless"
                ],
                "predictions": [
                    {
                        "score": 0.2525971588843409,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23573808244343553,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.23303591569163404,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2205208393145194,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.2144251504030675,
                        "answer": "skeptical",
                        "hit": false
                    },
                    {
                        "score": 0.2120844733386064,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sleeve"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6470571905374527
            },
            {
                "question verbose": "What is to soul ",
                "b": "soul",
                "expected answer": [
                    "soulless"
                ],
                "predictions": [
                    {
                        "score": 0.18206967616200828,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.17815120709050564,
                        "answer": "heaven",
                        "hit": false
                    },
                    {
                        "score": 0.1722982549333785,
                        "answer": "loosie",
                        "hit": false
                    },
                    {
                        "score": 0.17186307568235268,
                        "answer": "catalyst",
                        "hit": false
                    },
                    {
                        "score": 0.1709524304542447,
                        "answer": "explicitly",
                        "hit": false
                    },
                    {
                        "score": 0.17037279692013213,
                        "answer": "upcoming",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "soul"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5285995099693537
            },
            {
                "question verbose": "What is to speech ",
                "b": "speech",
                "expected answer": [
                    "speechless"
                ],
                "predictions": [
                    {
                        "score": 0.18507472137387607,
                        "answer": "museum",
                        "hit": false
                    },
                    {
                        "score": 0.17609711157593108,
                        "answer": "curator",
                        "hit": false
                    },
                    {
                        "score": 0.1726806524324264,
                        "answer": "message",
                        "hit": false
                    },
                    {
                        "score": 0.1547813320868542,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.15422000334625557,
                        "answer": "accord",
                        "hit": false
                    },
                    {
                        "score": 0.14849579738700627,
                        "answer": "context",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "speech"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5713846758008003
            },
            {
                "question verbose": "What is to spine ",
                "b": "spine",
                "expected answer": [
                    "spineless"
                ],
                "predictions": [
                    {
                        "score": 0.23850884539993164,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.20109776552315475,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.18991609863522474,
                        "answer": "heartless",
                        "hit": false
                    },
                    {
                        "score": 0.18906964502667625,
                        "answer": "timely",
                        "hit": false
                    },
                    {
                        "score": 0.18731113468239624,
                        "answer": "carol",
                        "hit": false
                    },
                    {
                        "score": 0.18727066038712806,
                        "answer": "shia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spine"
                ],
                "rank": 10824,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7843426764011383
            },
            {
                "question verbose": "What is to tact ",
                "b": "tact",
                "expected answer": [
                    "tactless"
                ],
                "predictions": [
                    {
                        "score": 0.871109348184861,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2149772284845657,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.19125346870011886,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1880175068389389,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.1789020895259662,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.17574627437492163,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tact"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to talent ",
                "b": "talent",
                "expected answer": [
                    "talentless"
                ],
                "predictions": [
                    {
                        "score": 0.17439961709455712,
                        "answer": "lebowski",
                        "hit": false
                    },
                    {
                        "score": 0.17108212934053474,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1664329398022402,
                        "answer": "sentinel",
                        "hit": false
                    },
                    {
                        "score": 0.1658792978254297,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.1599423911826337,
                        "answer": "react",
                        "hit": false
                    },
                    {
                        "score": 0.15448143069405304,
                        "answer": "fruit",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "talent"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5997318476438522
            },
            {
                "question verbose": "What is to thought ",
                "b": "thought",
                "expected answer": [
                    "thoughtless"
                ],
                "predictions": [
                    {
                        "score": 0.14314656082163363,
                        "answer": "bothered",
                        "hit": false
                    },
                    {
                        "score": 0.14239660432903123,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.1327735749755124,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.12659512057895966,
                        "answer": "insanely",
                        "hit": false
                    },
                    {
                        "score": 0.12523082992310855,
                        "answer": "bet",
                        "hit": false
                    },
                    {
                        "score": 0.12311473843015734,
                        "answer": "inundated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "thought"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5774021372199059
            },
            {
                "question verbose": "What is to tooth ",
                "b": "tooth",
                "expected answer": [
                    "toothless"
                ],
                "predictions": [
                    {
                        "score": 0.8714729299922148,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21742995236423043,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.19400134953370843,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.18183347507699432,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.17833075621830158,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.17173928606219377,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tooth"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to window ",
                "b": "window",
                "expected answer": [
                    "windowless"
                ],
                "predictions": [
                    {
                        "score": 0.22280819833156035,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.18956454312207677,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.18283998925933823,
                        "answer": "ultrabooks",
                        "hit": false
                    },
                    {
                        "score": 0.1766484387687141,
                        "answer": "launch",
                        "hit": false
                    },
                    {
                        "score": 0.17367114570676923,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.17315134237793478,
                        "answer": "recommend",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "window"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6103275716304779
            },
            {
                "question verbose": "What is to wit ",
                "b": "wit",
                "expected answer": [
                    "witless"
                ],
                "predictions": [
                    {
                        "score": 0.2334477780039278,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.2115548325662887,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.2114130790548621,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.2104181677147163,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.20264778112265153,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19856782414472493,
                        "answer": "usher",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wit"
                ],
                "rank": 7068,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7659717202186584
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D01 [noun+less_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "d424261c-5a8d-4f36-bce3-8dec38e187ed",
            "timestamp": "2020-10-22T15:56:57.328102"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to able ",
                "b": "able",
                "expected answer": [
                    "unable"
                ],
                "predictions": [
                    {
                        "score": 0.32626360476245825,
                        "answer": "veto",
                        "hit": false
                    },
                    {
                        "score": 0.3154303541763349,
                        "answer": "alter",
                        "hit": false
                    },
                    {
                        "score": 0.31068791467092305,
                        "answer": "apg",
                        "hit": false
                    },
                    {
                        "score": 0.30979658252031633,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.3086618981255998,
                        "answer": "awry",
                        "hit": false
                    },
                    {
                        "score": 0.3061609444486766,
                        "answer": "motorcycle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "able"
                ],
                "rank": 2993,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7326332479715347
            },
            {
                "question verbose": "What is to acceptable ",
                "b": "acceptable",
                "expected answer": [
                    "unacceptable"
                ],
                "predictions": [
                    {
                        "score": 0.5011348303340547,
                        "answer": "escaping",
                        "hit": false
                    },
                    {
                        "score": 0.48729870669415565,
                        "answer": "solvent",
                        "hit": false
                    },
                    {
                        "score": 0.4745407674416222,
                        "answer": "mtaz",
                        "hit": false
                    },
                    {
                        "score": 0.4742676485695269,
                        "answer": "morally",
                        "hit": false
                    },
                    {
                        "score": 0.46945322705158316,
                        "answer": "depiction",
                        "hit": false
                    },
                    {
                        "score": 0.46491596150541237,
                        "answer": "unreasonable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "acceptable"
                ],
                "rank": 2903,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8029042482376099
            },
            {
                "question verbose": "What is to affected ",
                "b": "affected",
                "expected answer": [
                    "unaffected"
                ],
                "predictions": [
                    {
                        "score": 0.4938612492747888,
                        "answer": "secrecy",
                        "hit": false
                    },
                    {
                        "score": 0.49109774478056123,
                        "answer": "lek",
                        "hit": false
                    },
                    {
                        "score": 0.4790289897207165,
                        "answer": "enlightened",
                        "hit": false
                    },
                    {
                        "score": 0.47868066009691096,
                        "answer": "scoped",
                        "hit": false
                    },
                    {
                        "score": 0.46999155650451896,
                        "answer": "domsticated",
                        "hit": false
                    },
                    {
                        "score": 0.4695458760695547,
                        "answer": "outsourcing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "affected"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5878384187817574
            },
            {
                "question verbose": "What is to authorized ",
                "b": "authorized",
                "expected answer": [
                    "unauthorized",
                    "unauthorised"
                ],
                "predictions": [
                    {
                        "score": 0.5441669678013915,
                        "answer": "submerge",
                        "hit": false
                    },
                    {
                        "score": 0.5432667862174703,
                        "answer": "framed",
                        "hit": false
                    },
                    {
                        "score": 0.5404120729642169,
                        "answer": "insofar",
                        "hit": false
                    },
                    {
                        "score": 0.5337790903327432,
                        "answer": "touchstone",
                        "hit": false
                    },
                    {
                        "score": 0.5331343759979111,
                        "answer": "completion",
                        "hit": false
                    },
                    {
                        "score": 0.5331091519677088,
                        "answer": "wooden",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "authorized"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6479163318872452
            },
            {
                "question verbose": "What is to available ",
                "b": "available",
                "expected answer": [
                    "unavailable"
                ],
                "predictions": [
                    {
                        "score": 0.33478734171586805,
                        "answer": "wellness",
                        "hit": false
                    },
                    {
                        "score": 0.3294537871236081,
                        "answer": "upgradeequipment",
                        "hit": false
                    },
                    {
                        "score": 0.31209993727102286,
                        "answer": "aukerman",
                        "hit": false
                    },
                    {
                        "score": 0.3098164986744206,
                        "answer": "charter",
                        "hit": false
                    },
                    {
                        "score": 0.3089997332655459,
                        "answer": "octane",
                        "hit": false
                    },
                    {
                        "score": 0.3067522390755533,
                        "answer": "needing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "available"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6032421514391899
            },
            {
                "question verbose": "What is to avoidable ",
                "b": "avoidable",
                "expected answer": [
                    "unavoidable"
                ],
                "predictions": [
                    {
                        "score": 0.7145196902034101,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.36705421521666765,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.3490648140562666,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.34274637775461986,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.33775411967398905,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.33440926617479433,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "avoidable"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to aware ",
                "b": "aware",
                "expected answer": [
                    "unaware"
                ],
                "predictions": [
                    {
                        "score": 0.3963166188425015,
                        "answer": "calvinistic",
                        "hit": false
                    },
                    {
                        "score": 0.3860545140439674,
                        "answer": "recognizing",
                        "hit": false
                    },
                    {
                        "score": 0.38435731760138314,
                        "answer": "hmmm",
                        "hit": false
                    },
                    {
                        "score": 0.3779125028897493,
                        "answer": "factcheckorg",
                        "hit": false
                    },
                    {
                        "score": 0.37002907157553977,
                        "answer": "sanctified",
                        "hit": false
                    },
                    {
                        "score": 0.36245608267297014,
                        "answer": "promised",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aware"
                ],
                "rank": 1441,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7151608467102051
            },
            {
                "question verbose": "What is to believable ",
                "b": "believable",
                "expected answer": [
                    "unbelievable"
                ],
                "predictions": [
                    {
                        "score": 0.5121036307764935,
                        "answer": "scoped",
                        "hit": false
                    },
                    {
                        "score": 0.5111478726924136,
                        "answer": "recognizing",
                        "hit": false
                    },
                    {
                        "score": 0.5109382946881559,
                        "answer": "pavia",
                        "hit": false
                    },
                    {
                        "score": 0.5072762833221005,
                        "answer": "elegance",
                        "hit": false
                    },
                    {
                        "score": 0.5017928290935448,
                        "answer": "scored",
                        "hit": false
                    },
                    {
                        "score": 0.49760763140730835,
                        "answer": "sketchiness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believable"
                ],
                "rank": 10424,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8472983539104462
            },
            {
                "question verbose": "What is to biased ",
                "b": "biased",
                "expected answer": [
                    "unbiased"
                ],
                "predictions": [
                    {
                        "score": 0.4770702518280142,
                        "answer": "seemsto",
                        "hit": false
                    },
                    {
                        "score": 0.4541990697886216,
                        "answer": "artificial",
                        "hit": false
                    },
                    {
                        "score": 0.4532248984114462,
                        "answer": "atm",
                        "hit": false
                    },
                    {
                        "score": 0.449025177763025,
                        "answer": "cult",
                        "hit": false
                    },
                    {
                        "score": 0.44542115487499956,
                        "answer": "ivy",
                        "hit": false
                    },
                    {
                        "score": 0.4359944400716493,
                        "answer": "heinous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "biased"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6295642852783203
            },
            {
                "question verbose": "What is to certain ",
                "b": "certain",
                "expected answer": [
                    "uncertain"
                ],
                "predictions": [
                    {
                        "score": 0.32048639082066155,
                        "answer": "morally",
                        "hit": false
                    },
                    {
                        "score": 0.310956306137423,
                        "answer": "mh",
                        "hit": false
                    },
                    {
                        "score": 0.3096550693353625,
                        "answer": "abusing",
                        "hit": false
                    },
                    {
                        "score": 0.30219709772015363,
                        "answer": "eternal",
                        "hit": false
                    },
                    {
                        "score": 0.30186103894353766,
                        "answer": "horrific",
                        "hit": false
                    },
                    {
                        "score": 0.30152500542947597,
                        "answer": "flame",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "certain"
                ],
                "rank": 1478,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6872639060020447
            },
            {
                "question verbose": "What is to changed ",
                "b": "changed",
                "expected answer": [
                    "unchanged"
                ],
                "predictions": [
                    {
                        "score": 0.3818949462711212,
                        "answer": "eternal",
                        "hit": false
                    },
                    {
                        "score": 0.37118622729041956,
                        "answer": "huffington",
                        "hit": false
                    },
                    {
                        "score": 0.36750223565398676,
                        "answer": "calvinistic",
                        "hit": false
                    },
                    {
                        "score": 0.36280766659774966,
                        "answer": "mamasitas",
                        "hit": false
                    },
                    {
                        "score": 0.3554179924885827,
                        "answer": "behaved",
                        "hit": false
                    },
                    {
                        "score": 0.35535597501764604,
                        "answer": "unreasonable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "changed"
                ],
                "rank": 2205,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7121904641389847
            },
            {
                "question verbose": "What is to comfortable ",
                "b": "comfortable",
                "expected answer": [
                    "uncomfortable"
                ],
                "predictions": [
                    {
                        "score": 0.38631541576190886,
                        "answer": "marital",
                        "hit": false
                    },
                    {
                        "score": 0.38206003993930165,
                        "answer": "ebates",
                        "hit": false
                    },
                    {
                        "score": 0.37686597409351963,
                        "answer": "perceptionopinion",
                        "hit": false
                    },
                    {
                        "score": 0.37430378564697087,
                        "answer": "dissect",
                        "hit": false
                    },
                    {
                        "score": 0.37239229159007,
                        "answer": "rallied",
                        "hit": false
                    },
                    {
                        "score": 0.3672686004758278,
                        "answer": "transcended",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "comfortable"
                ],
                "rank": 6007,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7212338447570801
            },
            {
                "question verbose": "What is to conditional ",
                "b": "conditional",
                "expected answer": [
                    "unconditional"
                ],
                "predictions": [
                    {
                        "score": 0.722116816770587,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34840075083262556,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.3328159228027152,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.33164000425427415,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.32664970688781847,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3224148449046045,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "conditional"
                ],
                "rank": 10356,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6240331530570984
            },
            {
                "question verbose": "What is to conscious ",
                "b": "conscious",
                "expected answer": [
                    "unconscious"
                ],
                "predictions": [
                    {
                        "score": 0.4706365629432782,
                        "answer": "genesis",
                        "hit": false
                    },
                    {
                        "score": 0.46839398307629937,
                        "answer": "someday",
                        "hit": false
                    },
                    {
                        "score": 0.45575698604686227,
                        "answer": "lek",
                        "hit": false
                    },
                    {
                        "score": 0.4551160736763869,
                        "answer": "normality",
                        "hit": false
                    },
                    {
                        "score": 0.4510728231867496,
                        "answer": "heartbroken",
                        "hit": false
                    },
                    {
                        "score": 0.45029051961554356,
                        "answer": "baal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "conscious"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5711939781904221
            },
            {
                "question verbose": "What is to controlled ",
                "b": "controlled",
                "expected answer": [
                    "uncontrolled"
                ],
                "predictions": [
                    {
                        "score": 0.4183382498460774,
                        "answer": "mandated",
                        "hit": false
                    },
                    {
                        "score": 0.39651712590722815,
                        "answer": "charter",
                        "hit": false
                    },
                    {
                        "score": 0.3883653809762364,
                        "answer": "bona",
                        "hit": false
                    },
                    {
                        "score": 0.3866114230625301,
                        "answer": "recognizes",
                        "hit": false
                    },
                    {
                        "score": 0.3818498066215074,
                        "answer": "edict",
                        "hit": false
                    },
                    {
                        "score": 0.37944569914793747,
                        "answer": "sphere",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "controlled"
                ],
                "rank": 1374,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.731017217040062
            },
            {
                "question verbose": "What is to desirable ",
                "b": "desirable",
                "expected answer": [
                    "undesirable"
                ],
                "predictions": [
                    {
                        "score": 0.7235389292357711,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.35727966161930913,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.33782830570353356,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3364642811090237,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.33315827103265405,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.32619260564348457,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "desirable"
                ],
                "rank": 8482,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6332297921180725
            },
            {
                "question verbose": "What is to employed ",
                "b": "employed",
                "expected answer": [
                    "unemployed"
                ],
                "predictions": [
                    {
                        "score": 0.43710292210318297,
                        "answer": "predicament",
                        "hit": false
                    },
                    {
                        "score": 0.42295020840874376,
                        "answer": "raping",
                        "hit": false
                    },
                    {
                        "score": 0.4173904900196369,
                        "answer": "terminate",
                        "hit": false
                    },
                    {
                        "score": 0.41528699769059335,
                        "answer": "bona",
                        "hit": false
                    },
                    {
                        "score": 0.41357695223284663,
                        "answer": "mtaz",
                        "hit": false
                    },
                    {
                        "score": 0.4127090790220796,
                        "answer": "superceding",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "employed"
                ],
                "rank": 6792,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7212826460599899
            },
            {
                "question verbose": "What is to expected ",
                "b": "expected",
                "expected answer": [
                    "unexpected"
                ],
                "predictions": [
                    {
                        "score": 0.39355923779009416,
                        "answer": "mh",
                        "hit": false
                    },
                    {
                        "score": 0.3811188652209346,
                        "answer": "increment",
                        "hit": false
                    },
                    {
                        "score": 0.3798761953192759,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.37649936288971353,
                        "answer": "capitalized",
                        "hit": false
                    },
                    {
                        "score": 0.3756861436530027,
                        "answer": "chronology",
                        "hit": false
                    },
                    {
                        "score": 0.37238193301314126,
                        "answer": "destabilize",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expected"
                ],
                "rank": 10319,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6343560814857483
            },
            {
                "question verbose": "What is to finished ",
                "b": "finished",
                "expected answer": [
                    "unfinished"
                ],
                "predictions": [
                    {
                        "score": 0.3903922746106229,
                        "answer": "utilised",
                        "hit": false
                    },
                    {
                        "score": 0.3842564663240862,
                        "answer": "stall",
                        "hit": false
                    },
                    {
                        "score": 0.3826278256478441,
                        "answer": "scored",
                        "hit": false
                    },
                    {
                        "score": 0.3824447548288194,
                        "answer": "ramble",
                        "hit": false
                    },
                    {
                        "score": 0.3823594346356065,
                        "answer": "doctoring",
                        "hit": false
                    },
                    {
                        "score": 0.3804501622723537,
                        "answer": "unreasonable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "finished"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5684300661087036
            },
            {
                "question verbose": "What is to forgettable ",
                "b": "forgettable",
                "expected answer": [
                    "unforgettable"
                ],
                "predictions": [
                    {
                        "score": 0.712277458509564,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3645318554569776,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.3523495865272171,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.34579168288347495,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.34435702696522297,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.33588625396866856,
                        "answer": "ingrate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "forgettable"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to fortunate ",
                "b": "fortunate",
                "expected answer": [
                    "unfortunate"
                ],
                "predictions": [
                    {
                        "score": 0.5472740447686469,
                        "answer": "decidedly",
                        "hit": false
                    },
                    {
                        "score": 0.5165718210021113,
                        "answer": "terminate",
                        "hit": false
                    },
                    {
                        "score": 0.5156909063306709,
                        "answer": "stimulous",
                        "hit": false
                    },
                    {
                        "score": 0.5139704163285688,
                        "answer": "checkout",
                        "hit": false
                    },
                    {
                        "score": 0.511098367432781,
                        "answer": "wellness",
                        "hit": false
                    },
                    {
                        "score": 0.5070484801148631,
                        "answer": "hr",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fortunate"
                ],
                "rank": 6588,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.817945808172226
            },
            {
                "question verbose": "What is to happy ",
                "b": "happy",
                "expected answer": [
                    "unhappy"
                ],
                "predictions": [
                    {
                        "score": 0.3666603823474348,
                        "answer": "synoptic",
                        "hit": false
                    },
                    {
                        "score": 0.35927070312409043,
                        "answer": "submerge",
                        "hit": false
                    },
                    {
                        "score": 0.3568649935803009,
                        "answer": "sophistry",
                        "hit": false
                    },
                    {
                        "score": 0.35510700337825185,
                        "answer": "wee",
                        "hit": false
                    },
                    {
                        "score": 0.35194035767917525,
                        "answer": "marry",
                        "hit": false
                    },
                    {
                        "score": 0.35052357324287725,
                        "answer": "decidedly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5431520976126194
            },
            {
                "question verbose": "What is to healthy ",
                "b": "healthy",
                "expected answer": [
                    "unhealthy"
                ],
                "predictions": [
                    {
                        "score": 0.38131369867804193,
                        "answer": "advising",
                        "hit": false
                    },
                    {
                        "score": 0.37988259470183855,
                        "answer": "paradigm",
                        "hit": false
                    },
                    {
                        "score": 0.375529486137343,
                        "answer": "marry",
                        "hit": false
                    },
                    {
                        "score": 0.3750936084050733,
                        "answer": "baal",
                        "hit": false
                    },
                    {
                        "score": 0.37415281272713347,
                        "answer": "smugler",
                        "hit": false
                    },
                    {
                        "score": 0.37271455941764287,
                        "answer": "visage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "healthy"
                ],
                "rank": 1249,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7494604140520096
            },
            {
                "question verbose": "What is to identified ",
                "b": "identified",
                "expected answer": [
                    "unidentified"
                ],
                "predictions": [
                    {
                        "score": 0.42215219986251384,
                        "answer": "danger",
                        "hit": false
                    },
                    {
                        "score": 0.40691821246748466,
                        "answer": "lek",
                        "hit": false
                    },
                    {
                        "score": 0.40406781195944774,
                        "answer": "escaping",
                        "hit": false
                    },
                    {
                        "score": 0.4025313398502079,
                        "answer": "idolatry",
                        "hit": false
                    },
                    {
                        "score": 0.40076947066993973,
                        "answer": "ideolgical",
                        "hit": false
                    },
                    {
                        "score": 0.38688828083130306,
                        "answer": "metropolitan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "identified"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.596056193113327
            },
            {
                "question verbose": "What is to intended ",
                "b": "intended",
                "expected answer": [
                    "unintended"
                ],
                "predictions": [
                    {
                        "score": 0.39435770517519464,
                        "answer": "voyager",
                        "hit": false
                    },
                    {
                        "score": 0.3914248050539861,
                        "answer": "ding",
                        "hit": false
                    },
                    {
                        "score": 0.3880189901393942,
                        "answer": "advising",
                        "hit": false
                    },
                    {
                        "score": 0.37043928420669353,
                        "answer": "doings",
                        "hit": false
                    },
                    {
                        "score": 0.36978405289996635,
                        "answer": "viewpointmovement",
                        "hit": false
                    },
                    {
                        "score": 0.3694170155562039,
                        "answer": "hr",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "intended"
                ],
                "rank": 4542,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7303639352321625
            },
            {
                "question verbose": "What is to interrupted ",
                "b": "interrupted",
                "expected answer": [
                    "uninterrupted"
                ],
                "predictions": [
                    {
                        "score": 0.5390420260739722,
                        "answer": "madman",
                        "hit": false
                    },
                    {
                        "score": 0.5060178815276015,
                        "answer": "terminate",
                        "hit": false
                    },
                    {
                        "score": 0.5051211488943422,
                        "answer": "mh",
                        "hit": false
                    },
                    {
                        "score": 0.5018883524456822,
                        "answer": "advising",
                        "hit": false
                    },
                    {
                        "score": 0.5017889173243717,
                        "answer": "checkout",
                        "hit": false
                    },
                    {
                        "score": 0.4999244032009263,
                        "answer": "doings",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interrupted"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6289225071668625
            },
            {
                "question verbose": "What is to known ",
                "b": "known",
                "expected answer": [
                    "unknown"
                ],
                "predictions": [
                    {
                        "score": 0.3324598840104456,
                        "answer": "involve",
                        "hit": false
                    },
                    {
                        "score": 0.32267493080558146,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.3113922314003055,
                        "answer": "utilised",
                        "hit": false
                    },
                    {
                        "score": 0.3113329928134298,
                        "answer": "behaved",
                        "hit": false
                    },
                    {
                        "score": 0.307012492317081,
                        "answer": "fossilized",
                        "hit": false
                    },
                    {
                        "score": 0.30555272653561244,
                        "answer": "inclined",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "known"
                ],
                "rank": 621,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6781086176633835
            },
            {
                "question verbose": "What is to lawful ",
                "b": "lawful",
                "expected answer": [
                    "unlawful"
                ],
                "predictions": [
                    {
                        "score": 0.7192971571074829,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3561262116969522,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.33536346485593893,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3308458388094788,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.32987768360056735,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.32771120791336045,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lawful"
                ],
                "rank": 4157,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6470324993133545
            },
            {
                "question verbose": "What is to lucky ",
                "b": "lucky",
                "expected answer": [
                    "unlucky"
                ],
                "predictions": [
                    {
                        "score": 0.4673461147813346,
                        "answer": "ideolgical",
                        "hit": false
                    },
                    {
                        "score": 0.4626215539138602,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.4549623137075594,
                        "answer": "decisionmaking",
                        "hit": false
                    },
                    {
                        "score": 0.443512248196084,
                        "answer": "barking",
                        "hit": false
                    },
                    {
                        "score": 0.4404380098844044,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.43994817903514616,
                        "answer": "leaner",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lucky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6323656737804413
            },
            {
                "question verbose": "What is to noticed ",
                "b": "noticed",
                "expected answer": [
                    "unnoticed"
                ],
                "predictions": [
                    {
                        "score": 0.4515707559845008,
                        "answer": "everywhen",
                        "hit": false
                    },
                    {
                        "score": 0.4450251813673961,
                        "answer": "transcended",
                        "hit": false
                    },
                    {
                        "score": 0.43890011786709515,
                        "answer": "seemsto",
                        "hit": false
                    },
                    {
                        "score": 0.4336092915720285,
                        "answer": "drastic",
                        "hit": false
                    },
                    {
                        "score": 0.42565816993676436,
                        "answer": "wtf",
                        "hit": false
                    },
                    {
                        "score": 0.41613106219169277,
                        "answer": "alter",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "noticed"
                ],
                "rank": 1435,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7601757943630219
            },
            {
                "question verbose": "What is to paid ",
                "b": "paid",
                "expected answer": [
                    "unpaid"
                ],
                "predictions": [
                    {
                        "score": 0.418160671943399,
                        "answer": "terminate",
                        "hit": false
                    },
                    {
                        "score": 0.40208542135544906,
                        "answer": "premium",
                        "hit": false
                    },
                    {
                        "score": 0.4020394217936727,
                        "answer": "reveler",
                        "hit": false
                    },
                    {
                        "score": 0.39522530427766744,
                        "answer": "bombing",
                        "hit": false
                    },
                    {
                        "score": 0.3951246992547352,
                        "answer": "superceding",
                        "hit": false
                    },
                    {
                        "score": 0.3941003221092254,
                        "answer": "legitimately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "paid"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.596386581659317
            },
            {
                "question verbose": "What is to pleasant ",
                "b": "pleasant",
                "expected answer": [
                    "unpleasant"
                ],
                "predictions": [
                    {
                        "score": 0.5499114812934811,
                        "answer": "utah",
                        "hit": false
                    },
                    {
                        "score": 0.5411061603589084,
                        "answer": "grove",
                        "hit": false
                    },
                    {
                        "score": 0.4980364404496764,
                        "answer": "orden",
                        "hit": false
                    },
                    {
                        "score": 0.4859137161138464,
                        "answer": "unreasonable",
                        "hit": false
                    },
                    {
                        "score": 0.4840338634380837,
                        "answer": "vested",
                        "hit": false
                    },
                    {
                        "score": 0.46561621370465095,
                        "answer": "respected",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pleasant"
                ],
                "rank": 6045,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6690257340669632
            },
            {
                "question verbose": "What is to popular ",
                "b": "popular",
                "expected answer": [
                    "unpopular"
                ],
                "predictions": [
                    {
                        "score": 0.3654072931346296,
                        "answer": "soooo",
                        "hit": false
                    },
                    {
                        "score": 0.3597101388689528,
                        "answer": "finale",
                        "hit": false
                    },
                    {
                        "score": 0.3565587334325005,
                        "answer": "huffington",
                        "hit": false
                    },
                    {
                        "score": 0.35352149683578954,
                        "answer": "rig",
                        "hit": false
                    },
                    {
                        "score": 0.35292728264303835,
                        "answer": "traitor",
                        "hit": false
                    },
                    {
                        "score": 0.3527455694193025,
                        "answer": "unnoticed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "popular"
                ],
                "rank": 7586,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.612834632396698
            },
            {
                "question verbose": "What is to predictable ",
                "b": "predictable",
                "expected answer": [
                    "unpredictable"
                ],
                "predictions": [
                    {
                        "score": 0.536316674633877,
                        "answer": "morally",
                        "hit": false
                    },
                    {
                        "score": 0.5304232602692441,
                        "answer": "ibelieber",
                        "hit": false
                    },
                    {
                        "score": 0.5303310947562807,
                        "answer": "eternal",
                        "hit": false
                    },
                    {
                        "score": 0.5302570512196109,
                        "answer": "toddler",
                        "hit": false
                    },
                    {
                        "score": 0.5238799704840332,
                        "answer": "scoped",
                        "hit": false
                    },
                    {
                        "score": 0.5229968309669346,
                        "answer": "submerge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "predictable"
                ],
                "rank": 8961,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8064783215522766
            },
            {
                "question verbose": "What is to published ",
                "b": "published",
                "expected answer": [
                    "unpublished"
                ],
                "predictions": [
                    {
                        "score": 0.4049193043223742,
                        "answer": "evading",
                        "hit": false
                    },
                    {
                        "score": 0.3952978321725916,
                        "answer": "assassin",
                        "hit": false
                    },
                    {
                        "score": 0.389399662324371,
                        "answer": "secrecy",
                        "hit": false
                    },
                    {
                        "score": 0.3828382671904759,
                        "answer": "reacting",
                        "hit": false
                    },
                    {
                        "score": 0.38252725237705265,
                        "answer": "eshelman",
                        "hit": false
                    },
                    {
                        "score": 0.37862262595024043,
                        "answer": "effectuate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "published"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5015606938395649
            },
            {
                "question verbose": "What is to realistic ",
                "b": "realistic",
                "expected answer": [
                    "unrealistic"
                ],
                "predictions": [
                    {
                        "score": 0.4683988093100891,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.45615306791044985,
                        "answer": "visage",
                        "hit": false
                    },
                    {
                        "score": 0.45473685092302885,
                        "answer": "stats",
                        "hit": false
                    },
                    {
                        "score": 0.4546362506589517,
                        "answer": "streetcar",
                        "hit": false
                    },
                    {
                        "score": 0.45118723958576995,
                        "answer": "calvinistic",
                        "hit": false
                    },
                    {
                        "score": 0.45054833866874794,
                        "answer": "advising",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "realistic"
                ],
                "rank": 10499,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.748876303434372
            },
            {
                "question verbose": "What is to reasonable ",
                "b": "reasonable",
                "expected answer": [
                    "unreasonable"
                ],
                "predictions": [
                    {
                        "score": 0.4214202621366068,
                        "answer": "analyse",
                        "hit": false
                    },
                    {
                        "score": 0.41265927849232753,
                        "answer": "salary",
                        "hit": false
                    },
                    {
                        "score": 0.4126350497595473,
                        "answer": "transcended",
                        "hit": false
                    },
                    {
                        "score": 0.4111986228381889,
                        "answer": "octane",
                        "hit": false
                    },
                    {
                        "score": 0.40472761455717726,
                        "answer": "unsoundness",
                        "hit": false
                    },
                    {
                        "score": 0.40231420493985026,
                        "answer": "pavia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reasonable"
                ],
                "rank": 239,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7454515099525452
            },
            {
                "question verbose": "What is to related ",
                "b": "related",
                "expected answer": [
                    "unrelated"
                ],
                "predictions": [
                    {
                        "score": 0.35499812062542424,
                        "answer": "traveller",
                        "hit": false
                    },
                    {
                        "score": 0.3437943015248576,
                        "answer": "boycoting",
                        "hit": false
                    },
                    {
                        "score": 0.34226366987332346,
                        "answer": "mtaz",
                        "hit": false
                    },
                    {
                        "score": 0.3408549740399255,
                        "answer": "foresee",
                        "hit": false
                    },
                    {
                        "score": 0.33603539736366567,
                        "answer": "socal",
                        "hit": false
                    },
                    {
                        "score": 0.3353360225160386,
                        "answer": "transcended",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "related"
                ],
                "rank": 1819,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7027506232261658
            },
            {
                "question verbose": "What is to reliable ",
                "b": "reliable",
                "expected answer": [
                    "unreliable"
                ],
                "predictions": [
                    {
                        "score": 0.5463766677623058,
                        "answer": "pitting",
                        "hit": false
                    },
                    {
                        "score": 0.5282020239456754,
                        "answer": "unreasonable",
                        "hit": false
                    },
                    {
                        "score": 0.5217209854023537,
                        "answer": "mamasitas",
                        "hit": false
                    },
                    {
                        "score": 0.520807936748056,
                        "answer": "reacting",
                        "hit": false
                    },
                    {
                        "score": 0.5144578920031501,
                        "answer": "secrecy",
                        "hit": false
                    },
                    {
                        "score": 0.49919599666557085,
                        "answer": "mainstay",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reliable"
                ],
                "rank": 10914,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7623296976089478
            },
            {
                "question verbose": "What is to resolved ",
                "b": "resolved",
                "expected answer": [
                    "unresolved"
                ],
                "predictions": [
                    {
                        "score": 0.5173289885016812,
                        "answer": "insanely",
                        "hit": false
                    },
                    {
                        "score": 0.515842943342132,
                        "answer": "advising",
                        "hit": false
                    },
                    {
                        "score": 0.49418454914167564,
                        "answer": "predicament",
                        "hit": false
                    },
                    {
                        "score": 0.49231973083550357,
                        "answer": "framed",
                        "hit": false
                    },
                    {
                        "score": 0.4923143528802259,
                        "answer": "heinous",
                        "hit": false
                    },
                    {
                        "score": 0.4831272549263721,
                        "answer": "unpleasant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "resolved"
                ],
                "rank": 4982,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8183215260505676
            },
            {
                "question verbose": "What is to restricted ",
                "b": "restricted",
                "expected answer": [
                    "unrestricted"
                ],
                "predictions": [
                    {
                        "score": 0.5286363438689031,
                        "answer": "mh",
                        "hit": false
                    },
                    {
                        "score": 0.5280461934643448,
                        "answer": "privacy",
                        "hit": false
                    },
                    {
                        "score": 0.5151161934256387,
                        "answer": "submerge",
                        "hit": false
                    },
                    {
                        "score": 0.5009733544974121,
                        "answer": "innate",
                        "hit": false
                    },
                    {
                        "score": 0.49744394385531365,
                        "answer": "lek",
                        "hit": false
                    },
                    {
                        "score": 0.4966042212957865,
                        "answer": "unpleasant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "restricted"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6203915327787399
            },
            {
                "question verbose": "What is to satisfactory ",
                "b": "satisfactory",
                "expected answer": [
                    "unsatisfactory"
                ],
                "predictions": [
                    {
                        "score": 0.7085582207730672,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3670990601738021,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.35074879610572685,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.34455562648863813,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.3370988700671808,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.32562871687715195,
                        "answer": "ingrate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "satisfactory"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to specified ",
                "b": "specified",
                "expected answer": [
                    "unspecified"
                ],
                "predictions": [
                    {
                        "score": 0.7125865328690677,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.35832272475207755,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.3380402842990454,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3373560308596915,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.3291740549785839,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.322713624008139,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "specified"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to successful ",
                "b": "successful",
                "expected answer": [
                    "unsuccessful"
                ],
                "predictions": [
                    {
                        "score": 0.38249750762331247,
                        "answer": "animating",
                        "hit": false
                    },
                    {
                        "score": 0.3781244844130582,
                        "answer": "definite",
                        "hit": false
                    },
                    {
                        "score": 0.3739977020539602,
                        "answer": "hunched",
                        "hit": false
                    },
                    {
                        "score": 0.3722459380234591,
                        "answer": "marital",
                        "hit": false
                    },
                    {
                        "score": 0.36864487331789264,
                        "answer": "heinous",
                        "hit": false
                    },
                    {
                        "score": 0.3662170496237875,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "successful"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6216649189591408
            },
            {
                "question verbose": "What is to suitable ",
                "b": "suitable",
                "expected answer": [
                    "unsuitable"
                ],
                "predictions": [
                    {
                        "score": 0.7114605639816748,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.35745342221650384,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.3423906744116819,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.33966664711627403,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3362673526364256,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3298520759115694,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "suitable"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to sustainable ",
                "b": "sustainable",
                "expected answer": [
                    "unsustainable"
                ],
                "predictions": [
                    {
                        "score": 0.46899719939590107,
                        "answer": "stiff",
                        "hit": false
                    },
                    {
                        "score": 0.45868458071844476,
                        "answer": "gitmo",
                        "hit": false
                    },
                    {
                        "score": 0.4522972215304018,
                        "answer": "veto",
                        "hit": false
                    },
                    {
                        "score": 0.44907818756688755,
                        "answer": "sophistry",
                        "hit": false
                    },
                    {
                        "score": 0.4403519402911575,
                        "answer": "horn",
                        "hit": false
                    },
                    {
                        "score": 0.4398306038782155,
                        "answer": "outlined",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sustainable"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.584089457988739
            },
            {
                "question verbose": "What is to used ",
                "b": "used",
                "expected answer": [
                    "unused"
                ],
                "predictions": [
                    {
                        "score": 0.33134990574244216,
                        "answer": "symptomatic",
                        "hit": false
                    },
                    {
                        "score": 0.3093709498689019,
                        "answer": "functionality",
                        "hit": false
                    },
                    {
                        "score": 0.30840234891494034,
                        "answer": "sbo",
                        "hit": false
                    },
                    {
                        "score": 0.2973330380413087,
                        "answer": "retired",
                        "hit": false
                    },
                    {
                        "score": 0.2951402856956008,
                        "answer": "debater",
                        "hit": false
                    },
                    {
                        "score": 0.29344128981613044,
                        "answer": "provoking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "used"
                ],
                "rank": 6467,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6551715731620789
            },
            {
                "question verbose": "What is to usual ",
                "b": "usual",
                "expected answer": [
                    "unusual"
                ],
                "predictions": [
                    {
                        "score": 0.3960760936331652,
                        "answer": "motorcycle",
                        "hit": false
                    },
                    {
                        "score": 0.385706855717498,
                        "answer": "clicheed",
                        "hit": false
                    },
                    {
                        "score": 0.35962551229533973,
                        "answer": "wheelchair",
                        "hit": false
                    },
                    {
                        "score": 0.3506056951830487,
                        "answer": "bushkerry",
                        "hit": false
                    },
                    {
                        "score": 0.3499438897691131,
                        "answer": "yell",
                        "hit": false
                    },
                    {
                        "score": 0.3470539002708573,
                        "answer": "completion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "usual"
                ],
                "rank": 2901,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.698139101266861
            },
            {
                "question verbose": "What is to veiled ",
                "b": "veiled",
                "expected answer": [
                    "unveiled"
                ],
                "predictions": [
                    {
                        "score": 0.7234913745748697,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3609106934464427,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.3439548583162123,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3407791905039831,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.33962445437467725,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.33887318471782985,
                        "answer": "ingrate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "veiled"
                ],
                "rank": 8297,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6097902581095695
            },
            {
                "question verbose": "What is to wanted ",
                "b": "wanted",
                "expected answer": [
                    "unwanted"
                ],
                "predictions": [
                    {
                        "score": 0.36218319742082433,
                        "answer": "shrooms",
                        "hit": false
                    },
                    {
                        "score": 0.34094903142454275,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.3385724614323152,
                        "answer": "capitol",
                        "hit": false
                    },
                    {
                        "score": 0.326356906066302,
                        "answer": "elmer",
                        "hit": false
                    },
                    {
                        "score": 0.3241659230115836,
                        "answer": "stampeded",
                        "hit": false
                    },
                    {
                        "score": 0.32249567786049205,
                        "answer": "cmm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wanted"
                ],
                "rank": 9703,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5937383249402046
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D02 [un+adj_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "df28ce65-7031-48ac-b915-e6af6b9491b9",
            "timestamp": "2020-10-22T15:56:59.582371"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to according ",
                "b": "according",
                "expected answer": [
                    "accordingly"
                ],
                "predictions": [
                    {
                        "score": 0.447054443938943,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.44509223535606496,
                        "answer": "persuaded",
                        "hit": false
                    },
                    {
                        "score": 0.44076926542560024,
                        "answer": "idolatry",
                        "hit": false
                    },
                    {
                        "score": 0.4317964540922199,
                        "answer": "depiction",
                        "hit": false
                    },
                    {
                        "score": 0.4202814505960632,
                        "answer": "framed",
                        "hit": false
                    },
                    {
                        "score": 0.41750200422698514,
                        "answer": "moviegoing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "according"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5845175236463547
            },
            {
                "question verbose": "What is to actual ",
                "b": "actual",
                "expected answer": [
                    "actually"
                ],
                "predictions": [
                    {
                        "score": 0.4006968039011697,
                        "answer": "prodigal",
                        "hit": false
                    },
                    {
                        "score": 0.39833630789461244,
                        "answer": "understood",
                        "hit": false
                    },
                    {
                        "score": 0.39075399809161143,
                        "answer": "spinoffs",
                        "hit": false
                    },
                    {
                        "score": 0.38554684843598724,
                        "answer": "dehydrates",
                        "hit": false
                    },
                    {
                        "score": 0.38440240042600005,
                        "answer": "tco",
                        "hit": false
                    },
                    {
                        "score": 0.3740470352697264,
                        "answer": "synoptic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "actual"
                ],
                "rank": 11215,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6141863316297531
            },
            {
                "question verbose": "What is to additional ",
                "b": "additional",
                "expected answer": [
                    "additionally"
                ],
                "predictions": [
                    {
                        "score": 0.46701460721760274,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.46612773935807994,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.45614010904606284,
                        "answer": "tacking",
                        "hit": false
                    },
                    {
                        "score": 0.44637191926803094,
                        "answer": "ethereal",
                        "hit": false
                    },
                    {
                        "score": 0.44614400973524154,
                        "answer": "osx",
                        "hit": false
                    },
                    {
                        "score": 0.4375312182696093,
                        "answer": "benevolence",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "additional"
                ],
                "rank": 11154,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6839986741542816
            },
            {
                "question verbose": "What is to apparent ",
                "b": "apparent",
                "expected answer": [
                    "apparently"
                ],
                "predictions": [
                    {
                        "score": 0.48578121747284003,
                        "answer": "forceful",
                        "hit": false
                    },
                    {
                        "score": 0.4816433448509053,
                        "answer": "elmer",
                        "hit": false
                    },
                    {
                        "score": 0.4806818956088311,
                        "answer": "pavia",
                        "hit": false
                    },
                    {
                        "score": 0.4715398799757143,
                        "answer": "gbsea",
                        "hit": false
                    },
                    {
                        "score": 0.4658700751321735,
                        "answer": "stuart",
                        "hit": false
                    },
                    {
                        "score": 0.46554820623288706,
                        "answer": "gcames",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apparent"
                ],
                "rank": 12217,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6935735642910004
            },
            {
                "question verbose": "What is to beautiful ",
                "b": "beautiful",
                "expected answer": [
                    "beautifully"
                ],
                "predictions": [
                    {
                        "score": 0.465226947949325,
                        "answer": "seriousness",
                        "hit": false
                    },
                    {
                        "score": 0.46448019744123936,
                        "answer": "cheering",
                        "hit": false
                    },
                    {
                        "score": 0.4538509657396443,
                        "answer": "collectible",
                        "hit": false
                    },
                    {
                        "score": 0.44583261335926716,
                        "answer": "wilfs",
                        "hit": false
                    },
                    {
                        "score": 0.4454809828621388,
                        "answer": "scripting",
                        "hit": false
                    },
                    {
                        "score": 0.4450423553711086,
                        "answer": "lisa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beautiful"
                ],
                "rank": 2597,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7644573748111725
            },
            {
                "question verbose": "What is to clinical ",
                "b": "clinical",
                "expected answer": [
                    "clinically"
                ],
                "predictions": [
                    {
                        "score": 0.4410860675777948,
                        "answer": "heaven",
                        "hit": false
                    },
                    {
                        "score": 0.43796881571386753,
                        "answer": "ripping",
                        "hit": false
                    },
                    {
                        "score": 0.43720553358787906,
                        "answer": "fence",
                        "hit": false
                    },
                    {
                        "score": 0.4357478629388378,
                        "answer": "duct",
                        "hit": false
                    },
                    {
                        "score": 0.4292358066940895,
                        "answer": "atheism",
                        "hit": false
                    },
                    {
                        "score": 0.4263094836113142,
                        "answer": "triple",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clinical"
                ],
                "rank": 8810,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.789198249578476
            },
            {
                "question verbose": "What is to creative ",
                "b": "creative",
                "expected answer": [
                    "creatively"
                ],
                "predictions": [
                    {
                        "score": 0.44482901215077314,
                        "answer": "forgiving",
                        "hit": false
                    },
                    {
                        "score": 0.43803020387316033,
                        "answer": "teeny",
                        "hit": false
                    },
                    {
                        "score": 0.4348047155076386,
                        "answer": "cryengine",
                        "hit": false
                    },
                    {
                        "score": 0.42876712877353057,
                        "answer": "marital",
                        "hit": false
                    },
                    {
                        "score": 0.4217620169617377,
                        "answer": "thou",
                        "hit": false
                    },
                    {
                        "score": 0.41911181698859434,
                        "answer": "scum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "creative"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.48650870379060507
            },
            {
                "question verbose": "What is to critical ",
                "b": "critical",
                "expected answer": [
                    "critically"
                ],
                "predictions": [
                    {
                        "score": 0.4326751204971213,
                        "answer": "shudder",
                        "hit": false
                    },
                    {
                        "score": 0.4279039292923859,
                        "answer": "fatten",
                        "hit": false
                    },
                    {
                        "score": 0.427118910261306,
                        "answer": "fest",
                        "hit": false
                    },
                    {
                        "score": 0.4248996243693096,
                        "answer": "visually",
                        "hit": false
                    },
                    {
                        "score": 0.42092484732857144,
                        "answer": "discovers",
                        "hit": false
                    },
                    {
                        "score": 0.411859359073368,
                        "answer": "cufflink",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "critical"
                ],
                "rank": 4110,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7015465795993805
            },
            {
                "question verbose": "What is to cultural ",
                "b": "cultural",
                "expected answer": [
                    "culturally"
                ],
                "predictions": [
                    {
                        "score": 0.41624964203928433,
                        "answer": "casabianca",
                        "hit": false
                    },
                    {
                        "score": 0.40853899424187085,
                        "answer": "scruffiness",
                        "hit": false
                    },
                    {
                        "score": 0.4085306520152118,
                        "answer": "vacant",
                        "hit": false
                    },
                    {
                        "score": 0.40840404342447323,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.40629354089405145,
                        "answer": "retains",
                        "hit": false
                    },
                    {
                        "score": 0.40602278573739886,
                        "answer": "pinned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cultural"
                ],
                "rank": 10337,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.693755716085434
            },
            {
                "question verbose": "What is to decided ",
                "b": "decided",
                "expected answer": [
                    "decidedly"
                ],
                "predictions": [
                    {
                        "score": 0.43137644100782185,
                        "answer": "recommendation",
                        "hit": false
                    },
                    {
                        "score": 0.4168971722362177,
                        "answer": "connoted",
                        "hit": false
                    },
                    {
                        "score": 0.41587225067430855,
                        "answer": "bellicek",
                        "hit": false
                    },
                    {
                        "score": 0.411391191161287,
                        "answer": "deceased",
                        "hit": false
                    },
                    {
                        "score": 0.40823282012027956,
                        "answer": "diarrhea",
                        "hit": false
                    },
                    {
                        "score": 0.4080774114051473,
                        "answer": "odark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "decided"
                ],
                "rank": 1871,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6949698626995087
            },
            {
                "question verbose": "What is to different ",
                "b": "different",
                "expected answer": [
                    "differently"
                ],
                "predictions": [
                    {
                        "score": 0.3835692468045541,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.36232082575969,
                        "answer": "africanization",
                        "hit": false
                    },
                    {
                        "score": 0.3534780567178758,
                        "answer": "depending",
                        "hit": false
                    },
                    {
                        "score": 0.344610029488435,
                        "answer": "pondering",
                        "hit": false
                    },
                    {
                        "score": 0.3433557604742962,
                        "answer": "fubar",
                        "hit": false
                    },
                    {
                        "score": 0.3408171201377529,
                        "answer": "galaxy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "different"
                ],
                "rank": 338,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6976899355649948
            },
            {
                "question verbose": "What is to digital ",
                "b": "digital",
                "expected answer": [
                    "digitally"
                ],
                "predictions": [
                    {
                        "score": 0.412772771533896,
                        "answer": "interruptive",
                        "hit": false
                    },
                    {
                        "score": 0.404695788466744,
                        "answer": "pinned",
                        "hit": false
                    },
                    {
                        "score": 0.4033530580409628,
                        "answer": "scripting",
                        "hit": false
                    },
                    {
                        "score": 0.4004917645079342,
                        "answer": "peed",
                        "hit": false
                    },
                    {
                        "score": 0.39780298250044943,
                        "answer": "clubspubsbars",
                        "hit": false
                    },
                    {
                        "score": 0.39603512140397185,
                        "answer": "quietly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "digital"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5064650573767722
            },
            {
                "question verbose": "What is to effective ",
                "b": "effective",
                "expected answer": [
                    "effectively"
                ],
                "predictions": [
                    {
                        "score": 0.4632730670294191,
                        "answer": "reverse",
                        "hit": false
                    },
                    {
                        "score": 0.4582550453764165,
                        "answer": "stained",
                        "hit": false
                    },
                    {
                        "score": 0.45761557473235226,
                        "answer": "highland",
                        "hit": false
                    },
                    {
                        "score": 0.45215039741457963,
                        "answer": "legion",
                        "hit": false
                    },
                    {
                        "score": 0.452137468885487,
                        "answer": "redeeming",
                        "hit": false
                    },
                    {
                        "score": 0.44682121294483623,
                        "answer": "mydesertgolf",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "effective"
                ],
                "rank": 5975,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7531385719776154
            },
            {
                "question verbose": "What is to environmental ",
                "b": "environmental",
                "expected answer": [
                    "environmentally"
                ],
                "predictions": [
                    {
                        "score": 0.5128635031458533,
                        "answer": "indication",
                        "hit": false
                    },
                    {
                        "score": 0.5123019357919472,
                        "answer": "moviegoing",
                        "hit": false
                    },
                    {
                        "score": 0.49867851030125787,
                        "answer": "pressuring",
                        "hit": false
                    },
                    {
                        "score": 0.4919420732072913,
                        "answer": "consulting",
                        "hit": false
                    },
                    {
                        "score": 0.4892180158623495,
                        "answer": "inflamed",
                        "hit": false
                    },
                    {
                        "score": 0.4829450042277751,
                        "answer": "playability",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "environmental"
                ],
                "rank": 3824,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.804897129535675
            },
            {
                "question verbose": "What is to extensive ",
                "b": "extensive",
                "expected answer": [
                    "extensively"
                ],
                "predictions": [
                    {
                        "score": 0.5523160342383223,
                        "answer": "viliations",
                        "hit": false
                    },
                    {
                        "score": 0.5304504794328371,
                        "answer": "cpu",
                        "hit": false
                    },
                    {
                        "score": 0.5240944712433477,
                        "answer": "lenghts",
                        "hit": false
                    },
                    {
                        "score": 0.5211494378270654,
                        "answer": "temerity",
                        "hit": false
                    },
                    {
                        "score": 0.5146747693432362,
                        "answer": "highland",
                        "hit": false
                    },
                    {
                        "score": 0.5114059936065909,
                        "answer": "slurvy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "extensive"
                ],
                "rank": 3266,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.809722363948822
            },
            {
                "question verbose": "What is to famous ",
                "b": "famous",
                "expected answer": [
                    "famously"
                ],
                "predictions": [
                    {
                        "score": 0.5760698309891271,
                        "answer": "ragged",
                        "hit": false
                    },
                    {
                        "score": 0.5544677203891937,
                        "answer": "caliber",
                        "hit": false
                    },
                    {
                        "score": 0.552415979767626,
                        "answer": "skinned",
                        "hit": false
                    },
                    {
                        "score": 0.5497827055418274,
                        "answer": "trackpads",
                        "hit": false
                    },
                    {
                        "score": 0.5496847380077305,
                        "answer": "zeekez",
                        "hit": false
                    },
                    {
                        "score": 0.5475822629122179,
                        "answer": "complimentary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "famous"
                ],
                "rank": 1679,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7951375842094421
            },
            {
                "question verbose": "What is to federal ",
                "b": "federal",
                "expected answer": [
                    "federally"
                ],
                "predictions": [
                    {
                        "score": 0.44775160911137796,
                        "answer": "paralysis",
                        "hit": false
                    },
                    {
                        "score": 0.43903645142199144,
                        "answer": "sphere",
                        "hit": false
                    },
                    {
                        "score": 0.42830436281028006,
                        "answer": "unconstitutional",
                        "hit": false
                    },
                    {
                        "score": 0.4188046123289585,
                        "answer": "somalia",
                        "hit": false
                    },
                    {
                        "score": 0.41267005671667534,
                        "answer": "indication",
                        "hit": false
                    },
                    {
                        "score": 0.40620447369212315,
                        "answer": "endorsing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "federal"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5661722719669342
            },
            {
                "question verbose": "What is to financial ",
                "b": "financial",
                "expected answer": [
                    "financially"
                ],
                "predictions": [
                    {
                        "score": 0.3922588970392156,
                        "answer": "dishonorable",
                        "hit": false
                    },
                    {
                        "score": 0.3913657176975226,
                        "answer": "meaningless",
                        "hit": false
                    },
                    {
                        "score": 0.3908147649573596,
                        "answer": "ecourage",
                        "hit": false
                    },
                    {
                        "score": 0.3858393639662045,
                        "answer": "fubar",
                        "hit": false
                    },
                    {
                        "score": 0.38153991873558063,
                        "answer": "resume",
                        "hit": false
                    },
                    {
                        "score": 0.37871137172220826,
                        "answer": "obtaining",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "financial"
                ],
                "rank": 2636,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6748174130916595
            },
            {
                "question verbose": "What is to global ",
                "b": "global",
                "expected answer": [
                    "globally"
                ],
                "predictions": [
                    {
                        "score": 0.4027982994824277,
                        "answer": "sometime",
                        "hit": false
                    },
                    {
                        "score": 0.3975184686894384,
                        "answer": "fortune",
                        "hit": false
                    },
                    {
                        "score": 0.3910355696679755,
                        "answer": "corby",
                        "hit": false
                    },
                    {
                        "score": 0.39093462589749456,
                        "answer": "slickly",
                        "hit": false
                    },
                    {
                        "score": 0.38111435304907854,
                        "answer": "metropolitan",
                        "hit": false
                    },
                    {
                        "score": 0.3803774672858423,
                        "answer": "dime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "global"
                ],
                "rank": 5269,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6890726536512375
            },
            {
                "question verbose": "What is to historical ",
                "b": "historical",
                "expected answer": [
                    "historically"
                ],
                "predictions": [
                    {
                        "score": 0.4416211476917239,
                        "answer": "heaven",
                        "hit": false
                    },
                    {
                        "score": 0.43730955539475014,
                        "answer": "espn",
                        "hit": false
                    },
                    {
                        "score": 0.42854261116122866,
                        "answer": "myopia",
                        "hit": false
                    },
                    {
                        "score": 0.3978029782919747,
                        "answer": "occasional",
                        "hit": false
                    },
                    {
                        "score": 0.3969568696556809,
                        "answer": "gorgeous",
                        "hit": false
                    },
                    {
                        "score": 0.3945966823593796,
                        "answer": "mydesertgolf",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "historical"
                ],
                "rank": 10262,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6772292107343674
            },
            {
                "question verbose": "What is to huge ",
                "b": "huge",
                "expected answer": [
                    "hugely"
                ],
                "predictions": [
                    {
                        "score": 0.35551469463979496,
                        "answer": "takumo",
                        "hit": false
                    },
                    {
                        "score": 0.3549879827677188,
                        "answer": "therewith",
                        "hit": false
                    },
                    {
                        "score": 0.35420015604424215,
                        "answer": "syriaturkey",
                        "hit": false
                    },
                    {
                        "score": 0.3422485379300496,
                        "answer": "flooded",
                        "hit": false
                    },
                    {
                        "score": 0.33881147294560204,
                        "answer": "edwina",
                        "hit": false
                    },
                    {
                        "score": 0.3347223839334469,
                        "answer": "misstatement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "huge"
                ],
                "rank": 11479,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6194743886590004
            },
            {
                "question verbose": "What is to immediate ",
                "b": "immediate",
                "expected answer": [
                    "immediately"
                ],
                "predictions": [
                    {
                        "score": 0.5153741674096335,
                        "answer": "dose",
                        "hit": false
                    },
                    {
                        "score": 0.4630162073804332,
                        "answer": "highland",
                        "hit": false
                    },
                    {
                        "score": 0.46278937762121175,
                        "answer": "intimidated",
                        "hit": false
                    },
                    {
                        "score": 0.4620205469108892,
                        "answer": "soundbites",
                        "hit": false
                    },
                    {
                        "score": 0.45670606072191294,
                        "answer": "misogynistic",
                        "hit": false
                    },
                    {
                        "score": 0.4558773034111987,
                        "answer": "mydesertgolf",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "immediate"
                ],
                "rank": 15089,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6252830922603607
            },
            {
                "question verbose": "What is to important ",
                "b": "important",
                "expected answer": [
                    "importantly"
                ],
                "predictions": [
                    {
                        "score": 0.38269404541468915,
                        "answer": "shudder",
                        "hit": false
                    },
                    {
                        "score": 0.3681769022220339,
                        "answer": "turfi",
                        "hit": false
                    },
                    {
                        "score": 0.3591902238082078,
                        "answer": "potus",
                        "hit": false
                    },
                    {
                        "score": 0.35707002541676236,
                        "answer": "headache",
                        "hit": false
                    },
                    {
                        "score": 0.35050295472895276,
                        "answer": "vga",
                        "hit": false
                    },
                    {
                        "score": 0.35035126972806185,
                        "answer": "dinosaur",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "important"
                ],
                "rank": 13469,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.620089091360569
            },
            {
                "question verbose": "What is to increasing ",
                "b": "increasing",
                "expected answer": [
                    "increasingly"
                ],
                "predictions": [
                    {
                        "score": 0.48752411623746306,
                        "answer": "hearted",
                        "hit": false
                    },
                    {
                        "score": 0.4824141066580553,
                        "answer": "pvp",
                        "hit": false
                    },
                    {
                        "score": 0.4715287519720274,
                        "answer": "misstruths",
                        "hit": false
                    },
                    {
                        "score": 0.46666654002967994,
                        "answer": "challenger",
                        "hit": false
                    },
                    {
                        "score": 0.463583376395632,
                        "answer": "antithesis",
                        "hit": false
                    },
                    {
                        "score": 0.46335534684569085,
                        "answer": "vacant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "increasing"
                ],
                "rank": 5615,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7171448469161987
            },
            {
                "question verbose": "What is to interesting ",
                "b": "interesting",
                "expected answer": [
                    "interestingly"
                ],
                "predictions": [
                    {
                        "score": 0.4464303683949239,
                        "answer": "tsk",
                        "hit": false
                    },
                    {
                        "score": 0.4414753210718106,
                        "answer": "annoying",
                        "hit": false
                    },
                    {
                        "score": 0.4298561284826882,
                        "answer": "shudder",
                        "hit": false
                    },
                    {
                        "score": 0.4291528024728714,
                        "answer": "gilgamesh",
                        "hit": false
                    },
                    {
                        "score": 0.42637453570901224,
                        "answer": "antique",
                        "hit": false
                    },
                    {
                        "score": 0.4231447752365969,
                        "answer": "frankenstorm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interesting"
                ],
                "rank": 1209,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7002832591533661
            },
            {
                "question verbose": "What is to internal ",
                "b": "internal",
                "expected answer": [
                    "internally"
                ],
                "predictions": [
                    {
                        "score": 0.5185130255630825,
                        "answer": "portrayed",
                        "hit": false
                    },
                    {
                        "score": 0.5000856721674584,
                        "answer": "frankenstorm",
                        "hit": false
                    },
                    {
                        "score": 0.49639011831492624,
                        "answer": "crescendo",
                        "hit": false
                    },
                    {
                        "score": 0.4920261444925188,
                        "answer": "dating",
                        "hit": false
                    },
                    {
                        "score": 0.4871701482498662,
                        "answer": "mute",
                        "hit": false
                    },
                    {
                        "score": 0.48704391315321094,
                        "answer": "caliber",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "internal"
                ],
                "rank": 10416,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700884252786636
            },
            {
                "question verbose": "What is to international ",
                "b": "international",
                "expected answer": [
                    "internationally"
                ],
                "predictions": [
                    {
                        "score": 0.44510812609259887,
                        "answer": "traditionally",
                        "hit": false
                    },
                    {
                        "score": 0.4259921492616108,
                        "answer": "eritrea",
                        "hit": false
                    },
                    {
                        "score": 0.42494352287667614,
                        "answer": "romneycare",
                        "hit": false
                    },
                    {
                        "score": 0.4184787429801786,
                        "answer": "pressuring",
                        "hit": false
                    },
                    {
                        "score": 0.41363600570440023,
                        "answer": "reluctantly",
                        "hit": false
                    },
                    {
                        "score": 0.4134053049648564,
                        "answer": "dereklowe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "international"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5560233294963837
            },
            {
                "question verbose": "What is to legal ",
                "b": "legal",
                "expected answer": [
                    "legally"
                ],
                "predictions": [
                    {
                        "score": 0.4084426638766664,
                        "answer": "hammer",
                        "hit": false
                    },
                    {
                        "score": 0.3996641297789209,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3970381055274444,
                        "answer": "endorsing",
                        "hit": false
                    },
                    {
                        "score": 0.38892067379141904,
                        "answer": "differing",
                        "hit": false
                    },
                    {
                        "score": 0.38143850722419786,
                        "answer": "formulated",
                        "hit": false
                    },
                    {
                        "score": 0.37726269271341917,
                        "answer": "touchstone",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "legal"
                ],
                "rank": 2004,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7294147461652756
            },
            {
                "question verbose": "What is to mental ",
                "b": "mental",
                "expected answer": [
                    "mentally"
                ],
                "predictions": [
                    {
                        "score": 0.43478227537095293,
                        "answer": "seemsto",
                        "hit": false
                    },
                    {
                        "score": 0.4260690191360514,
                        "answer": "apologize",
                        "hit": false
                    },
                    {
                        "score": 0.42574354257442837,
                        "answer": "tb",
                        "hit": false
                    },
                    {
                        "score": 0.4177595022975956,
                        "answer": "hitchens",
                        "hit": false
                    },
                    {
                        "score": 0.41210247020735136,
                        "answer": "emo",
                        "hit": false
                    },
                    {
                        "score": 0.41089531619639375,
                        "answer": "caliber",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mental"
                ],
                "rank": 6957,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6850970387458801
            },
            {
                "question verbose": "What is to nice ",
                "b": "nice",
                "expected answer": [
                    "nicely"
                ],
                "predictions": [
                    {
                        "score": 0.42754691133276435,
                        "answer": "rmoney",
                        "hit": false
                    },
                    {
                        "score": 0.4051929227521167,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.40449959688955517,
                        "answer": "pithy",
                        "hit": false
                    },
                    {
                        "score": 0.4038627671352119,
                        "answer": "vikes",
                        "hit": false
                    },
                    {
                        "score": 0.40264128039164787,
                        "answer": "cheering",
                        "hit": false
                    },
                    {
                        "score": 0.3967838256684235,
                        "answer": "medicate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nice"
                ],
                "rank": 2304,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6614512503147125
            },
            {
                "question verbose": "What is to obvious ",
                "b": "obvious",
                "expected answer": [
                    "obviously"
                ],
                "predictions": [
                    {
                        "score": 0.42368020669941564,
                        "answer": "flopper",
                        "hit": false
                    },
                    {
                        "score": 0.41704562196649275,
                        "answer": "fest",
                        "hit": false
                    },
                    {
                        "score": 0.4167844060428572,
                        "answer": "ignored",
                        "hit": false
                    },
                    {
                        "score": 0.4148764424674241,
                        "answer": "marathoner",
                        "hit": false
                    },
                    {
                        "score": 0.4143217805559926,
                        "answer": "unadulterated",
                        "hit": false
                    },
                    {
                        "score": 0.41264863213149683,
                        "answer": "inaugural",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "obvious"
                ],
                "rank": 14111,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6037677004933357
            },
            {
                "question verbose": "What is to physical ",
                "b": "physical",
                "expected answer": [
                    "physically"
                ],
                "predictions": [
                    {
                        "score": 0.44659456759592836,
                        "answer": "pale",
                        "hit": false
                    },
                    {
                        "score": 0.4450661062445847,
                        "answer": "automatically",
                        "hit": false
                    },
                    {
                        "score": 0.4415528547745906,
                        "answer": "moral",
                        "hit": false
                    },
                    {
                        "score": 0.4334304580326138,
                        "answer": "apologize",
                        "hit": false
                    },
                    {
                        "score": 0.4269608777200032,
                        "answer": "dancing",
                        "hit": false
                    },
                    {
                        "score": 0.4242041721771331,
                        "answer": "jewish",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "physical"
                ],
                "rank": 1993,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7173393070697784
            },
            {
                "question verbose": "What is to political ",
                "b": "political",
                "expected answer": [
                    "politically"
                ],
                "predictions": [
                    {
                        "score": 0.39561820628293426,
                        "answer": "cowardice",
                        "hit": false
                    },
                    {
                        "score": 0.3873087294135076,
                        "answer": "falsehood",
                        "hit": false
                    },
                    {
                        "score": 0.3784391316672086,
                        "answer": "accurately",
                        "hit": false
                    },
                    {
                        "score": 0.3757448513627425,
                        "answer": "freethought",
                        "hit": false
                    },
                    {
                        "score": 0.3642327083886602,
                        "answer": "fest",
                        "hit": false
                    },
                    {
                        "score": 0.36002082479898395,
                        "answer": "anyall",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "political"
                ],
                "rank": 12013,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6270011216402054
            },
            {
                "question verbose": "What is to popular ",
                "b": "popular",
                "expected answer": [
                    "popularly"
                ],
                "predictions": [
                    {
                        "score": 0.4324170166785285,
                        "answer": "rig",
                        "hit": false
                    },
                    {
                        "score": 0.43025994524038563,
                        "answer": "snuck",
                        "hit": false
                    },
                    {
                        "score": 0.42633237895726156,
                        "answer": "unnoticed",
                        "hit": false
                    },
                    {
                        "score": 0.41507932369652445,
                        "answer": "limp",
                        "hit": false
                    },
                    {
                        "score": 0.41128565323457783,
                        "answer": "soooo",
                        "hit": false
                    },
                    {
                        "score": 0.40810124406765896,
                        "answer": "electoral",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "popular"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5628613531589508
            },
            {
                "question verbose": "What is to practical ",
                "b": "practical",
                "expected answer": [
                    "practically"
                ],
                "predictions": [
                    {
                        "score": 0.6555010312925628,
                        "answer": "caliber",
                        "hit": false
                    },
                    {
                        "score": 0.6062054612400076,
                        "answer": "highland",
                        "hit": false
                    },
                    {
                        "score": 0.5878585005463434,
                        "answer": "excert",
                        "hit": false
                    },
                    {
                        "score": 0.5857930846985802,
                        "answer": "interruptive",
                        "hit": false
                    },
                    {
                        "score": 0.5855907649506252,
                        "answer": "htpc",
                        "hit": false
                    },
                    {
                        "score": 0.5816371089870073,
                        "answer": "opnion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "practical"
                ],
                "rank": 3007,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7877405881881714
            },
            {
                "question verbose": "What is to previous ",
                "b": "previous",
                "expected answer": [
                    "previously"
                ],
                "predictions": [
                    {
                        "score": 0.5384069231154274,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.5229988994002918,
                        "answer": "volatility",
                        "hit": false
                    },
                    {
                        "score": 0.4977491855409875,
                        "answer": "rhetoric",
                        "hit": false
                    },
                    {
                        "score": 0.47774810477221463,
                        "answer": "pad",
                        "hit": false
                    },
                    {
                        "score": 0.4750229430152769,
                        "answer": "dag",
                        "hit": false
                    },
                    {
                        "score": 0.4691880083011256,
                        "answer": "bakken",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "previous"
                ],
                "rank": 9632,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7430588155984879
            },
            {
                "question verbose": "What is to rare ",
                "b": "rare",
                "expected answer": [
                    "rarely"
                ],
                "predictions": [
                    {
                        "score": 0.5483550714845841,
                        "answer": "gbsea",
                        "hit": false
                    },
                    {
                        "score": 0.5410218202660774,
                        "answer": "caliber",
                        "hit": false
                    },
                    {
                        "score": 0.5063940919894889,
                        "answer": "swoop",
                        "hit": false
                    },
                    {
                        "score": 0.5054909774813857,
                        "answer": "polarity",
                        "hit": false
                    },
                    {
                        "score": 0.5043363310613654,
                        "answer": "concise",
                        "hit": false
                    },
                    {
                        "score": 0.500847380994793,
                        "answer": "dejected",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rare"
                ],
                "rank": 11710,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7392687797546387
            },
            {
                "question verbose": "What is to regional ",
                "b": "regional",
                "expected answer": [
                    "regionally"
                ],
                "predictions": [
                    {
                        "score": 0.4738953527257883,
                        "answer": "magnum",
                        "hit": false
                    },
                    {
                        "score": 0.47284268204080915,
                        "answer": "chilean",
                        "hit": false
                    },
                    {
                        "score": 0.4701937893037069,
                        "answer": "mecatronica",
                        "hit": false
                    },
                    {
                        "score": 0.46390453884026933,
                        "answer": "reportsan",
                        "hit": false
                    },
                    {
                        "score": 0.4572607185561873,
                        "answer": "atheism",
                        "hit": false
                    },
                    {
                        "score": 0.45545395474885964,
                        "answer": "cycled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "regional"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5334601253271103
            },
            {
                "question verbose": "What is to serious ",
                "b": "serious",
                "expected answer": [
                    "seriously"
                ],
                "predictions": [
                    {
                        "score": 0.4357558583529667,
                        "answer": "predisposition",
                        "hit": false
                    },
                    {
                        "score": 0.42377359661367425,
                        "answer": "bogus",
                        "hit": false
                    },
                    {
                        "score": 0.41138154521901404,
                        "answer": "gbsea",
                        "hit": false
                    },
                    {
                        "score": 0.4099787098961129,
                        "answer": "opnion",
                        "hit": false
                    },
                    {
                        "score": 0.4088265834882244,
                        "answer": "confronting",
                        "hit": false
                    },
                    {
                        "score": 0.390896887691483,
                        "answer": "stormtrooper",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "serious"
                ],
                "rank": 1190,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6697909086942673
            },
            {
                "question verbose": "What is to sexual ",
                "b": "sexual",
                "expected answer": [
                    "sexually"
                ],
                "predictions": [
                    {
                        "score": 0.52022551876564,
                        "answer": "reverse",
                        "hit": false
                    },
                    {
                        "score": 0.5127879047570367,
                        "answer": "completing",
                        "hit": false
                    },
                    {
                        "score": 0.509825643361063,
                        "answer": "inhumane",
                        "hit": false
                    },
                    {
                        "score": 0.5052614163154707,
                        "answer": "ant",
                        "hit": false
                    },
                    {
                        "score": 0.5045491585416687,
                        "answer": "sexuality",
                        "hit": false
                    },
                    {
                        "score": 0.5009879697997702,
                        "answer": "highland",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sexual"
                ],
                "rank": 4009,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7970952987670898
            },
            {
                "question verbose": "What is to significant ",
                "b": "significant",
                "expected answer": [
                    "significantly"
                ],
                "predictions": [
                    {
                        "score": 0.42832373259745776,
                        "answer": "pound",
                        "hit": false
                    },
                    {
                        "score": 0.4179089338617512,
                        "answer": "detention",
                        "hit": false
                    },
                    {
                        "score": 0.4017065696384369,
                        "answer": "disputings",
                        "hit": false
                    },
                    {
                        "score": 0.3961455083878603,
                        "answer": "tvg",
                        "hit": false
                    },
                    {
                        "score": 0.3941477818589372,
                        "answer": "sexuality",
                        "hit": false
                    },
                    {
                        "score": 0.39270659146180414,
                        "answer": "swamping",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "significant"
                ],
                "rank": 13415,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6099639236927032
            },
            {
                "question verbose": "What is to similar ",
                "b": "similar",
                "expected answer": [
                    "similarly"
                ],
                "predictions": [
                    {
                        "score": 0.38837525045306914,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.3782347801874928,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.3758108116156943,
                        "answer": "stripped",
                        "hit": false
                    },
                    {
                        "score": 0.3747337975018635,
                        "answer": "dehydrates",
                        "hit": false
                    },
                    {
                        "score": 0.3741092179506114,
                        "answer": "audio",
                        "hit": false
                    },
                    {
                        "score": 0.37391881677728867,
                        "answer": "enthused",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "similar"
                ],
                "rank": 14400,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.661216676235199
            },
            {
                "question verbose": "What is to strong ",
                "b": "strong",
                "expected answer": [
                    "strongly"
                ],
                "predictions": [
                    {
                        "score": 0.39177764941951765,
                        "answer": "couldcan",
                        "hit": false
                    },
                    {
                        "score": 0.38536060002935707,
                        "answer": "tactician",
                        "hit": false
                    },
                    {
                        "score": 0.381241507409327,
                        "answer": "acquittal",
                        "hit": false
                    },
                    {
                        "score": 0.36525991896238896,
                        "answer": "moral",
                        "hit": false
                    },
                    {
                        "score": 0.36242384679936634,
                        "answer": "hundredth",
                        "hit": false
                    },
                    {
                        "score": 0.3622305617978382,
                        "answer": "transcript",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strong"
                ],
                "rank": 1654,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7064681351184845
            },
            {
                "question verbose": "What is to subsequent ",
                "b": "subsequent",
                "expected answer": [
                    "subsequently"
                ],
                "predictions": [
                    {
                        "score": 0.6188369079486749,
                        "answer": "seemsto",
                        "hit": false
                    },
                    {
                        "score": 0.5853341283771187,
                        "answer": "safely",
                        "hit": false
                    },
                    {
                        "score": 0.5816027533027179,
                        "answer": "reverse",
                        "hit": false
                    },
                    {
                        "score": 0.5800446178856377,
                        "answer": "belong",
                        "hit": false
                    },
                    {
                        "score": 0.5753391113609644,
                        "answer": "caliber",
                        "hit": false
                    },
                    {
                        "score": 0.5694135075044748,
                        "answer": "synoptic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "subsequent"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.637227401137352
            },
            {
                "question verbose": "What is to successful ",
                "b": "successful",
                "expected answer": [
                    "successfully"
                ],
                "predictions": [
                    {
                        "score": 0.4622084346751455,
                        "answer": "caliber",
                        "hit": false
                    },
                    {
                        "score": 0.4619133790053663,
                        "answer": "curfew",
                        "hit": false
                    },
                    {
                        "score": 0.4451243021694741,
                        "answer": "upgraded",
                        "hit": false
                    },
                    {
                        "score": 0.4430101155047915,
                        "answer": "stiff",
                        "hit": false
                    },
                    {
                        "score": 0.44240342954536127,
                        "answer": "definite",
                        "hit": false
                    },
                    {
                        "score": 0.43988507063005794,
                        "answer": "unacceptable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "successful"
                ],
                "rank": 9381,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6612915098667145
            },
            {
                "question verbose": "What is to traditional ",
                "b": "traditional",
                "expected answer": [
                    "traditionally"
                ],
                "predictions": [
                    {
                        "score": 0.45363175994632116,
                        "answer": "delivering",
                        "hit": false
                    },
                    {
                        "score": 0.4300375482534644,
                        "answer": "assumtions",
                        "hit": false
                    },
                    {
                        "score": 0.423295062891068,
                        "answer": "hearted",
                        "hit": false
                    },
                    {
                        "score": 0.42311630907127257,
                        "answer": "crystallize",
                        "hit": false
                    },
                    {
                        "score": 0.4223546552347447,
                        "answer": "actress",
                        "hit": false
                    },
                    {
                        "score": 0.42203407283754085,
                        "answer": "pedos",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "traditional"
                ],
                "rank": 5949,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6943708658218384
            },
            {
                "question verbose": "What is to typical ",
                "b": "typical",
                "expected answer": [
                    "typically"
                ],
                "predictions": [
                    {
                        "score": 0.4433950878804756,
                        "answer": "fpsps",
                        "hit": false
                    },
                    {
                        "score": 0.43363070357209776,
                        "answer": "hitchens",
                        "hit": false
                    },
                    {
                        "score": 0.42516614456797625,
                        "answer": "operate",
                        "hit": false
                    },
                    {
                        "score": 0.4135602601283529,
                        "answer": "criminality",
                        "hit": false
                    },
                    {
                        "score": 0.411104538090589,
                        "answer": "intoxicating",
                        "hit": false
                    },
                    {
                        "score": 0.41059790807420843,
                        "answer": "reflection",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "typical"
                ],
                "rank": 11074,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6608715802431107
            },
            {
                "question verbose": "What is to unique ",
                "b": "unique",
                "expected answer": [
                    "uniquely"
                ],
                "predictions": [
                    {
                        "score": 0.42257618078883097,
                        "answer": "coughed",
                        "hit": false
                    },
                    {
                        "score": 0.4131659399121372,
                        "answer": "curfew",
                        "hit": false
                    },
                    {
                        "score": 0.40574176404415496,
                        "answer": "assertion",
                        "hit": false
                    },
                    {
                        "score": 0.4020141925314618,
                        "answer": "doc",
                        "hit": false
                    },
                    {
                        "score": 0.40189784758981945,
                        "answer": "definite",
                        "hit": false
                    },
                    {
                        "score": 0.40133048947826133,
                        "answer": "misdated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "unique"
                ],
                "rank": 795,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7544892430305481
            },
            {
                "question verbose": "What is to virtual ",
                "b": "virtual",
                "expected answer": [
                    "virtually"
                ],
                "predictions": [
                    {
                        "score": 0.5429426122464867,
                        "answer": "settlement",
                        "hit": false
                    },
                    {
                        "score": 0.5417751800656091,
                        "answer": "submerge",
                        "hit": false
                    },
                    {
                        "score": 0.5359265585576688,
                        "answer": "locale",
                        "hit": false
                    },
                    {
                        "score": 0.5355289597490035,
                        "answer": "fewseveral",
                        "hit": false
                    },
                    {
                        "score": 0.5275258431308381,
                        "answer": "stinking",
                        "hit": false
                    },
                    {
                        "score": 0.5264900049822915,
                        "answer": "tho",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "virtual"
                ],
                "rank": 9296,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7208453714847565
            },
            {
                "question verbose": "What is to visual ",
                "b": "visual",
                "expected answer": [
                    "visually"
                ],
                "predictions": [
                    {
                        "score": 0.5416644430838236,
                        "answer": "fillibustering",
                        "hit": false
                    },
                    {
                        "score": 0.5408370234696196,
                        "answer": "whitney",
                        "hit": false
                    },
                    {
                        "score": 0.5359674119350458,
                        "answer": "ragged",
                        "hit": false
                    },
                    {
                        "score": 0.5356532791149288,
                        "answer": "interruptive",
                        "hit": false
                    },
                    {
                        "score": 0.5311170719822846,
                        "answer": "gbsea",
                        "hit": false
                    },
                    {
                        "score": 0.5296228819432679,
                        "answer": "actress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "visual"
                ],
                "rank": 153,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8137988448143005
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D03 [adj+ly_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "5a5e19f1-e4cd-4c67-a6a6-242fbd783c76",
            "timestamp": "2020-10-22T15:57:01.146763"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to ambitious ",
                "b": "ambitious",
                "expected answer": [
                    "overambitious",
                    "over-ambitious"
                ],
                "predictions": [
                    {
                        "score": 0.8200693236310131,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19993596306007994,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.18808988211940386,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16193625097458827,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.16105162857032856,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15533587355128056,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ambitious"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to arching ",
                "b": "arching",
                "expected answer": [
                    "overarching",
                    "over-arching"
                ],
                "predictions": [
                    {
                        "score": 0.8205445979097994,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19420693168580744,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1894587484272183,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16323928443577287,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15834789804980592,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.15685278445782097,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "arching"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to booked ",
                "b": "booked",
                "expected answer": [
                    "overbooked",
                    "over-booked"
                ],
                "predictions": [
                    {
                        "score": 0.8203265378584785,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19498869369520802,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1867809629289258,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1622880451689368,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15676534608264264,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.15204124545320208,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "booked"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to charged ",
                "b": "charged",
                "expected answer": [
                    "overcharged",
                    "over-charged"
                ],
                "predictions": [
                    {
                        "score": 0.17002922339205456,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.16362420880040407,
                        "answer": "trillion",
                        "hit": false
                    },
                    {
                        "score": 0.16231460318753552,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.16203915362271118,
                        "answer": "matthew",
                        "hit": false
                    },
                    {
                        "score": 0.1579159061485644,
                        "answer": "addressing",
                        "hit": false
                    },
                    {
                        "score": 0.15687279331606424,
                        "answer": "repeal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "charged"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5510005503892899
            },
            {
                "question verbose": "What is to compensated ",
                "b": "compensated",
                "expected answer": [
                    "overcompensated",
                    "over-compensated"
                ],
                "predictions": [
                    {
                        "score": 0.8204995902349138,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19381162093324156,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.19197351922607173,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16443069500705715,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15822676409652014,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1563118070100114,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "compensated"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to confident ",
                "b": "confident",
                "expected answer": [
                    "overconfident",
                    "over-confident"
                ],
                "predictions": [
                    {
                        "score": 0.2366995090049919,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.21667106876839848,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.20603329388119904,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1925474876482324,
                        "answer": "blocked",
                        "hit": false
                    },
                    {
                        "score": 0.18814787809029462,
                        "answer": "levin",
                        "hit": false
                    },
                    {
                        "score": 0.18786275740476704,
                        "answer": "cleared",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "confident"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6464177668094635
            },
            {
                "question verbose": "What is to cooked ",
                "b": "cooked",
                "expected answer": [
                    "overcooked",
                    "over-cooked"
                ],
                "predictions": [
                    {
                        "score": 0.8201120069180938,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19388873726826228,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.19322841055704731,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16583690497940645,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16231891545189822,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.15358645510971874,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cooked"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to crowded ",
                "b": "crowded",
                "expected answer": [
                    "overcrowded",
                    "over-crowded"
                ],
                "predictions": [
                    {
                        "score": 0.21783501224966856,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1818322341150207,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.17812415499449158,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.17412408009674252,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.1704956790736112,
                        "answer": "fried",
                        "hit": false
                    },
                    {
                        "score": 0.16835821329487646,
                        "answer": "murphy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crowded"
                ],
                "rank": 1630,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.861627459526062
            },
            {
                "question verbose": "What is to developed ",
                "b": "developed",
                "expected answer": [
                    "overdeveloped",
                    "over-developed"
                ],
                "predictions": [
                    {
                        "score": 0.20687244744402966,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17409272472137327,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.16312937164450103,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.16262512542375357,
                        "answer": "additional",
                        "hit": false
                    },
                    {
                        "score": 0.16197262741857854,
                        "answer": "teach",
                        "hit": false
                    },
                    {
                        "score": 0.15554800996747847,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "developed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6279003769159317
            },
            {
                "question verbose": "What is to done ",
                "b": "done",
                "expected answer": [
                    "overdone",
                    "over-done"
                ],
                "predictions": [
                    {
                        "score": 0.16879208261218984,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1373149867749033,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.13441376078014097,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.13329116695952584,
                        "answer": "cleared",
                        "hit": false
                    },
                    {
                        "score": 0.129842931124278,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.1282420834398398,
                        "answer": "plenary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "done"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6043147072196007
            },
            {
                "question verbose": "What is to dressed ",
                "b": "dressed",
                "expected answer": [
                    "overdressed",
                    "over-dressed"
                ],
                "predictions": [
                    {
                        "score": 0.20174285906153228,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.18429180466608286,
                        "answer": "dispose",
                        "hit": false
                    },
                    {
                        "score": 0.18374081375992551,
                        "answer": "sinking",
                        "hit": false
                    },
                    {
                        "score": 0.18362323568134653,
                        "answer": "utah",
                        "hit": false
                    },
                    {
                        "score": 0.17984439775381847,
                        "answer": "murphy",
                        "hit": false
                    },
                    {
                        "score": 0.17496536049753134,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dressed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6076096519827843
            },
            {
                "question verbose": "What is to enthusiastic ",
                "b": "enthusiastic",
                "expected answer": [
                    "overenthusiastic",
                    "over-enthusiastic"
                ],
                "predictions": [
                    {
                        "score": 0.2619866552652201,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1967780285128239,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.19643803612413094,
                        "answer": "west",
                        "hit": false
                    },
                    {
                        "score": 0.18197391048771563,
                        "answer": "fro",
                        "hit": false
                    },
                    {
                        "score": 0.17974665264597461,
                        "answer": "plenary",
                        "hit": false
                    },
                    {
                        "score": 0.17694134476196718,
                        "answer": "capped",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enthusiastic"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6619700044393539
            },
            {
                "question verbose": "What is to excited ",
                "b": "excited",
                "expected answer": [
                    "overexcited",
                    "over-excited"
                ],
                "predictions": [
                    {
                        "score": 0.19339011151244942,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1791447166429689,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.17430396223911895,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.15872775281284018,
                        "answer": "hopeychangey",
                        "hit": false
                    },
                    {
                        "score": 0.15670869566840753,
                        "answer": "depressed",
                        "hit": false
                    },
                    {
                        "score": 0.15522341915236293,
                        "answer": "impression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "excited"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6195364147424698
            },
            {
                "question verbose": "What is to exposed ",
                "b": "exposed",
                "expected answer": [
                    "overexposed",
                    "over-exposed"
                ],
                "predictions": [
                    {
                        "score": 0.17112715211049784,
                        "answer": "overturned",
                        "hit": false
                    },
                    {
                        "score": 0.1680803664189514,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.16695220339734612,
                        "answer": "plenary",
                        "hit": false
                    },
                    {
                        "score": 0.15826431837978958,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.1571087141881294,
                        "answer": "dose",
                        "hit": false
                    },
                    {
                        "score": 0.15645020850342306,
                        "answer": "murphy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "exposed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5911334231495857
            },
            {
                "question verbose": "What is to filled ",
                "b": "filled",
                "expected answer": [
                    "overfilled",
                    "over-filled"
                ],
                "predictions": [
                    {
                        "score": 0.15221038362576286,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.15191977004079363,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.13581933140728625,
                        "answer": "warped",
                        "hit": false
                    },
                    {
                        "score": 0.13571833509609005,
                        "answer": "pharma",
                        "hit": false
                    },
                    {
                        "score": 0.13543080501490765,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1353564410788353,
                        "answer": "weber",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "filled"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5792425945401192
            },
            {
                "question verbose": "What is to grown ",
                "b": "grown",
                "expected answer": [
                    "overgrown",
                    "over-grown"
                ],
                "predictions": [
                    {
                        "score": 0.16164309321170572,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1509710879918631,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.1503704718728728,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.14996415448863562,
                        "answer": "smarter",
                        "hit": false
                    },
                    {
                        "score": 0.14604482217637033,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.14359289279074083,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grown"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5888300463557243
            },
            {
                "question verbose": "What is to heard ",
                "b": "heard",
                "expected answer": [
                    "overheard",
                    "over-heard"
                ],
                "predictions": [
                    {
                        "score": 0.13102005303621636,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.12759774824691894,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.12377312833170102,
                        "answer": "mlk",
                        "hit": false
                    },
                    {
                        "score": 0.12332889532595052,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.12112174444122065,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.11970506538701307,
                        "answer": "overzealous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "heard"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5424094013869762
            },
            {
                "question verbose": "What is to heated ",
                "b": "heated",
                "expected answer": [
                    "overheated",
                    "over-heated"
                ],
                "predictions": [
                    {
                        "score": 0.8204265274951973,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19635031847635687,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.19363372722062894,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.16553231279472633,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.159449639537349,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.1589519195876585,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "heated"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to inflated ",
                "b": "inflated",
                "expected answer": [
                    "overinflated",
                    "over-inflated"
                ],
                "predictions": [
                    {
                        "score": 0.20847970660146223,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19911155226995833,
                        "answer": "wakeup",
                        "hit": false
                    },
                    {
                        "score": 0.18903599053891942,
                        "answer": "cleared",
                        "hit": false
                    },
                    {
                        "score": 0.18694454841374422,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.18163024474820988,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.18158745699206685,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inflated"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6289482265710831
            },
            {
                "question verbose": "What is to laid ",
                "b": "laid",
                "expected answer": [
                    "overlaid",
                    "over-laid"
                ],
                "predictions": [
                    {
                        "score": 0.1997333572229014,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19159045906513195,
                        "answer": "overturned",
                        "hit": false
                    },
                    {
                        "score": 0.18397553847489811,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.18195697212272077,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.17764767410517335,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.17648347230573344,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "laid"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6233907043933868
            },
            {
                "question verbose": "What is to loaded ",
                "b": "loaded",
                "expected answer": [
                    "overloaded",
                    "over-loaded"
                ],
                "predictions": [
                    {
                        "score": 0.17473438846293257,
                        "answer": "apt",
                        "hit": false
                    },
                    {
                        "score": 0.17296758090586287,
                        "answer": "gay",
                        "hit": false
                    },
                    {
                        "score": 0.17078325004018133,
                        "answer": "subtext",
                        "hit": false
                    },
                    {
                        "score": 0.1682414115086894,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16644208844358638,
                        "answer": "murphy",
                        "hit": false
                    },
                    {
                        "score": 0.16403876748523816,
                        "answer": "enron",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "loaded"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6040607690811157
            },
            {
                "question verbose": "What is to optimistic ",
                "b": "optimistic",
                "expected answer": [
                    "overoptimistic",
                    "over-optimistic"
                ],
                "predictions": [
                    {
                        "score": 0.23387796364350796,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22174881899337875,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.22051272053315785,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.1998462464766505,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.19883101935158412,
                        "answer": "repeal",
                        "hit": false
                    },
                    {
                        "score": 0.19869888913404352,
                        "answer": "overturned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "optimistic"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6447004973888397
            },
            {
                "question verbose": "What is to paid ",
                "b": "paid",
                "expected answer": [
                    "overpaid",
                    "over-paid"
                ],
                "predictions": [
                    {
                        "score": 0.17725486425209316,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.1699776955891287,
                        "answer": "repeal",
                        "hit": false
                    },
                    {
                        "score": 0.16868174626067758,
                        "answer": "legislation",
                        "hit": false
                    },
                    {
                        "score": 0.16802674344232763,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.16796000371244355,
                        "answer": "overturned",
                        "hit": false
                    },
                    {
                        "score": 0.16716966697790558,
                        "answer": "balanced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "paid"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.596386581659317
            },
            {
                "question verbose": "What is to painted ",
                "b": "painted",
                "expected answer": [
                    "overpainted",
                    "over-painted"
                ],
                "predictions": [
                    {
                        "score": 0.20882452707414156,
                        "answer": "murphy",
                        "hit": false
                    },
                    {
                        "score": 0.19973416964002294,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.19570460729865224,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.19422753880506965,
                        "answer": "assholishness",
                        "hit": false
                    },
                    {
                        "score": 0.19090655467156556,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.189126133814805,
                        "answer": "plenary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "painted"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6006001904606819
            },
            {
                "question verbose": "What is to played ",
                "b": "played",
                "expected answer": [
                    "overplayed",
                    "over-played"
                ],
                "predictions": [
                    {
                        "score": 0.17664068714655512,
                        "answer": "guy",
                        "hit": false
                    },
                    {
                        "score": 0.17520497852998476,
                        "answer": "cleared",
                        "hit": false
                    },
                    {
                        "score": 0.16508135920161277,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.16222043816121037,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16049518855540387,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.16019076976179292,
                        "answer": "genre",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "played"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6003421545028687
            },
            {
                "question verbose": "What is to populated ",
                "b": "populated",
                "expected answer": [
                    "overpopulated",
                    "over-populated"
                ],
                "predictions": [
                    {
                        "score": 0.8202699364858577,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19623359617144062,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1942150515608468,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.16462336384174955,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.16115744265526485,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15822568738420692,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "populated"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to powered ",
                "b": "powered",
                "expected answer": [
                    "overpowered",
                    "over-powered"
                ],
                "predictions": [
                    {
                        "score": 0.14995063206670578,
                        "answer": "fried",
                        "hit": false
                    },
                    {
                        "score": 0.14992635767878365,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.14024963985955702,
                        "answer": "dose",
                        "hit": false
                    },
                    {
                        "score": 0.13806955842366003,
                        "answer": "denouement",
                        "hit": false
                    },
                    {
                        "score": 0.13724485460045546,
                        "answer": "amplifier",
                        "hit": false
                    },
                    {
                        "score": 0.13366354884352885,
                        "answer": "reasonable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "powered"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5409205257892609
            },
            {
                "question verbose": "What is to protected ",
                "b": "protected",
                "expected answer": [
                    "overprotected",
                    "over-protected"
                ],
                "predictions": [
                    {
                        "score": 0.2533612655830645,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25278835925037335,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.22896914628914541,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.22742165261923775,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.2231495259703863,
                        "answer": "helpless",
                        "hit": false
                    },
                    {
                        "score": 0.21994738886040635,
                        "answer": "overzealous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "protected"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6567138135433197
            },
            {
                "question verbose": "What is to protective ",
                "b": "protective",
                "expected answer": [
                    "overprotective",
                    "over-protective"
                ],
                "predictions": [
                    {
                        "score": 0.277963128761342,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24730631769966224,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.24046962679942724,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.23173840087136086,
                        "answer": "legislation",
                        "hit": false
                    },
                    {
                        "score": 0.23162008762101766,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2225379228806038,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "protective"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6718480736017227
            },
            {
                "question verbose": "What is to qualified ",
                "b": "qualified",
                "expected answer": [
                    "overqualified",
                    "over-qualified"
                ],
                "predictions": [
                    {
                        "score": 0.2664613059177758,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2503946794975151,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.23357350687782474,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.23057058406303213,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.22590626265686178,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.21344725175348553,
                        "answer": "legislation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "qualified"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6648811548948288
            },
            {
                "question verbose": "What is to represented ",
                "b": "represented",
                "expected answer": [
                    "overrepresented",
                    "over-represented"
                ],
                "predictions": [
                    {
                        "score": 0.254481489750193,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.23499137061868036,
                        "answer": "utah",
                        "hit": false
                    },
                    {
                        "score": 0.2084225116022462,
                        "answer": "denied",
                        "hit": false
                    },
                    {
                        "score": 0.20813077119252973,
                        "answer": "summum",
                        "hit": false
                    },
                    {
                        "score": 0.20444538585508953,
                        "answer": "fannie",
                        "hit": false
                    },
                    {
                        "score": 0.20422616975717586,
                        "answer": "offset",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "represented"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6030563861131668
            },
            {
                "question verbose": "What is to saturated ",
                "b": "saturated",
                "expected answer": [
                    "oversaturated",
                    "over-saturated"
                ],
                "predictions": [
                    {
                        "score": 0.8200807933464226,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19391759235158304,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.18643585302491655,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.1654589032026706,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.1630820808752226,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.1597337742282991,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "saturated"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to shadowed ",
                "b": "shadowed",
                "expected answer": [
                    "overshadowed",
                    "over-shadowed"
                ],
                "predictions": [
                    {
                        "score": 0.30862295229803266,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2344795112298126,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.22494136511098334,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.22082585773027769,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.21294331798138294,
                        "answer": "helpless",
                        "hit": false
                    },
                    {
                        "score": 0.21270160970133342,
                        "answer": "crystallize",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shadowed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6908230036497116
            },
            {
                "question verbose": "What is to simplified ",
                "b": "simplified",
                "expected answer": [
                    "oversimplified",
                    "over-simplified"
                ],
                "predictions": [
                    {
                        "score": 0.3308640247079496,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2615135409742167,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.23686172673862305,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.2342670074708039,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.21495566976051547,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.21396017595971767,
                        "answer": "overzealous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "simplified"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7045268267393112
            },
            {
                "question verbose": "What is to sized ",
                "b": "sized",
                "expected answer": [
                    "oversized",
                    "over-sized"
                ],
                "predictions": [
                    {
                        "score": 0.16429937261932676,
                        "answer": "fried",
                        "hit": false
                    },
                    {
                        "score": 0.1482942812188988,
                        "answer": "denouement",
                        "hit": false
                    },
                    {
                        "score": 0.14787098648322197,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.14780139420386126,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.1465698403813463,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.1457938761198201,
                        "answer": "undeveloped",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sized"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5821147263050079
            },
            {
                "question verbose": "What is to sold ",
                "b": "sold",
                "expected answer": [
                    "oversold",
                    "over-sold"
                ],
                "predictions": [
                    {
                        "score": 0.15436985669290273,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.152069651561926,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.15138257977445194,
                        "answer": "frank",
                        "hit": false
                    },
                    {
                        "score": 0.14923899295701445,
                        "answer": "legislation",
                        "hit": false
                    },
                    {
                        "score": 0.1462792650054395,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.1388340110751853,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sold"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5839228257536888
            },
            {
                "question verbose": "What is to spent ",
                "b": "spent",
                "expected answer": [
                    "overspent",
                    "over-spent"
                ],
                "predictions": [
                    {
                        "score": 0.21097300154533727,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.1617046108317055,
                        "answer": "taint",
                        "hit": false
                    },
                    {
                        "score": 0.16129324533356004,
                        "answer": "lewd",
                        "hit": false
                    },
                    {
                        "score": 0.15779844703410686,
                        "answer": "cleared",
                        "hit": false
                    },
                    {
                        "score": 0.15292432202180703,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.15073939074810064,
                        "answer": "jetting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spent"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5945201218128204
            },
            {
                "question verbose": "What is to stated ",
                "b": "stated",
                "expected answer": [
                    "overstated",
                    "over-stated"
                ],
                "predictions": [
                    {
                        "score": 0.17659859722522542,
                        "answer": "addressing",
                        "hit": false
                    },
                    {
                        "score": 0.1704166396103919,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.16551198869735229,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16428863125521093,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.15820577279037773,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.15670520281884162,
                        "answer": "granted",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stated"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6015774831175804
            },
            {
                "question verbose": "What is to stimulated ",
                "b": "stimulated",
                "expected answer": [
                    "overstimulated",
                    "over-stimulated"
                ],
                "predictions": [
                    {
                        "score": 0.8203784016551066,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1930354186122016,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.18915224858573773,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16576186360492123,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.16137060186531185,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.160273213907495,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stimulated"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to stocked ",
                "b": "stocked",
                "expected answer": [
                    "overstocked",
                    "over-stocked"
                ],
                "predictions": [
                    {
                        "score": 0.2562358242124182,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2272798161057476,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.2127572406177372,
                        "answer": "cleared",
                        "hit": false
                    },
                    {
                        "score": 0.21262398014480682,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.20797772486152147,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.20282202437471533,
                        "answer": "overzealous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stocked"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6585218161344528
            },
            {
                "question verbose": "What is to strained ",
                "b": "strained",
                "expected answer": [
                    "overstrained",
                    "over-strained"
                ],
                "predictions": [
                    {
                        "score": 0.1967223533312334,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.19590826876251996,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.1901617976165967,
                        "answer": "murphy",
                        "hit": false
                    },
                    {
                        "score": 0.1868858141208561,
                        "answer": "moneyproperty",
                        "hit": false
                    },
                    {
                        "score": 0.18582310964008178,
                        "answer": "oath",
                        "hit": false
                    },
                    {
                        "score": 0.1843942893851564,
                        "answer": "endorsing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strained"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5893780291080475
            },
            {
                "question verbose": "What is to stressed ",
                "b": "stressed",
                "expected answer": [
                    "overstressed",
                    "over-stressed"
                ],
                "predictions": [
                    {
                        "score": 0.2564264037752722,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23075227857713979,
                        "answer": "legislation",
                        "hit": false
                    },
                    {
                        "score": 0.2141423132520361,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.21291217484996142,
                        "answer": "fried",
                        "hit": false
                    },
                    {
                        "score": 0.21284928197268704,
                        "answer": "depressed",
                        "hit": false
                    },
                    {
                        "score": 0.2112808623096126,
                        "answer": "fannie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stressed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6584721058607101
            },
            {
                "question verbose": "What is to stretched ",
                "b": "stretched",
                "expected answer": [
                    "overstretched",
                    "over-stretched"
                ],
                "predictions": [
                    {
                        "score": 0.23922363513385184,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22820888277964363,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.2142719553268133,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2132521028937768,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2125638427854635,
                        "answer": "lord",
                        "hit": false
                    },
                    {
                        "score": 0.21176245674734404,
                        "answer": "overzealous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stretched"
                ],
                "rank": 14573,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8385520875453949
            },
            {
                "question verbose": "What is to subscribed ",
                "b": "subscribed",
                "expected answer": [
                    "oversubscribed",
                    "over-subscribed"
                ],
                "predictions": [
                    {
                        "score": 0.8201281971825001,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.19667280341629462,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.19013692426185247,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.16522436800817142,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.16521855940029015,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15794731248500857,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "subscribed"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to taken ",
                "b": "taken",
                "expected answer": [
                    "overtaken",
                    "over-taken"
                ],
                "predictions": [
                    {
                        "score": 0.11404693182650501,
                        "answer": "hungry",
                        "hit": false
                    },
                    {
                        "score": 0.11236176040833536,
                        "answer": "denouement",
                        "hit": false
                    },
                    {
                        "score": 0.11144481833473921,
                        "answer": "bcad",
                        "hit": false
                    },
                    {
                        "score": 0.11124342534164113,
                        "answer": "protects",
                        "hit": false
                    },
                    {
                        "score": 0.11022549586028701,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.11009448056956068,
                        "answer": "cleared",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "taken"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5469913333654404
            },
            {
                "question verbose": "What is to thrown ",
                "b": "thrown",
                "expected answer": [
                    "overthrown",
                    "over-thrown"
                ],
                "predictions": [
                    {
                        "score": 0.17844667642028858,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.1726684365025595,
                        "answer": "fried",
                        "hit": false
                    },
                    {
                        "score": 0.16921528850609643,
                        "answer": "fearful",
                        "hit": false
                    },
                    {
                        "score": 0.16752886602080305,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16646437825620733,
                        "answer": "pound",
                        "hit": false
                    },
                    {
                        "score": 0.16626335881378693,
                        "answer": "sipping",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "thrown"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.600828044116497
            },
            {
                "question verbose": "What is to turned ",
                "b": "turned",
                "expected answer": [
                    "overturned",
                    "over-turned"
                ],
                "predictions": [
                    {
                        "score": 0.15213783526736,
                        "answer": "eventually",
                        "hit": false
                    },
                    {
                        "score": 0.14346966224190025,
                        "answer": "overzealous",
                        "hit": false
                    },
                    {
                        "score": 0.13242411507418178,
                        "answer": "joke",
                        "hit": false
                    },
                    {
                        "score": 0.12618544739162207,
                        "answer": "big",
                        "hit": false
                    },
                    {
                        "score": 0.12427430802298835,
                        "answer": "shell",
                        "hit": false
                    },
                    {
                        "score": 0.12392621291012278,
                        "answer": "club",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "turned"
                ],
                "rank": 5628,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.653438538312912
            },
            {
                "question verbose": "What is to used ",
                "b": "used",
                "expected answer": [
                    "overused",
                    "over-used"
                ],
                "predictions": [
                    {
                        "score": 0.15907384812835776,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11757304821005839,
                        "answer": "checkmate",
                        "hit": false
                    },
                    {
                        "score": 0.11697804867054139,
                        "answer": "drug",
                        "hit": false
                    },
                    {
                        "score": 0.10602774563085632,
                        "answer": "increasingly",
                        "hit": false
                    },
                    {
                        "score": 0.09957090503532635,
                        "answer": "pressure",
                        "hit": false
                    },
                    {
                        "score": 0.09233212611956716,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "used"
                ],
                "rank": 6062,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.652613565325737
            },
            {
                "question verbose": "What is to written ",
                "b": "written",
                "expected answer": [
                    "overwritten",
                    "over-written"
                ],
                "predictions": [
                    {
                        "score": 0.11523655531649975,
                        "answer": "mar",
                        "hit": false
                    },
                    {
                        "score": 0.11457309731977619,
                        "answer": "puffed",
                        "hit": false
                    },
                    {
                        "score": 0.11306469060836886,
                        "answer": "wakeup",
                        "hit": false
                    },
                    {
                        "score": 0.11154617025638482,
                        "answer": "crystallize",
                        "hit": false
                    },
                    {
                        "score": 0.10942951710445743,
                        "answer": "pissed",
                        "hit": false
                    },
                    {
                        "score": 0.10783811384917064,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "written"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5289678834378719
            },
            {
                "question verbose": "What is to zealous ",
                "b": "zealous",
                "expected answer": [
                    "overzealous",
                    "over-zealous"
                ],
                "predictions": [
                    {
                        "score": 0.822034518298285,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17769540167565032,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.16748640352541277,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.16588534977706268,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1549435143933149,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.14501537671134376,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "zealous"
                ],
                "rank": 2071,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6643489599227905
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D04 [over+adj_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "02fa9673-34ff-441a-aa61-24e3ea344b7b",
            "timestamp": "2020-10-22T15:57:02.639625"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to amazing ",
                "b": "amazing",
                "expected answer": [
                    "amazingness"
                ],
                "predictions": [
                    {
                        "score": 0.25923258156544515,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.2263416445281917,
                        "answer": "generosity",
                        "hit": false
                    },
                    {
                        "score": 0.22165304435200853,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20440239283803013,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.20150903989854585,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.19126602312695581,
                        "answer": "levying",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "amazing"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5503771379590034
            },
            {
                "question verbose": "What is to attractive ",
                "b": "attractive",
                "expected answer": [
                    "attractiveness"
                ],
                "predictions": [
                    {
                        "score": 0.3352682921126469,
                        "answer": "ml",
                        "hit": false
                    },
                    {
                        "score": 0.31037528521523944,
                        "answer": "pray",
                        "hit": false
                    },
                    {
                        "score": 0.294754422196006,
                        "answer": "enacted",
                        "hit": false
                    },
                    {
                        "score": 0.29371044469639473,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2780208142953961,
                        "answer": "heal",
                        "hit": false
                    },
                    {
                        "score": 0.2761209342789342,
                        "answer": "odark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "attractive"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6021748185157776
            },
            {
                "question verbose": "What is to aware ",
                "b": "aware",
                "expected answer": [
                    "awareness"
                ],
                "predictions": [
                    {
                        "score": 0.297189751128776,
                        "answer": "truly",
                        "hit": false
                    },
                    {
                        "score": 0.2769822427918371,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.27082044662259663,
                        "answer": "pray",
                        "hit": false
                    },
                    {
                        "score": 0.26690934108699177,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.2631119249490885,
                        "answer": "unto",
                        "hit": false
                    },
                    {
                        "score": 0.2619696118411306,
                        "answer": "peer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aware"
                ],
                "rank": 2732,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6771022528409958
            },
            {
                "question verbose": "What is to broken ",
                "b": "broken",
                "expected answer": [
                    "brokenness"
                ],
                "predictions": [
                    {
                        "score": 0.2828833300110122,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.27727904701323114,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2771515992635422,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2717946672462904,
                        "answer": "outweigh",
                        "hit": false
                    },
                    {
                        "score": 0.2710773714014598,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2704445366725087,
                        "answer": "commiseration",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "broken"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6393754482269287
            },
            {
                "question verbose": "What is to careful ",
                "b": "careful",
                "expected answer": [
                    "carefulness"
                ],
                "predictions": [
                    {
                        "score": 0.3017709093169044,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.299866696693636,
                        "answer": "ecourage",
                        "hit": false
                    },
                    {
                        "score": 0.2997235474200719,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.29729203093697,
                        "answer": "awareness",
                        "hit": false
                    },
                    {
                        "score": 0.2836568814066606,
                        "answer": "vest",
                        "hit": false
                    },
                    {
                        "score": 0.28310191303769405,
                        "answer": "premature",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "careful"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5845240354537964
            },
            {
                "question verbose": "What is to cheap ",
                "b": "cheap",
                "expected answer": [
                    "cheapness"
                ],
                "predictions": [
                    {
                        "score": 0.2975058745082565,
                        "answer": "speeding",
                        "hit": false
                    },
                    {
                        "score": 0.287749640059266,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.28409046621579953,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.2825500836557654,
                        "answer": "enacted",
                        "hit": false
                    },
                    {
                        "score": 0.2772714594672685,
                        "answer": "ml",
                        "hit": false
                    },
                    {
                        "score": 0.2740010026033647,
                        "answer": "ingrate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cheap"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6501016020774841
            },
            {
                "question verbose": "What is to competitive ",
                "b": "competitive",
                "expected answer": [
                    "competitiveness"
                ],
                "predictions": [
                    {
                        "score": 0.3209855773142321,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.31427528840085933,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3036484289113994,
                        "answer": "detract",
                        "hit": false
                    },
                    {
                        "score": 0.30169660375769014,
                        "answer": "outweigh",
                        "hit": false
                    },
                    {
                        "score": 0.297417850223922,
                        "answer": "wbc",
                        "hit": false
                    },
                    {
                        "score": 0.29216561775063465,
                        "answer": "supporttime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "competitive"
                ],
                "rank": 322,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7686489224433899
            },
            {
                "question verbose": "What is to connected ",
                "b": "connected",
                "expected answer": [
                    "connectedness"
                ],
                "predictions": [
                    {
                        "score": 0.26317954118506093,
                        "answer": "detract",
                        "hit": false
                    },
                    {
                        "score": 0.25612538108343624,
                        "answer": "choc",
                        "hit": false
                    },
                    {
                        "score": 0.25151730372598674,
                        "answer": "monavie",
                        "hit": false
                    },
                    {
                        "score": 0.25011878381265085,
                        "answer": "stinking",
                        "hit": false
                    },
                    {
                        "score": 0.24642995384622202,
                        "answer": "mami",
                        "hit": false
                    },
                    {
                        "score": 0.24446424576784184,
                        "answer": "borrower",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "connected"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5504708997905254
            },
            {
                "question verbose": "What is to conscious ",
                "b": "conscious",
                "expected answer": [
                    "consciousness"
                ],
                "predictions": [
                    {
                        "score": 0.3162657250904081,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.3107176807787346,
                        "answer": "forgive",
                        "hit": false
                    },
                    {
                        "score": 0.3101589254889923,
                        "answer": "pray",
                        "hit": false
                    },
                    {
                        "score": 0.30792197183783837,
                        "answer": "competitiveness",
                        "hit": false
                    },
                    {
                        "score": 0.3035099186205758,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3014490936173235,
                        "answer": "pusher",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "conscious"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5711939781904221
            },
            {
                "question verbose": "What is to creative ",
                "b": "creative",
                "expected answer": [
                    "creativeness"
                ],
                "predictions": [
                    {
                        "score": 0.227792392680332,
                        "answer": "myopia",
                        "hit": false
                    },
                    {
                        "score": 0.22657487400420706,
                        "answer": "succumb",
                        "hit": false
                    },
                    {
                        "score": 0.22653440644620285,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.2188870731306926,
                        "answer": "cryengine",
                        "hit": false
                    },
                    {
                        "score": 0.2164458115266736,
                        "answer": "compare",
                        "hit": false
                    },
                    {
                        "score": 0.21564688423772438,
                        "answer": "madly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "creative"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.48650870379060507
            },
            {
                "question verbose": "What is to dangerous ",
                "b": "dangerous",
                "expected answer": [
                    "dangerousness"
                ],
                "predictions": [
                    {
                        "score": 0.2580037103400299,
                        "answer": "looming",
                        "hit": false
                    },
                    {
                        "score": 0.24131869749787224,
                        "answer": "credibility",
                        "hit": false
                    },
                    {
                        "score": 0.22936593121961013,
                        "answer": "wanting",
                        "hit": false
                    },
                    {
                        "score": 0.22911743547883145,
                        "answer": "ayn",
                        "hit": false
                    },
                    {
                        "score": 0.22672515097022916,
                        "answer": "recovers",
                        "hit": false
                    },
                    {
                        "score": 0.22518017174421992,
                        "answer": "authorize",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dangerous"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5809931978583336
            },
            {
                "question verbose": "What is to devoted ",
                "b": "devoted",
                "expected answer": [
                    "devotedness"
                ],
                "predictions": [
                    {
                        "score": 0.3054984562498528,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.3015990645083322,
                        "answer": "stinking",
                        "hit": false
                    },
                    {
                        "score": 0.29811756363825065,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.28830714819232345,
                        "answer": "roomful",
                        "hit": false
                    },
                    {
                        "score": 0.28005047799739763,
                        "answer": "collaborative",
                        "hit": false
                    },
                    {
                        "score": 0.2777610273116995,
                        "answer": "diaper",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "devoted"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6265328675508499
            },
            {
                "question verbose": "What is to directed ",
                "b": "directed",
                "expected answer": [
                    "directedness"
                ],
                "predictions": [
                    {
                        "score": 0.2420451113395144,
                        "answer": "imagining",
                        "hit": false
                    },
                    {
                        "score": 0.23278360420069705,
                        "answer": "struggling",
                        "hit": false
                    },
                    {
                        "score": 0.23126705972726203,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2273815536390812,
                        "answer": "heath",
                        "hit": false
                    },
                    {
                        "score": 0.2248390407570518,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.22149670213870043,
                        "answer": "velocity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "directed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6082722917199135
            },
            {
                "question verbose": "What is to distinct ",
                "b": "distinct",
                "expected answer": [
                    "distinctness"
                ],
                "predictions": [
                    {
                        "score": 0.3417735702532646,
                        "answer": "vest",
                        "hit": false
                    },
                    {
                        "score": 0.31506486226641234,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3064149118891503,
                        "answer": "recovers",
                        "hit": false
                    },
                    {
                        "score": 0.3058458650604263,
                        "answer": "borrower",
                        "hit": false
                    },
                    {
                        "score": 0.3031142318452915,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.2987562467573671,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "distinct"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6527463793754578
            },
            {
                "question verbose": "What is to distinctive ",
                "b": "distinctive",
                "expected answer": [
                    "distinctiveness"
                ],
                "predictions": [
                    {
                        "score": 0.9165363186406229,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34275838177241874,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3170701824695837,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2933173425355485,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2635421963826492,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.25892788900398905,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "distinctive"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to effective ",
                "b": "effective",
                "expected answer": [
                    "effectiveness"
                ],
                "predictions": [
                    {
                        "score": 0.29148039556246447,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2783711636514715,
                        "answer": "faculty",
                        "hit": false
                    },
                    {
                        "score": 0.2665891912884246,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2624801304082431,
                        "answer": "outweigh",
                        "hit": false
                    },
                    {
                        "score": 0.2619695571332932,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2595873322404567,
                        "answer": "happiness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "effective"
                ],
                "rank": 7054,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7255772799253464
            },
            {
                "question verbose": "What is to extensive ",
                "b": "extensive",
                "expected answer": [
                    "extensiveness"
                ],
                "predictions": [
                    {
                        "score": 0.3013535701489917,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.27214427169344446,
                        "answer": "enacted",
                        "hit": false
                    },
                    {
                        "score": 0.2719073676554744,
                        "answer": "intrusion",
                        "hit": false
                    },
                    {
                        "score": 0.26205124522250717,
                        "answer": "bcad",
                        "hit": false
                    },
                    {
                        "score": 0.26174750614958125,
                        "answer": "faculty",
                        "hit": false
                    },
                    {
                        "score": 0.2616211813089545,
                        "answer": "pusher",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "extensive"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.563436321914196
            },
            {
                "question verbose": "What is to fixed ",
                "b": "fixed",
                "expected answer": [
                    "fixedness"
                ],
                "predictions": [
                    {
                        "score": 0.3025108760124798,
                        "answer": "fewest",
                        "hit": false
                    },
                    {
                        "score": 0.28837764502235574,
                        "answer": "pusher",
                        "hit": false
                    },
                    {
                        "score": 0.2879721523621418,
                        "answer": "recovers",
                        "hit": false
                    },
                    {
                        "score": 0.28457841633213626,
                        "answer": "turner",
                        "hit": false
                    },
                    {
                        "score": 0.28424059533814283,
                        "answer": "faculty",
                        "hit": false
                    },
                    {
                        "score": 0.2832122986478724,
                        "answer": "visage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fixed"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6147142350673676
            },
            {
                "question verbose": "What is to foreign ",
                "b": "foreign",
                "expected answer": [
                    "foreignness"
                ],
                "predictions": [
                    {
                        "score": 0.3072107065985368,
                        "answer": "commiseration",
                        "hit": false
                    },
                    {
                        "score": 0.2939284175632218,
                        "answer": "competitiveness",
                        "hit": false
                    },
                    {
                        "score": 0.27309607814033104,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.272612167786134,
                        "answer": "remind",
                        "hit": false
                    },
                    {
                        "score": 0.26808263464543053,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2625153368667736,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "foreign"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6455429643392563
            },
            {
                "question verbose": "What is to happy ",
                "b": "happy",
                "expected answer": [
                    "happiness"
                ],
                "predictions": [
                    {
                        "score": 0.24679606929340153,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.22848967273889925,
                        "answer": "heal",
                        "hit": false
                    },
                    {
                        "score": 0.2282435553337567,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2122946130211007,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.21216933149135866,
                        "answer": "bet",
                        "hit": false
                    },
                    {
                        "score": 0.2115699159567284,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happy"
                ],
                "rank": 3352,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6345144212245941
            },
            {
                "question verbose": "What is to helpful ",
                "b": "helpful",
                "expected answer": [
                    "helpfulness"
                ],
                "predictions": [
                    {
                        "score": 0.33444701286625883,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.3162268483018892,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2943917788021811,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29390027305213845,
                        "answer": "unto",
                        "hit": false
                    },
                    {
                        "score": 0.28093421152496323,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.28021824465727957,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "helpful"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6631671637296677
            },
            {
                "question verbose": "What is to hidden ",
                "b": "hidden",
                "expected answer": [
                    "hiddenness"
                ],
                "predictions": [
                    {
                        "score": 0.3377875636893247,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.30710202310744206,
                        "answer": "ecourage",
                        "hit": false
                    },
                    {
                        "score": 0.29205612313171453,
                        "answer": "generosity",
                        "hit": false
                    },
                    {
                        "score": 0.2887558635147128,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.2861246937262887,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2794045898895115,
                        "answer": "bleed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hidden"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5923128426074982
            },
            {
                "question verbose": "What is to hot ",
                "b": "hot",
                "expected answer": [
                    "hotness"
                ],
                "predictions": [
                    {
                        "score": 0.20069549215164056,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.1952537858248366,
                        "answer": "amart",
                        "hit": false
                    },
                    {
                        "score": 0.1938766959902607,
                        "answer": "bleed",
                        "hit": false
                    },
                    {
                        "score": 0.18694622206439926,
                        "answer": "nextone",
                        "hit": false
                    },
                    {
                        "score": 0.1802116696051979,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.17979976217134117,
                        "answer": "critique",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hot"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5451088920235634
            },
            {
                "question verbose": "What is to huge ",
                "b": "huge",
                "expected answer": [
                    "hugeness"
                ],
                "predictions": [
                    {
                        "score": 0.20308818949477295,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.20300108001365721,
                        "answer": "estimated",
                        "hit": false
                    },
                    {
                        "score": 0.18598280715625518,
                        "answer": "speeding",
                        "hit": false
                    },
                    {
                        "score": 0.18545365007771994,
                        "answer": "policymakers",
                        "hit": false
                    },
                    {
                        "score": 0.18247825681216723,
                        "answer": "modernize",
                        "hit": false
                    },
                    {
                        "score": 0.18131432255497068,
                        "answer": "attempt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "huge"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5399163849651814
            },
            {
                "question verbose": "What is to impressive ",
                "b": "impressive",
                "expected answer": [
                    "impressiveness"
                ],
                "predictions": [
                    {
                        "score": 0.3006676784703784,
                        "answer": "pray",
                        "hit": false
                    },
                    {
                        "score": 0.29751142530123653,
                        "answer": "unto",
                        "hit": false
                    },
                    {
                        "score": 0.29227734872928246,
                        "answer": "credibility",
                        "hit": false
                    },
                    {
                        "score": 0.2913172003996891,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.28515185808884647,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.28482732316646664,
                        "answer": "breed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "impressive"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.592102162539959
            },
            {
                "question verbose": "What is to innovative ",
                "b": "innovative",
                "expected answer": [
                    "innovativeness"
                ],
                "predictions": [
                    {
                        "score": 0.3546996030413549,
                        "answer": "peer",
                        "hit": false
                    },
                    {
                        "score": 0.33808113257672334,
                        "answer": "turner",
                        "hit": false
                    },
                    {
                        "score": 0.33162791486855986,
                        "answer": "succumb",
                        "hit": false
                    },
                    {
                        "score": 0.33118935005836575,
                        "answer": "faculty",
                        "hit": false
                    },
                    {
                        "score": 0.32531713995957945,
                        "answer": "fewest",
                        "hit": false
                    },
                    {
                        "score": 0.32388220093388875,
                        "answer": "trauma",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "innovative"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6285997480154037
            },
            {
                "question verbose": "What is to interesting ",
                "b": "interesting",
                "expected answer": [
                    "interestingness"
                ],
                "predictions": [
                    {
                        "score": 0.23208900143442768,
                        "answer": "stats",
                        "hit": false
                    },
                    {
                        "score": 0.22821232617969597,
                        "answer": "unto",
                        "hit": false
                    },
                    {
                        "score": 0.22344039035190708,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.22017557336650642,
                        "answer": "genius",
                        "hit": false
                    },
                    {
                        "score": 0.21926743404992463,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.21847657351596478,
                        "answer": "unbridled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interesting"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5849374905228615
            },
            {
                "question verbose": "What is to mad ",
                "b": "mad",
                "expected answer": [
                    "madness"
                ],
                "predictions": [
                    {
                        "score": 0.29392972135393286,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2921531240340194,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.2912234098816005,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.27753884972479964,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.2769986558065774,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2693522716273402,
                        "answer": "forgive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mad"
                ],
                "rank": 9206,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6809529960155487
            },
            {
                "question verbose": "What is to marked ",
                "b": "marked",
                "expected answer": [
                    "markedness"
                ],
                "predictions": [
                    {
                        "score": 0.3653748251142918,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.3407538157315451,
                        "answer": "outweigh",
                        "hit": false
                    },
                    {
                        "score": 0.335227710792874,
                        "answer": "competitiveness",
                        "hit": false
                    },
                    {
                        "score": 0.32550530606108813,
                        "answer": "stinking",
                        "hit": false
                    },
                    {
                        "score": 0.31763653176542084,
                        "answer": "vest",
                        "hit": false
                    },
                    {
                        "score": 0.3170142719477254,
                        "answer": "laity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marked"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6087110787630081
            },
            {
                "question verbose": "What is to massive ",
                "b": "massive",
                "expected answer": [
                    "massiveness"
                ],
                "predictions": [
                    {
                        "score": 0.27828831847847824,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2626763856549579,
                        "answer": "niceness",
                        "hit": false
                    },
                    {
                        "score": 0.2579482846830412,
                        "answer": "generosity",
                        "hit": false
                    },
                    {
                        "score": 0.2533615189174542,
                        "answer": "salary",
                        "hit": false
                    },
                    {
                        "score": 0.25184959159503434,
                        "answer": "awareness",
                        "hit": false
                    },
                    {
                        "score": 0.25054512026799414,
                        "answer": "happiness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "massive"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6543697118759155
            },
            {
                "question verbose": "What is to nice ",
                "b": "nice",
                "expected answer": [
                    "niceness"
                ],
                "predictions": [
                    {
                        "score": 0.23459487588367472,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2326442078460443,
                        "answer": "cheering",
                        "hit": false
                    },
                    {
                        "score": 0.22278723910833342,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2100062482614628,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.20897956350099023,
                        "answer": "jimi",
                        "hit": false
                    },
                    {
                        "score": 0.20844791914822755,
                        "answer": "worry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nice"
                ],
                "rank": 2741,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6417264491319656
            },
            {
                "question verbose": "What is to obvious ",
                "b": "obvious",
                "expected answer": [
                    "obviousness"
                ],
                "predictions": [
                    {
                        "score": 0.2314849740795356,
                        "answer": "spill",
                        "hit": false
                    },
                    {
                        "score": 0.2166439988518499,
                        "answer": "speeding",
                        "hit": false
                    },
                    {
                        "score": 0.21550739184256826,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.21383350817038624,
                        "answer": "scary",
                        "hit": false
                    },
                    {
                        "score": 0.2128802977413429,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.2088233391184095,
                        "answer": "papi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "obvious"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5441366359591484
            },
            {
                "question verbose": "What is to odd ",
                "b": "odd",
                "expected answer": [
                    "oddness"
                ],
                "predictions": [
                    {
                        "score": 0.284368068936492,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.2789331094124962,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.27510866841251885,
                        "answer": "forgive",
                        "hit": false
                    },
                    {
                        "score": 0.27215417141444176,
                        "answer": "unto",
                        "hit": false
                    },
                    {
                        "score": 0.2699643458426746,
                        "answer": "stats",
                        "hit": false
                    },
                    {
                        "score": 0.2658308966168097,
                        "answer": "flame",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "odd"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6056981608271599
            },
            {
                "question verbose": "What is to prepared ",
                "b": "prepared",
                "expected answer": [
                    "preparedness"
                ],
                "predictions": [
                    {
                        "score": 0.2658614935265427,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.25387189272247057,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24393876139449192,
                        "answer": "generation",
                        "hit": false
                    },
                    {
                        "score": 0.24300421664462787,
                        "answer": "stats",
                        "hit": false
                    },
                    {
                        "score": 0.24016598801474737,
                        "answer": "papi",
                        "hit": false
                    },
                    {
                        "score": 0.23554464511984888,
                        "answer": "heal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prepared"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6409040838479996
            },
            {
                "question verbose": "What is to pure ",
                "b": "pure",
                "expected answer": [
                    "pureness"
                ],
                "predictions": [
                    {
                        "score": 0.2933255767336205,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.28121953744575834,
                        "answer": "constitutes",
                        "hit": false
                    },
                    {
                        "score": 0.2791463126200583,
                        "answer": "unite",
                        "hit": false
                    },
                    {
                        "score": 0.2787391986987408,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.2745935958150192,
                        "answer": "succumb",
                        "hit": false
                    },
                    {
                        "score": 0.27423763989342964,
                        "answer": "knowledgeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pure"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5443156324326992
            },
            {
                "question verbose": "What is to random ",
                "b": "random",
                "expected answer": [
                    "randomness"
                ],
                "predictions": [
                    {
                        "score": 0.24826170473164844,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24770647635102855,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2448711286016114,
                        "answer": "cheering",
                        "hit": false
                    },
                    {
                        "score": 0.24270380622051457,
                        "answer": "greek",
                        "hit": false
                    },
                    {
                        "score": 0.23914426379254258,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.2319743645790726,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "random"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6375161856412888
            },
            {
                "question verbose": "What is to rare ",
                "b": "rare",
                "expected answer": [
                    "rareness"
                ],
                "predictions": [
                    {
                        "score": 0.2961080617298692,
                        "answer": "bleed",
                        "hit": false
                    },
                    {
                        "score": 0.2946727028007575,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.28518856670741843,
                        "answer": "constitutes",
                        "hit": false
                    },
                    {
                        "score": 0.267803764580745,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.26736623297307366,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.26149018560025017,
                        "answer": "unto",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rare"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6415678113698959
            },
            {
                "question verbose": "What is to reasonable ",
                "b": "reasonable",
                "expected answer": [
                    "reasonableness"
                ],
                "predictions": [
                    {
                        "score": 0.3067255861667768,
                        "answer": "salary",
                        "hit": false
                    },
                    {
                        "score": 0.30033171734912656,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.293843210363209,
                        "answer": "remind",
                        "hit": false
                    },
                    {
                        "score": 0.28288569471238584,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2769319501029774,
                        "answer": "outweigh",
                        "hit": false
                    },
                    {
                        "score": 0.2742721569547465,
                        "answer": "competitiveness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reasonable"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6389851719141006
            },
            {
                "question verbose": "What is to related ",
                "b": "related",
                "expected answer": [
                    "relatedness"
                ],
                "predictions": [
                    {
                        "score": 0.26291665048789836,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.25482641160816855,
                        "answer": "wanting",
                        "hit": false
                    },
                    {
                        "score": 0.2205244019336695,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.21916214661587888,
                        "answer": "mother",
                        "hit": false
                    },
                    {
                        "score": 0.21598855597557912,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.21506909998238716,
                        "answer": "legitimacy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "related"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5755203440785408
            },
            {
                "question verbose": "What is to righteous ",
                "b": "righteous",
                "expected answer": [
                    "righteousness"
                ],
                "predictions": [
                    {
                        "score": 0.917541435358934,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34396493808150064,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3170018679782443,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2847306246043184,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2624859666514599,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.26050258878187704,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "righteous"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to sacred ",
                "b": "sacred",
                "expected answer": [
                    "sacredness"
                ],
                "predictions": [
                    {
                        "score": 0.43687380692462763,
                        "answer": "vest",
                        "hit": false
                    },
                    {
                        "score": 0.43248334576812614,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.4045066765752752,
                        "answer": "outweigh",
                        "hit": false
                    },
                    {
                        "score": 0.40055964801627464,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.39617844125303736,
                        "answer": "shipped",
                        "hit": false
                    },
                    {
                        "score": 0.37540390485772274,
                        "answer": "roomful",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sacred"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6587668210268021
            },
            {
                "question verbose": "What is to sad ",
                "b": "sad",
                "expected answer": [
                    "sadness"
                ],
                "predictions": [
                    {
                        "score": 0.25666420914703963,
                        "answer": "truly",
                        "hit": false
                    },
                    {
                        "score": 0.23491054967808184,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.23389800567772767,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2274277478819762,
                        "answer": "heal",
                        "hit": false
                    },
                    {
                        "score": 0.22581911369963673,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.22143792650235278,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sad"
                ],
                "rank": 2564,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7539563477039337
            },
            {
                "question verbose": "What is to same ",
                "b": "same",
                "expected answer": [
                    "sameness"
                ],
                "predictions": [
                    {
                        "score": 0.9178126097852708,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3400595416072637,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.323643620906141,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2995786941708324,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.26827772054739146,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.25154503527488864,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "same"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to serious ",
                "b": "serious",
                "expected answer": [
                    "seriousness"
                ],
                "predictions": [
                    {
                        "score": 0.24569523635572474,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.22289509464219773,
                        "answer": "hornet",
                        "hit": false
                    },
                    {
                        "score": 0.22023293703505115,
                        "answer": "niceness",
                        "hit": false
                    },
                    {
                        "score": 0.2160748418183246,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.20924079780876637,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.20638610789269343,
                        "answer": "paternity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "serious"
                ],
                "rank": 1494,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.664062112569809
            },
            {
                "question verbose": "What is to situated ",
                "b": "situated",
                "expected answer": [
                    "situatedness"
                ],
                "predictions": [
                    {
                        "score": 0.38136704973791213,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.31089180171729414,
                        "answer": "velocity",
                        "hit": false
                    },
                    {
                        "score": 0.30642788243166275,
                        "answer": "semester",
                        "hit": false
                    },
                    {
                        "score": 0.2995945421683468,
                        "answer": "pusher",
                        "hit": false
                    },
                    {
                        "score": 0.29503686627480447,
                        "answer": "dispose",
                        "hit": false
                    },
                    {
                        "score": 0.2924572169112954,
                        "answer": "allocation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "situated"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6016842946410179
            },
            {
                "question verbose": "What is to strange ",
                "b": "strange",
                "expected answer": [
                    "strangeness"
                ],
                "predictions": [
                    {
                        "score": 0.31710247255167195,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2855264113388298,
                        "answer": "mh",
                        "hit": false
                    },
                    {
                        "score": 0.28032313246975366,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.2788553304634169,
                        "answer": "truly",
                        "hit": false
                    },
                    {
                        "score": 0.2773382797409069,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.2722570002416368,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strange"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6177984699606895
            },
            {
                "question verbose": "What is to unique ",
                "b": "unique",
                "expected answer": [
                    "uniqueness"
                ],
                "predictions": [
                    {
                        "score": 0.22948776174949448,
                        "answer": "culture",
                        "hit": false
                    },
                    {
                        "score": 0.2292245274262119,
                        "answer": "awareness",
                        "hit": false
                    },
                    {
                        "score": 0.22634044300007647,
                        "answer": "peer",
                        "hit": false
                    },
                    {
                        "score": 0.2260044069056399,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2250828991618359,
                        "answer": "altar",
                        "hit": false
                    },
                    {
                        "score": 0.22419868227926779,
                        "answer": "seaman",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "unique"
                ],
                "rank": 1059,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7377552390098572
            },
            {
                "question verbose": "What is to useful ",
                "b": "useful",
                "expected answer": [
                    "usefulness"
                ],
                "predictions": [
                    {
                        "score": 0.35913619750849074,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2891420383649195,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.28890075746751376,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.28534685276877503,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2454065853463399,
                        "answer": "assumption",
                        "hit": false
                    },
                    {
                        "score": 0.2366904976427288,
                        "answer": "happiness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "useful"
                ],
                "rank": 2545,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7911520600318909
            },
            {
                "question verbose": "What is to vast ",
                "b": "vast",
                "expected answer": [
                    "vastness"
                ],
                "predictions": [
                    {
                        "score": 0.3315800708705934,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2970904956963905,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.2780998076902765,
                        "answer": "heritage",
                        "hit": false
                    },
                    {
                        "score": 0.2612434547042233,
                        "answer": "pioneer",
                        "hit": false
                    },
                    {
                        "score": 0.25430305218280597,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.2526817949766033,
                        "answer": "fewest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vast"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5478322841227055
            },
            {
                "question verbose": "What is to weak ",
                "b": "weak",
                "expected answer": [
                    "weakness"
                ],
                "predictions": [
                    {
                        "score": 0.2744646866179105,
                        "answer": "storing",
                        "hit": false
                    },
                    {
                        "score": 0.26798840798364587,
                        "answer": "awareness",
                        "hit": false
                    },
                    {
                        "score": 0.24460213035427614,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.24374022552552557,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.24301909615135167,
                        "answer": "happiness",
                        "hit": false
                    },
                    {
                        "score": 0.24193393726439325,
                        "answer": "obtuse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weak"
                ],
                "rank": 5461,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7184322625398636
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D05 [adj+ness_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "19d61487-361a-4373-b036-44c33cc37e14",
            "timestamp": "2020-10-22T15:57:04.749621"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to acquire ",
                "b": "acquire",
                "expected answer": [
                    "reacquire"
                ],
                "predictions": [
                    {
                        "score": 0.2933294346275481,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2856791478602961,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.25320406289567665,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.25292041115921227,
                        "answer": "drink",
                        "hit": false
                    },
                    {
                        "score": 0.24945384456448555,
                        "answer": "smashing",
                        "hit": false
                    },
                    {
                        "score": 0.2478396677666008,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "acquire"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5971764996647835
            },
            {
                "question verbose": "What is to adjust ",
                "b": "adjust",
                "expected answer": [
                    "readjust"
                ],
                "predictions": [
                    {
                        "score": 0.3337460723600376,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.33138416092871975,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.3313334211286185,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.3241987197736397,
                        "answer": "atleast",
                        "hit": false
                    },
                    {
                        "score": 0.3084588644313285,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.30759421506366813,
                        "answer": "learner",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adjust"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6323782056570053
            },
            {
                "question verbose": "What is to appear ",
                "b": "appear",
                "expected answer": [
                    "reappear"
                ],
                "predictions": [
                    {
                        "score": 0.3030206790862845,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2996996241713933,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.27899849010896244,
                        "answer": "atleast",
                        "hit": false
                    },
                    {
                        "score": 0.2768618845343521,
                        "answer": "genius",
                        "hit": false
                    },
                    {
                        "score": 0.26899607945443055,
                        "answer": "uninformed",
                        "hit": false
                    },
                    {
                        "score": 0.26718632894295236,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appear"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6846228390932083
            },
            {
                "question verbose": "What is to apply ",
                "b": "apply",
                "expected answer": [
                    "reapply"
                ],
                "predictions": [
                    {
                        "score": 0.27636164946721653,
                        "answer": "arguing",
                        "hit": false
                    },
                    {
                        "score": 0.27102723613031743,
                        "answer": "necessarily",
                        "hit": false
                    },
                    {
                        "score": 0.26166195586886487,
                        "answer": "communicate",
                        "hit": false
                    },
                    {
                        "score": 0.2567926056207181,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.2502947673949595,
                        "answer": "indicate",
                        "hit": false
                    },
                    {
                        "score": 0.250284733378036,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apply"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5968583226203918
            },
            {
                "question verbose": "What is to appoint ",
                "b": "appoint",
                "expected answer": [
                    "reappoint"
                ],
                "predictions": [
                    {
                        "score": 0.8345101666591993,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2482424830452043,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.22048626721112274,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.21815168008906569,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.214215848863037,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.20944456239060058,
                        "answer": "wedgie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appoint"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to arrange ",
                "b": "arrange",
                "expected answer": [
                    "rearrange"
                ],
                "predictions": [
                    {
                        "score": 0.8348891783379121,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2548153575933132,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.22455302230571847,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.22153783759543058,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.21599711561165202,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.21597598770272575,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "arrange"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to assess ",
                "b": "assess",
                "expected answer": [
                    "reassess"
                ],
                "predictions": [
                    {
                        "score": 0.8344930404200731,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2506525685436371,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.22268714539155576,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.21996260939949686,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.2191504378199807,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.2148096245894403,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "assess"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to assign ",
                "b": "assign",
                "expected answer": [
                    "reassign"
                ],
                "predictions": [
                    {
                        "score": 0.3196442648431129,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.3174004724820416,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.31639420204479457,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.3132344133582166,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.30842829636327906,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.3011602163465752,
                        "answer": "atleast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "assign"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6763179302215576
            },
            {
                "question verbose": "What is to assure ",
                "b": "assure",
                "expected answer": [
                    "reassure"
                ],
                "predictions": [
                    {
                        "score": 0.2761553795691824,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.24459555824609355,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.23686966871863596,
                        "answer": "hypnosis",
                        "hit": false
                    },
                    {
                        "score": 0.23269545103949857,
                        "answer": "mantra",
                        "hit": false
                    },
                    {
                        "score": 0.22992286511553045,
                        "answer": "sc",
                        "hit": false
                    },
                    {
                        "score": 0.2280056963394151,
                        "answer": "linux",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "assure"
                ],
                "rank": 7309,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7891928851604462
            },
            {
                "question verbose": "What is to calculate ",
                "b": "calculate",
                "expected answer": [
                    "recalculate"
                ],
                "predictions": [
                    {
                        "score": 0.2884673653517095,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.2844308621051303,
                        "answer": "atleast",
                        "hit": false
                    },
                    {
                        "score": 0.28128528554912574,
                        "answer": "genius",
                        "hit": false
                    },
                    {
                        "score": 0.2784231561851396,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.27048133656281725,
                        "answer": "understood",
                        "hit": false
                    },
                    {
                        "score": 0.26976386409825676,
                        "answer": "redeeming",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "calculate"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6527416557073593
            },
            {
                "question verbose": "What is to cognize ",
                "b": "cognize",
                "expected answer": [
                    "recognize"
                ],
                "predictions": [
                    {
                        "score": 0.8360042127384728,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24070690454967256,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.20759018931555448,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.19878039705720688,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.19543983619463734,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.1945119089640347,
                        "answer": "procedurally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cognize"
                ],
                "rank": 776,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6667893975973129
            },
            {
                "question verbose": "What is to commend ",
                "b": "commend",
                "expected answer": [
                    "recommend"
                ],
                "predictions": [
                    {
                        "score": 0.8358424197716852,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24512690970849518,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.20505304423617982,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.20156986634042004,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.20093143243649117,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.20045495039168595,
                        "answer": "lose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "commend"
                ],
                "rank": 307,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.68461012840271
            },
            {
                "question verbose": "What is to configure ",
                "b": "configure",
                "expected answer": [
                    "reconfigure"
                ],
                "predictions": [
                    {
                        "score": 0.8346360440396481,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2634511021226492,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.21728997851947188,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.21643672145772752,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.2161540003869439,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.21125341386128885,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "configure"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to confirm ",
                "b": "confirm",
                "expected answer": [
                    "reconfirm"
                ],
                "predictions": [
                    {
                        "score": 0.2986355573424267,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.27613446177040407,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.27071310646815344,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.26789726302859734,
                        "answer": "suggests",
                        "hit": false
                    },
                    {
                        "score": 0.26507076820144343,
                        "answer": "manme",
                        "hit": false
                    },
                    {
                        "score": 0.26354479319952395,
                        "answer": "rewrite",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "confirm"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6521879732608795
            },
            {
                "question verbose": "What is to connect ",
                "b": "connect",
                "expected answer": [
                    "reconnect"
                ],
                "predictions": [
                    {
                        "score": 0.2601697606338517,
                        "answer": "hypnosis",
                        "hit": false
                    },
                    {
                        "score": 0.25969029229526747,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.25671471087355313,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.2484929787913815,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.2434009880438576,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.2429679079949254,
                        "answer": "atleast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "connect"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6163900420069695
            },
            {
                "question verbose": "What is to consider ",
                "b": "consider",
                "expected answer": [
                    "reconsider"
                ],
                "predictions": [
                    {
                        "score": 0.25191976082510664,
                        "answer": "nordling",
                        "hit": false
                    },
                    {
                        "score": 0.2332927000535878,
                        "answer": "genius",
                        "hit": false
                    },
                    {
                        "score": 0.22579726207897802,
                        "answer": "appreciate",
                        "hit": false
                    },
                    {
                        "score": 0.22278782787669008,
                        "answer": "badmouthing",
                        "hit": false
                    },
                    {
                        "score": 0.2194616281766656,
                        "answer": "surety",
                        "hit": false
                    },
                    {
                        "score": 0.2194079400781479,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consider"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6062342002987862
            },
            {
                "question verbose": "What is to create ",
                "b": "create",
                "expected answer": [
                    "recreate"
                ],
                "predictions": [
                    {
                        "score": 0.21183946401221307,
                        "answer": "stating",
                        "hit": false
                    },
                    {
                        "score": 0.18382327782419808,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.180501042352525,
                        "answer": "demonstrated",
                        "hit": false
                    },
                    {
                        "score": 0.17972828322769563,
                        "answer": "respond",
                        "hit": false
                    },
                    {
                        "score": 0.17769732495204413,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.17682462825256356,
                        "answer": "drown",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "create"
                ],
                "rank": 531,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.687082514166832
            },
            {
                "question verbose": "What is to decorate ",
                "b": "decorate",
                "expected answer": [
                    "redecorate"
                ],
                "predictions": [
                    {
                        "score": 0.8345830100607445,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23797406977621186,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2201550009532687,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.21250505853765078,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.21024199353677234,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.21022947982356355,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "decorate"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to deem ",
                "b": "deem",
                "expected answer": [
                    "redeem"
                ],
                "predictions": [
                    {
                        "score": 0.8366323330306287,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2502499034164201,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.20570468338358733,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.20458659937954954,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20398557191352523,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.20276527767396002,
                        "answer": "wedgie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deem"
                ],
                "rank": 132,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7352716773748398
            },
            {
                "question verbose": "What is to define ",
                "b": "define",
                "expected answer": [
                    "redefine"
                ],
                "predictions": [
                    {
                        "score": 0.2645680381269067,
                        "answer": "fundamental",
                        "hit": false
                    },
                    {
                        "score": 0.2620628426940387,
                        "answer": "highly",
                        "hit": false
                    },
                    {
                        "score": 0.25083885786425064,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.2270870757705706,
                        "answer": "suggesting",
                        "hit": false
                    },
                    {
                        "score": 0.2270814347278265,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2249213134326208,
                        "answer": "nerve",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "define"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5958474799990654
            },
            {
                "question verbose": "What is to develop ",
                "b": "develop",
                "expected answer": [
                    "redevelop"
                ],
                "predictions": [
                    {
                        "score": 0.3088465454599669,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.28694630725345144,
                        "answer": "html",
                        "hit": false
                    },
                    {
                        "score": 0.277163092334675,
                        "answer": "linux",
                        "hit": false
                    },
                    {
                        "score": 0.27559591241185927,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27546592896239847,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.263405989118778,
                        "answer": "wedgie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develop"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6677109003067017
            },
            {
                "question verbose": "What is to discover ",
                "b": "discover",
                "expected answer": [
                    "rediscover"
                ],
                "predictions": [
                    {
                        "score": 0.26509723069182495,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.2507130602605686,
                        "answer": "cryengine",
                        "hit": false
                    },
                    {
                        "score": 0.2502354823369862,
                        "answer": "dejected",
                        "hit": false
                    },
                    {
                        "score": 0.24695506844408402,
                        "answer": "genius",
                        "hit": false
                    },
                    {
                        "score": 0.24532104552453693,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2436344315088692,
                        "answer": "rewrite",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "discover"
                ],
                "rank": 1948,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8371936976909637
            },
            {
                "question verbose": "What is to distribute ",
                "b": "distribute",
                "expected answer": [
                    "redistribute"
                ],
                "predictions": [
                    {
                        "score": 0.3223387164802869,
                        "answer": "cryengine",
                        "hit": false
                    },
                    {
                        "score": 0.2937891698845767,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2909086079488719,
                        "answer": "rewrite",
                        "hit": false
                    },
                    {
                        "score": 0.2905365283146148,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.2754466653002898,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.27375701228095717,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "distribute"
                ],
                "rank": 209,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8220023810863495
            },
            {
                "question verbose": "What is to emerge ",
                "b": "emerge",
                "expected answer": [
                    "reemerge"
                ],
                "predictions": [
                    {
                        "score": 0.2789047788350091,
                        "answer": "savior",
                        "hit": false
                    },
                    {
                        "score": 0.2762884015160951,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.27187404099245166,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2605092205180697,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.25849456387020703,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.250065669465005,
                        "answer": "necessarily",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "emerge"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5919557884335518
            },
            {
                "question verbose": "What is to engage ",
                "b": "engage",
                "expected answer": [
                    "reengage"
                ],
                "predictions": [
                    {
                        "score": 0.3239002886259426,
                        "answer": "necessarily",
                        "hit": false
                    },
                    {
                        "score": 0.3012045632113247,
                        "answer": "arguing",
                        "hit": false
                    },
                    {
                        "score": 0.2925915724991282,
                        "answer": "wanting",
                        "hit": false
                    },
                    {
                        "score": 0.27479727048468705,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.2650551803558628,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2586619962072786,
                        "answer": "resolved",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "engage"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6381287723779678
            },
            {
                "question verbose": "What is to establish ",
                "b": "establish",
                "expected answer": [
                    "reestablish"
                ],
                "predictions": [
                    {
                        "score": 0.25430701915359694,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.24940834242237117,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2459683307740134,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.24283435378057422,
                        "answer": "oppress",
                        "hit": false
                    },
                    {
                        "score": 0.23227468334765236,
                        "answer": "commonplace",
                        "hit": false
                    },
                    {
                        "score": 0.2304959948597013,
                        "answer": "exagerate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "establish"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6518961191177368
            },
            {
                "question verbose": "What is to evaluate ",
                "b": "evaluate",
                "expected answer": [
                    "reevaluate"
                ],
                "predictions": [
                    {
                        "score": 0.3084902539862073,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.3069260052543164,
                        "answer": "recognize",
                        "hit": false
                    },
                    {
                        "score": 0.3040976160796273,
                        "answer": "hypnosis",
                        "hit": false
                    },
                    {
                        "score": 0.29898094186849133,
                        "answer": "linux",
                        "hit": false
                    },
                    {
                        "score": 0.29779880035718137,
                        "answer": "fist",
                        "hit": false
                    },
                    {
                        "score": 0.2800461908297176,
                        "answer": "cryengine",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "evaluate"
                ],
                "rank": 1915,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7837676107883453
            },
            {
                "question verbose": "What is to examine ",
                "b": "examine",
                "expected answer": [
                    "reexamine"
                ],
                "predictions": [
                    {
                        "score": 0.30687122949069,
                        "answer": "specific",
                        "hit": false
                    },
                    {
                        "score": 0.3044217226924652,
                        "answer": "stick",
                        "hit": false
                    },
                    {
                        "score": 0.28171655719318306,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.2796432268666172,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.27745538686137405,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.2763706420424049,
                        "answer": "pity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "examine"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6564575880765915
            },
            {
                "question verbose": "What is to generate ",
                "b": "generate",
                "expected answer": [
                    "regenerate"
                ],
                "predictions": [
                    {
                        "score": 0.2866139762979682,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2575611625023933,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.2513065965385156,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24672049682656336,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.24069496496163942,
                        "answer": "forgive",
                        "hit": false
                    },
                    {
                        "score": 0.2384199226870335,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "generate"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6530273407697678
            },
            {
                "question verbose": "What is to grow ",
                "b": "grow",
                "expected answer": [
                    "regrow"
                ],
                "predictions": [
                    {
                        "score": 0.2500174443985783,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.24654423262226102,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.22341153833175126,
                        "answer": "goverment",
                        "hit": false
                    },
                    {
                        "score": 0.21586535891130407,
                        "answer": "unbridled",
                        "hit": false
                    },
                    {
                        "score": 0.21404732480211167,
                        "answer": "forgive",
                        "hit": false
                    },
                    {
                        "score": 0.21215514282730794,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grow"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6291625648736954
            },
            {
                "question verbose": "What is to install ",
                "b": "install",
                "expected answer": [
                    "reinstall"
                ],
                "predictions": [
                    {
                        "score": 0.3281886210665978,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.30515945019675167,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3045563176014304,
                        "answer": "rewrite",
                        "hit": false
                    },
                    {
                        "score": 0.30275592629023446,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.2990692254071571,
                        "answer": "hypnosis",
                        "hit": false
                    },
                    {
                        "score": 0.29767688422136906,
                        "answer": "disingenuous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "install"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6385581791400909
            },
            {
                "question verbose": "What is to integrate ",
                "b": "integrate",
                "expected answer": [
                    "reintegrate"
                ],
                "predictions": [
                    {
                        "score": 0.21912920795681076,
                        "answer": "rewrite",
                        "hit": false
                    },
                    {
                        "score": 0.21392061873258364,
                        "answer": "communicate",
                        "hit": false
                    },
                    {
                        "score": 0.2112664072324464,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20817604577303706,
                        "answer": "observer",
                        "hit": false
                    },
                    {
                        "score": 0.20730340847149872,
                        "answer": "drink",
                        "hit": false
                    },
                    {
                        "score": 0.20707170087846283,
                        "answer": "redeem",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "integrate"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5737661719322205
            },
            {
                "question verbose": "What is to interpret ",
                "b": "interpret",
                "expected answer": [
                    "reinterpret"
                ],
                "predictions": [
                    {
                        "score": 0.30885976651674574,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.3035022137846596,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2996048042319984,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.29742376003615856,
                        "answer": "observer",
                        "hit": false
                    },
                    {
                        "score": 0.29548115574205897,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.29493709576422045,
                        "answer": "earning",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interpret"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.666520819067955
            },
            {
                "question verbose": "What is to introduce ",
                "b": "introduce",
                "expected answer": [
                    "reintroduce"
                ],
                "predictions": [
                    {
                        "score": 0.26603937938832173,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.2407330292922944,
                        "answer": "truly",
                        "hit": false
                    },
                    {
                        "score": 0.23323896813320685,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2299047359283042,
                        "answer": "recreate",
                        "hit": false
                    },
                    {
                        "score": 0.2267492656423233,
                        "answer": "characteristic",
                        "hit": false
                    },
                    {
                        "score": 0.22201833164142173,
                        "answer": "hateful",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "introduce"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5827931612730026
            },
            {
                "question verbose": "What is to invest ",
                "b": "invest",
                "expected answer": [
                    "reinvest"
                ],
                "predictions": [
                    {
                        "score": 0.32187542866584956,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.3101061737248873,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2998039178510714,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.2992901789613075,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.290718350215046,
                        "answer": "disingenuous",
                        "hit": false
                    },
                    {
                        "score": 0.28884004478811703,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "invest"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6888108104467392
            },
            {
                "question verbose": "What is to investigate ",
                "b": "investigate",
                "expected answer": [
                    "reinvestigate"
                ],
                "predictions": [
                    {
                        "score": 0.2467193743442118,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.23772781972585724,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.23426215052312888,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.23237241599465508,
                        "answer": "decide",
                        "hit": false
                    },
                    {
                        "score": 0.23224169574052542,
                        "answer": "morally",
                        "hit": false
                    },
                    {
                        "score": 0.2318421191700486,
                        "answer": "lib",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "investigate"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5774509981274605
            },
            {
                "question verbose": "What is to learn ",
                "b": "learn",
                "expected answer": [
                    "relearn"
                ],
                "predictions": [
                    {
                        "score": 0.2870808360239646,
                        "answer": "hypnosis",
                        "hit": false
                    },
                    {
                        "score": 0.25625632371436874,
                        "answer": "wedgie",
                        "hit": false
                    },
                    {
                        "score": 0.2505692271113371,
                        "answer": "cryengine",
                        "hit": false
                    },
                    {
                        "score": 0.2501633073813051,
                        "answer": "html",
                        "hit": false
                    },
                    {
                        "score": 0.2471113562924824,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.24705579398987323,
                        "answer": "unbridled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "learn"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5812326818704605
            },
            {
                "question verbose": "What is to locate ",
                "b": "locate",
                "expected answer": [
                    "relocate"
                ],
                "predictions": [
                    {
                        "score": 0.8346943165375487,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25616782417572004,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.21748895881647287,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.2172702293492512,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.21724244132374546,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.21221015545751593,
                        "answer": "goal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locate"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to marry ",
                "b": "marry",
                "expected answer": [
                    "remarry"
                ],
                "predictions": [
                    {
                        "score": 0.3495576450017733,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.3479752543515901,
                        "answer": "savior",
                        "hit": false
                    },
                    {
                        "score": 0.33434985851870097,
                        "answer": "fist",
                        "hit": false
                    },
                    {
                        "score": 0.32900918364320353,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.317013937353131,
                        "answer": "ape",
                        "hit": false
                    },
                    {
                        "score": 0.3117615788296205,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.686234325170517
            },
            {
                "question verbose": "What is to negotiate ",
                "b": "negotiate",
                "expected answer": [
                    "renegotiate"
                ],
                "predictions": [
                    {
                        "score": 0.23397028052664384,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2208288794831929,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.22064375052011329,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.21413271475043585,
                        "answer": "highly",
                        "hit": false
                    },
                    {
                        "score": 0.21402970487359185,
                        "answer": "evaders",
                        "hit": false
                    },
                    {
                        "score": 0.21252029558334243,
                        "answer": "accountable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "negotiate"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5802903845906258
            },
            {
                "question verbose": "What is to occur ",
                "b": "occur",
                "expected answer": [
                    "reoccur"
                ],
                "predictions": [
                    {
                        "score": 0.32875625864441765,
                        "answer": "savior",
                        "hit": false
                    },
                    {
                        "score": 0.31860459662279333,
                        "answer": "necessarily",
                        "hit": false
                    },
                    {
                        "score": 0.30988965114946293,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3074989056574091,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30433825995518404,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.30356578637248655,
                        "answer": "smarter",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "occur"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6791394650936127
            },
            {
                "question verbose": "What is to organize ",
                "b": "organize",
                "expected answer": [
                    "reorganize",
                    "reorganise"
                ],
                "predictions": [
                    {
                        "score": 0.31326707252293684,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3014720547729799,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.29955792125134684,
                        "answer": "recreate",
                        "hit": false
                    },
                    {
                        "score": 0.2918211155956334,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.29173575944421454,
                        "answer": "scrabble",
                        "hit": false
                    },
                    {
                        "score": 0.2912707990726109,
                        "answer": "nordling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "organize"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6690833270549774
            },
            {
                "question verbose": "What is to publish ",
                "b": "publish",
                "expected answer": [
                    "republish"
                ],
                "predictions": [
                    {
                        "score": 0.3131403479553445,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.3052983443883147,
                        "answer": "rewrite",
                        "hit": false
                    },
                    {
                        "score": 0.29706550630710177,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2905578563547064,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.28789614437534594,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.2874521485299257,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publish"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.630352258682251
            },
            {
                "question verbose": "What is to send ",
                "b": "send",
                "expected answer": [
                    "resend"
                ],
                "predictions": [
                    {
                        "score": 0.3876383011030773,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25164043495741195,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.229058030141338,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.21864911301740228,
                        "answer": "prefer",
                        "hit": false
                    },
                    {
                        "score": 0.21845183469422594,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.21573132844161194,
                        "answer": "semi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "send"
                ],
                "rank": 1436,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7334662228822708
            },
            {
                "question verbose": "What is to solve ",
                "b": "solve",
                "expected answer": [
                    "resolve"
                ],
                "predictions": [
                    {
                        "score": 0.34248547422916303,
                        "answer": "bipartisan",
                        "hit": false
                    },
                    {
                        "score": 0.2968303080613891,
                        "answer": "taxing",
                        "hit": false
                    },
                    {
                        "score": 0.27677661115067304,
                        "answer": "obama",
                        "hit": false
                    },
                    {
                        "score": 0.2711486827442066,
                        "answer": "goal",
                        "hit": false
                    },
                    {
                        "score": 0.26910165254917195,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2685130281617853,
                        "answer": "atleast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "solve"
                ],
                "rank": 817,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8266362249851227
            },
            {
                "question verbose": "What is to submit ",
                "b": "submit",
                "expected answer": [
                    "resubmit"
                ],
                "predictions": [
                    {
                        "score": 0.2670373694047606,
                        "answer": "devs",
                        "hit": false
                    },
                    {
                        "score": 0.25093274655378506,
                        "answer": "vikes",
                        "hit": false
                    },
                    {
                        "score": 0.24578008730163692,
                        "answer": "maker",
                        "hit": false
                    },
                    {
                        "score": 0.23990740252051035,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23459072890081117,
                        "answer": "juntao",
                        "hit": false
                    },
                    {
                        "score": 0.23321418469978128,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "submit"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6459440290927887
            },
            {
                "question verbose": "What is to tell ",
                "b": "tell",
                "expected answer": [
                    "retell"
                ],
                "predictions": [
                    {
                        "score": 0.25241975045137244,
                        "answer": "rmoney",
                        "hit": false
                    },
                    {
                        "score": 0.23292134111653634,
                        "answer": "delicately",
                        "hit": false
                    },
                    {
                        "score": 0.22464016480386922,
                        "answer": "liar",
                        "hit": false
                    },
                    {
                        "score": 0.22435042105620912,
                        "answer": "badmouthing",
                        "hit": false
                    },
                    {
                        "score": 0.2071850225739411,
                        "answer": "surety",
                        "hit": false
                    },
                    {
                        "score": 0.20700413504874027,
                        "answer": "zines",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tell"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5753644853830338
            },
            {
                "question verbose": "What is to unite ",
                "b": "unite",
                "expected answer": [
                    "reunite"
                ],
                "predictions": [
                    {
                        "score": 0.3334659787570954,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.325353013716769,
                        "answer": "unbridled",
                        "hit": false
                    },
                    {
                        "score": 0.30787849181617477,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.3032319386672126,
                        "answer": "characteristic",
                        "hit": false
                    },
                    {
                        "score": 0.30125845417326697,
                        "answer": "pity",
                        "hit": false
                    },
                    {
                        "score": 0.30048898279082414,
                        "answer": "recreate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "unite"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6162363290786743
            },
            {
                "question verbose": "What is to upload ",
                "b": "upload",
                "expected answer": [
                    "reupload"
                ],
                "predictions": [
                    {
                        "score": 0.26481900096751615,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.25188203831767675,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.2361519001997397,
                        "answer": "console",
                        "hit": false
                    },
                    {
                        "score": 0.23257619340971133,
                        "answer": "impetuosity",
                        "hit": false
                    },
                    {
                        "score": 0.23119688417416967,
                        "answer": "snarky",
                        "hit": false
                    },
                    {
                        "score": 0.22940476098202023,
                        "answer": "atleast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "upload"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5556297563016415
            },
            {
                "question verbose": "What is to write ",
                "b": "write",
                "expected answer": [
                    "rewrite"
                ],
                "predictions": [
                    {
                        "score": 0.2971210308928573,
                        "answer": "prefer",
                        "hit": false
                    },
                    {
                        "score": 0.25837687944648136,
                        "answer": "hypnosis",
                        "hit": false
                    },
                    {
                        "score": 0.233847411071821,
                        "answer": "accepting",
                        "hit": false
                    },
                    {
                        "score": 0.22231093832980117,
                        "answer": "procedurally",
                        "hit": false
                    },
                    {
                        "score": 0.2219904404431329,
                        "answer": "fist",
                        "hit": false
                    },
                    {
                        "score": 0.2207994998314144,
                        "answer": "cared",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "write"
                ],
                "rank": 435,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7132407426834106
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D06 [re+verb_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "86c0fc53-5615-4ffe-8f14-09d561bb1ad8",
            "timestamp": "2020-10-22T15:57:06.574090"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to accept ",
                "b": "accept",
                "expected answer": [
                    "acceptable"
                ],
                "predictions": [
                    {
                        "score": 0.3183306089253322,
                        "answer": "donated",
                        "hit": false
                    },
                    {
                        "score": 0.299164969900587,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.29886324756102317,
                        "answer": "involve",
                        "hit": false
                    },
                    {
                        "score": 0.2889996745886169,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2886319627452649,
                        "answer": "placement",
                        "hit": false
                    },
                    {
                        "score": 0.28632669465346927,
                        "answer": "museum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "accept"
                ],
                "rank": 9780,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.676767498254776
            },
            {
                "question verbose": "What is to achieve ",
                "b": "achieve",
                "expected answer": [
                    "achieveable"
                ],
                "predictions": [
                    {
                        "score": 0.38030735573614977,
                        "answer": "torture",
                        "hit": false
                    },
                    {
                        "score": 0.37996871274006766,
                        "answer": "conducted",
                        "hit": false
                    },
                    {
                        "score": 0.3726676069919983,
                        "answer": "identical",
                        "hit": false
                    },
                    {
                        "score": 0.3561569349046085,
                        "answer": "schwab",
                        "hit": false
                    },
                    {
                        "score": 0.352846870307266,
                        "answer": "gaining",
                        "hit": false
                    },
                    {
                        "score": 0.35184969393319687,
                        "answer": "efficacy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "achieve"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5589854381978512
            },
            {
                "question verbose": "What is to adapt ",
                "b": "adapt",
                "expected answer": [
                    "adaptable"
                ],
                "predictions": [
                    {
                        "score": 0.32392974557647286,
                        "answer": "gaining",
                        "hit": false
                    },
                    {
                        "score": 0.3176704186862252,
                        "answer": "substantiating",
                        "hit": false
                    },
                    {
                        "score": 0.31652412672453045,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.31525435809675795,
                        "answer": "simultaneous",
                        "hit": false
                    },
                    {
                        "score": 0.3129849760161799,
                        "answer": "watershed",
                        "hit": false
                    },
                    {
                        "score": 0.3104915142505885,
                        "answer": "algal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adapt"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5817369893193245
            },
            {
                "question verbose": "What is to adjust ",
                "b": "adjust",
                "expected answer": [
                    "adjustable"
                ],
                "predictions": [
                    {
                        "score": 0.4527786055971272,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.43109556920680925,
                        "answer": "roosevelt",
                        "hit": false
                    },
                    {
                        "score": 0.43095635936355664,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.4297183858173913,
                        "answer": "framing",
                        "hit": false
                    },
                    {
                        "score": 0.42576315213178556,
                        "answer": "reformist",
                        "hit": false
                    },
                    {
                        "score": 0.42260732869428747,
                        "answer": "retains",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adjust"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6323782056570053
            },
            {
                "question verbose": "What is to admire ",
                "b": "admire",
                "expected answer": [
                    "admirable"
                ],
                "predictions": [
                    {
                        "score": 0.36234930806147936,
                        "answer": "predisposition",
                        "hit": false
                    },
                    {
                        "score": 0.3562728045720209,
                        "answer": "wakeup",
                        "hit": false
                    },
                    {
                        "score": 0.34498599973795363,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3378793251193711,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.3363033396263066,
                        "answer": "fotographer",
                        "hit": false
                    },
                    {
                        "score": 0.3338316414322774,
                        "answer": "insanely",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "admire"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6488704085350037
            },
            {
                "question verbose": "What is to adore ",
                "b": "adore",
                "expected answer": [
                    "adorable"
                ],
                "predictions": [
                    {
                        "score": 0.29771289511933846,
                        "answer": "reformist",
                        "hit": false
                    },
                    {
                        "score": 0.27215341542449933,
                        "answer": "scent",
                        "hit": false
                    },
                    {
                        "score": 0.2656850653718969,
                        "answer": "andrew",
                        "hit": false
                    },
                    {
                        "score": 0.26317683500687505,
                        "answer": "overhand",
                        "hit": false
                    },
                    {
                        "score": 0.2621277365215389,
                        "answer": "frank",
                        "hit": false
                    },
                    {
                        "score": 0.2600754489436007,
                        "answer": "roosevelt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adore"
                ],
                "rank": 7949,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7066907584667206
            },
            {
                "question verbose": "What is to advise ",
                "b": "advise",
                "expected answer": [
                    "advisable"
                ],
                "predictions": [
                    {
                        "score": 0.48857280892873717,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.4695941579172783,
                        "answer": "livid",
                        "hit": false
                    },
                    {
                        "score": 0.46343531861015536,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.4620625117523009,
                        "answer": "roosevelt",
                        "hit": false
                    },
                    {
                        "score": 0.4545427587282064,
                        "answer": "ronald",
                        "hit": false
                    },
                    {
                        "score": 0.4457932504988942,
                        "answer": "framing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "advise"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6392468214035034
            },
            {
                "question verbose": "What is to afford ",
                "b": "afford",
                "expected answer": [
                    "affordable"
                ],
                "predictions": [
                    {
                        "score": 0.35321418053794,
                        "answer": "precinct",
                        "hit": false
                    },
                    {
                        "score": 0.3285232970999927,
                        "answer": "disdain",
                        "hit": false
                    },
                    {
                        "score": 0.3245004708101103,
                        "answer": "moviegoing",
                        "hit": false
                    },
                    {
                        "score": 0.31488229583127775,
                        "answer": "destabilize",
                        "hit": false
                    },
                    {
                        "score": 0.31004908500374834,
                        "answer": "franklin",
                        "hit": false
                    },
                    {
                        "score": 0.3051693874296507,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "afford"
                ],
                "rank": 12393,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6739372909069061
            },
            {
                "question verbose": "What is to avoid ",
                "b": "avoid",
                "expected answer": [
                    "avoidable"
                ],
                "predictions": [
                    {
                        "score": 0.30080074081327757,
                        "answer": "frank",
                        "hit": false
                    },
                    {
                        "score": 0.29582427343385553,
                        "answer": "massive",
                        "hit": false
                    },
                    {
                        "score": 0.295457723119751,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28705750194851276,
                        "answer": "penalized",
                        "hit": false
                    },
                    {
                        "score": 0.2835535429707919,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2822665419096906,
                        "answer": "storing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "avoid"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6605902463197708
            },
            {
                "question verbose": "What is to believe ",
                "b": "believe",
                "expected answer": [
                    "believeable",
                    "believable"
                ],
                "predictions": [
                    {
                        "score": 0.22243373618472959,
                        "answer": "flee",
                        "hit": false
                    },
                    {
                        "score": 0.21910914718801688,
                        "answer": "defendant",
                        "hit": false
                    },
                    {
                        "score": 0.2187913113585901,
                        "answer": "flotilla",
                        "hit": false
                    },
                    {
                        "score": 0.21350092950773625,
                        "answer": "whopper",
                        "hit": false
                    },
                    {
                        "score": 0.2071994774721422,
                        "answer": "waking",
                        "hit": false
                    },
                    {
                        "score": 0.2055905311838135,
                        "answer": "warhol",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believe"
                ],
                "rank": 557,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5253567472100258
            },
            {
                "question verbose": "What is to consider ",
                "b": "consider",
                "expected answer": [
                    "considerable"
                ],
                "predictions": [
                    {
                        "score": 0.28696078704636974,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.258609441347717,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.24410463076586977,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.24238649048508837,
                        "answer": "ka",
                        "hit": false
                    },
                    {
                        "score": 0.2371839076914115,
                        "answer": "travis",
                        "hit": false
                    },
                    {
                        "score": 0.2361771529717937,
                        "answer": "tights",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consider"
                ],
                "rank": 202,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6652977764606476
            },
            {
                "question verbose": "What is to contain ",
                "b": "contain",
                "expected answer": [
                    "containable"
                ],
                "predictions": [
                    {
                        "score": 0.40877738314536466,
                        "answer": "historic",
                        "hit": false
                    },
                    {
                        "score": 0.3964962259821292,
                        "answer": "travis",
                        "hit": false
                    },
                    {
                        "score": 0.3809831332310564,
                        "answer": "sal",
                        "hit": false
                    },
                    {
                        "score": 0.37589753026280465,
                        "answer": "komen",
                        "hit": false
                    },
                    {
                        "score": 0.37163096041997634,
                        "answer": "delivery",
                        "hit": false
                    },
                    {
                        "score": 0.37159369668838516,
                        "answer": "delaware",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "contain"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.61268300563097
            },
            {
                "question verbose": "What is to define ",
                "b": "define",
                "expected answer": [
                    "definable"
                ],
                "predictions": [
                    {
                        "score": 0.42352280823555866,
                        "answer": "lawsuit",
                        "hit": false
                    },
                    {
                        "score": 0.39833971571019117,
                        "answer": "strut",
                        "hit": false
                    },
                    {
                        "score": 0.3750578932964931,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.36580045092036945,
                        "answer": "multicultural",
                        "hit": false
                    },
                    {
                        "score": 0.3639148104474093,
                        "answer": "altar",
                        "hit": false
                    },
                    {
                        "score": 0.36242251902007006,
                        "answer": "considerable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "define"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5958474799990654
            },
            {
                "question verbose": "What is to deliver ",
                "b": "deliver",
                "expected answer": [
                    "deliverable"
                ],
                "predictions": [
                    {
                        "score": 0.40327067705876873,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.3530195499384764,
                        "answer": "capped",
                        "hit": false
                    },
                    {
                        "score": 0.3396355656424555,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3349666438528809,
                        "answer": "ronald",
                        "hit": false
                    },
                    {
                        "score": 0.32771441765346915,
                        "answer": "whopper",
                        "hit": false
                    },
                    {
                        "score": 0.32536986831589426,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deliver"
                ],
                "rank": 10285,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7186125814914703
            },
            {
                "question verbose": "What is to discover ",
                "b": "discover",
                "expected answer": [
                    "discoverable"
                ],
                "predictions": [
                    {
                        "score": 0.4411703176895512,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.4217407453550628,
                        "answer": "paste",
                        "hit": false
                    },
                    {
                        "score": 0.4174279869750639,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.39336236889794013,
                        "answer": "andrei",
                        "hit": false
                    },
                    {
                        "score": 0.38077191121146375,
                        "answer": "fickle",
                        "hit": false
                    },
                    {
                        "score": 0.38006717464509293,
                        "answer": "inhabitant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "discover"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6031466647982597
            },
            {
                "question verbose": "What is to dispose ",
                "b": "dispose",
                "expected answer": [
                    "disposable"
                ],
                "predictions": [
                    {
                        "score": 0.4841040275329107,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.43734117423992386,
                        "answer": "komen",
                        "hit": false
                    },
                    {
                        "score": 0.43169472576591433,
                        "answer": "msc",
                        "hit": false
                    },
                    {
                        "score": 0.4298053569889793,
                        "answer": "stolen",
                        "hit": false
                    },
                    {
                        "score": 0.42523138749621564,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.4233235746997764,
                        "answer": "moisture",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dispose"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6727862507104874
            },
            {
                "question verbose": "What is to download ",
                "b": "download",
                "expected answer": [
                    "downloadable"
                ],
                "predictions": [
                    {
                        "score": 0.3624500182625873,
                        "answer": "gettysburg",
                        "hit": false
                    },
                    {
                        "score": 0.36011816983452066,
                        "answer": "pincher",
                        "hit": false
                    },
                    {
                        "score": 0.35757568851161364,
                        "answer": "strut",
                        "hit": false
                    },
                    {
                        "score": 0.3570852954647533,
                        "answer": "retains",
                        "hit": false
                    },
                    {
                        "score": 0.3551749680768028,
                        "answer": "wh",
                        "hit": false
                    },
                    {
                        "score": 0.3539031460662256,
                        "answer": "inhabitant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "download"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6093892976641655
            },
            {
                "question verbose": "What is to edit ",
                "b": "edit",
                "expected answer": [
                    "editable"
                ],
                "predictions": [
                    {
                        "score": 0.30789971862429033,
                        "answer": "komen",
                        "hit": false
                    },
                    {
                        "score": 0.3031253766438778,
                        "answer": "gaining",
                        "hit": false
                    },
                    {
                        "score": 0.3023216050950456,
                        "answer": "allegory",
                        "hit": false
                    },
                    {
                        "score": 0.3015830967469934,
                        "answer": "expansiveness",
                        "hit": false
                    },
                    {
                        "score": 0.29958619305840684,
                        "answer": "promoting",
                        "hit": false
                    },
                    {
                        "score": 0.29888492114758364,
                        "answer": "rosen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "edit"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6101584061980247
            },
            {
                "question verbose": "What is to enjoy ",
                "b": "enjoy",
                "expected answer": [
                    "enjoyable"
                ],
                "predictions": [
                    {
                        "score": 0.27661911942630246,
                        "answer": "journalism",
                        "hit": false
                    },
                    {
                        "score": 0.2654333417672799,
                        "answer": "plagued",
                        "hit": false
                    },
                    {
                        "score": 0.2619757917497597,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2588001820338752,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.25753901535306983,
                        "answer": "nostalgia",
                        "hit": false
                    },
                    {
                        "score": 0.25515510642236683,
                        "answer": "fotographer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enjoy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.582465149462223
            },
            {
                "question verbose": "What is to execute ",
                "b": "execute",
                "expected answer": [
                    "executable"
                ],
                "predictions": [
                    {
                        "score": 0.47554395472183986,
                        "answer": "pamela",
                        "hit": false
                    },
                    {
                        "score": 0.45366405372330265,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.4393174930143786,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.4356342068140593,
                        "answer": "andrei",
                        "hit": false
                    },
                    {
                        "score": 0.4352050638969418,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.4302702353868038,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "execute"
                ],
                "rank": 3052,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8930242955684662
            },
            {
                "question verbose": "What is to expand ",
                "b": "expand",
                "expected answer": [
                    "expandable"
                ],
                "predictions": [
                    {
                        "score": 0.4296101954772912,
                        "answer": "rosen",
                        "hit": false
                    },
                    {
                        "score": 0.4256968908833687,
                        "answer": "defy",
                        "hit": false
                    },
                    {
                        "score": 0.4220960868665671,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.41869233887575735,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.411073641291999,
                        "answer": "nationalist",
                        "hit": false
                    },
                    {
                        "score": 0.4024187764217644,
                        "answer": "reformist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expand"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.64692422747612
            },
            {
                "question verbose": "What is to expect ",
                "b": "expect",
                "expected answer": [
                    "expectable"
                ],
                "predictions": [
                    {
                        "score": 0.3183252139031892,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.299990437465011,
                        "answer": "east",
                        "hit": false
                    },
                    {
                        "score": 0.2995402126415622,
                        "answer": "considerable",
                        "hit": false
                    },
                    {
                        "score": 0.2791535403878805,
                        "answer": "destabilize",
                        "hit": false
                    },
                    {
                        "score": 0.2739319909918494,
                        "answer": "whopper",
                        "hit": false
                    },
                    {
                        "score": 0.27192006090044263,
                        "answer": "cliff",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expect"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6000476330518723
            },
            {
                "question verbose": "What is to explain ",
                "b": "explain",
                "expected answer": [
                    "explainable"
                ],
                "predictions": [
                    {
                        "score": 0.2997128736706214,
                        "answer": "endows",
                        "hit": false
                    },
                    {
                        "score": 0.2929281588049537,
                        "answer": "understandable",
                        "hit": false
                    },
                    {
                        "score": 0.29020546664828034,
                        "answer": "paint",
                        "hit": false
                    },
                    {
                        "score": 0.28982246421813307,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.28847777888597687,
                        "answer": "pass",
                        "hit": false
                    },
                    {
                        "score": 0.28813958839060916,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "explain"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6563650965690613
            },
            {
                "question verbose": "What is to extend ",
                "b": "extend",
                "expected answer": [
                    "extendable"
                ],
                "predictions": [
                    {
                        "score": 0.40489188352491423,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.38018877274909907,
                        "answer": "rosen",
                        "hit": false
                    },
                    {
                        "score": 0.3701734810170882,
                        "answer": "frequency",
                        "hit": false
                    },
                    {
                        "score": 0.3694959141619409,
                        "answer": "leo",
                        "hit": false
                    },
                    {
                        "score": 0.3646474003282572,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.36351920290056905,
                        "answer": "inhabitant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "extend"
                ],
                "rank": 6850,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7695546746253967
            },
            {
                "question verbose": "What is to foresee ",
                "b": "foresee",
                "expected answer": [
                    "foreseeable"
                ],
                "predictions": [
                    {
                        "score": 0.4800540896835273,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.46373869159980763,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.4508587343107364,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.4404804288890357,
                        "answer": "inclined",
                        "hit": false
                    },
                    {
                        "score": 0.4403642357220312,
                        "answer": "framing",
                        "hit": false
                    },
                    {
                        "score": 0.44003608303321995,
                        "answer": "rosen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "foresee"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6846202611923218
            },
            {
                "question verbose": "What is to identify ",
                "b": "identify",
                "expected answer": [
                    "identifiable"
                ],
                "predictions": [
                    {
                        "score": 0.3499274093467776,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.3188715127110728,
                        "answer": "naturalist",
                        "hit": false
                    },
                    {
                        "score": 0.3047816361429681,
                        "answer": "fighting",
                        "hit": false
                    },
                    {
                        "score": 0.2967488386435782,
                        "answer": "questionnaire",
                        "hit": false
                    },
                    {
                        "score": 0.2949174722889266,
                        "answer": "lockout",
                        "hit": false
                    },
                    {
                        "score": 0.2937967333185022,
                        "answer": "euro",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "identify"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5975830182433128
            },
            {
                "question verbose": "What is to imagine ",
                "b": "imagine",
                "expected answer": [
                    "imaginable"
                ],
                "predictions": [
                    {
                        "score": 0.33504337844321264,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3323263466089357,
                        "answer": "greater",
                        "hit": false
                    },
                    {
                        "score": 0.3280137550770688,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3246930673941611,
                        "answer": "toddler",
                        "hit": false
                    },
                    {
                        "score": 0.31178851915196176,
                        "answer": "livid",
                        "hit": false
                    },
                    {
                        "score": 0.3048894785720562,
                        "answer": "scent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "imagine"
                ],
                "rank": 4977,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6474065333604813
            },
            {
                "question verbose": "What is to improve ",
                "b": "improve",
                "expected answer": [
                    "improvable"
                ],
                "predictions": [
                    {
                        "score": 0.2904811919667869,
                        "answer": "referendum",
                        "hit": false
                    },
                    {
                        "score": 0.29003274163127046,
                        "answer": "digress",
                        "hit": false
                    },
                    {
                        "score": 0.2877128413919634,
                        "answer": "itif",
                        "hit": false
                    },
                    {
                        "score": 0.2862873025168687,
                        "answer": "national",
                        "hit": false
                    },
                    {
                        "score": 0.2815979006219061,
                        "answer": "trademark",
                        "hit": false
                    },
                    {
                        "score": 0.28099025858234644,
                        "answer": "reformist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improve"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6177749186754227
            },
            {
                "question verbose": "What is to inflate ",
                "b": "inflate",
                "expected answer": [
                    "inflatable"
                ],
                "predictions": [
                    {
                        "score": 0.4845384047948334,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.4094776600201085,
                        "answer": "gilchrest",
                        "hit": false
                    },
                    {
                        "score": 0.40580650908042526,
                        "answer": "rosen",
                        "hit": false
                    },
                    {
                        "score": 0.4035904257357853,
                        "answer": "frank",
                        "hit": false
                    },
                    {
                        "score": 0.39172496697285647,
                        "answer": "precinct",
                        "hit": false
                    },
                    {
                        "score": 0.3916402310260937,
                        "answer": "considerable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inflate"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5755965039134026
            },
            {
                "question verbose": "What is to learn ",
                "b": "learn",
                "expected answer": [
                    "learnable"
                ],
                "predictions": [
                    {
                        "score": 0.22653115856727607,
                        "answer": "altar",
                        "hit": false
                    },
                    {
                        "score": 0.22077156463865372,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.22062635752571313,
                        "answer": "enhancing",
                        "hit": false
                    },
                    {
                        "score": 0.2192858569012892,
                        "answer": "intrigued",
                        "hit": false
                    },
                    {
                        "score": 0.2120133785183374,
                        "answer": "fotographer",
                        "hit": false
                    },
                    {
                        "score": 0.21187081158636323,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "learn"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5812326818704605
            },
            {
                "question verbose": "What is to maintain ",
                "b": "maintain",
                "expected answer": [
                    "maintainable"
                ],
                "predictions": [
                    {
                        "score": 0.33853491555889453,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.33819858650379236,
                        "answer": "allegory",
                        "hit": false
                    },
                    {
                        "score": 0.3364439367992413,
                        "answer": "substantial",
                        "hit": false
                    },
                    {
                        "score": 0.3354041000528776,
                        "answer": "limbaugh",
                        "hit": false
                    },
                    {
                        "score": 0.32915777494078063,
                        "answer": "enzyme",
                        "hit": false
                    },
                    {
                        "score": 0.3271351575108867,
                        "answer": "pincher",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "maintain"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6032395586371422
            },
            {
                "question verbose": "What is to manage ",
                "b": "manage",
                "expected answer": [
                    "manageable"
                ],
                "predictions": [
                    {
                        "score": 0.3492148790269716,
                        "answer": "wrinkle",
                        "hit": false
                    },
                    {
                        "score": 0.336352762323298,
                        "answer": "reformist",
                        "hit": false
                    },
                    {
                        "score": 0.3280942528297942,
                        "answer": "paste",
                        "hit": false
                    },
                    {
                        "score": 0.3207409825412816,
                        "answer": "raking",
                        "hit": false
                    },
                    {
                        "score": 0.31825908742807923,
                        "answer": "rosen",
                        "hit": false
                    },
                    {
                        "score": 0.3117604339138235,
                        "answer": "komen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manage"
                ],
                "rank": 8339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7791473865509033
            },
            {
                "question verbose": "What is to observe ",
                "b": "observe",
                "expected answer": [
                    "observable"
                ],
                "predictions": [
                    {
                        "score": 0.375485163973432,
                        "answer": "toddler",
                        "hit": false
                    },
                    {
                        "score": 0.3589412836026198,
                        "answer": "pamela",
                        "hit": false
                    },
                    {
                        "score": 0.3504837797401195,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3473432562836194,
                        "answer": "irishman",
                        "hit": false
                    },
                    {
                        "score": 0.34403000034179987,
                        "answer": "roosevelt",
                        "hit": false
                    },
                    {
                        "score": 0.337509999052032,
                        "answer": "ordering",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "observe"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6270125061273575
            },
            {
                "question verbose": "What is to perform ",
                "b": "perform",
                "expected answer": [
                    "performable"
                ],
                "predictions": [
                    {
                        "score": 0.3295409755010663,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.325136197147347,
                        "answer": "inheritance",
                        "hit": false
                    },
                    {
                        "score": 0.32332431813213064,
                        "answer": "livid",
                        "hit": false
                    },
                    {
                        "score": 0.3221183546912622,
                        "answer": "rosen",
                        "hit": false
                    },
                    {
                        "score": 0.3219523370289346,
                        "answer": "moderate",
                        "hit": false
                    },
                    {
                        "score": 0.31889233936502676,
                        "answer": "andrei",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "perform"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6414122134447098
            },
            {
                "question verbose": "What is to predict ",
                "b": "predict",
                "expected answer": [
                    "predictable"
                ],
                "predictions": [
                    {
                        "score": 0.30358497130609974,
                        "answer": "sporting",
                        "hit": false
                    },
                    {
                        "score": 0.297912736906099,
                        "answer": "allegory",
                        "hit": false
                    },
                    {
                        "score": 0.29703556360729433,
                        "answer": "wrinkle",
                        "hit": false
                    },
                    {
                        "score": 0.2966304201355592,
                        "answer": "pamela",
                        "hit": false
                    },
                    {
                        "score": 0.2958429444138623,
                        "answer": "plagued",
                        "hit": false
                    },
                    {
                        "score": 0.29088464740587344,
                        "answer": "unstacking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "predict"
                ],
                "rank": 10312,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7549121081829071
            },
            {
                "question verbose": "What is to prefer ",
                "b": "prefer",
                "expected answer": [
                    "preferable"
                ],
                "predictions": [
                    {
                        "score": 0.3593487175722293,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3190528053862222,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.29662670723247486,
                        "answer": "somalia",
                        "hit": false
                    },
                    {
                        "score": 0.289984418194176,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2868479966924372,
                        "answer": "wakeup",
                        "hit": false
                    },
                    {
                        "score": 0.2849849415401928,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prefer"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6949627995491028
            },
            {
                "question verbose": "What is to prevent ",
                "b": "prevent",
                "expected answer": [
                    "preventable"
                ],
                "predictions": [
                    {
                        "score": 0.3420388023716299,
                        "answer": "stable",
                        "hit": false
                    },
                    {
                        "score": 0.33680466471874765,
                        "answer": "offset",
                        "hit": false
                    },
                    {
                        "score": 0.33320117179037545,
                        "answer": "delivery",
                        "hit": false
                    },
                    {
                        "score": 0.3256800341247239,
                        "answer": "initiative",
                        "hit": false
                    },
                    {
                        "score": 0.32427425275139843,
                        "answer": "gulf",
                        "hit": false
                    },
                    {
                        "score": 0.3227803026328618,
                        "answer": "revenue",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prevent"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5659578889608383
            },
            {
                "question verbose": "What is to protect ",
                "b": "protect",
                "expected answer": [
                    "protectable"
                ],
                "predictions": [
                    {
                        "score": 0.31338467427877537,
                        "answer": "identical",
                        "hit": false
                    },
                    {
                        "score": 0.3026493947253601,
                        "answer": "donated",
                        "hit": false
                    },
                    {
                        "score": 0.3005972926571562,
                        "answer": "strut",
                        "hit": false
                    },
                    {
                        "score": 0.2925727095686219,
                        "answer": "fraternal",
                        "hit": false
                    },
                    {
                        "score": 0.29171125520186364,
                        "answer": "summum",
                        "hit": false
                    },
                    {
                        "score": 0.2908813193320022,
                        "answer": "efficacy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "protect"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5954209864139557
            },
            {
                "question verbose": "What is to publish ",
                "b": "publish",
                "expected answer": [
                    "publishable"
                ],
                "predictions": [
                    {
                        "score": 0.4522997154526315,
                        "answer": "inclined",
                        "hit": false
                    },
                    {
                        "score": 0.4506472841784477,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.42682237280278973,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.42550986158810933,
                        "answer": "sponging",
                        "hit": false
                    },
                    {
                        "score": 0.4246400554286947,
                        "answer": "expansiveness",
                        "hit": false
                    },
                    {
                        "score": 0.42241597799644776,
                        "answer": "catalyst",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publish"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.630352258682251
            },
            {
                "question verbose": "What is to recognize ",
                "b": "recognize",
                "expected answer": [
                    "recognizable",
                    "recognisable"
                ],
                "predictions": [
                    {
                        "score": 0.323646032737777,
                        "answer": "arguing",
                        "hit": false
                    },
                    {
                        "score": 0.3172047678848484,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.3145361805713397,
                        "answer": "framing",
                        "hit": false
                    },
                    {
                        "score": 0.31278072659733247,
                        "answer": "strut",
                        "hit": false
                    },
                    {
                        "score": 0.31091560304547744,
                        "answer": "irreparable",
                        "hit": false
                    },
                    {
                        "score": 0.3106206697039649,
                        "answer": "powerless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "recognize"
                ],
                "rank": 4087,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7664175927639008
            },
            {
                "question verbose": "What is to recommend ",
                "b": "recommend",
                "expected answer": [
                    "recommendable"
                ],
                "predictions": [
                    {
                        "score": 0.34028831662742504,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3181902616063057,
                        "answer": "gaining",
                        "hit": false
                    },
                    {
                        "score": 0.31667215993159037,
                        "answer": "inclined",
                        "hit": false
                    },
                    {
                        "score": 0.3124242112188635,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.29452672234056415,
                        "answer": "destabilizing",
                        "hit": false
                    },
                    {
                        "score": 0.2915428592988299,
                        "answer": "mooncakes",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "recommend"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.68461012840271
            },
            {
                "question verbose": "What is to rely ",
                "b": "rely",
                "expected answer": [
                    "reliable"
                ],
                "predictions": [
                    {
                        "score": 0.3474224236152294,
                        "answer": "premature",
                        "hit": false
                    },
                    {
                        "score": 0.33175521228007415,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.3198950998159147,
                        "answer": "livid",
                        "hit": false
                    },
                    {
                        "score": 0.3186646896329308,
                        "answer": "lieu",
                        "hit": false
                    },
                    {
                        "score": 0.3173211367385875,
                        "answer": "pass",
                        "hit": false
                    },
                    {
                        "score": 0.31635649067454136,
                        "answer": "danger",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rely"
                ],
                "rank": 5252,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6687605082988739
            },
            {
                "question verbose": "What is to renew ",
                "b": "renew",
                "expected answer": [
                    "renewable"
                ],
                "predictions": [
                    {
                        "score": 0.9378015802323648,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32879325436124174,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.32006598126276925,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2896079087454848,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.27818810892156753,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.27799298735830297,
                        "answer": "inhabitant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "renew"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to replace ",
                "b": "replace",
                "expected answer": [
                    "replaceable"
                ],
                "predictions": [
                    {
                        "score": 0.3374167351761911,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.33569159425037254,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.33547201558474915,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.33380335607074696,
                        "answer": "abolish",
                        "hit": false
                    },
                    {
                        "score": 0.33278162564564756,
                        "answer": "auxiliary",
                        "hit": false
                    },
                    {
                        "score": 0.3233228574164429,
                        "answer": "destabilize",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "replace"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6729118078947067
            },
            {
                "question verbose": "What is to represent ",
                "b": "represent",
                "expected answer": [
                    "representable"
                ],
                "predictions": [
                    {
                        "score": 0.4053977632036743,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.40338507656817735,
                        "answer": "reformist",
                        "hit": false
                    },
                    {
                        "score": 0.38877526950992447,
                        "answer": "delusion",
                        "hit": false
                    },
                    {
                        "score": 0.3830684069724045,
                        "answer": "decline",
                        "hit": false
                    },
                    {
                        "score": 0.3790406427025212,
                        "answer": "watershed",
                        "hit": false
                    },
                    {
                        "score": 0.37216039527037365,
                        "answer": "cwp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "represent"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5306524503976107
            },
            {
                "question verbose": "What is to survive ",
                "b": "survive",
                "expected answer": [
                    "survivable"
                ],
                "predictions": [
                    {
                        "score": 0.31266971050011066,
                        "answer": "nextone",
                        "hit": false
                    },
                    {
                        "score": 0.3021388229087327,
                        "answer": "naturalist",
                        "hit": false
                    },
                    {
                        "score": 0.28894152208156715,
                        "answer": "pamela",
                        "hit": false
                    },
                    {
                        "score": 0.2883250870870563,
                        "answer": "tempered",
                        "hit": false
                    },
                    {
                        "score": 0.2835998850415031,
                        "answer": "televised",
                        "hit": false
                    },
                    {
                        "score": 0.2813678228880864,
                        "answer": "raid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "survive"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5937359184026718
            },
            {
                "question verbose": "What is to sustain ",
                "b": "sustain",
                "expected answer": [
                    "sustainable"
                ],
                "predictions": [
                    {
                        "score": 0.45783724307692075,
                        "answer": "inhabitant",
                        "hit": false
                    },
                    {
                        "score": 0.4380602358552861,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.4359377883633052,
                        "answer": "msc",
                        "hit": false
                    },
                    {
                        "score": 0.4332703561215526,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.4244068420271912,
                        "answer": "milquetoast",
                        "hit": false
                    },
                    {
                        "score": 0.41977212656587953,
                        "answer": "delusion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sustain"
                ],
                "rank": 13655,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7674933671951294
            },
            {
                "question verbose": "What is to understand ",
                "b": "understand",
                "expected answer": [
                    "understandable"
                ],
                "predictions": [
                    {
                        "score": 0.2932614316175284,
                        "answer": "raised",
                        "hit": false
                    },
                    {
                        "score": 0.2494737643634108,
                        "answer": "pamela",
                        "hit": false
                    },
                    {
                        "score": 0.2494635770057691,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.24763826140679074,
                        "answer": "traitor",
                        "hit": false
                    },
                    {
                        "score": 0.2354014241278557,
                        "answer": "leaner",
                        "hit": false
                    },
                    {
                        "score": 0.23349219820844028,
                        "answer": "aplomb",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "understand"
                ],
                "rank": 2563,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6790194362401962
            },
            {
                "question verbose": "What is to vary ",
                "b": "vary",
                "expected answer": [
                    "variable"
                ],
                "predictions": [
                    {
                        "score": 0.38059018756017865,
                        "answer": "pamela",
                        "hit": false
                    },
                    {
                        "score": 0.3689931902143816,
                        "answer": "ruffled",
                        "hit": false
                    },
                    {
                        "score": 0.36552394044245573,
                        "answer": "substantival",
                        "hit": false
                    },
                    {
                        "score": 0.36100613704608675,
                        "answer": "clarence",
                        "hit": false
                    },
                    {
                        "score": 0.35937300521105897,
                        "answer": "separately",
                        "hit": false
                    },
                    {
                        "score": 0.3467946953695662,
                        "answer": "auxiliary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vary"
                ],
                "rank": 2894,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7105544954538345
            },
            {
                "question verbose": "What is to write ",
                "b": "write",
                "expected answer": [
                    "writeable",
                    "writable"
                ],
                "predictions": [
                    {
                        "score": 0.24347528012600977,
                        "answer": "inclined",
                        "hit": false
                    },
                    {
                        "score": 0.24195592246010778,
                        "answer": "wakeup",
                        "hit": false
                    },
                    {
                        "score": 0.23954883147831682,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.23869348543525035,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2386397490609329,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.23791446726576557,
                        "answer": "livecast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "write"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6295433044433594
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D07 [verb+able_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "eb526f88-228c-429c-8839-ef5565070562",
            "timestamp": "2020-10-22T15:57:08.392981"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to achieve ",
                "b": "achieve",
                "expected answer": [
                    "achiever"
                ],
                "predictions": [
                    {
                        "score": 0.4571256716215093,
                        "answer": "significantly",
                        "hit": false
                    },
                    {
                        "score": 0.45637726043220916,
                        "answer": "mann",
                        "hit": false
                    },
                    {
                        "score": 0.44945784474022954,
                        "answer": "milkshake",
                        "hit": false
                    },
                    {
                        "score": 0.4425245732748033,
                        "answer": "builder",
                        "hit": false
                    },
                    {
                        "score": 0.4390299947918654,
                        "answer": "nasrc",
                        "hit": false
                    },
                    {
                        "score": 0.4382222195236799,
                        "answer": "jb",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "achieve"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5589854381978512
            },
            {
                "question verbose": "What is to advertise ",
                "b": "advertise",
                "expected answer": [
                    "advertiser"
                ],
                "predictions": [
                    {
                        "score": 0.5567133881278646,
                        "answer": "espousal",
                        "hit": false
                    },
                    {
                        "score": 0.5355930580922335,
                        "answer": "suitably",
                        "hit": false
                    },
                    {
                        "score": 0.5325591764742178,
                        "answer": "menacing",
                        "hit": false
                    },
                    {
                        "score": 0.5298752675027024,
                        "answer": "notably",
                        "hit": false
                    },
                    {
                        "score": 0.5233770083301389,
                        "answer": "misuse",
                        "hit": false
                    },
                    {
                        "score": 0.5224136356897372,
                        "answer": "legally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "advertise"
                ],
                "rank": 9723,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8440293371677399
            },
            {
                "question verbose": "What is to announce ",
                "b": "announce",
                "expected answer": [
                    "announcer"
                ],
                "predictions": [
                    {
                        "score": 0.678862709155989,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30756795415919164,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.3063263815991418,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.29938420867421944,
                        "answer": "auxiliary",
                        "hit": false
                    },
                    {
                        "score": 0.2986744651802227,
                        "answer": "adopting",
                        "hit": false
                    },
                    {
                        "score": 0.29802811728357387,
                        "answer": "etch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "announce"
                ],
                "rank": 526,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7046765834093094
            },
            {
                "question verbose": "What is to bake ",
                "b": "bake",
                "expected answer": [
                    "baker"
                ],
                "predictions": [
                    {
                        "score": 0.6794399609335877,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30873555050144147,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.30492869207938983,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2984171277274281,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2962311204955063,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.29299757168083723,
                        "answer": "auxiliary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bake"
                ],
                "rank": 8360,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5716405436396599
            },
            {
                "question verbose": "What is to begin ",
                "b": "begin",
                "expected answer": [
                    "beginner"
                ],
                "predictions": [
                    {
                        "score": 0.3636961919945963,
                        "answer": "unexpected",
                        "hit": false
                    },
                    {
                        "score": 0.3565899775797501,
                        "answer": "replete",
                        "hit": false
                    },
                    {
                        "score": 0.35146872993572736,
                        "answer": "rutland",
                        "hit": false
                    },
                    {
                        "score": 0.34628020302727236,
                        "answer": "worring",
                        "hit": false
                    },
                    {
                        "score": 0.3461021353725766,
                        "answer": "traveled",
                        "hit": false
                    },
                    {
                        "score": 0.3413596543841364,
                        "answer": "fiercer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "begin"
                ],
                "rank": 1543,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6686501055955887
            },
            {
                "question verbose": "What is to believe ",
                "b": "believe",
                "expected answer": [
                    "believer"
                ],
                "predictions": [
                    {
                        "score": 0.3345014350190415,
                        "answer": "uncouth",
                        "hit": false
                    },
                    {
                        "score": 0.3038184358112838,
                        "answer": "lastly",
                        "hit": false
                    },
                    {
                        "score": 0.2999165355734391,
                        "answer": "projection",
                        "hit": false
                    },
                    {
                        "score": 0.2969789555873485,
                        "answer": "espousal",
                        "hit": false
                    },
                    {
                        "score": 0.2851982373883322,
                        "answer": "sam",
                        "hit": false
                    },
                    {
                        "score": 0.2827443314643597,
                        "answer": "mj",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believe"
                ],
                "rank": 5096,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6782099902629852
            },
            {
                "question verbose": "What is to borrow ",
                "b": "borrow",
                "expected answer": [
                    "borrower"
                ],
                "predictions": [
                    {
                        "score": 0.5536404901790197,
                        "answer": "filing",
                        "hit": false
                    },
                    {
                        "score": 0.5368882731550985,
                        "answer": "terror",
                        "hit": false
                    },
                    {
                        "score": 0.5307547680640339,
                        "answer": "garca",
                        "hit": false
                    },
                    {
                        "score": 0.526111662085352,
                        "answer": "aide",
                        "hit": false
                    },
                    {
                        "score": 0.525160982453533,
                        "answer": "pinette",
                        "hit": false
                    },
                    {
                        "score": 0.5224696357632229,
                        "answer": "treadmarks",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "borrow"
                ],
                "rank": 6851,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.802405446767807
            },
            {
                "question verbose": "What is to choreograph ",
                "b": "choreograph",
                "expected answer": [
                    "choreographer"
                ],
                "predictions": [
                    {
                        "score": 0.6696292613998343,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31490861323498626,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.31063507831986586,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3003810335552266,
                        "answer": "behalf",
                        "hit": false
                    },
                    {
                        "score": 0.2998785287838237,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.2984146968733906,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "choreograph"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to compose ",
                "b": "compose",
                "expected answer": [
                    "composer"
                ],
                "predictions": [
                    {
                        "score": 0.5299315022797311,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.5292947433306637,
                        "answer": "defender",
                        "hit": false
                    },
                    {
                        "score": 0.5254473035655152,
                        "answer": "parked",
                        "hit": false
                    },
                    {
                        "score": 0.5228707683135901,
                        "answer": "arxivorg",
                        "hit": false
                    },
                    {
                        "score": 0.5197509891223389,
                        "answer": "toughest",
                        "hit": false
                    },
                    {
                        "score": 0.51473927035452,
                        "answer": "toussaint",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "compose"
                ],
                "rank": 9373,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7950328290462494
            },
            {
                "question verbose": "What is to consume ",
                "b": "consume",
                "expected answer": [
                    "consumer"
                ],
                "predictions": [
                    {
                        "score": 0.6758242163702773,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30887921980884014,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.3012409071154145,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.2950837426586822,
                        "answer": "adopting",
                        "hit": false
                    },
                    {
                        "score": 0.294964085072291,
                        "answer": "observer",
                        "hit": false
                    },
                    {
                        "score": 0.29409871128437765,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consume"
                ],
                "rank": 9242,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5990035608410835
            },
            {
                "question verbose": "What is to contend ",
                "b": "contend",
                "expected answer": [
                    "contender"
                ],
                "predictions": [
                    {
                        "score": 0.5300641131163191,
                        "answer": "treadmarks",
                        "hit": false
                    },
                    {
                        "score": 0.525675559000368,
                        "answer": "pinette",
                        "hit": false
                    },
                    {
                        "score": 0.524152024415905,
                        "answer": "sal",
                        "hit": false
                    },
                    {
                        "score": 0.5082908091590418,
                        "answer": "filing",
                        "hit": false
                    },
                    {
                        "score": 0.5015890502307799,
                        "answer": "inheritance",
                        "hit": false
                    },
                    {
                        "score": 0.49878217378534045,
                        "answer": "manuel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "contend"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6690245866775513
            },
            {
                "question verbose": "What is to defend ",
                "b": "defend",
                "expected answer": [
                    "defender"
                ],
                "predictions": [
                    {
                        "score": 0.4138526777403689,
                        "answer": "jeep",
                        "hit": false
                    },
                    {
                        "score": 0.3810194500426127,
                        "answer": "wannstedt",
                        "hit": false
                    },
                    {
                        "score": 0.3668968819629506,
                        "answer": "inept",
                        "hit": false
                    },
                    {
                        "score": 0.3665861811083849,
                        "answer": "essence",
                        "hit": false
                    },
                    {
                        "score": 0.3652065766326174,
                        "answer": "mockable",
                        "hit": false
                    },
                    {
                        "score": 0.36410557761142986,
                        "answer": "rosenberg",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "defend"
                ],
                "rank": 73,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7232977896928787
            },
            {
                "question verbose": "What is to deliver ",
                "b": "deliver",
                "expected answer": [
                    "deliverer"
                ],
                "predictions": [
                    {
                        "score": 0.4414602888752405,
                        "answer": "toughest",
                        "hit": false
                    },
                    {
                        "score": 0.4177336123546789,
                        "answer": "arndt",
                        "hit": false
                    },
                    {
                        "score": 0.4137259039844567,
                        "answer": "samford",
                        "hit": false
                    },
                    {
                        "score": 0.4106737817315771,
                        "answer": "oldman",
                        "hit": false
                    },
                    {
                        "score": 0.4104386683429054,
                        "answer": "fri",
                        "hit": false
                    },
                    {
                        "score": 0.40900942317763567,
                        "answer": "vie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deliver"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6614734828472137
            },
            {
                "question verbose": "What is to destroy ",
                "b": "destroy",
                "expected answer": [
                    "destroyer"
                ],
                "predictions": [
                    {
                        "score": 0.41404367515168566,
                        "answer": "fiercer",
                        "hit": false
                    },
                    {
                        "score": 0.41046460726485323,
                        "answer": "surety",
                        "hit": false
                    },
                    {
                        "score": 0.40232068316929415,
                        "answer": "gambit",
                        "hit": false
                    },
                    {
                        "score": 0.39958214351602905,
                        "answer": "escape",
                        "hit": false
                    },
                    {
                        "score": 0.38632784195944536,
                        "answer": "envisioned",
                        "hit": false
                    },
                    {
                        "score": 0.38610124548581526,
                        "answer": "mothering",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "destroy"
                ],
                "rank": 11753,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.657329648733139
            },
            {
                "question verbose": "What is to determine ",
                "b": "determine",
                "expected answer": [
                    "determiner"
                ],
                "predictions": [
                    {
                        "score": 0.40418441704960345,
                        "answer": "raindrop",
                        "hit": false
                    },
                    {
                        "score": 0.40173773898662857,
                        "answer": "glycoside",
                        "hit": false
                    },
                    {
                        "score": 0.40130333223218145,
                        "answer": "disruption",
                        "hit": false
                    },
                    {
                        "score": 0.3841147793705036,
                        "answer": "internally",
                        "hit": false
                    },
                    {
                        "score": 0.3814463124792044,
                        "answer": "thinner",
                        "hit": false
                    },
                    {
                        "score": 0.37831484748123173,
                        "answer": "milkshake",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "determine"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5825532525777817
            },
            {
                "question verbose": "What is to develop ",
                "b": "develop",
                "expected answer": [
                    "developer"
                ],
                "predictions": [
                    {
                        "score": 0.37646337638918276,
                        "answer": "ebb",
                        "hit": false
                    },
                    {
                        "score": 0.3561505046931304,
                        "answer": "owed",
                        "hit": false
                    },
                    {
                        "score": 0.34037575207207427,
                        "answer": "narcotic",
                        "hit": false
                    },
                    {
                        "score": 0.33503206375076744,
                        "answer": "edwina",
                        "hit": false
                    },
                    {
                        "score": 0.3282134675170395,
                        "answer": "excert",
                        "hit": false
                    },
                    {
                        "score": 0.32521257987556557,
                        "answer": "franswaa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develop"
                ],
                "rank": 853,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.72481869161129
            },
            {
                "question verbose": "What is to discover ",
                "b": "discover",
                "expected answer": [
                    "discoverer"
                ],
                "predictions": [
                    {
                        "score": 0.4842823350827819,
                        "answer": "cobbled",
                        "hit": false
                    },
                    {
                        "score": 0.4697315634669608,
                        "answer": "milkshake",
                        "hit": false
                    },
                    {
                        "score": 0.4661972841045407,
                        "answer": "houston",
                        "hit": false
                    },
                    {
                        "score": 0.46600613986726935,
                        "answer": "inept",
                        "hit": false
                    },
                    {
                        "score": 0.4612826915274074,
                        "answer": "defender",
                        "hit": false
                    },
                    {
                        "score": 0.45918304217546063,
                        "answer": "bloodstream",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "discover"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6031466647982597
            },
            {
                "question verbose": "What is to eat ",
                "b": "eat",
                "expected answer": [
                    "eater"
                ],
                "predictions": [
                    {
                        "score": 0.40673298687455073,
                        "answer": "rosenberg",
                        "hit": false
                    },
                    {
                        "score": 0.37596812288022563,
                        "answer": "vitamin",
                        "hit": false
                    },
                    {
                        "score": 0.3740314086699393,
                        "answer": "stranger",
                        "hit": false
                    },
                    {
                        "score": 0.36879277648959996,
                        "answer": "girth",
                        "hit": false
                    },
                    {
                        "score": 0.3663235203867879,
                        "answer": "slj",
                        "hit": false
                    },
                    {
                        "score": 0.3565445371197531,
                        "answer": "forceful",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "eat"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6269963830709457
            },
            {
                "question verbose": "What is to entertain ",
                "b": "entertain",
                "expected answer": [
                    "entertainer"
                ],
                "predictions": [
                    {
                        "score": 0.5356586798603861,
                        "answer": "rosenberg",
                        "hit": false
                    },
                    {
                        "score": 0.5265514585219192,
                        "answer": "wannstedt",
                        "hit": false
                    },
                    {
                        "score": 0.5206135493429911,
                        "answer": "builder",
                        "hit": false
                    },
                    {
                        "score": 0.5195183020824455,
                        "answer": "methane",
                        "hit": false
                    },
                    {
                        "score": 0.5129808897419604,
                        "answer": "auxiliary",
                        "hit": false
                    },
                    {
                        "score": 0.5114194013568273,
                        "answer": "aquinas",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "entertain"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6319607943296432
            },
            {
                "question verbose": "What is to examine ",
                "b": "examine",
                "expected answer": [
                    "examiner"
                ],
                "predictions": [
                    {
                        "score": 0.3889803411926897,
                        "answer": "artificial",
                        "hit": false
                    },
                    {
                        "score": 0.3866807738065816,
                        "answer": "succesfully",
                        "hit": false
                    },
                    {
                        "score": 0.3853232054770878,
                        "answer": "inept",
                        "hit": false
                    },
                    {
                        "score": 0.3845779355134837,
                        "answer": "defender",
                        "hit": false
                    },
                    {
                        "score": 0.38188164132884816,
                        "answer": "obey",
                        "hit": false
                    },
                    {
                        "score": 0.3789187125216177,
                        "answer": "heroin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "examine"
                ],
                "rank": 2957,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7539819478988647
            },
            {
                "question verbose": "What is to explore ",
                "b": "explore",
                "expected answer": [
                    "explorer"
                ],
                "predictions": [
                    {
                        "score": 0.342707593343277,
                        "answer": "subjectivity",
                        "hit": false
                    },
                    {
                        "score": 0.3336510538166373,
                        "answer": "reinventions",
                        "hit": false
                    },
                    {
                        "score": 0.332691913949211,
                        "answer": "uncouth",
                        "hit": false
                    },
                    {
                        "score": 0.32637657837608297,
                        "answer": "unwavering",
                        "hit": false
                    },
                    {
                        "score": 0.32571509507378604,
                        "answer": "stretched",
                        "hit": false
                    },
                    {
                        "score": 0.3247631946047707,
                        "answer": "legally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "explore"
                ],
                "rank": 6604,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6876609772443771
            },
            {
                "question verbose": "What is to follow ",
                "b": "follow",
                "expected answer": [
                    "follower"
                ],
                "predictions": [
                    {
                        "score": 0.32560754121047736,
                        "answer": "izabella",
                        "hit": false
                    },
                    {
                        "score": 0.29471627745405715,
                        "answer": "builder",
                        "hit": false
                    },
                    {
                        "score": 0.2931139762338805,
                        "answer": "disseminating",
                        "hit": false
                    },
                    {
                        "score": 0.29266436182329597,
                        "answer": "debuted",
                        "hit": false
                    },
                    {
                        "score": 0.2905797088271312,
                        "answer": "latent",
                        "hit": false
                    },
                    {
                        "score": 0.29026810341885767,
                        "answer": "procession",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "follow"
                ],
                "rank": 2355,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6543455868959427
            },
            {
                "question verbose": "What is to interpret ",
                "b": "interpret",
                "expected answer": [
                    "interpreter"
                ],
                "predictions": [
                    {
                        "score": 0.5841995271476732,
                        "answer": "rosenberg",
                        "hit": false
                    },
                    {
                        "score": 0.583385322690825,
                        "answer": "filing",
                        "hit": false
                    },
                    {
                        "score": 0.567569213447932,
                        "answer": "forceful",
                        "hit": false
                    },
                    {
                        "score": 0.5664836169486293,
                        "answer": "cne",
                        "hit": false
                    },
                    {
                        "score": 0.5586426981066862,
                        "answer": "tufekci",
                        "hit": false
                    },
                    {
                        "score": 0.5575139638776156,
                        "answer": "methane",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interpret"
                ],
                "rank": 7016,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8392962515354156
            },
            {
                "question verbose": "What is to intrude ",
                "b": "intrude",
                "expected answer": [
                    "intruder"
                ],
                "predictions": [
                    {
                        "score": 0.6650607532493713,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3183885189763787,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.3092785442383113,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30472027117513434,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.29798501193876015,
                        "answer": "osx",
                        "hit": false
                    },
                    {
                        "score": 0.29427296897621047,
                        "answer": "adopting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "intrude"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to learn ",
                "b": "learn",
                "expected answer": [
                    "learner"
                ],
                "predictions": [
                    {
                        "score": 0.3059780981933337,
                        "answer": "assumtions",
                        "hit": false
                    },
                    {
                        "score": 0.29514152486490536,
                        "answer": "abhor",
                        "hit": false
                    },
                    {
                        "score": 0.2942346760953624,
                        "answer": "boston",
                        "hit": false
                    },
                    {
                        "score": 0.28748023976606274,
                        "answer": "reinventions",
                        "hit": false
                    },
                    {
                        "score": 0.28601521230537597,
                        "answer": "cobbled",
                        "hit": false
                    },
                    {
                        "score": 0.2848620952986469,
                        "answer": "lastly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "learn"
                ],
                "rank": 3592,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7515198886394501
            },
            {
                "question verbose": "What is to listen ",
                "b": "listen",
                "expected answer": [
                    "listener"
                ],
                "predictions": [
                    {
                        "score": 0.4532344256618249,
                        "answer": "symphony",
                        "hit": false
                    },
                    {
                        "score": 0.44446438588637455,
                        "answer": "attending",
                        "hit": false
                    },
                    {
                        "score": 0.4005779452555113,
                        "answer": "mansfield",
                        "hit": false
                    },
                    {
                        "score": 0.3935630120563311,
                        "answer": "broadway",
                        "hit": false
                    },
                    {
                        "score": 0.38050977375899575,
                        "answer": "rock",
                        "hit": false
                    },
                    {
                        "score": 0.3778962382989359,
                        "answer": "drilled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "listen"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.645231768488884
            },
            {
                "question verbose": "What is to lose ",
                "b": "lose",
                "expected answer": [
                    "loser"
                ],
                "predictions": [
                    {
                        "score": 0.36082629244306097,
                        "answer": "credibility",
                        "hit": false
                    },
                    {
                        "score": 0.358475774185441,
                        "answer": "hypocrite",
                        "hit": false
                    },
                    {
                        "score": 0.34611342779631754,
                        "answer": "bidderi",
                        "hit": false
                    },
                    {
                        "score": 0.34564432135588685,
                        "answer": "smashing",
                        "hit": false
                    },
                    {
                        "score": 0.3439937357085216,
                        "answer": "auxiliary",
                        "hit": false
                    },
                    {
                        "score": 0.3426144684588327,
                        "answer": "astute",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lose"
                ],
                "rank": 8906,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7109441757202148
            },
            {
                "question verbose": "What is to manage ",
                "b": "manage",
                "expected answer": [
                    "manager"
                ],
                "predictions": [
                    {
                        "score": 0.41348482331629083,
                        "answer": "impervious",
                        "hit": false
                    },
                    {
                        "score": 0.41163450593153916,
                        "answer": "legally",
                        "hit": false
                    },
                    {
                        "score": 0.4087292229517232,
                        "answer": "defender",
                        "hit": false
                    },
                    {
                        "score": 0.4079696109831486,
                        "answer": "pale",
                        "hit": false
                    },
                    {
                        "score": 0.40564311239907613,
                        "answer": "bedford",
                        "hit": false
                    },
                    {
                        "score": 0.40372027731838145,
                        "answer": "concession",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manage"
                ],
                "rank": 14178,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6019986271858215
            },
            {
                "question verbose": "What is to molest ",
                "b": "molest",
                "expected answer": [
                    "molester"
                ],
                "predictions": [
                    {
                        "score": 0.6663988665860119,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31116987125308304,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.30739477599657994,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.30448752783020144,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.299804767061492,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.29400638741301915,
                        "answer": "auxiliary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "molest"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to mourn ",
                "b": "mourn",
                "expected answer": [
                    "mourner"
                ],
                "predictions": [
                    {
                        "score": 0.42643672640587055,
                        "answer": "wooden",
                        "hit": false
                    },
                    {
                        "score": 0.41604035044122484,
                        "answer": "invovles",
                        "hit": false
                    },
                    {
                        "score": 0.41532099523766236,
                        "answer": "darker",
                        "hit": false
                    },
                    {
                        "score": 0.40559683624099874,
                        "answer": "lotsa",
                        "hit": false
                    },
                    {
                        "score": 0.40530456646302365,
                        "answer": "sox",
                        "hit": false
                    },
                    {
                        "score": 0.4038534578152314,
                        "answer": "moviegoing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mourn"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6433322131633759
            },
            {
                "question verbose": "What is to observe ",
                "b": "observe",
                "expected answer": [
                    "observer"
                ],
                "predictions": [
                    {
                        "score": 0.5208327316326712,
                        "answer": "hast",
                        "hit": false
                    },
                    {
                        "score": 0.4784955084769254,
                        "answer": "yea",
                        "hit": false
                    },
                    {
                        "score": 0.47755627724180616,
                        "answer": "idk",
                        "hit": false
                    },
                    {
                        "score": 0.4663489731539792,
                        "answer": "lecturing",
                        "hit": false
                    },
                    {
                        "score": 0.4656825154144963,
                        "answer": "shew",
                        "hit": false
                    },
                    {
                        "score": 0.46497565742120317,
                        "answer": "forgave",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "observe"
                ],
                "rank": 1206,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8052834868431091
            },
            {
                "question verbose": "What is to offend ",
                "b": "offend",
                "expected answer": [
                    "offender"
                ],
                "predictions": [
                    {
                        "score": 0.6815917460545692,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.329624832902123,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.3201357379540056,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.3163863419675009,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.3063406724999141,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.304509618268204,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "offend"
                ],
                "rank": 7466,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6127672642469406
            },
            {
                "question verbose": "What is to organise ",
                "b": "organise",
                "expected answer": [
                    "organiser"
                ],
                "predictions": [
                    {
                        "score": 0.6681830603043296,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3219539040929272,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.3107806518757226,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.30251018609701286,
                        "answer": "auxiliary",
                        "hit": false
                    },
                    {
                        "score": 0.2951828649062931,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.2938744715741627,
                        "answer": "adopting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "organise"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to organize ",
                "b": "organize",
                "expected answer": [
                    "organizer"
                ],
                "predictions": [
                    {
                        "score": 0.5986158279069053,
                        "answer": "methane",
                        "hit": false
                    },
                    {
                        "score": 0.5900390733340966,
                        "answer": "filing",
                        "hit": false
                    },
                    {
                        "score": 0.5715009180645072,
                        "answer": "peep",
                        "hit": false
                    },
                    {
                        "score": 0.5703013041366657,
                        "answer": "oshaughnessy",
                        "hit": false
                    },
                    {
                        "score": 0.5671003404617002,
                        "answer": "tufekci",
                        "hit": false
                    },
                    {
                        "score": 0.5662157047074768,
                        "answer": "sal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "organize"
                ],
                "rank": 5170,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7401264905929565
            },
            {
                "question verbose": "What is to perform ",
                "b": "perform",
                "expected answer": [
                    "performer"
                ],
                "predictions": [
                    {
                        "score": 0.4114019530746453,
                        "answer": "tat",
                        "hit": false
                    },
                    {
                        "score": 0.3983801708213396,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.3946665075179265,
                        "answer": "accountant",
                        "hit": false
                    },
                    {
                        "score": 0.3831768712721149,
                        "answer": "cne",
                        "hit": false
                    },
                    {
                        "score": 0.3819565538528104,
                        "answer": "aide",
                        "hit": false
                    },
                    {
                        "score": 0.3781763470914178,
                        "answer": "walter",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "perform"
                ],
                "rank": 7864,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7140740901231766
            },
            {
                "question verbose": "What is to preach ",
                "b": "preach",
                "expected answer": [
                    "preacher"
                ],
                "predictions": [
                    {
                        "score": 0.5220783397711403,
                        "answer": "bidderi",
                        "hit": false
                    },
                    {
                        "score": 0.5154643416333534,
                        "answer": "espousal",
                        "hit": false
                    },
                    {
                        "score": 0.5105975943531368,
                        "answer": "astute",
                        "hit": false
                    },
                    {
                        "score": 0.509316838303095,
                        "answer": "weiwei",
                        "hit": false
                    },
                    {
                        "score": 0.504150136146759,
                        "answer": "sexuality",
                        "hit": false
                    },
                    {
                        "score": 0.495796085330491,
                        "answer": "adopting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "preach"
                ],
                "rank": 2864,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8644487857818604
            },
            {
                "question verbose": "What is to promote ",
                "b": "promote",
                "expected answer": [
                    "promoter"
                ],
                "predictions": [
                    {
                        "score": 0.42279081042881184,
                        "answer": "partly",
                        "hit": false
                    },
                    {
                        "score": 0.38359278410899417,
                        "answer": "doctrine",
                        "hit": false
                    },
                    {
                        "score": 0.374425477102355,
                        "answer": "cononline",
                        "hit": false
                    },
                    {
                        "score": 0.36610815786937756,
                        "answer": "attentive",
                        "hit": false
                    },
                    {
                        "score": 0.3614089835187277,
                        "answer": "unwashed",
                        "hit": false
                    },
                    {
                        "score": 0.36118310760681577,
                        "answer": "formulate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "promote"
                ],
                "rank": 10143,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6416102200746536
            },
            {
                "question verbose": "What is to provide ",
                "b": "provide",
                "expected answer": [
                    "provider"
                ],
                "predictions": [
                    {
                        "score": 0.33407258461937556,
                        "answer": "reinventions",
                        "hit": false
                    },
                    {
                        "score": 0.3307135029958253,
                        "answer": "equine",
                        "hit": false
                    },
                    {
                        "score": 0.32579804017321634,
                        "answer": "subjectivity",
                        "hit": false
                    },
                    {
                        "score": 0.3236429985996111,
                        "answer": "manticore",
                        "hit": false
                    },
                    {
                        "score": 0.314561199641063,
                        "answer": "amieable",
                        "hit": false
                    },
                    {
                        "score": 0.3104262684434902,
                        "answer": "laity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "provide"
                ],
                "rank": 13352,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7084004133939743
            },
            {
                "question verbose": "What is to publish ",
                "b": "publish",
                "expected answer": [
                    "publisher"
                ],
                "predictions": [
                    {
                        "score": 0.5938527874709162,
                        "answer": "rosenberg",
                        "hit": false
                    },
                    {
                        "score": 0.5509917261205712,
                        "answer": "abbott",
                        "hit": false
                    },
                    {
                        "score": 0.5446762719718908,
                        "answer": "milkshake",
                        "hit": false
                    },
                    {
                        "score": 0.5425519260478814,
                        "answer": "availability",
                        "hit": false
                    },
                    {
                        "score": 0.5409668700089489,
                        "answer": "illiterate",
                        "hit": false
                    },
                    {
                        "score": 0.5408793438539901,
                        "answer": "tufekci",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publish"
                ],
                "rank": 8374,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8419391214847565
            },
            {
                "question verbose": "What is to receive ",
                "b": "receive",
                "expected answer": [
                    "receiver"
                ],
                "predictions": [
                    {
                        "score": 0.3483239607694291,
                        "answer": "unjust",
                        "hit": false
                    },
                    {
                        "score": 0.3453935425595437,
                        "answer": "walter",
                        "hit": false
                    },
                    {
                        "score": 0.3404498212460427,
                        "answer": "foster",
                        "hit": false
                    },
                    {
                        "score": 0.34039466658725764,
                        "answer": "commencement",
                        "hit": false
                    },
                    {
                        "score": 0.33739277332981665,
                        "answer": "symbosis",
                        "hit": false
                    },
                    {
                        "score": 0.335314203254681,
                        "answer": "methane",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receive"
                ],
                "rank": 11170,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6139839217066765
            },
            {
                "question verbose": "What is to recommend ",
                "b": "recommend",
                "expected answer": [
                    "recommender"
                ],
                "predictions": [
                    {
                        "score": 0.4224693278944678,
                        "answer": "legally",
                        "hit": false
                    },
                    {
                        "score": 0.41330635725405157,
                        "answer": "izabella",
                        "hit": false
                    },
                    {
                        "score": 0.40742920032519986,
                        "answer": "vance",
                        "hit": false
                    },
                    {
                        "score": 0.40555223502883,
                        "answer": "static",
                        "hit": false
                    },
                    {
                        "score": 0.40170427413325016,
                        "answer": "forexpros",
                        "hit": false
                    },
                    {
                        "score": 0.3976861780663449,
                        "answer": "guided",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "recommend"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.68461012840271
            },
            {
                "question verbose": "What is to send ",
                "b": "send",
                "expected answer": [
                    "sender"
                ],
                "predictions": [
                    {
                        "score": 0.3562554632685551,
                        "answer": "informing",
                        "hit": false
                    },
                    {
                        "score": 0.329669351257433,
                        "answer": "clint",
                        "hit": false
                    },
                    {
                        "score": 0.32771529401928784,
                        "answer": "tactical",
                        "hit": false
                    },
                    {
                        "score": 0.3253676650559112,
                        "answer": "penelope",
                        "hit": false
                    },
                    {
                        "score": 0.3253243618625882,
                        "answer": "sabans",
                        "hit": false
                    },
                    {
                        "score": 0.3244760433572386,
                        "answer": "tacoma",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "send"
                ],
                "rank": 3919,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6495119780302048
            },
            {
                "question verbose": "What is to skydive ",
                "b": "skydive",
                "expected answer": [
                    "skydiver"
                ],
                "predictions": [
                    {
                        "score": 0.6665872188069798,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3143447563873726,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.3047803666228643,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.3031321142751863,
                        "answer": "adopting",
                        "hit": false
                    },
                    {
                        "score": 0.30306310454290536,
                        "answer": "menacing",
                        "hit": false
                    },
                    {
                        "score": 0.30293674318856967,
                        "answer": "auxiliary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "skydive"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to slay ",
                "b": "slay",
                "expected answer": [
                    "slayer"
                ],
                "predictions": [
                    {
                        "score": 0.6686620617706874,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31874870360279445,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3161660249098757,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.30716038376205446,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.30604429162888513,
                        "answer": "adopting",
                        "hit": false
                    },
                    {
                        "score": 0.30452477509516845,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "slay"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to speak ",
                "b": "speak",
                "expected answer": [
                    "speaker"
                ],
                "predictions": [
                    {
                        "score": 0.37574051374955675,
                        "answer": "adopting",
                        "hit": false
                    },
                    {
                        "score": 0.3616450816677301,
                        "answer": "documented",
                        "hit": false
                    },
                    {
                        "score": 0.35546077189345454,
                        "answer": "edwina",
                        "hit": false
                    },
                    {
                        "score": 0.3553453883032141,
                        "answer": "communism",
                        "hit": false
                    },
                    {
                        "score": 0.344872692337473,
                        "answer": "curator",
                        "hit": false
                    },
                    {
                        "score": 0.3439900239273081,
                        "answer": "undeniable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "speak"
                ],
                "rank": 5957,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6512984037399292
            },
            {
                "question verbose": "What is to subscribe ",
                "b": "subscribe",
                "expected answer": [
                    "subscriber"
                ],
                "predictions": [
                    {
                        "score": 0.562858865637603,
                        "answer": "eshelman",
                        "hit": false
                    },
                    {
                        "score": 0.5432514230044999,
                        "answer": "westwood",
                        "hit": false
                    },
                    {
                        "score": 0.5388846422725582,
                        "answer": "intro",
                        "hit": false
                    },
                    {
                        "score": 0.537769026362321,
                        "answer": "revealing",
                        "hit": false
                    },
                    {
                        "score": 0.5372796859221735,
                        "answer": "newton",
                        "hit": false
                    },
                    {
                        "score": 0.537167094540763,
                        "answer": "contingent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "subscribe"
                ],
                "rank": 904,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8078015744686127
            },
            {
                "question verbose": "What is to suffer ",
                "b": "suffer",
                "expected answer": [
                    "sufferer"
                ],
                "predictions": [
                    {
                        "score": 0.4500030956259798,
                        "answer": "espousal",
                        "hit": false
                    },
                    {
                        "score": 0.4494750512493029,
                        "answer": "rosie",
                        "hit": false
                    },
                    {
                        "score": 0.4467752010227628,
                        "answer": "testimony",
                        "hit": false
                    },
                    {
                        "score": 0.4404854428356072,
                        "answer": "unjust",
                        "hit": false
                    },
                    {
                        "score": 0.4288377244486947,
                        "answer": "blocking",
                        "hit": false
                    },
                    {
                        "score": 0.42703219091120215,
                        "answer": "drum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "suffer"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6456108391284943
            },
            {
                "question verbose": "What is to teach ",
                "b": "teach",
                "expected answer": [
                    "teacher"
                ],
                "predictions": [
                    {
                        "score": 0.4185083986456653,
                        "answer": "befriends",
                        "hit": false
                    },
                    {
                        "score": 0.40005748861669355,
                        "answer": "reinventions",
                        "hit": false
                    },
                    {
                        "score": 0.39880783184405444,
                        "answer": "vice",
                        "hit": false
                    },
                    {
                        "score": 0.39024055269874813,
                        "answer": "consist",
                        "hit": false
                    },
                    {
                        "score": 0.38894428464567604,
                        "answer": "semester",
                        "hit": false
                    },
                    {
                        "score": 0.387980344110137,
                        "answer": "mcintosh",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "teach"
                ],
                "rank": 6013,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7590824067592621
            },
            {
                "question verbose": "What is to tell ",
                "b": "tell",
                "expected answer": [
                    "teller"
                ],
                "predictions": [
                    {
                        "score": 0.3527412187870792,
                        "answer": "reproduction",
                        "hit": false
                    },
                    {
                        "score": 0.3406084624149866,
                        "answer": "frustration",
                        "hit": false
                    },
                    {
                        "score": 0.3320059830206468,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.3226662507750243,
                        "answer": "paint",
                        "hit": false
                    },
                    {
                        "score": 0.32180492716513404,
                        "answer": "rebuttal",
                        "hit": false
                    },
                    {
                        "score": 0.3204689784176977,
                        "answer": "hypocrite",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tell"
                ],
                "rank": 3167,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6879055500030518
            },
            {
                "question verbose": "What is to write ",
                "b": "write",
                "expected answer": [
                    "writer"
                ],
                "predictions": [
                    {
                        "score": 0.3595083442499304,
                        "answer": "polite",
                        "hit": false
                    },
                    {
                        "score": 0.3415788255223924,
                        "answer": "toning",
                        "hit": false
                    },
                    {
                        "score": 0.32935187528835,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.3257819728052241,
                        "answer": "caving",
                        "hit": false
                    },
                    {
                        "score": 0.32470369215344796,
                        "answer": "outraged",
                        "hit": false
                    },
                    {
                        "score": 0.32435345306241353,
                        "answer": "corzine",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "write"
                ],
                "rank": 11250,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6054741814732552
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D08 [verb+er_irreg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "bdc6d1b5-1c7f-4401-a5b3-a21d713129f2",
            "timestamp": "2020-10-22T15:57:10.314648"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to accuse ",
                "b": "accuse",
                "expected answer": [
                    "accusation"
                ],
                "predictions": [
                    {
                        "score": 0.4506031930052678,
                        "answer": "imaginative",
                        "hit": false
                    },
                    {
                        "score": 0.43383792115494707,
                        "answer": "intersecting",
                        "hit": false
                    },
                    {
                        "score": 0.43224477142574796,
                        "answer": "donald",
                        "hit": false
                    },
                    {
                        "score": 0.4272623183280271,
                        "answer": "needle",
                        "hit": false
                    },
                    {
                        "score": 0.4266573882178519,
                        "answer": "beneifts",
                        "hit": false
                    },
                    {
                        "score": 0.4194782059261573,
                        "answer": "manticore",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "accuse"
                ],
                "rank": 8208,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7942385673522949
            },
            {
                "question verbose": "What is to admire ",
                "b": "admire",
                "expected answer": [
                    "admiration"
                ],
                "predictions": [
                    {
                        "score": 0.3875851788570959,
                        "answer": "imaginative",
                        "hit": false
                    },
                    {
                        "score": 0.38334433658218153,
                        "answer": "nwords",
                        "hit": false
                    },
                    {
                        "score": 0.3791713862863923,
                        "answer": "alex",
                        "hit": false
                    },
                    {
                        "score": 0.3739393957953857,
                        "answer": "actress",
                        "hit": false
                    },
                    {
                        "score": 0.37200125726370975,
                        "answer": "peripheral",
                        "hit": false
                    },
                    {
                        "score": 0.3716927481648096,
                        "answer": "surgeon",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "admire"
                ],
                "rank": 6201,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7570404410362244
            },
            {
                "question verbose": "What is to allege ",
                "b": "allege",
                "expected answer": [
                    "allegation"
                ],
                "predictions": [
                    {
                        "score": 0.5691494321363688,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27173187700662327,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.26497178579582953,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2624573631410147,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25906143797593423,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2563335982286025,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allege"
                ],
                "rank": 10224,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6321069747209549
            },
            {
                "question verbose": "What is to aspire ",
                "b": "aspire",
                "expected answer": [
                    "aspiration"
                ],
                "predictions": [
                    {
                        "score": 0.4883244963975092,
                        "answer": "peripheral",
                        "hit": false
                    },
                    {
                        "score": 0.4803518404977283,
                        "answer": "evolutionist",
                        "hit": false
                    },
                    {
                        "score": 0.4721446068759299,
                        "answer": "ironic",
                        "hit": false
                    },
                    {
                        "score": 0.47026008177651835,
                        "answer": "outed",
                        "hit": false
                    },
                    {
                        "score": 0.46927811806766195,
                        "answer": "illegitmate",
                        "hit": false
                    },
                    {
                        "score": 0.45832932833415274,
                        "answer": "harbaugh",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aspire"
                ],
                "rank": 12908,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7991925776004791
            },
            {
                "question verbose": "What is to authorize ",
                "b": "authorize",
                "expected answer": [
                    "authorization",
                    "authorisation"
                ],
                "predictions": [
                    {
                        "score": 0.47612512038059773,
                        "answer": "manuel",
                        "hit": false
                    },
                    {
                        "score": 0.46512896786119295,
                        "answer": "disproportion",
                        "hit": false
                    },
                    {
                        "score": 0.4595062919723453,
                        "answer": "whopper",
                        "hit": false
                    },
                    {
                        "score": 0.4584806688855256,
                        "answer": "reed",
                        "hit": false
                    },
                    {
                        "score": 0.45429076970023097,
                        "answer": "sworn",
                        "hit": false
                    },
                    {
                        "score": 0.45272294044504624,
                        "answer": "enron",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "authorize"
                ],
                "rank": 3414,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8877021968364716
            },
            {
                "question verbose": "What is to characterize ",
                "b": "characterize",
                "expected answer": [
                    "characterization",
                    "characterisation"
                ],
                "predictions": [
                    {
                        "score": 0.5706422840862592,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27015205520540064,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.26390245796314493,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.26166533557448873,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.25536544145452594,
                        "answer": "befriends",
                        "hit": false
                    },
                    {
                        "score": 0.252654849224779,
                        "answer": "enron",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "characterize"
                ],
                "rank": 8746,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6278046071529388
            },
            {
                "question verbose": "What is to civilize ",
                "b": "civilize",
                "expected answer": [
                    "civilization",
                    "civilisation"
                ],
                "predictions": [
                    {
                        "score": 0.5704549186634913,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2579263954107423,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2495779905922505,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.24936511450367743,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2475010360293858,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.24737467581310513,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "civilize"
                ],
                "rank": 8807,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5927631258964539
            },
            {
                "question verbose": "What is to colonize ",
                "b": "colonize",
                "expected answer": [
                    "colonization"
                ],
                "predictions": [
                    {
                        "score": 0.4881577133915566,
                        "answer": "housemate",
                        "hit": false
                    },
                    {
                        "score": 0.47969928186682376,
                        "answer": "mature",
                        "hit": false
                    },
                    {
                        "score": 0.47589887314121687,
                        "answer": "upstate",
                        "hit": false
                    },
                    {
                        "score": 0.47486444636703184,
                        "answer": "arxivorg",
                        "hit": false
                    },
                    {
                        "score": 0.46976066904087865,
                        "answer": "marcus",
                        "hit": false
                    },
                    {
                        "score": 0.4683269949913059,
                        "answer": "simbel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "colonize"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6217444241046906
            },
            {
                "question verbose": "What is to compile ",
                "b": "compile",
                "expected answer": [
                    "compilation"
                ],
                "predictions": [
                    {
                        "score": 0.5602956893717256,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26782093491689224,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.2593796729100094,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25892693481779455,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.25723628197980164,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2546292580721232,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "compile"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to compute ",
                "b": "compute",
                "expected answer": [
                    "computation"
                ],
                "predictions": [
                    {
                        "score": 0.5603781352477344,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28768903994908523,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2695731484775312,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.2693574926711925,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.26845976501262103,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.26767683013151194,
                        "answer": "restrict",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "compute"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to condense ",
                "b": "condense",
                "expected answer": [
                    "condensation"
                ],
                "predictions": [
                    {
                        "score": 0.5616896965294292,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26560752946980243,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2651338796699446,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2647808071836747,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2592579730215229,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.25593373625203075,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "condense"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to configure ",
                "b": "configure",
                "expected answer": [
                    "configuration"
                ],
                "predictions": [
                    {
                        "score": 0.5712034435499658,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2592513874990818,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2575491886232875,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.257267558840279,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2544512794769738,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.2522031828227271,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "configure"
                ],
                "rank": 8499,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6016287505626678
            },
            {
                "question verbose": "What is to consult ",
                "b": "consult",
                "expected answer": [
                    "consulation"
                ],
                "predictions": [
                    {
                        "score": 0.44679864947565,
                        "answer": "conscious",
                        "hit": false
                    },
                    {
                        "score": 0.4389519929147151,
                        "answer": "rita",
                        "hit": false
                    },
                    {
                        "score": 0.4325938067031091,
                        "answer": "evacuating",
                        "hit": false
                    },
                    {
                        "score": 0.430049953186615,
                        "answer": "renting",
                        "hit": false
                    },
                    {
                        "score": 0.42625850419750744,
                        "answer": "costa",
                        "hit": false
                    },
                    {
                        "score": 0.4230553885180298,
                        "answer": "islamist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consult"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6128313764929771
            },
            {
                "question verbose": "What is to continue ",
                "b": "continue",
                "expected answer": [
                    "continuation"
                ],
                "predictions": [
                    {
                        "score": 0.2656763391761196,
                        "answer": "phased",
                        "hit": false
                    },
                    {
                        "score": 0.26198745354774555,
                        "answer": "quik",
                        "hit": false
                    },
                    {
                        "score": 0.26001796223408535,
                        "answer": "annually",
                        "hit": false
                    },
                    {
                        "score": 0.25969205736400064,
                        "answer": "converge",
                        "hit": false
                    },
                    {
                        "score": 0.25967243354782354,
                        "answer": "secure",
                        "hit": false
                    },
                    {
                        "score": 0.259245017226624,
                        "answer": "gallows",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continue"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5698804259300232
            },
            {
                "question verbose": "What is to customize ",
                "b": "customize",
                "expected answer": [
                    "customization"
                ],
                "predictions": [
                    {
                        "score": 0.5608285862533339,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2677512547484456,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.2663066474889276,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.25995671316711383,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2561436681643508,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.25275059473742545,
                        "answer": "befriends",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "customize"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to declare ",
                "b": "declare",
                "expected answer": [
                    "declaration"
                ],
                "predictions": [
                    {
                        "score": 0.46926210402763824,
                        "answer": "tradition",
                        "hit": false
                    },
                    {
                        "score": 0.4620174698000471,
                        "answer": "thy",
                        "hit": false
                    },
                    {
                        "score": 0.4522702703015042,
                        "answer": "saint",
                        "hit": false
                    },
                    {
                        "score": 0.4260168214500845,
                        "answer": "hop",
                        "hit": false
                    },
                    {
                        "score": 0.4256292423239589,
                        "answer": "testimony",
                        "hit": false
                    },
                    {
                        "score": 0.4143382557989131,
                        "answer": "uncouth",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "declare"
                ],
                "rank": 11320,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6841677129268646
            },
            {
                "question verbose": "What is to degrade ",
                "b": "degrade",
                "expected answer": [
                    "degradation"
                ],
                "predictions": [
                    {
                        "score": 0.566948319540677,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26723595497904834,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25986990690187584,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2540979119967896,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2530881421299732,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.24916420760917085,
                        "answer": "enron",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "degrade"
                ],
                "rank": 149,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6864149868488312
            },
            {
                "question verbose": "What is to deprive ",
                "b": "deprive",
                "expected answer": [
                    "deprivation"
                ],
                "predictions": [
                    {
                        "score": 0.4800548438106295,
                        "answer": "crosstown",
                        "hit": false
                    },
                    {
                        "score": 0.4684256761607404,
                        "answer": "upstate",
                        "hit": false
                    },
                    {
                        "score": 0.4643782908518821,
                        "answer": "secrecy",
                        "hit": false
                    },
                    {
                        "score": 0.4617475883355848,
                        "answer": "equine",
                        "hit": false
                    },
                    {
                        "score": 0.4596438292908487,
                        "answer": "butter",
                        "hit": false
                    },
                    {
                        "score": 0.4582338576974177,
                        "answer": "oklahoma",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deprive"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6479296386241913
            },
            {
                "question verbose": "What is to derive ",
                "b": "derive",
                "expected answer": [
                    "derivation"
                ],
                "predictions": [
                    {
                        "score": 0.5595573002051558,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2682228876082932,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2628696730123244,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.25974784816932067,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.2562107612821657,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.25518067522053456,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "derive"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to determine ",
                "b": "determine",
                "expected answer": [
                    "determination"
                ],
                "predictions": [
                    {
                        "score": 0.35140107458945324,
                        "answer": "location",
                        "hit": false
                    },
                    {
                        "score": 0.3469552300182782,
                        "answer": "supplier",
                        "hit": false
                    },
                    {
                        "score": 0.3226459102354227,
                        "answer": "equine",
                        "hit": false
                    },
                    {
                        "score": 0.3116862738510683,
                        "answer": "sourced",
                        "hit": false
                    },
                    {
                        "score": 0.30979667097313596,
                        "answer": "amuse",
                        "hit": false
                    },
                    {
                        "score": 0.3082032130430451,
                        "answer": "manic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "determine"
                ],
                "rank": 13923,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6219481974840164
            },
            {
                "question verbose": "What is to examine ",
                "b": "examine",
                "expected answer": [
                    "examination"
                ],
                "predictions": [
                    {
                        "score": 0.4003497044615906,
                        "answer": "grief",
                        "hit": false
                    },
                    {
                        "score": 0.3795415513104803,
                        "answer": "guthrie",
                        "hit": false
                    },
                    {
                        "score": 0.37471949606576216,
                        "answer": "setter",
                        "hit": false
                    },
                    {
                        "score": 0.3702939619739879,
                        "answer": "manticore",
                        "hit": false
                    },
                    {
                        "score": 0.36994903414951696,
                        "answer": "resque",
                        "hit": false
                    },
                    {
                        "score": 0.3687836028751289,
                        "answer": "considers",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "examine"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6564575880765915
            },
            {
                "question verbose": "What is to expire ",
                "b": "expire",
                "expected answer": [
                    "expiration"
                ],
                "predictions": [
                    {
                        "score": 0.5594723012148373,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26740157954869703,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2634158684840232,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.26037483910587317,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.25627894471283463,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2555058308698626,
                        "answer": "befriends",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expire"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to explore ",
                "b": "explore",
                "expected answer": [
                    "exploration"
                ],
                "predictions": [
                    {
                        "score": 0.3637977925458295,
                        "answer": "grief",
                        "hit": false
                    },
                    {
                        "score": 0.34874468185221275,
                        "answer": "gifting",
                        "hit": false
                    },
                    {
                        "score": 0.34496917508828867,
                        "answer": "exploit",
                        "hit": false
                    },
                    {
                        "score": 0.3414306413100056,
                        "answer": "conscious",
                        "hit": false
                    },
                    {
                        "score": 0.34040781456101876,
                        "answer": "nirvanaah",
                        "hit": false
                    },
                    {
                        "score": 0.33698051738527984,
                        "answer": "pagan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "explore"
                ],
                "rank": 3184,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7135794162750244
            },
            {
                "question verbose": "What is to globalize ",
                "b": "globalize",
                "expected answer": [
                    "globalization",
                    "globalisation"
                ],
                "predictions": [
                    {
                        "score": 0.5694965829645935,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26622551752905493,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.26371610502797727,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2569230295108382,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2566886453345355,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.25451834190072525,
                        "answer": "befriends",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "globalize"
                ],
                "rank": 6501,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6439120471477509
            },
            {
                "question verbose": "What is to illumine ",
                "b": "illumine",
                "expected answer": [
                    "illumination"
                ],
                "predictions": [
                    {
                        "score": 0.5594859818559301,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27261049296520196,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.25997633106993795,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25950981346253077,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2590920049999742,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.25886798754638557,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "illumine"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to imagine ",
                "b": "imagine",
                "expected answer": [
                    "imagination"
                ],
                "predictions": [
                    {
                        "score": 0.3461620805260688,
                        "answer": "forceful",
                        "hit": false
                    },
                    {
                        "score": 0.3452683522891093,
                        "answer": "prick",
                        "hit": false
                    },
                    {
                        "score": 0.34318448635346993,
                        "answer": "jsut",
                        "hit": false
                    },
                    {
                        "score": 0.3409146720348294,
                        "answer": "polite",
                        "hit": false
                    },
                    {
                        "score": 0.33788661676002224,
                        "answer": "skirt",
                        "hit": false
                    },
                    {
                        "score": 0.3367867812615514,
                        "answer": "pinette",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "imagine"
                ],
                "rank": 13457,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6849476993083954
            },
            {
                "question verbose": "What is to improvize ",
                "b": "improvize",
                "expected answer": [
                    "improvization",
                    "improvisation"
                ],
                "predictions": [
                    {
                        "score": 0.5594790253023026,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26361925430619687,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2625037875565677,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2595172806123875,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.25866225862304365,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.2571203893795387,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improvize"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to inspire ",
                "b": "inspire",
                "expected answer": [
                    "inspiration"
                ],
                "predictions": [
                    {
                        "score": 0.40827807960643636,
                        "answer": "activision",
                        "hit": false
                    },
                    {
                        "score": 0.40759404804719446,
                        "answer": "owns",
                        "hit": false
                    },
                    {
                        "score": 0.40584730972896443,
                        "answer": "recognizable",
                        "hit": false
                    },
                    {
                        "score": 0.40230471712675553,
                        "answer": "funnel",
                        "hit": false
                    },
                    {
                        "score": 0.4000486143637757,
                        "answer": "peripheral",
                        "hit": false
                    },
                    {
                        "score": 0.39956326220716354,
                        "answer": "orchard",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inspire"
                ],
                "rank": 2866,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8005858957767487
            },
            {
                "question verbose": "What is to install ",
                "b": "install",
                "expected answer": [
                    "installation",
                    "instalation"
                ],
                "predictions": [
                    {
                        "score": 0.46648717027348346,
                        "answer": "activision",
                        "hit": false
                    },
                    {
                        "score": 0.46539918372009914,
                        "answer": "esoteric",
                        "hit": false
                    },
                    {
                        "score": 0.4607546199697991,
                        "answer": "infringement",
                        "hit": false
                    },
                    {
                        "score": 0.4582057091899002,
                        "answer": "gentle",
                        "hit": false
                    },
                    {
                        "score": 0.4522943804714267,
                        "answer": "peripheral",
                        "hit": false
                    },
                    {
                        "score": 0.4516734587262663,
                        "answer": "scoot",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "install"
                ],
                "rank": 10227,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.810308575630188
            },
            {
                "question verbose": "What is to maximize ",
                "b": "maximize",
                "expected answer": [
                    "maximization",
                    "maximisation"
                ],
                "predictions": [
                    {
                        "score": 0.5201679897848835,
                        "answer": "kaminska",
                        "hit": false
                    },
                    {
                        "score": 0.5048728018458374,
                        "answer": "hidalgos",
                        "hit": false
                    },
                    {
                        "score": 0.495415224752337,
                        "answer": "heredity",
                        "hit": false
                    },
                    {
                        "score": 0.49286602996416373,
                        "answer": "officer",
                        "hit": false
                    },
                    {
                        "score": 0.4911986597577339,
                        "answer": "pupil",
                        "hit": false
                    },
                    {
                        "score": 0.4911164250704766,
                        "answer": "petrohawk",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "maximize"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5872932970523834
            },
            {
                "question verbose": "What is to minimize ",
                "b": "minimize",
                "expected answer": [
                    "minimization",
                    "minimisation"
                ],
                "predictions": [
                    {
                        "score": 0.5124666421705355,
                        "answer": "explosion",
                        "hit": false
                    },
                    {
                        "score": 0.5067291764506657,
                        "answer": "kaminska",
                        "hit": false
                    },
                    {
                        "score": 0.4988347871517632,
                        "answer": "housemate",
                        "hit": false
                    },
                    {
                        "score": 0.4985431390869121,
                        "answer": "childrens",
                        "hit": false
                    },
                    {
                        "score": 0.49480479204963235,
                        "answer": "activision",
                        "hit": false
                    },
                    {
                        "score": 0.49230876751846814,
                        "answer": "contingent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "minimize"
                ],
                "rank": 10202,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8336057066917419
            },
            {
                "question verbose": "What is to modernize ",
                "b": "modernize",
                "expected answer": [
                    "modernization",
                    "modernisation"
                ],
                "predictions": [
                    {
                        "score": 0.4995397356897741,
                        "answer": "renting",
                        "hit": false
                    },
                    {
                        "score": 0.4877992865505451,
                        "answer": "pupil",
                        "hit": false
                    },
                    {
                        "score": 0.4796130139660346,
                        "answer": "roman",
                        "hit": false
                    },
                    {
                        "score": 0.4702754487999461,
                        "answer": "stella",
                        "hit": false
                    },
                    {
                        "score": 0.46920778933103147,
                        "answer": "physic",
                        "hit": false
                    },
                    {
                        "score": 0.46768257600977403,
                        "answer": "equine",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "modernize"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6694784760475159
            },
            {
                "question verbose": "What is to oblige ",
                "b": "oblige",
                "expected answer": [
                    "obligation"
                ],
                "predictions": [
                    {
                        "score": 0.5695914494281595,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28097321489561455,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2650073126909726,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.2606365074678918,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.25947047509804727,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25517029650455125,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "oblige"
                ],
                "rank": 9190,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6231856793165207
            },
            {
                "question verbose": "What is to observe ",
                "b": "observe",
                "expected answer": [
                    "observation"
                ],
                "predictions": [
                    {
                        "score": 0.42812485554224156,
                        "answer": "oscar",
                        "hit": false
                    },
                    {
                        "score": 0.41214402864761535,
                        "answer": "wig",
                        "hit": false
                    },
                    {
                        "score": 0.41134253805313414,
                        "answer": "gi",
                        "hit": false
                    },
                    {
                        "score": 0.4099884221103399,
                        "answer": "thankless",
                        "hit": false
                    },
                    {
                        "score": 0.4014801330315694,
                        "answer": "rds",
                        "hit": false
                    },
                    {
                        "score": 0.40004096559023095,
                        "answer": "radar",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "observe"
                ],
                "rank": 6457,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7486859261989594
            },
            {
                "question verbose": "What is to occupy ",
                "b": "occupy",
                "expected answer": [
                    "occupation"
                ],
                "predictions": [
                    {
                        "score": 0.5672131492955168,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2604096186363991,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2589460048885593,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.2566020268715955,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.25048593023611543,
                        "answer": "vamp",
                        "hit": false
                    },
                    {
                        "score": 0.24650959409969303,
                        "answer": "livecast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "occupy"
                ],
                "rank": 1065,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6964745968580246
            },
            {
                "question verbose": "What is to optimize ",
                "b": "optimize",
                "expected answer": [
                    "optimization",
                    "optimisation"
                ],
                "predictions": [
                    {
                        "score": 0.4856437446048632,
                        "answer": "stella",
                        "hit": false
                    },
                    {
                        "score": 0.47758422897527786,
                        "answer": "scandinavia",
                        "hit": false
                    },
                    {
                        "score": 0.4712211285931584,
                        "answer": "islamist",
                        "hit": false
                    },
                    {
                        "score": 0.4703132761795418,
                        "answer": "kaminska",
                        "hit": false
                    },
                    {
                        "score": 0.46688965897688584,
                        "answer": "guthrie",
                        "hit": false
                    },
                    {
                        "score": 0.4664811402447644,
                        "answer": "garca",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "optimize"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5793909579515457
            },
            {
                "question verbose": "What is to organize ",
                "b": "organize",
                "expected answer": [
                    "organization"
                ],
                "predictions": [
                    {
                        "score": 0.5193712995624408,
                        "answer": "beneifts",
                        "hit": false
                    },
                    {
                        "score": 0.505668910794685,
                        "answer": "housemate",
                        "hit": false
                    },
                    {
                        "score": 0.491576154825357,
                        "answer": "bombast",
                        "hit": false
                    },
                    {
                        "score": 0.4914181934931573,
                        "answer": "sip",
                        "hit": false
                    },
                    {
                        "score": 0.48573145226452014,
                        "answer": "squashed",
                        "hit": false
                    },
                    {
                        "score": 0.4853368851540991,
                        "answer": "gi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "organize"
                ],
                "rank": 13266,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7156393826007843
            },
            {
                "question verbose": "What is to perspire ",
                "b": "perspire",
                "expected answer": [
                    "perspiration"
                ],
                "predictions": [
                    {
                        "score": 0.5595968386861015,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26978555781523167,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2672113658630556,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.26717726294070554,
                        "answer": "vamp",
                        "hit": false
                    },
                    {
                        "score": 0.2644685016444832,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2640303430438271,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "perspire"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to prepare ",
                "b": "prepare",
                "expected answer": [
                    "preparation"
                ],
                "predictions": [
                    {
                        "score": 0.42683208883158236,
                        "answer": "hitter",
                        "hit": false
                    },
                    {
                        "score": 0.42189604419849736,
                        "answer": "darker",
                        "hit": false
                    },
                    {
                        "score": 0.41066607281786566,
                        "answer": "loft",
                        "hit": false
                    },
                    {
                        "score": 0.4002571385068567,
                        "answer": "gamecock",
                        "hit": false
                    },
                    {
                        "score": 0.398570543125005,
                        "answer": "riding",
                        "hit": false
                    },
                    {
                        "score": 0.3985506965131641,
                        "answer": "bulldog",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prepare"
                ],
                "rank": 314,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8324974477291107
            },
            {
                "question verbose": "What is to privatize ",
                "b": "privatize",
                "expected answer": [
                    "privatization",
                    "privatisation"
                ],
                "predictions": [
                    {
                        "score": 0.46935254094318973,
                        "answer": "scarey",
                        "hit": false
                    },
                    {
                        "score": 0.46628628989139465,
                        "answer": "recycled",
                        "hit": false
                    },
                    {
                        "score": 0.46343891456948494,
                        "answer": "yogurt",
                        "hit": false
                    },
                    {
                        "score": 0.4592110481071386,
                        "answer": "limelight",
                        "hit": false
                    },
                    {
                        "score": 0.45691028360999236,
                        "answer": "grief",
                        "hit": false
                    },
                    {
                        "score": 0.4559060518227682,
                        "answer": "paste",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "privatize"
                ],
                "rank": 13988,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7555902302265167
            },
            {
                "question verbose": "What is to randomize ",
                "b": "randomize",
                "expected answer": [
                    "randomization",
                    "randomisation"
                ],
                "predictions": [
                    {
                        "score": 0.5599469938825719,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2780460962123174,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.27435709929649205,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.27049832567206145,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.26656898198799345,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.2659310781314299,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "randomize"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to realize ",
                "b": "realize",
                "expected answer": [
                    "realization",
                    "realisation"
                ],
                "predictions": [
                    {
                        "score": 0.36048651488563466,
                        "answer": "faithful",
                        "hit": false
                    },
                    {
                        "score": 0.3391346361470239,
                        "answer": "giveth",
                        "hit": false
                    },
                    {
                        "score": 0.3218108167540755,
                        "answer": "replay",
                        "hit": false
                    },
                    {
                        "score": 0.3109993936533302,
                        "answer": "exhibition",
                        "hit": false
                    },
                    {
                        "score": 0.3105416872920181,
                        "answer": "checker",
                        "hit": false
                    },
                    {
                        "score": 0.3100218563644109,
                        "answer": "michele",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "realize"
                ],
                "rank": 11882,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6637175232172012
            },
            {
                "question verbose": "What is to reorganize ",
                "b": "reorganize",
                "expected answer": [
                    "reorganization",
                    "reorganisation"
                ],
                "predictions": [
                    {
                        "score": 0.5598185449558121,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27197597492267,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.269024893365782,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.254184483147829,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.25166174617297016,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.24854394983116143,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reorganize"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to restore ",
                "b": "restore",
                "expected answer": [
                    "restoration"
                ],
                "predictions": [
                    {
                        "score": 0.557507010332855,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26809865907376035,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.26376114098179326,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.256994372780477,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2564872698101352,
                        "answer": "vamp",
                        "hit": false
                    },
                    {
                        "score": 0.2538882434525487,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "restore"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to specialize ",
                "b": "specialize",
                "expected answer": [
                    "specialization",
                    "specialisation"
                ],
                "predictions": [
                    {
                        "score": 0.5592438903438086,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26173553651010945,
                        "answer": "livecast",
                        "hit": false
                    },
                    {
                        "score": 0.2597083761326832,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2569911841713675,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.2559877451592884,
                        "answer": "vamp",
                        "hit": false
                    },
                    {
                        "score": 0.2500495334061118,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "specialize"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to stabilize ",
                "b": "stabilize",
                "expected answer": [
                    "stabilization"
                ],
                "predictions": [
                    {
                        "score": 0.479748070547505,
                        "answer": "addiction",
                        "hit": false
                    },
                    {
                        "score": 0.4723320091726467,
                        "answer": "cougs",
                        "hit": false
                    },
                    {
                        "score": 0.465432995951844,
                        "answer": "defensive",
                        "hit": false
                    },
                    {
                        "score": 0.46471280301855966,
                        "answer": "upstate",
                        "hit": false
                    },
                    {
                        "score": 0.4645172079718754,
                        "answer": "shuttling",
                        "hit": false
                    },
                    {
                        "score": 0.46423298753262165,
                        "answer": "coordinator",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stabilize"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5974986106157303
            },
            {
                "question verbose": "What is to standardize ",
                "b": "standardize",
                "expected answer": [
                    "standardization",
                    "standardisation"
                ],
                "predictions": [
                    {
                        "score": 0.5584954858542952,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.265222096696958,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.265134402788402,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2612429676479059,
                        "answer": "vamp",
                        "hit": false
                    },
                    {
                        "score": 0.2602950033316509,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.255340071019507,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "standardize"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to starve ",
                "b": "starve",
                "expected answer": [
                    "starvation"
                ],
                "predictions": [
                    {
                        "score": 0.5595392190272628,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26746624318509815,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2671153867051339,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.26047482261426397,
                        "answer": "vamp",
                        "hit": false
                    },
                    {
                        "score": 0.25807459210292977,
                        "answer": "enron",
                        "hit": false
                    },
                    {
                        "score": 0.25374207067474175,
                        "answer": "clueless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "starve"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to utilize ",
                "b": "utilize",
                "expected answer": [
                    "utilization",
                    "utilisation"
                ],
                "predictions": [
                    {
                        "score": 0.49450634378392405,
                        "answer": "manticore",
                        "hit": false
                    },
                    {
                        "score": 0.47208505151384456,
                        "answer": "equine",
                        "hit": false
                    },
                    {
                        "score": 0.47205780390159213,
                        "answer": "abound",
                        "hit": false
                    },
                    {
                        "score": 0.4717076138419841,
                        "answer": "unjust",
                        "hit": false
                    },
                    {
                        "score": 0.4672187865146312,
                        "answer": "disabling",
                        "hit": false
                    },
                    {
                        "score": 0.46495251213459965,
                        "answer": "wand",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "utilize"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6654877811670303
            },
            {
                "question verbose": "What is to visualize ",
                "b": "visualize",
                "expected answer": [
                    "visualization"
                ],
                "predictions": [
                    {
                        "score": 0.5688493060199991,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26780804332770675,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2674383322537258,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.264136212294716,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.2632071299570351,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.25551804684124746,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "visualize"
                ],
                "rank": 6116,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6282167732715607
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D09 [verb+tion_irreg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "05abe0da-ecfa-4966-870a-bccb9dd9d4f6",
            "timestamp": "2020-10-22T15:57:12.039565"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to accomplish ",
                "b": "accomplish",
                "expected answer": [
                    "accomplishment"
                ],
                "predictions": [
                    {
                        "score": 0.3994218894144073,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.38936103411943296,
                        "answer": "persian",
                        "hit": false
                    },
                    {
                        "score": 0.38179495694011545,
                        "answer": "referral",
                        "hit": false
                    },
                    {
                        "score": 0.375782709086966,
                        "answer": "inducing",
                        "hit": false
                    },
                    {
                        "score": 0.3727506763614094,
                        "answer": "blogging",
                        "hit": false
                    },
                    {
                        "score": 0.37012769933080847,
                        "answer": "armenia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "accomplish"
                ],
                "rank": 2940,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.767911821603775
            },
            {
                "question verbose": "What is to achieve ",
                "b": "achieve",
                "expected answer": [
                    "achievement"
                ],
                "predictions": [
                    {
                        "score": 0.485840355003907,
                        "answer": "nursing",
                        "hit": false
                    },
                    {
                        "score": 0.4574537919189716,
                        "answer": "excellence",
                        "hit": false
                    },
                    {
                        "score": 0.45734938768989625,
                        "answer": "contract",
                        "hit": false
                    },
                    {
                        "score": 0.4479859158251427,
                        "answer": "incubator",
                        "hit": false
                    },
                    {
                        "score": 0.4431600523897811,
                        "answer": "socialize",
                        "hit": false
                    },
                    {
                        "score": 0.4401107187637493,
                        "answer": "dissemination",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "achieve"
                ],
                "rank": 8169,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6577844172716141
            },
            {
                "question verbose": "What is to acknowledge ",
                "b": "acknowledge",
                "expected answer": [
                    "acknowledgement"
                ],
                "predictions": [
                    {
                        "score": 0.43475210626630545,
                        "answer": "rampage",
                        "hit": false
                    },
                    {
                        "score": 0.4269772156251355,
                        "answer": "bamboozled",
                        "hit": false
                    },
                    {
                        "score": 0.420091556699764,
                        "answer": "madicatist",
                        "hit": false
                    },
                    {
                        "score": 0.41408226946004384,
                        "answer": "shakespeare",
                        "hit": false
                    },
                    {
                        "score": 0.4103809012468329,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.4095911723433235,
                        "answer": "escalator",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "acknowledge"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5828816443681717
            },
            {
                "question verbose": "What is to adjust ",
                "b": "adjust",
                "expected answer": [
                    "adjustment"
                ],
                "predictions": [
                    {
                        "score": 0.5253925971476942,
                        "answer": "escalator",
                        "hit": false
                    },
                    {
                        "score": 0.510301099205338,
                        "answer": "universal",
                        "hit": false
                    },
                    {
                        "score": 0.5102311348515407,
                        "answer": "anniversary",
                        "hit": false
                    },
                    {
                        "score": 0.5057923985389017,
                        "answer": "asia",
                        "hit": false
                    },
                    {
                        "score": 0.5007030463576333,
                        "answer": "rubbing",
                        "hit": false
                    },
                    {
                        "score": 0.5006946159293257,
                        "answer": "greedy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adjust"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6323782056570053
            },
            {
                "question verbose": "What is to advertise ",
                "b": "advertise",
                "expected answer": [
                    "advertisement",
                    "advertizement"
                ],
                "predictions": [
                    {
                        "score": 0.518165491107819,
                        "answer": "arthur",
                        "hit": false
                    },
                    {
                        "score": 0.48582861986816717,
                        "answer": "eco",
                        "hit": false
                    },
                    {
                        "score": 0.48364392183417426,
                        "answer": "etiology",
                        "hit": false
                    },
                    {
                        "score": 0.48052719514912523,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.48020988119094543,
                        "answer": "publicize",
                        "hit": false
                    },
                    {
                        "score": 0.48008435678042005,
                        "answer": "revealing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "advertise"
                ],
                "rank": 5763,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8474395871162415
            },
            {
                "question verbose": "What is to agree ",
                "b": "agree",
                "expected answer": [
                    "agreement"
                ],
                "predictions": [
                    {
                        "score": 0.33740326736497894,
                        "answer": "ungrateful",
                        "hit": false
                    },
                    {
                        "score": 0.3288624660816402,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.3226936180981921,
                        "answer": "prior",
                        "hit": false
                    },
                    {
                        "score": 0.31141993530920586,
                        "answer": "thereof",
                        "hit": false
                    },
                    {
                        "score": 0.3113030377971098,
                        "answer": "foremost",
                        "hit": false
                    },
                    {
                        "score": 0.30931459997367294,
                        "answer": "asserts",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "agree"
                ],
                "rank": 254,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6901862472295761
            },
            {
                "question verbose": "What is to align ",
                "b": "align",
                "expected answer": [
                    "alignment"
                ],
                "predictions": [
                    {
                        "score": 0.475192469298731,
                        "answer": "impacting",
                        "hit": false
                    },
                    {
                        "score": 0.45535776991426025,
                        "answer": "disappointment",
                        "hit": false
                    },
                    {
                        "score": 0.4523651981729071,
                        "answer": "saftey",
                        "hit": false
                    },
                    {
                        "score": 0.44827059386314067,
                        "answer": "champagne",
                        "hit": false
                    },
                    {
                        "score": 0.4434519389633598,
                        "answer": "trial",
                        "hit": false
                    },
                    {
                        "score": 0.4411772076699791,
                        "answer": "ox",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "align"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5218747053295374
            },
            {
                "question verbose": "What is to amend ",
                "b": "amend",
                "expected answer": [
                    "amendment"
                ],
                "predictions": [
                    {
                        "score": 0.516837198532954,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26560065542002625,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.26370608385788996,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.26004032591337944,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.25791359134037417,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.25706235661244264,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "amend"
                ],
                "rank": 10737,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5728883817791939
            },
            {
                "question verbose": "What is to amuse ",
                "b": "amuse",
                "expected answer": [
                    "amusement"
                ],
                "predictions": [
                    {
                        "score": 0.4906919464216169,
                        "answer": "escalator",
                        "hit": false
                    },
                    {
                        "score": 0.4852235507677457,
                        "answer": "irrespective",
                        "hit": false
                    },
                    {
                        "score": 0.4790296866010041,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.4782505652641898,
                        "answer": "socialize",
                        "hit": false
                    },
                    {
                        "score": 0.4779514033853741,
                        "answer": "heredity",
                        "hit": false
                    },
                    {
                        "score": 0.4732375018599842,
                        "answer": "preventive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "amuse"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.633429765701294
            },
            {
                "question verbose": "What is to announce ",
                "b": "announce",
                "expected answer": [
                    "announcement"
                ],
                "predictions": [
                    {
                        "score": 0.5167057499249891,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27665420984937594,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2691328848794993,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2496486768980507,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.24673285020005287,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.23692816730601582,
                        "answer": "founded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "announce"
                ],
                "rank": 14155,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5527533404529095
            },
            {
                "question verbose": "What is to appoint ",
                "b": "appoint",
                "expected answer": [
                    "appointment"
                ],
                "predictions": [
                    {
                        "score": 0.5145133933952656,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2691671425387591,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2593542412165639,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.2574227627063246,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.25700456601091226,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.24205435758883442,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appoint"
                ],
                "rank": 3699,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6770518571138382
            },
            {
                "question verbose": "What is to arrange ",
                "b": "arrange",
                "expected answer": [
                    "arrangement"
                ],
                "predictions": [
                    {
                        "score": 0.516959719005171,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2727630482137403,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2619480980896588,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2555099098026198,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.2515914283704787,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.23737013932499842,
                        "answer": "heritage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "arrange"
                ],
                "rank": 7314,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6762621402740479
            },
            {
                "question verbose": "What is to assess ",
                "b": "assess",
                "expected answer": [
                    "assessment"
                ],
                "predictions": [
                    {
                        "score": 0.5156044726812301,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27714436965303413,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2611838545427491,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2604504126921839,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2542641128528725,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24076338977227532,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "assess"
                ],
                "rank": 5145,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5898821130394936
            },
            {
                "question verbose": "What is to assign ",
                "b": "assign",
                "expected answer": [
                    "assignment"
                ],
                "predictions": [
                    {
                        "score": 0.5480350298214633,
                        "answer": "revealing",
                        "hit": false
                    },
                    {
                        "score": 0.5405523834352607,
                        "answer": "cincinnati",
                        "hit": false
                    },
                    {
                        "score": 0.5374271141534243,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.5310424496121302,
                        "answer": "consolidation",
                        "hit": false
                    },
                    {
                        "score": 0.5302045889386788,
                        "answer": "contingent",
                        "hit": false
                    },
                    {
                        "score": 0.5280912454251521,
                        "answer": "agricultural",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "assign"
                ],
                "rank": 1361,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8380709290504456
            },
            {
                "question verbose": "What is to commit ",
                "b": "commit",
                "expected answer": [
                    "commitment"
                ],
                "predictions": [
                    {
                        "score": 0.4543860766772449,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.44677627382523244,
                        "answer": "ninth",
                        "hit": false
                    },
                    {
                        "score": 0.4460608749588391,
                        "answer": "justified",
                        "hit": false
                    },
                    {
                        "score": 0.44532757768632536,
                        "answer": "attached",
                        "hit": false
                    },
                    {
                        "score": 0.4448386213635886,
                        "answer": "semantic",
                        "hit": false
                    },
                    {
                        "score": 0.4432786527172258,
                        "answer": "escalator",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "commit"
                ],
                "rank": 5710,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.778310239315033
            },
            {
                "question verbose": "What is to detach ",
                "b": "detach",
                "expected answer": [
                    "detachment"
                ],
                "predictions": [
                    {
                        "score": 0.5001270633243831,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2763211274174836,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.271461215114173,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2648532971008717,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.25726569103615066,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24348174580168372,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "detach"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to develop ",
                "b": "develop",
                "expected answer": [
                    "development"
                ],
                "predictions": [
                    {
                        "score": 0.3648854669470749,
                        "answer": "assessment",
                        "hit": false
                    },
                    {
                        "score": 0.3627194376549294,
                        "answer": "benefiting",
                        "hit": false
                    },
                    {
                        "score": 0.3505792017644874,
                        "answer": "socialize",
                        "hit": false
                    },
                    {
                        "score": 0.3503653628603805,
                        "answer": "hallelujah",
                        "hit": false
                    },
                    {
                        "score": 0.33492465251231984,
                        "answer": "ponytail",
                        "hit": false
                    },
                    {
                        "score": 0.333346793324386,
                        "answer": "incompetence",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develop"
                ],
                "rank": 6878,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6577016264200211
            },
            {
                "question verbose": "What is to disagree ",
                "b": "disagree",
                "expected answer": [
                    "disagreement"
                ],
                "predictions": [
                    {
                        "score": 0.36741002900152064,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.3545406296462528,
                        "answer": "pansy",
                        "hit": false
                    },
                    {
                        "score": 0.35416103865642296,
                        "answer": "reserved",
                        "hit": false
                    },
                    {
                        "score": 0.35353180677859125,
                        "answer": "wi",
                        "hit": false
                    },
                    {
                        "score": 0.3522957086832093,
                        "answer": "jeopardy",
                        "hit": false
                    },
                    {
                        "score": 0.34049967564076516,
                        "answer": "disseminating",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "disagree"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6583538800477982
            },
            {
                "question verbose": "What is to disappoint ",
                "b": "disappoint",
                "expected answer": [
                    "disappointment"
                ],
                "predictions": [
                    {
                        "score": 0.49944675929283805,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.49313937531882135,
                        "answer": "disenfranchised",
                        "hit": false
                    },
                    {
                        "score": 0.48164891697538026,
                        "answer": "entertained",
                        "hit": false
                    },
                    {
                        "score": 0.4813514688921273,
                        "answer": "carved",
                        "hit": false
                    },
                    {
                        "score": 0.4805588519606123,
                        "answer": "factbook",
                        "hit": false
                    },
                    {
                        "score": 0.47669203817326083,
                        "answer": "sincerity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "disappoint"
                ],
                "rank": 4137,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.797439455986023
            },
            {
                "question verbose": "What is to displace ",
                "b": "displace",
                "expected answer": [
                    "displacement"
                ],
                "predictions": [
                    {
                        "score": 0.5039854252813416,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2806102541582307,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2689513442583466,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.26580358818597943,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.24974995174575643,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24857674838106747,
                        "answer": "founded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "displace"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to embarrass ",
                "b": "embarrass",
                "expected answer": [
                    "embarrassment"
                ],
                "predictions": [
                    {
                        "score": 0.5814179801843588,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.5757082444417415,
                        "answer": "civic",
                        "hit": false
                    },
                    {
                        "score": 0.5624718008471943,
                        "answer": "dakota",
                        "hit": false
                    },
                    {
                        "score": 0.5600940339547444,
                        "answer": "commerce",
                        "hit": false
                    },
                    {
                        "score": 0.5587565542628561,
                        "answer": "crater",
                        "hit": false
                    },
                    {
                        "score": 0.5582978087706083,
                        "answer": "boat",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "embarrass"
                ],
                "rank": 2089,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8070906400680542
            },
            {
                "question verbose": "What is to encourage ",
                "b": "encourage",
                "expected answer": [
                    "encouragement"
                ],
                "predictions": [
                    {
                        "score": 0.4280545585851483,
                        "answer": "activist",
                        "hit": false
                    },
                    {
                        "score": 0.37530562284765434,
                        "answer": "agenda",
                        "hit": false
                    },
                    {
                        "score": 0.3711224598790732,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.3682121347543526,
                        "answer": "incompetence",
                        "hit": false
                    },
                    {
                        "score": 0.36693587711170317,
                        "answer": "authoritative",
                        "hit": false
                    },
                    {
                        "score": 0.35950681046613486,
                        "answer": "fundamental",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "encourage"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6619144231081009
            },
            {
                "question verbose": "What is to endorse ",
                "b": "endorse",
                "expected answer": [
                    "endorsement"
                ],
                "predictions": [
                    {
                        "score": 0.47231465309329235,
                        "answer": "distortion",
                        "hit": false
                    },
                    {
                        "score": 0.4368509575505275,
                        "answer": "geneva",
                        "hit": false
                    },
                    {
                        "score": 0.4305539947100258,
                        "answer": "contraceptive",
                        "hit": false
                    },
                    {
                        "score": 0.4265480934951784,
                        "answer": "psrc",
                        "hit": false
                    },
                    {
                        "score": 0.4197413540157044,
                        "answer": "frieze",
                        "hit": false
                    },
                    {
                        "score": 0.41942742893235396,
                        "answer": "eliminated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "endorse"
                ],
                "rank": 10284,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8000059425830841
            },
            {
                "question verbose": "What is to enforce ",
                "b": "enforce",
                "expected answer": [
                    "enforcement"
                ],
                "predictions": [
                    {
                        "score": 0.6100906752994348,
                        "answer": "contraceptive",
                        "hit": false
                    },
                    {
                        "score": 0.5931214638984237,
                        "answer": "agricultural",
                        "hit": false
                    },
                    {
                        "score": 0.589738488606198,
                        "answer": "cocoa",
                        "hit": false
                    },
                    {
                        "score": 0.5846502215697743,
                        "answer": "averaged",
                        "hit": false
                    },
                    {
                        "score": 0.5842407766829539,
                        "answer": "argentinian",
                        "hit": false
                    },
                    {
                        "score": 0.5804618617777209,
                        "answer": "presbyterian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enforce"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6381581723690033
            },
            {
                "question verbose": "What is to engage ",
                "b": "engage",
                "expected answer": [
                    "engagement"
                ],
                "predictions": [
                    {
                        "score": 0.42797734951561417,
                        "answer": "museum",
                        "hit": false
                    },
                    {
                        "score": 0.40504239847474677,
                        "answer": "curator",
                        "hit": false
                    },
                    {
                        "score": 0.40146343840143395,
                        "answer": "activist",
                        "hit": false
                    },
                    {
                        "score": 0.3929863506638291,
                        "answer": "consisted",
                        "hit": false
                    },
                    {
                        "score": 0.3917365300408632,
                        "answer": "rhetorical",
                        "hit": false
                    },
                    {
                        "score": 0.387615546440465,
                        "answer": "domain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "engage"
                ],
                "rank": 11650,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6030787900090218
            },
            {
                "question verbose": "What is to enhance ",
                "b": "enhance",
                "expected answer": [
                    "enhancement"
                ],
                "predictions": [
                    {
                        "score": 0.5633325677603856,
                        "answer": "advocate",
                        "hit": false
                    },
                    {
                        "score": 0.51207879461844,
                        "answer": "mlm",
                        "hit": false
                    },
                    {
                        "score": 0.49297926026019556,
                        "answer": "yourtangocom",
                        "hit": false
                    },
                    {
                        "score": 0.4569158263169081,
                        "answer": "impolitic",
                        "hit": false
                    },
                    {
                        "score": 0.45513939765568523,
                        "answer": "esophagus",
                        "hit": false
                    },
                    {
                        "score": 0.44844482122865875,
                        "answer": "impacting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enhance"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5632772743701935
            },
            {
                "question verbose": "What is to enjoy ",
                "b": "enjoy",
                "expected answer": [
                    "enjoyment"
                ],
                "predictions": [
                    {
                        "score": 0.3631184444864707,
                        "answer": "worried",
                        "hit": false
                    },
                    {
                        "score": 0.3480638498163746,
                        "answer": "ridge",
                        "hit": false
                    },
                    {
                        "score": 0.3395571204872603,
                        "answer": "visitor",
                        "hit": false
                    },
                    {
                        "score": 0.3364512027448942,
                        "answer": "residential",
                        "hit": false
                    },
                    {
                        "score": 0.33485584487825815,
                        "answer": "vibe",
                        "hit": false
                    },
                    {
                        "score": 0.3333357504244994,
                        "answer": "curvature",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enjoy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.582465149462223
            },
            {
                "question verbose": "What is to enlighten ",
                "b": "enlighten",
                "expected answer": [
                    "enlightenment"
                ],
                "predictions": [
                    {
                        "score": 0.4801926061812007,
                        "answer": "shelf",
                        "hit": false
                    },
                    {
                        "score": 0.47086083266153217,
                        "answer": "doulgeris",
                        "hit": false
                    },
                    {
                        "score": 0.458694082821718,
                        "answer": "reselling",
                        "hit": false
                    },
                    {
                        "score": 0.45199811848483695,
                        "answer": "alamo",
                        "hit": false
                    },
                    {
                        "score": 0.4473476915419101,
                        "answer": "spider",
                        "hit": false
                    },
                    {
                        "score": 0.44604102483898256,
                        "answer": "joblocom",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enlighten"
                ],
                "rank": 2677,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8052232563495636
            },
            {
                "question verbose": "What is to enroll ",
                "b": "enroll",
                "expected answer": [
                    "enrollment",
                    "enrolment"
                ],
                "predictions": [
                    {
                        "score": 0.5151839642483262,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25792452710187275,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.24900374385800278,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2475391294726044,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24606842652137081,
                        "answer": "shia",
                        "hit": false
                    },
                    {
                        "score": 0.23743320956377056,
                        "answer": "instrumental",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enroll"
                ],
                "rank": 6384,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5799200534820557
            },
            {
                "question verbose": "What is to entertain ",
                "b": "entertain",
                "expected answer": [
                    "entertainment"
                ],
                "predictions": [
                    {
                        "score": 0.5317069638146473,
                        "answer": "scout",
                        "hit": false
                    },
                    {
                        "score": 0.5203431844381287,
                        "answer": "dashing",
                        "hit": false
                    },
                    {
                        "score": 0.5176107500074032,
                        "answer": "alamo",
                        "hit": false
                    },
                    {
                        "score": 0.5167580829946502,
                        "answer": "acwrimo",
                        "hit": false
                    },
                    {
                        "score": 0.5157586282806387,
                        "answer": "pentathlon",
                        "hit": false
                    },
                    {
                        "score": 0.515293241590139,
                        "answer": "escalator",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "entertain"
                ],
                "rank": 11139,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7172641158103943
            },
            {
                "question verbose": "What is to entitle ",
                "b": "entitle",
                "expected answer": [
                    "entitlement"
                ],
                "predictions": [
                    {
                        "score": 0.5171899769721582,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27808376268537127,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2745521929585652,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.25996120984189697,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.2576789314077909,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.24525438441664973,
                        "answer": "segregation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "entitle"
                ],
                "rank": 7706,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6076903939247131
            },
            {
                "question verbose": "What is to equip ",
                "b": "equip",
                "expected answer": [
                    "equipment"
                ],
                "predictions": [
                    {
                        "score": 0.45340148394663904,
                        "answer": "excellence",
                        "hit": false
                    },
                    {
                        "score": 0.4254192945280623,
                        "answer": "mccombs",
                        "hit": false
                    },
                    {
                        "score": 0.4235879041287424,
                        "answer": "gainfully",
                        "hit": false
                    },
                    {
                        "score": 0.42024308947985806,
                        "answer": "booth",
                        "hit": false
                    },
                    {
                        "score": 0.418922848364318,
                        "answer": "brink",
                        "hit": false
                    },
                    {
                        "score": 0.41362995042261397,
                        "answer": "jargon",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "equip"
                ],
                "rank": 5114,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7465465515851974
            },
            {
                "question verbose": "What is to establish ",
                "b": "establish",
                "expected answer": [
                    "establishment"
                ],
                "predictions": [
                    {
                        "score": 0.522947100125099,
                        "answer": "consequently",
                        "hit": false
                    },
                    {
                        "score": 0.521943409083902,
                        "answer": "missionary",
                        "hit": false
                    },
                    {
                        "score": 0.5207361723963946,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.5157286941025294,
                        "answer": "rudy",
                        "hit": false
                    },
                    {
                        "score": 0.5107735861449819,
                        "answer": "presbyterian",
                        "hit": false
                    },
                    {
                        "score": 0.5033012748154995,
                        "answer": "armed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "establish"
                ],
                "rank": 7926,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7289333641529083
            },
            {
                "question verbose": "What is to excite ",
                "b": "excite",
                "expected answer": [
                    "excitement"
                ],
                "predictions": [
                    {
                        "score": 0.4989969898762081,
                        "answer": "ox",
                        "hit": false
                    },
                    {
                        "score": 0.47306249610437906,
                        "answer": "champagne",
                        "hit": false
                    },
                    {
                        "score": 0.4724362997498364,
                        "answer": "endorses",
                        "hit": false
                    },
                    {
                        "score": 0.4723702293239482,
                        "answer": "promoter",
                        "hit": false
                    },
                    {
                        "score": 0.47236934435723643,
                        "answer": "bni",
                        "hit": false
                    },
                    {
                        "score": 0.47052917250807813,
                        "answer": "incamera",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "excite"
                ],
                "rank": 11610,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7966547608375549
            },
            {
                "question verbose": "What is to fulfil ",
                "b": "fulfil",
                "expected answer": [
                    "fulfilment"
                ],
                "predictions": [
                    {
                        "score": 0.49645757144221714,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2820809883699678,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2681510959766909,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2652546204990741,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2551033020375227,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24982539441806856,
                        "answer": "shia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fulfil"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to fulfill ",
                "b": "fulfill",
                "expected answer": [
                    "fulfillment"
                ],
                "predictions": [
                    {
                        "score": 0.5172608626260428,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27807257178904754,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2669330354915576,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.26444536785688033,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2525893446720152,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2521638793664535,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fulfill"
                ],
                "rank": 7560,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6499939858913422
            },
            {
                "question verbose": "What is to harass ",
                "b": "harass",
                "expected answer": [
                    "harassment"
                ],
                "predictions": [
                    {
                        "score": 0.500801829394187,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2752296584362501,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.27336055112495034,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2602903464477207,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.260020900453358,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.25515680785817024,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "harass"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to impair ",
                "b": "impair",
                "expected answer": [
                    "impairment"
                ],
                "predictions": [
                    {
                        "score": 0.5007603693009856,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27832305060357637,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2668643066051966,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2643247000372111,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2570456478965159,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24406891488441387,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "impair"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to improve ",
                "b": "improve",
                "expected answer": [
                    "improvement"
                ],
                "predictions": [
                    {
                        "score": 0.37408244236562355,
                        "answer": "assessing",
                        "hit": false
                    },
                    {
                        "score": 0.3597023288368302,
                        "answer": "mccombs",
                        "hit": false
                    },
                    {
                        "score": 0.35698383401347555,
                        "answer": "nursing",
                        "hit": false
                    },
                    {
                        "score": 0.34594786361034413,
                        "answer": "excellence",
                        "hit": false
                    },
                    {
                        "score": 0.34175639194573393,
                        "answer": "blead",
                        "hit": false
                    },
                    {
                        "score": 0.34123565301978803,
                        "answer": "specialist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improve"
                ],
                "rank": 5529,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.725611001253128
            },
            {
                "question verbose": "What is to infringe ",
                "b": "infringe",
                "expected answer": [
                    "infringement"
                ],
                "predictions": [
                    {
                        "score": 0.5191442425785625,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2748937907493732,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2642804343487616,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.25274467118278054,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2525227110153207,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.2505999972951153,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "infringe"
                ],
                "rank": 9681,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5780071914196014
            },
            {
                "question verbose": "What is to invest ",
                "b": "invest",
                "expected answer": [
                    "investment"
                ],
                "predictions": [
                    {
                        "score": 0.5406737854380262,
                        "answer": "rehearing",
                        "hit": false
                    },
                    {
                        "score": 0.535823394577662,
                        "answer": "scandinavia",
                        "hit": false
                    },
                    {
                        "score": 0.5321051014401925,
                        "answer": "consolidation",
                        "hit": false
                    },
                    {
                        "score": 0.5295692637788428,
                        "answer": "peep",
                        "hit": false
                    },
                    {
                        "score": 0.5248725983767257,
                        "answer": "robertson",
                        "hit": false
                    },
                    {
                        "score": 0.5243274274360762,
                        "answer": "impacting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "invest"
                ],
                "rank": 7801,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.745281919836998
            },
            {
                "question verbose": "What is to involve ",
                "b": "involve",
                "expected answer": [
                    "involvement"
                ],
                "predictions": [
                    {
                        "score": 0.5201746889093781,
                        "answer": "prohibiting",
                        "hit": false
                    },
                    {
                        "score": 0.5100670487865326,
                        "answer": "clause",
                        "hit": false
                    },
                    {
                        "score": 0.5038451630677673,
                        "answer": "protection",
                        "hit": false
                    },
                    {
                        "score": 0.4888698926685742,
                        "answer": "finley",
                        "hit": false
                    },
                    {
                        "score": 0.48517686597207543,
                        "answer": "violate",
                        "hit": false
                    },
                    {
                        "score": 0.4743431835209525,
                        "answer": "averaged",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involve"
                ],
                "rank": 92,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8444053828716278
            },
            {
                "question verbose": "What is to manage ",
                "b": "manage",
                "expected answer": [
                    "management"
                ],
                "predictions": [
                    {
                        "score": 0.4588326193047773,
                        "answer": "edit",
                        "hit": false
                    },
                    {
                        "score": 0.4484639260319742,
                        "answer": "upgrade",
                        "hit": false
                    },
                    {
                        "score": 0.437101080752614,
                        "answer": "electric",
                        "hit": false
                    },
                    {
                        "score": 0.41889451789019755,
                        "answer": "escalator",
                        "hit": false
                    },
                    {
                        "score": 0.41831786271158716,
                        "answer": "doh",
                        "hit": false
                    },
                    {
                        "score": 0.40832920380700544,
                        "answer": "provider",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manage"
                ],
                "rank": 8713,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7098580151796341
            },
            {
                "question verbose": "What is to punish ",
                "b": "punish",
                "expected answer": [
                    "punishment"
                ],
                "predictions": [
                    {
                        "score": 0.4694856908371198,
                        "answer": "broadcasting",
                        "hit": false
                    },
                    {
                        "score": 0.44562824179635485,
                        "answer": "inefficient",
                        "hit": false
                    },
                    {
                        "score": 0.4437114232708972,
                        "answer": "consolidate",
                        "hit": false
                    },
                    {
                        "score": 0.44206644978557386,
                        "answer": "aquinas",
                        "hit": false
                    },
                    {
                        "score": 0.4390509615277,
                        "answer": "brink",
                        "hit": false
                    },
                    {
                        "score": 0.43871823495325,
                        "answer": "embarrassment",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "punish"
                ],
                "rank": 11563,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7523476481437683
            },
            {
                "question verbose": "What is to redevelop ",
                "b": "redevelop",
                "expected answer": [
                    "redevelopment"
                ],
                "predictions": [
                    {
                        "score": 0.5014308023954668,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2734746959742885,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2715570308802425,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.26067017900325024,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.2559091704587275,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2473824988476505,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "redevelop"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to reimburse ",
                "b": "reimburse",
                "expected answer": [
                    "reimbursement"
                ],
                "predictions": [
                    {
                        "score": 0.5155243260614495,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2829501837591366,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.26913600473036603,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2642425155089393,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.26273852521669383,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24654150214257056,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reimburse"
                ],
                "rank": 2270,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6448604315519333
            },
            {
                "question verbose": "What is to reinforce ",
                "b": "reinforce",
                "expected answer": [
                    "reinforcement"
                ],
                "predictions": [
                    {
                        "score": 0.5163503329535662,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2571643185140696,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.25390671721897734,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.24833149020922435,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2412567271360242,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.23789990605447467,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reinforce"
                ],
                "rank": 1870,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.653355285525322
            },
            {
                "question verbose": "What is to replace ",
                "b": "replace",
                "expected answer": [
                    "replacement"
                ],
                "predictions": [
                    {
                        "score": 0.4508763429490718,
                        "answer": "edit",
                        "hit": false
                    },
                    {
                        "score": 0.39033149557951713,
                        "answer": "outlet",
                        "hit": false
                    },
                    {
                        "score": 0.36659610761452976,
                        "answer": "mileage",
                        "hit": false
                    },
                    {
                        "score": 0.36618762471928107,
                        "answer": "absorbing",
                        "hit": false
                    },
                    {
                        "score": 0.36365823154995364,
                        "answer": "semantic",
                        "hit": false
                    },
                    {
                        "score": 0.3621263658525955,
                        "answer": "precision",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "replace"
                ],
                "rank": 14409,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6521380543708801
            },
            {
                "question verbose": "What is to require ",
                "b": "require",
                "expected answer": [
                    "requirement"
                ],
                "predictions": [
                    {
                        "score": 0.35019619992088313,
                        "answer": "agreement",
                        "hit": false
                    },
                    {
                        "score": 0.3444016904943689,
                        "answer": "leone",
                        "hit": false
                    },
                    {
                        "score": 0.32445066521940147,
                        "answer": "entered",
                        "hit": false
                    },
                    {
                        "score": 0.32443374640612016,
                        "answer": "employer",
                        "hit": false
                    },
                    {
                        "score": 0.323090558055746,
                        "answer": "prohibiting",
                        "hit": false
                    },
                    {
                        "score": 0.3187413299013241,
                        "answer": "blogging",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "require"
                ],
                "rank": 1867,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6861696988344193
            },
            {
                "question verbose": "What is to resent ",
                "b": "resent",
                "expected answer": [
                    "resentment"
                ],
                "predictions": [
                    {
                        "score": 0.5173531321411113,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26429873530536957,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2596981405519177,
                        "answer": "allocation",
                        "hit": false
                    },
                    {
                        "score": 0.2573678162934108,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.25219547204149856,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.24125757710814888,
                        "answer": "founded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "resent"
                ],
                "rank": 12783,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6187717318534851
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "2_Derivational_morphology",
            "subcategory": "D10 [verb+ment_irreg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "65a520ef-11bd-4e8b-8045-c22cdcfed86f",
            "timestamp": "2020-10-22T15:57:13.800948"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to abuja ",
                "b": "abuja",
                "expected answer": [
                    "nigeria"
                ],
                "predictions": [
                    {
                        "score": 0.3731157821780764,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3149266792495195,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3023239334793661,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2961489829665825,
                        "answer": "matthew",
                        "hit": false
                    },
                    {
                        "score": 0.2948312662065953,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29128947150381884,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "abuja"
                ],
                "rank": 4011,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6046404018998146
            },
            {
                "question verbose": "What is to amman ",
                "b": "amman",
                "expected answer": [
                    "jordan"
                ],
                "predictions": [
                    {
                        "score": 0.37543847141663883,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3106895112256022,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.3033517952693714,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3019802810758813,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29993966441306946,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2960147707775671,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "amman"
                ],
                "rank": 4687,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5890912562608719
            },
            {
                "question verbose": "What is to ankara ",
                "b": "ankara",
                "expected answer": [
                    "turkey"
                ],
                "predictions": [
                    {
                        "score": 0.3732053853722693,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3129668063389203,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3003154530091245,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2894088462551928,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2874757877936615,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2873479204503529,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ankara"
                ],
                "rank": 1633,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6434292942285538
            },
            {
                "question verbose": "What is to athens ",
                "b": "athens",
                "expected answer": [
                    "greece"
                ],
                "predictions": [
                    {
                        "score": 0.3627682018882501,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3103756923883961,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3095469259210883,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3061771085387846,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30330757527309865,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.3018495018637799,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "athens"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to baghdad ",
                "b": "baghdad",
                "expected answer": [
                    "iraq"
                ],
                "predictions": [
                    {
                        "score": 0.37314635318367667,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30030599165348876,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.29486148660064004,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29093105195301744,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.2887199597669807,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2783262767755594,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "baghdad"
                ],
                "rank": 1649,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6259661465883255
            },
            {
                "question verbose": "What is to bangkok ",
                "b": "bangkok",
                "expected answer": [
                    "thailand"
                ],
                "predictions": [
                    {
                        "score": 0.375722382175523,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30517093294471226,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30073501248685275,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3000422936012825,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2997205687533268,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29962130613356064,
                        "answer": "disappear",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bangkok"
                ],
                "rank": 15316,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.45868297293782234
            },
            {
                "question verbose": "What is to beijing ",
                "b": "beijing",
                "expected answer": [
                    "china"
                ],
                "predictions": [
                    {
                        "score": 0.5736479934075069,
                        "answer": "momr",
                        "hit": false
                    },
                    {
                        "score": 0.5723719741766187,
                        "answer": "northern",
                        "hit": false
                    },
                    {
                        "score": 0.5721279511362333,
                        "answer": "legg",
                        "hit": false
                    },
                    {
                        "score": 0.5705645126964403,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.5664819999294105,
                        "answer": "occurrence",
                        "hit": false
                    },
                    {
                        "score": 0.564652923161724,
                        "answer": "averaged",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beijing"
                ],
                "rank": 9941,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7123949825763702
            },
            {
                "question verbose": "What is to beirut ",
                "b": "beirut",
                "expected answer": [
                    "lebanon"
                ],
                "predictions": [
                    {
                        "score": 0.37433112417365766,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3089768645034774,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3078472844707447,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.3029516874120735,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3016792169397223,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.301286070358564,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beirut"
                ],
                "rank": 2343,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613738551735878
            },
            {
                "question verbose": "What is to belgrade ",
                "b": "belgrade",
                "expected answer": [
                    "serbia"
                ],
                "predictions": [
                    {
                        "score": 0.3608376395637047,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3037464381054546,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30144240351447604,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2960942335559953,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.29577430350111233,
                        "answer": "aid",
                        "hit": false
                    },
                    {
                        "score": 0.29303474109691163,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "belgrade"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to berlin ",
                "b": "berlin",
                "expected answer": [
                    "germany"
                ],
                "predictions": [
                    {
                        "score": 0.3762407997455626,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.316304355452563,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3116646979293096,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.30956454206171014,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3048819637595242,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.301972109378387,
                        "answer": "founded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "berlin"
                ],
                "rank": 5782,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6076404824852943
            },
            {
                "question verbose": "What is to bern ",
                "b": "bern",
                "expected answer": [
                    "switzerland"
                ],
                "predictions": [
                    {
                        "score": 0.3751175317577771,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3094762770447832,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3016041835362783,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.301333027111582,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.29673232283191187,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2958333984707173,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bern"
                ],
                "rank": 2023,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6361023485660553
            },
            {
                "question verbose": "What is to brussels ",
                "b": "brussels",
                "expected answer": [
                    "belgium"
                ],
                "predictions": [
                    {
                        "score": 0.3607414008969888,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32084152325137416,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3139884842813711,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3095733191455088,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.3087952643531373,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.30558704614846555,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brussels"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to bucharest ",
                "b": "bucharest",
                "expected answer": [
                    "romania"
                ],
                "predictions": [
                    {
                        "score": 0.3613172810659051,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3114224749000199,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30295514921081435,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3026254265768345,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2988071945074167,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2960771679220032,
                        "answer": "aid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bucharest"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to budapest ",
                "b": "budapest",
                "expected answer": [
                    "hungary"
                ],
                "predictions": [
                    {
                        "score": 0.3629283926014696,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31319994019882147,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.31011829567985555,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.305863292122091,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.3057020932602946,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2966417951926115,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "budapest"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cairo ",
                "b": "cairo",
                "expected answer": [
                    "egypt"
                ],
                "predictions": [
                    {
                        "score": 0.556531713204974,
                        "answer": "scholaryale",
                        "hit": false
                    },
                    {
                        "score": 0.5520547549590304,
                        "answer": "incarcerated",
                        "hit": false
                    },
                    {
                        "score": 0.5469727849585192,
                        "answer": "ani",
                        "hit": false
                    },
                    {
                        "score": 0.5390634333323475,
                        "answer": "marie",
                        "hit": false
                    },
                    {
                        "score": 0.5346446629614381,
                        "answer": "crispus",
                        "hit": false
                    },
                    {
                        "score": 0.5343674307218752,
                        "answer": "sorriest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cairo"
                ],
                "rank": 2026,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7452627271413803
            },
            {
                "question verbose": "What is to canberra ",
                "b": "canberra",
                "expected answer": [
                    "australia"
                ],
                "predictions": [
                    {
                        "score": 0.37608557002259535,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30582676123543484,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3045111545667952,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.30433520321013824,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.30041124028025296,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2960998554522707,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "canberra"
                ],
                "rank": 11571,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.56623225659132
            },
            {
                "question verbose": "What is to conakry ",
                "b": "conakry",
                "expected answer": [
                    "guinea"
                ],
                "predictions": [
                    {
                        "score": 0.37600686540105865,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3061305231518719,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30496536102664584,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.296944666883835,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2965800439001577,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2896852463526982,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "conakry"
                ],
                "rank": 217,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.663913756608963
            },
            {
                "question verbose": "What is to copenhagen ",
                "b": "copenhagen",
                "expected answer": [
                    "denmark"
                ],
                "predictions": [
                    {
                        "score": 0.3734614031415743,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30868504609440583,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.29650214281169807,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29299308504286387,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29260558549557275,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.2855629644835013,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "copenhagen"
                ],
                "rank": 4410,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6134062111377716
            },
            {
                "question verbose": "What is to damascus ",
                "b": "damascus",
                "expected answer": [
                    "syria"
                ],
                "predictions": [
                    {
                        "score": 0.39554515373341625,
                        "answer": "understandably",
                        "hit": false
                    },
                    {
                        "score": 0.38613991967697825,
                        "answer": "basest",
                        "hit": false
                    },
                    {
                        "score": 0.3819568866897399,
                        "answer": "phillip",
                        "hit": false
                    },
                    {
                        "score": 0.3798544630083922,
                        "answer": "gripped",
                        "hit": false
                    },
                    {
                        "score": 0.374638963723054,
                        "answer": "geographic",
                        "hit": false
                    },
                    {
                        "score": 0.37308555768251134,
                        "answer": "steady",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "damascus"
                ],
                "rank": 2247,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7336676269769669
            },
            {
                "question verbose": "What is to dhaka ",
                "b": "dhaka",
                "expected answer": [
                    "bangladesh"
                ],
                "predictions": [
                    {
                        "score": 0.3618835757699424,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30523684185846145,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3038139867176793,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2997642224239439,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2992889225549258,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29482574446451754,
                        "answer": "proposed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dhaka"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dublin ",
                "b": "dublin",
                "expected answer": [
                    "ireland"
                ],
                "predictions": [
                    {
                        "score": 0.5621891249758305,
                        "answer": "leo",
                        "hit": false
                    },
                    {
                        "score": 0.5620115954121588,
                        "answer": "supervisor",
                        "hit": false
                    },
                    {
                        "score": 0.5617557934595997,
                        "answer": "somerville",
                        "hit": false
                    },
                    {
                        "score": 0.5536746759326154,
                        "answer": "totaled",
                        "hit": false
                    },
                    {
                        "score": 0.5491090250617805,
                        "answer": "topolsky",
                        "hit": false
                    },
                    {
                        "score": 0.5462666174838151,
                        "answer": "reveler",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dublin"
                ],
                "rank": 1757,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.745933473110199
            },
            {
                "question verbose": "What is to hanoi ",
                "b": "hanoi",
                "expected answer": [
                    "vietnam"
                ],
                "predictions": [
                    {
                        "score": 0.37517782691220924,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31133223423000367,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30980860964299123,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.30617221622350893,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29926211211613024,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2992117108273843,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hanoi"
                ],
                "rank": 6206,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6299059838056564
            },
            {
                "question verbose": "What is to havana ",
                "b": "havana",
                "expected answer": [
                    "cuba"
                ],
                "predictions": [
                    {
                        "score": 0.3737205626040444,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3063370736907266,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.28914514210264736,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.28789408197120486,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2876824055512581,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2864840503366662,
                        "answer": "disappear",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "havana"
                ],
                "rank": 2213,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6744038164615631
            },
            {
                "question verbose": "What is to helsinki ",
                "b": "helsinki",
                "expected answer": [
                    "finland"
                ],
                "predictions": [
                    {
                        "score": 0.3615770896753305,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31359187211726275,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3081288188110966,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.30358867018703867,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.30294279202389734,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30224947846762085,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "helsinki"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to islamabad ",
                "b": "islamabad",
                "expected answer": [
                    "pakistan"
                ],
                "predictions": [
                    {
                        "score": 0.3629145488005881,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.310479134857853,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3020214713272776,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3007134810042968,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.300234435590198,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2968978949406878,
                        "answer": "aid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "islamabad"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to jakarta ",
                "b": "jakarta",
                "expected answer": [
                    "indonesia"
                ],
                "predictions": [
                    {
                        "score": 0.36384233608833927,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3136966807172485,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30943010683229955,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30601025669507587,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3031214140370624,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2966531570838752,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jakarta"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to kabul ",
                "b": "kabul",
                "expected answer": [
                    "afghanistan"
                ],
                "predictions": [
                    {
                        "score": 0.36158411412105185,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3086366964896692,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3019230566464376,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29874103217239845,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.29833975770730004,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29236832736310747,
                        "answer": "aid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kabul"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to kiev ",
                "b": "kiev",
                "expected answer": [
                    "ukraine"
                ],
                "predictions": [
                    {
                        "score": 0.3644560995276923,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3038055958556266,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3009577809801884,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.3006881611327175,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.29518363290982325,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2915480923754421,
                        "answer": "aid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kiev"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to kingston ",
                "b": "kingston",
                "expected answer": [
                    "jamaica"
                ],
                "predictions": [
                    {
                        "score": 0.37439090483844334,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3064844441326177,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30562462522419986,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.29686738522808,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29481747230101124,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.28801982506180723,
                        "answer": "disappear",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kingston"
                ],
                "rank": 7433,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6075325012207031
            },
            {
                "question verbose": "What is to lima ",
                "b": "lima",
                "expected answer": [
                    "peru"
                ],
                "predictions": [
                    {
                        "score": 0.3747940298076527,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3136675554346304,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3111063838879919,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2986810502607078,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.29420740952793245,
                        "answer": "matthew",
                        "hit": false
                    },
                    {
                        "score": 0.29400460661553385,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lima"
                ],
                "rank": 1442,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6422961503267288
            },
            {
                "question verbose": "What is to lisbon ",
                "b": "lisbon",
                "expected answer": [
                    "portugal"
                ],
                "predictions": [
                    {
                        "score": 0.36329861813055053,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31230385915711223,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3089117891604511,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30602141490505763,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.30556279852346496,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.30255824875433635,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lisbon"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to london ",
                "b": "london",
                "expected answer": [
                    "england",
                    "uk",
                    "britain"
                ],
                "predictions": [
                    {
                        "score": 0.5687656932414246,
                        "answer": "grew",
                        "hit": false
                    },
                    {
                        "score": 0.5615129839553679,
                        "answer": "daley",
                        "hit": false
                    },
                    {
                        "score": 0.559461295949484,
                        "answer": "chamber",
                        "hit": false
                    },
                    {
                        "score": 0.555833324208677,
                        "answer": "hokkaido",
                        "hit": false
                    },
                    {
                        "score": 0.5551077201539559,
                        "answer": "morgan",
                        "hit": false
                    },
                    {
                        "score": 0.5522029191981255,
                        "answer": "quebec",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "london"
                ],
                "rank": 678,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8335675001144409
            },
            {
                "question verbose": "What is to madrid ",
                "b": "madrid",
                "expected answer": [
                    "spain"
                ],
                "predictions": [
                    {
                        "score": 0.37706180426531916,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30217373229724814,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2967330022483279,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29321384232064845,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.28995131910021815,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2877286902909915,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "madrid"
                ],
                "rank": 832,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6382187753915787
            },
            {
                "question verbose": "What is to manila ",
                "b": "manila",
                "expected answer": [
                    "philippines"
                ],
                "predictions": [
                    {
                        "score": 0.3625526834039508,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31469081786957953,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3034931559551003,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.30113883883224224,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30084425173151585,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29744734226439395,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manila"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to moscow ",
                "b": "moscow",
                "expected answer": [
                    "russia"
                ],
                "predictions": [
                    {
                        "score": 0.6043901116716571,
                        "answer": "attu",
                        "hit": false
                    },
                    {
                        "score": 0.6020039574906082,
                        "answer": "kenya",
                        "hit": false
                    },
                    {
                        "score": 0.5791723201962798,
                        "answer": "crossing",
                        "hit": false
                    },
                    {
                        "score": 0.5752428851781299,
                        "answer": "halted",
                        "hit": false
                    },
                    {
                        "score": 0.5743732389488786,
                        "answer": "actuary",
                        "hit": false
                    },
                    {
                        "score": 0.5741519538768981,
                        "answer": "hokkaido",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "moscow"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6623065769672394
            },
            {
                "question verbose": "What is to nairobi ",
                "b": "nairobi",
                "expected answer": [
                    "kenya"
                ],
                "predictions": [
                    {
                        "score": 0.3748950968124529,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3099522954529923,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.299189152089777,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.29429173021905464,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29410677159882415,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.28742779243089095,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nairobi"
                ],
                "rank": 561,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6339173763990402
            },
            {
                "question verbose": "What is to oslo ",
                "b": "oslo",
                "expected answer": [
                    "norway"
                ],
                "predictions": [
                    {
                        "score": 0.5072524951669002,
                        "answer": "georgia",
                        "hit": false
                    },
                    {
                        "score": 0.4850668934770965,
                        "answer": "boehlert",
                        "hit": false
                    },
                    {
                        "score": 0.482578532171276,
                        "answer": "dissemination",
                        "hit": false
                    },
                    {
                        "score": 0.47934361792544805,
                        "answer": "morocco",
                        "hit": false
                    },
                    {
                        "score": 0.47818996524922514,
                        "answer": "urged",
                        "hit": false
                    },
                    {
                        "score": 0.47675918572035014,
                        "answer": "primus",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "oslo"
                ],
                "rank": 4602,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7077690064907074
            },
            {
                "question verbose": "What is to ottawa ",
                "b": "ottawa",
                "expected answer": [
                    "canada"
                ],
                "predictions": [
                    {
                        "score": 0.374377375739429,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3052410113040656,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.29317488464907915,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.29070431513691963,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.288994561984962,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.28833044991265094,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ottawa"
                ],
                "rank": 3815,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613965593278408
            },
            {
                "question verbose": "What is to paris ",
                "b": "paris",
                "expected answer": [
                    "france"
                ],
                "predictions": [
                    {
                        "score": 0.46726273216207376,
                        "answer": "acwrimo",
                        "hit": false
                    },
                    {
                        "score": 0.465195192854647,
                        "answer": "assembled",
                        "hit": false
                    },
                    {
                        "score": 0.46113959243026637,
                        "answer": "jamaica",
                        "hit": false
                    },
                    {
                        "score": 0.4607766605879326,
                        "answer": "wed",
                        "hit": false
                    },
                    {
                        "score": 0.4592175423231002,
                        "answer": "bicentennial",
                        "hit": false
                    },
                    {
                        "score": 0.45193740690245077,
                        "answer": "jeb",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "paris"
                ],
                "rank": 1045,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7235354036092758
            },
            {
                "question verbose": "What is to rome ",
                "b": "rome",
                "expected answer": [
                    "italy"
                ],
                "predictions": [
                    {
                        "score": 0.6591884237987184,
                        "answer": "momr",
                        "hit": false
                    },
                    {
                        "score": 0.6424032708338213,
                        "answer": "attu",
                        "hit": false
                    },
                    {
                        "score": 0.6391333762652833,
                        "answer": "france",
                        "hit": false
                    },
                    {
                        "score": 0.6249410936333354,
                        "answer": "incarcerated",
                        "hit": false
                    },
                    {
                        "score": 0.6210571061273097,
                        "answer": "saltpeter",
                        "hit": false
                    },
                    {
                        "score": 0.6149719409412229,
                        "answer": "crater",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rome"
                ],
                "rank": 9060,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.763618141412735
            },
            {
                "question verbose": "What is to santiago ",
                "b": "santiago",
                "expected answer": [
                    "chile"
                ],
                "predictions": [
                    {
                        "score": 0.37432873385352383,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3067394961963907,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.296283222884424,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.295967550685937,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.29523371584087443,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2909951229292947,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "santiago"
                ],
                "rank": 12578,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5507184118032455
            },
            {
                "question verbose": "What is to sofia ",
                "b": "sofia",
                "expected answer": [
                    "bulgaria"
                ],
                "predictions": [
                    {
                        "score": 0.3625478487482476,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3089784954088249,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3087685913296096,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3086764157775076,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.30746149249250965,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2992321349439404,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sofia"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to stockholm ",
                "b": "stockholm",
                "expected answer": [
                    "sweden"
                ],
                "predictions": [
                    {
                        "score": 0.3751703655011525,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3026457289082767,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.29907795470227677,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.29783276873696685,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.29599727500751954,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2935523254319033,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stockholm"
                ],
                "rank": 14587,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5348562560975552
            },
            {
                "question verbose": "What is to taipei ",
                "b": "taipei",
                "expected answer": [
                    "taiwan"
                ],
                "predictions": [
                    {
                        "score": 0.616553608625699,
                        "answer": "momr",
                        "hit": false
                    },
                    {
                        "score": 0.5884224527697364,
                        "answer": "ireland",
                        "hit": false
                    },
                    {
                        "score": 0.5881053335001757,
                        "answer": "fort",
                        "hit": false
                    },
                    {
                        "score": 0.5865867112978741,
                        "answer": "regional",
                        "hit": false
                    },
                    {
                        "score": 0.5788146961290148,
                        "answer": "scandinavia",
                        "hit": false
                    },
                    {
                        "score": 0.577873224048307,
                        "answer": "speedway",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "taipei"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6016831174492836
            },
            {
                "question verbose": "What is to tbilisi ",
                "b": "tbilisi",
                "expected answer": [
                    "georgia"
                ],
                "predictions": [
                    {
                        "score": 0.3738618701998018,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30473412318050463,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30190387230386845,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2965536044528672,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.2836841576522425,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.27981469076237214,
                        "answer": "aid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tbilisi"
                ],
                "rank": 854,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6362631618976593
            },
            {
                "question verbose": "What is to tehran ",
                "b": "tehran",
                "expected answer": [
                    "iran"
                ],
                "predictions": [
                    {
                        "score": 0.37363474260441903,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.309284847783941,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30814416586275056,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30370434986937217,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.2949064261864676,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2939774363225008,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tehran"
                ],
                "rank": 4283,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5896319821476936
            },
            {
                "question verbose": "What is to tokyo ",
                "b": "tokyo",
                "expected answer": [
                    "japan"
                ],
                "predictions": [
                    {
                        "score": 0.5088577746053805,
                        "answer": "sap",
                        "hit": false
                    },
                    {
                        "score": 0.48763703637575995,
                        "answer": "extraction",
                        "hit": false
                    },
                    {
                        "score": 0.4838354086240234,
                        "answer": "driest",
                        "hit": false
                    },
                    {
                        "score": 0.47811687482101245,
                        "answer": "acwrimo",
                        "hit": false
                    },
                    {
                        "score": 0.47646698627938355,
                        "answer": "arthur",
                        "hit": false
                    },
                    {
                        "score": 0.47503325685477443,
                        "answer": "catlett",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tokyo"
                ],
                "rank": 11740,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6907829344272614
            },
            {
                "question verbose": "What is to vienna ",
                "b": "vienna",
                "expected answer": [
                    "austria"
                ],
                "predictions": [
                    {
                        "score": 0.3617764281988843,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3196458286131091,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.31572068655116525,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3054032067949595,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.30101244912485603,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.2951214224177083,
                        "answer": "matthew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vienna"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to warsaw ",
                "b": "warsaw",
                "expected answer": [
                    "poland"
                ],
                "predictions": [
                    {
                        "score": 0.37755512377831624,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3095904754214259,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30648734581807835,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.30423983014153005,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.3037514050117745,
                        "answer": "eligible",
                        "hit": false
                    },
                    {
                        "score": 0.30193107423828563,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "warsaw"
                ],
                "rank": 13774,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5560841225087643
            },
            {
                "question verbose": "What is to zagreb ",
                "b": "zagreb",
                "expected answer": [
                    "croatia"
                ],
                "predictions": [
                    {
                        "score": 0.36325505978556794,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30861879376951923,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.30756788759119,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.30713545876458376,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.30669575687120154,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.29742766894320466,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "zagreb"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E01 [country - capital].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "1d8755f7-106f-4ee7-9a97-1a20c196f4d3",
            "timestamp": "2020-10-22T15:57:15.293275"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to andorra ",
                "b": "andorra",
                "expected answer": [
                    "catalan"
                ],
                "predictions": [
                    {
                        "score": 0.47433406269323825,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3149465481812974,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2900935904316386,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2852341745287473,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2775199853629763,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.26560796783375984,
                        "answer": "forgive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "andorra"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to argentina ",
                "b": "argentina",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.48066462092937984,
                        "answer": "spanish",
                        "hit": true
                    },
                    {
                        "score": 0.42009886949761815,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.4177873980807548,
                        "answer": "diluted",
                        "hit": false
                    },
                    {
                        "score": 0.41718804045914365,
                        "answer": "improvedrealized",
                        "hit": false
                    },
                    {
                        "score": 0.41493116935092034,
                        "answer": "irreversible",
                        "hit": false
                    },
                    {
                        "score": 0.41387192994781496,
                        "answer": "dingell",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "argentina"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7677216529846191
            },
            {
                "question verbose": "What is to australia ",
                "b": "australia",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.3088470406849089,
                        "answer": "billboard",
                        "hit": false
                    },
                    {
                        "score": 0.3027100492988199,
                        "answer": "underling",
                        "hit": false
                    },
                    {
                        "score": 0.3025314769975995,
                        "answer": "supe",
                        "hit": false
                    },
                    {
                        "score": 0.2979361273852985,
                        "answer": "imac",
                        "hit": false
                    },
                    {
                        "score": 0.29572279291587955,
                        "answer": "mankind",
                        "hit": false
                    },
                    {
                        "score": 0.29501619884522046,
                        "answer": "ealing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "australia"
                ],
                "rank": 56,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6406063586473465
            },
            {
                "question verbose": "What is to austria ",
                "b": "austria",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.48874152598622433,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31175784617857244,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29419597559674904,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2878778963205428,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28616169572006916,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2706051320520889,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "austria"
                ],
                "rank": 1916,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to bahamas ",
                "b": "bahamas",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.48794969476235395,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3076113763619305,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2916623584683056,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.28175881886187665,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.27931830411611036,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2630508738898536,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bahamas"
                ],
                "rank": 65,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to bangladesh ",
                "b": "bangladesh",
                "expected answer": [
                    "bengali",
                    "bangla"
                ],
                "predictions": [
                    {
                        "score": 0.4752380360432883,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3298108251666173,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29943624247337514,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2987977275628854,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2946658881331661,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2767948125700076,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bangladesh"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to barbados ",
                "b": "barbados",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.3847532569127245,
                        "answer": "downright",
                        "hit": false
                    },
                    {
                        "score": 0.3821019338388983,
                        "answer": "alarming",
                        "hit": false
                    },
                    {
                        "score": 0.37948679462869134,
                        "answer": "wordpress",
                        "hit": false
                    },
                    {
                        "score": 0.3769158612075936,
                        "answer": "german",
                        "hit": false
                    },
                    {
                        "score": 0.37445034057562826,
                        "answer": "compose",
                        "hit": false
                    },
                    {
                        "score": 0.3737952262880935,
                        "answer": "hull",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "barbados"
                ],
                "rank": 1965,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6273108273744583
            },
            {
                "question verbose": "What is to belize ",
                "b": "belize",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.4884329868229196,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3151220862245366,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3100185218765262,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.29872464266224946,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2969248513250611,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28264406869164693,
                        "answer": "devs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "belize"
                ],
                "rank": 102,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to bolivia ",
                "b": "bolivia",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.48722259684419356,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32235278824937463,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3029392085076355,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2906911148404249,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28573725726369653,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.27282242278013585,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bolivia"
                ],
                "rank": 199,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6059737130999565
            },
            {
                "question verbose": "What is to brazil ",
                "b": "brazil",
                "expected answer": [
                    "portuguese"
                ],
                "predictions": [
                    {
                        "score": 0.4899321080616348,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3180094029925047,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2985606093543211,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2916605958053075,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2802119937616926,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.26815616318766083,
                        "answer": "allow",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brazil"
                ],
                "rank": 2394,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6399046033620834
            },
            {
                "question verbose": "What is to cambodia ",
                "b": "cambodia",
                "expected answer": [
                    "khmer"
                ],
                "predictions": [
                    {
                        "score": 0.496005938512185,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.48884817410357995,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.4813206115581435,
                        "answer": "resentment",
                        "hit": false
                    },
                    {
                        "score": 0.46934760248065366,
                        "answer": "diluted",
                        "hit": false
                    },
                    {
                        "score": 0.4673096932135591,
                        "answer": "poppa",
                        "hit": false
                    },
                    {
                        "score": 0.46334749292672234,
                        "answer": "alarming",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cambodia"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5746055096387863
            },
            {
                "question verbose": "What is to canada ",
                "b": "canada",
                "expected answer": [
                    "english",
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.3612021821067092,
                        "answer": "earning",
                        "hit": false
                    },
                    {
                        "score": 0.3230492308794874,
                        "answer": "server",
                        "hit": false
                    },
                    {
                        "score": 0.3152835153223918,
                        "answer": "thriving",
                        "hit": false
                    },
                    {
                        "score": 0.31195033321599464,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.3106520450789668,
                        "answer": "someday",
                        "hit": false
                    },
                    {
                        "score": 0.31036734883645156,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "canada"
                ],
                "rank": 379,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6341772079467773
            },
            {
                "question verbose": "What is to chile ",
                "b": "chile",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.39080620516549797,
                        "answer": "billboard",
                        "hit": false
                    },
                    {
                        "score": 0.3755255651183383,
                        "answer": "spanish",
                        "hit": true
                    },
                    {
                        "score": 0.3212509443294675,
                        "answer": "sticker",
                        "hit": false
                    },
                    {
                        "score": 0.3070655576997817,
                        "answer": "rain",
                        "hit": false
                    },
                    {
                        "score": 0.30593319930703794,
                        "answer": "compose",
                        "hit": false
                    },
                    {
                        "score": 0.30590546028037763,
                        "answer": "french",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chile"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7085615694522858
            },
            {
                "question verbose": "What is to colombia ",
                "b": "colombia",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.4879774979058575,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3166201898355012,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30413705774594435,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2930233831137493,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2775875489552288,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.276879790692644,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "colombia"
                ],
                "rank": 197,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6059737130999565
            },
            {
                "question verbose": "What is to cuba ",
                "b": "cuba",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.4623240693363948,
                        "answer": "anonymous",
                        "hit": false
                    },
                    {
                        "score": 0.4332134722465954,
                        "answer": "diluted",
                        "hit": false
                    },
                    {
                        "score": 0.4329416250315492,
                        "answer": "thriving",
                        "hit": false
                    },
                    {
                        "score": 0.42943495884835126,
                        "answer": "sacrifice",
                        "hit": false
                    },
                    {
                        "score": 0.42750071047280014,
                        "answer": "pledger",
                        "hit": false
                    },
                    {
                        "score": 0.41309143899262285,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cuba"
                ],
                "rank": 117,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6955932527780533
            },
            {
                "question verbose": "What is to cyprus ",
                "b": "cyprus",
                "expected answer": [
                    "greek",
                    "turkish"
                ],
                "predictions": [
                    {
                        "score": 0.4877399116316584,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3189663791459661,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2900557306371379,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2898670065525814,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2779466741832213,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2690473885554116,
                        "answer": "forgive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cyprus"
                ],
                "rank": 797,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6984416842460632
            },
            {
                "question verbose": "What is to denmark ",
                "b": "denmark",
                "expected answer": [
                    "danish",
                    "faroese",
                    "greenlandic",
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.4906259949275015,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.4629909004004481,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.46255202178007937,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.4570576345649077,
                        "answer": "garment",
                        "hit": false
                    },
                    {
                        "score": 0.45379554013174483,
                        "answer": "softest",
                        "hit": false
                    },
                    {
                        "score": 0.4435000084400418,
                        "answer": "amart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "denmark"
                ],
                "rank": 16,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6134062111377716
            },
            {
                "question verbose": "What is to ecuador ",
                "b": "ecuador",
                "expected answer": [
                    "spanish",
                    "quechua"
                ],
                "predictions": [
                    {
                        "score": 0.48787450012761135,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31449116881189293,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30572034878275217,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.29187312794273074,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28907364012066367,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28355966592771703,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ecuador"
                ],
                "rank": 230,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6059737130999565
            },
            {
                "question verbose": "What is to egypt ",
                "b": "egypt",
                "expected answer": [
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.3577541846764565,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.3495396607109151,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.34453930915773917,
                        "answer": "paragraph",
                        "hit": false
                    },
                    {
                        "score": 0.333068681097522,
                        "answer": "sticker",
                        "hit": false
                    },
                    {
                        "score": 0.3243455240600584,
                        "answer": "judicious",
                        "hit": false
                    },
                    {
                        "score": 0.3228132388339051,
                        "answer": "recommended",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "egypt"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5606923289597034
            },
            {
                "question verbose": "What is to ethiopia ",
                "b": "ethiopia",
                "expected answer": [
                    "amharic"
                ],
                "predictions": [
                    {
                        "score": 0.4747677838880546,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3297788814954056,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3055142529438306,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.3037187247157059,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.30267313709629695,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2966592352195717,
                        "answer": "devs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ethiopia"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to fiji ",
                "b": "fiji",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.4884876733076294,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3275572154181757,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3028823545198795,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2991040579076177,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2987880891666395,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28136107098720015,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fiji"
                ],
                "rank": 122,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to guadeloupe ",
                "b": "guadeloupe",
                "expected answer": [
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.48778148450381315,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30559633340349046,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3007379533328952,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.29423261772076337,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28090004493399023,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.26629276531946294,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guadeloupe"
                ],
                "rank": 786,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to guam ",
                "b": "guam",
                "expected answer": [
                    "english",
                    "chamorro"
                ],
                "predictions": [
                    {
                        "score": 0.48887214833320175,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3143763755268816,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3033671611884347,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2984130212155641,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.29012361807953646,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2799604024898032,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guam"
                ],
                "rank": 93,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to guatemala ",
                "b": "guatemala",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.5893454118447568,
                        "answer": "spanish",
                        "hit": true
                    },
                    {
                        "score": 0.5673699414066682,
                        "answer": "saddam",
                        "hit": false
                    },
                    {
                        "score": 0.5617012020625527,
                        "answer": "irreversible",
                        "hit": false
                    },
                    {
                        "score": 0.5504822579862654,
                        "answer": "api",
                        "hit": false
                    },
                    {
                        "score": 0.5487515933867084,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.5486759520671902,
                        "answer": "improvedrealized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guatemala"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8246654570102692
            },
            {
                "question verbose": "What is to guyana ",
                "b": "guyana",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.48828354861917017,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32539641322951385,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3046050629582699,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.301876061836476,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2933147318704468,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.27430997761284925,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guyana"
                ],
                "rank": 106,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to haiti ",
                "b": "haiti",
                "expected answer": [
                    "creole",
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.45745694584925184,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.44628442538610436,
                        "answer": "french",
                        "hit": true
                    },
                    {
                        "score": 0.4439507699284591,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.4246238770462064,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.4162870909199873,
                        "answer": "garment",
                        "hit": false
                    },
                    {
                        "score": 0.4063319867632011,
                        "answer": "specialist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "haiti"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5770797878503799
            },
            {
                "question verbose": "What is to iran ",
                "b": "iran",
                "expected answer": [
                    "persian"
                ],
                "predictions": [
                    {
                        "score": 0.2737570381383606,
                        "answer": "webcast",
                        "hit": false
                    },
                    {
                        "score": 0.2713512967386638,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.2649040775059828,
                        "answer": "framework",
                        "hit": false
                    },
                    {
                        "score": 0.2641291887037039,
                        "answer": "oppress",
                        "hit": false
                    },
                    {
                        "score": 0.2510406049412547,
                        "answer": "sticker",
                        "hit": false
                    },
                    {
                        "score": 0.24727210893060622,
                        "answer": "wordpress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "iran"
                ],
                "rank": 5276,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7540556192398071
            },
            {
                "question verbose": "What is to iraq ",
                "b": "iraq",
                "expected answer": [
                    "arabic",
                    "kurdish"
                ],
                "predictions": [
                    {
                        "score": 0.38644614406739275,
                        "answer": "encourage",
                        "hit": false
                    },
                    {
                        "score": 0.38234840145589694,
                        "answer": "folder",
                        "hit": false
                    },
                    {
                        "score": 0.3787220856044045,
                        "answer": "diluted",
                        "hit": false
                    },
                    {
                        "score": 0.37560488636543143,
                        "answer": "attests",
                        "hit": false
                    },
                    {
                        "score": 0.358727415991687,
                        "answer": "shepherd",
                        "hit": false
                    },
                    {
                        "score": 0.35762285955754813,
                        "answer": "poppa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "iraq"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6259661465883255
            },
            {
                "question verbose": "What is to ireland ",
                "b": "ireland",
                "expected answer": [
                    "english",
                    "irish",
                    "gaelic"
                ],
                "predictions": [
                    {
                        "score": 0.4766091870098177,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.41316085231226335,
                        "answer": "downright",
                        "hit": false
                    },
                    {
                        "score": 0.4025183967484029,
                        "answer": "billboard",
                        "hit": false
                    },
                    {
                        "score": 0.4017541376421684,
                        "answer": "softest",
                        "hit": false
                    },
                    {
                        "score": 0.3895031199681783,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.3860217357630797,
                        "answer": "scholarly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ireland"
                ],
                "rank": 479,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6469767391681671
            },
            {
                "question verbose": "What is to israel ",
                "b": "israel",
                "expected answer": [
                    "hebrew",
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.35204138744400415,
                        "answer": "scholarly",
                        "hit": false
                    },
                    {
                        "score": 0.3412120604060522,
                        "answer": "mankind",
                        "hit": false
                    },
                    {
                        "score": 0.32830817737807927,
                        "answer": "shepherd",
                        "hit": false
                    },
                    {
                        "score": 0.3262951055755532,
                        "answer": "bamboozle",
                        "hit": false
                    },
                    {
                        "score": 0.3248224578907259,
                        "answer": "insane",
                        "hit": false
                    },
                    {
                        "score": 0.32389517887206576,
                        "answer": "acumen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "israel"
                ],
                "rank": 2759,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7168306112289429
            },
            {
                "question verbose": "What is to jamaica ",
                "b": "jamaica",
                "expected answer": [
                    "english",
                    "creole"
                ],
                "predictions": [
                    {
                        "score": 0.45133349508226517,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.4133475976391725,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.4127190336364493,
                        "answer": "mankind",
                        "hit": false
                    },
                    {
                        "score": 0.4027532020752223,
                        "answer": "overgrowth",
                        "hit": false
                    },
                    {
                        "score": 0.4025739939146903,
                        "answer": "whats",
                        "hit": false
                    },
                    {
                        "score": 0.3951696787155431,
                        "answer": "saddam",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jamaica"
                ],
                "rank": 703,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6511422842741013
            },
            {
                "question verbose": "What is to jordan ",
                "b": "jordan",
                "expected answer": [
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.3854307835155967,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.37438671400447077,
                        "answer": "paragraph",
                        "hit": false
                    },
                    {
                        "score": 0.3344280357813388,
                        "answer": "mexican",
                        "hit": false
                    },
                    {
                        "score": 0.33274129893894944,
                        "answer": "cook",
                        "hit": false
                    },
                    {
                        "score": 0.3310591812494016,
                        "answer": "adopting",
                        "hit": false
                    },
                    {
                        "score": 0.32699412162745656,
                        "answer": "grand",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jordan"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5890912562608719
            },
            {
                "question verbose": "What is to kazakhstan ",
                "b": "kazakhstan",
                "expected answer": [
                    "kazak",
                    "qazaq",
                    "russian"
                ],
                "predictions": [
                    {
                        "score": 0.47441432450438237,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32159946088321456,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3034411833706772,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3022398321593445,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2982176100142763,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2704598331569457,
                        "answer": "devs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kazakhstan"
                ],
                "rank": 2998,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to kosovo ",
                "b": "kosovo",
                "expected answer": [
                    "albanian",
                    "serbian"
                ],
                "predictions": [
                    {
                        "score": 0.48584277285660427,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3036435230776656,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2960482342244096,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2848079903797069,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.27034476899391596,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.26824753772946597,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kosovo"
                ],
                "rank": 682,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6982732266187668
            },
            {
                "question verbose": "What is to kuwait ",
                "b": "kuwait",
                "expected answer": [
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.5033376090476697,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.49075934743695543,
                        "answer": "scholarly",
                        "hit": false
                    },
                    {
                        "score": 0.45113842144691224,
                        "answer": "german",
                        "hit": false
                    },
                    {
                        "score": 0.4496333344044492,
                        "answer": "wordpress",
                        "hit": false
                    },
                    {
                        "score": 0.44711498693116636,
                        "answer": "amart",
                        "hit": false
                    },
                    {
                        "score": 0.44355326188147676,
                        "answer": "alarming",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kuwait"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6123209446668625
            },
            {
                "question verbose": "What is to mexico ",
                "b": "mexico",
                "expected answer": [
                    "spanish",
                    "nahuatl"
                ],
                "predictions": [
                    {
                        "score": 0.3772640966632845,
                        "answer": "server",
                        "hit": false
                    },
                    {
                        "score": 0.3617276697971248,
                        "answer": "alice",
                        "hit": false
                    },
                    {
                        "score": 0.3601010232904184,
                        "answer": "lousy",
                        "hit": false
                    },
                    {
                        "score": 0.35920463637479816,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.35896504297677406,
                        "answer": "compose",
                        "hit": false
                    },
                    {
                        "score": 0.35789893751390905,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mexico"
                ],
                "rank": 910,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6430424749851227
            },
            {
                "question verbose": "What is to moldova ",
                "b": "moldova",
                "expected answer": [
                    "moldovan",
                    "romanian"
                ],
                "predictions": [
                    {
                        "score": 0.47417049144746737,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3244698292400828,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3046702560703295,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.3008942929981966,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.29645598907713583,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2749418923343353,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "moldova"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to morocco ",
                "b": "morocco",
                "expected answer": [
                    "berber",
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.4854669675884757,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.4018031490621441,
                        "answer": "premised",
                        "hit": false
                    },
                    {
                        "score": 0.3973632052708005,
                        "answer": "api",
                        "hit": false
                    },
                    {
                        "score": 0.39486015829086774,
                        "answer": "cartel",
                        "hit": false
                    },
                    {
                        "score": 0.39413425663233814,
                        "answer": "scholarly",
                        "hit": false
                    },
                    {
                        "score": 0.3928929824019646,
                        "answer": "shortens",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "morocco"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6133670881390572
            },
            {
                "question verbose": "What is to mozambique ",
                "b": "mozambique",
                "expected answer": [
                    "portuguese"
                ],
                "predictions": [
                    {
                        "score": 0.4883031081275497,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.317273021679825,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2994230201273924,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2950193973873459,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.29047511718669633,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2749617411499809,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mozambique"
                ],
                "rank": 2483,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6399046033620834
            },
            {
                "question verbose": "What is to netherlands ",
                "b": "netherlands",
                "expected answer": [
                    "dutch",
                    "frisian"
                ],
                "predictions": [
                    {
                        "score": 0.4328691165304816,
                        "answer": "billboard",
                        "hit": false
                    },
                    {
                        "score": 0.4059522297329131,
                        "answer": "german",
                        "hit": false
                    },
                    {
                        "score": 0.4020242910814089,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.40121257446602393,
                        "answer": "economonitor",
                        "hit": false
                    },
                    {
                        "score": 0.39353187534822764,
                        "answer": "scholarly",
                        "hit": false
                    },
                    {
                        "score": 0.3858091783277652,
                        "answer": "acumen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "netherlands"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6064204424619675
            },
            {
                "question verbose": "What is to nicaragua ",
                "b": "nicaragua",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.48809508447177147,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3190538035699454,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30261062963412044,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.300038289404584,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.29503325043578804,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28307092210094437,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nicaragua"
                ],
                "rank": 221,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6059737130999565
            },
            {
                "question verbose": "What is to norway ",
                "b": "norway",
                "expected answer": [
                    "norwegian",
                    "bokml",
                    "bokmal",
                    "nynorsk"
                ],
                "predictions": [
                    {
                        "score": 0.33580405493692256,
                        "answer": "downright",
                        "hit": false
                    },
                    {
                        "score": 0.31553973709244365,
                        "answer": "passing",
                        "hit": false
                    },
                    {
                        "score": 0.3057329907281176,
                        "answer": "gracefully",
                        "hit": false
                    },
                    {
                        "score": 0.29961176782987037,
                        "answer": "weatherization",
                        "hit": false
                    },
                    {
                        "score": 0.29855639760813396,
                        "answer": "amart",
                        "hit": false
                    },
                    {
                        "score": 0.2969434140044161,
                        "answer": "alarming",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "norway"
                ],
                "rank": 599,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8117541670799255
            },
            {
                "question verbose": "What is to palestine ",
                "b": "palestine",
                "expected answer": [
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.4736672285635763,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32038692792901546,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30762054740800115,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.30674658539229377,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.291507768781514,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2868825714792361,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "palestine"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to peru ",
                "b": "peru",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.4525554102065001,
                        "answer": "spanish",
                        "hit": true
                    },
                    {
                        "score": 0.43706628200595105,
                        "answer": "acumen",
                        "hit": false
                    },
                    {
                        "score": 0.4332239509307751,
                        "answer": "scholarly",
                        "hit": false
                    },
                    {
                        "score": 0.43262554683849197,
                        "answer": "shepherd",
                        "hit": false
                    },
                    {
                        "score": 0.4299392056891394,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.4282682303948709,
                        "answer": "specialist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "peru"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7504059970378876
            },
            {
                "question verbose": "What is to philippines ",
                "b": "philippines",
                "expected answer": [
                    "tagalog",
                    "filipino"
                ],
                "predictions": [
                    {
                        "score": 0.47408136022701275,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.307232730792388,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2964481023487455,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2954489697529427,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2887642254733108,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.270640191199826,
                        "answer": "osx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "philippines"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to switzerland ",
                "b": "switzerland",
                "expected answer": [
                    "german",
                    "french",
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.4763948832966627,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.40113329026602135,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.38578039937932646,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.3834817204400062,
                        "answer": "amart",
                        "hit": false
                    },
                    {
                        "score": 0.38028140822012924,
                        "answer": "darling",
                        "hit": false
                    },
                    {
                        "score": 0.3797793908154985,
                        "answer": "saddam",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "switzerland"
                ],
                "rank": 18,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7669264078140259
            },
            {
                "question verbose": "What is to syria ",
                "b": "syria",
                "expected answer": [
                    "arabic"
                ],
                "predictions": [
                    {
                        "score": 0.48968031236420845,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.4518867722576713,
                        "answer": "pledger",
                        "hit": false
                    },
                    {
                        "score": 0.44187738871195426,
                        "answer": "flourish",
                        "hit": false
                    },
                    {
                        "score": 0.4405929639202297,
                        "answer": "english",
                        "hit": false
                    },
                    {
                        "score": 0.4392105822753597,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.4371835842860088,
                        "answer": "sticker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "syria"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6557343304157257
            },
            {
                "question verbose": "What is to taiwan ",
                "b": "taiwan",
                "expected answer": [
                    "chinese"
                ],
                "predictions": [
                    {
                        "score": 0.483733472084488,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3224968970198072,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2906150788040228,
                        "answer": "sounded",
                        "hit": false
                    },
                    {
                        "score": 0.2850940491850106,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2808805329140652,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2682677043669058,
                        "answer": "devs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "taiwan"
                ],
                "rank": 590,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7123090624809265
            },
            {
                "question verbose": "What is to usa ",
                "b": "usa",
                "expected answer": [
                    "english"
                ],
                "predictions": [
                    {
                        "score": 0.39250097183158605,
                        "answer": "science",
                        "hit": false
                    },
                    {
                        "score": 0.38687864455479976,
                        "answer": "scholarly",
                        "hit": false
                    },
                    {
                        "score": 0.3728890986513138,
                        "answer": "acumen",
                        "hit": false
                    },
                    {
                        "score": 0.36513902450778246,
                        "answer": "fragmented",
                        "hit": false
                    },
                    {
                        "score": 0.36431670006112654,
                        "answer": "spanish",
                        "hit": false
                    },
                    {
                        "score": 0.3469816464249264,
                        "answer": "entertainment",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "usa"
                ],
                "rank": 317,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6280020028352737
            },
            {
                "question verbose": "What is to venezuela ",
                "b": "venezuela",
                "expected answer": [
                    "spanish"
                ],
                "predictions": [
                    {
                        "score": 0.33422348428753346,
                        "answer": "sticker",
                        "hit": false
                    },
                    {
                        "score": 0.33311134883340043,
                        "answer": "diluted",
                        "hit": false
                    },
                    {
                        "score": 0.32931045590392666,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.3267946204413605,
                        "answer": "wordpress",
                        "hit": false
                    },
                    {
                        "score": 0.3198547797743099,
                        "answer": "compose",
                        "hit": false
                    },
                    {
                        "score": 0.3144598698254914,
                        "answer": "albanian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "venezuela"
                ],
                "rank": 459,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6287335604429245
            }
        ],
        "result": {
            "cnt_questions_correct": 3,
            "cnt_questions_total": 50,
            "accuracy": 0.06
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E02 [country - language].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "564e57c9-b060-4264-b382-1b902772655b",
            "timestamp": "2020-10-22T15:57:16.859613"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to aberdeen ",
                "b": "aberdeen",
                "expected answer": [
                    "aberdeenshire"
                ],
                "predictions": [
                    {
                        "score": 0.5896702179297579,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1730187523753358,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16566955756379928,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16339603704388367,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16153655550664509,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15782576599640535,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aberdeen"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to bath ",
                "b": "bath",
                "expected answer": [
                    "somerset"
                ],
                "predictions": [
                    {
                        "score": 0.27926828117818886,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.2343009168146769,
                        "answer": "haigh",
                        "hit": false
                    },
                    {
                        "score": 0.22797527216514762,
                        "answer": "tasman",
                        "hit": false
                    },
                    {
                        "score": 0.2256377336053281,
                        "answer": "nirvanaah",
                        "hit": false
                    },
                    {
                        "score": 0.21780668194389508,
                        "answer": "dublin",
                        "hit": false
                    },
                    {
                        "score": 0.2157018432701642,
                        "answer": "mangare",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bath"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5966296494007111
            },
            {
                "question verbose": "What is to belfast ",
                "b": "belfast",
                "expected answer": [
                    "antrim"
                ],
                "predictions": [
                    {
                        "score": 0.5896065925564425,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.18073504278652347,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16779447236680695,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16669705005504828,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16402402760832327,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16047444465913424,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "belfast"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to birmingham ",
                "b": "birmingham",
                "expected answer": [
                    "midlands"
                ],
                "predictions": [
                    {
                        "score": 0.24835998542973062,
                        "answer": "west",
                        "hit": false
                    },
                    {
                        "score": 0.24488615323325255,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.23268827324632763,
                        "answer": "rose",
                        "hit": false
                    },
                    {
                        "score": 0.22918813132812663,
                        "answer": "virginia",
                        "hit": false
                    },
                    {
                        "score": 0.21244773566098618,
                        "answer": "japan",
                        "hit": false
                    },
                    {
                        "score": 0.20986325492413171,
                        "answer": "hokkaido",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "birmingham"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.552478302270174
            },
            {
                "question verbose": "What is to bradford ",
                "b": "bradford",
                "expected answer": [
                    "yorkshire"
                ],
                "predictions": [
                    {
                        "score": 0.29613377161883414,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.2534356712070628,
                        "answer": "pond",
                        "hit": false
                    },
                    {
                        "score": 0.2506971252470231,
                        "answer": "harding",
                        "hit": false
                    },
                    {
                        "score": 0.2434144503292777,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.24275996894012042,
                        "answer": "kent",
                        "hit": false
                    },
                    {
                        "score": 0.23709824611571878,
                        "answer": "amtrak",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bradford"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6290103793144226
            },
            {
                "question verbose": "What is to brighton ",
                "b": "brighton",
                "expected answer": [
                    "sussex"
                ],
                "predictions": [
                    {
                        "score": 0.5891746802209329,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16734090165136822,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1614433702867431,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.15671405735451746,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15528710070908702,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15525806317414054,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brighton"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cambridge ",
                "b": "cambridge",
                "expected answer": [
                    "cambridgeshire"
                ],
                "predictions": [
                    {
                        "score": 0.25436823227241384,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.2195358619639517,
                        "answer": "nationwide",
                        "hit": false
                    },
                    {
                        "score": 0.21547718768609955,
                        "answer": "pension",
                        "hit": false
                    },
                    {
                        "score": 0.21335464771716126,
                        "answer": "wisconsin",
                        "hit": false
                    },
                    {
                        "score": 0.20872436053790291,
                        "answer": "dublin",
                        "hit": false
                    },
                    {
                        "score": 0.20805699874939282,
                        "answer": "raided",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cambridge"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6404339969158173
            },
            {
                "question verbose": "What is to canterbury ",
                "b": "canterbury",
                "expected answer": [
                    "kent"
                ],
                "predictions": [
                    {
                        "score": 0.5931286070785677,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.160539882383046,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.14803689557621721,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1452541619216955,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.14316349829481353,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.13753919469370104,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "canterbury"
                ],
                "rank": 5185,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6264211535453796
            },
            {
                "question verbose": "What is to cardiff ",
                "b": "cardiff",
                "expected answer": [
                    "glamorgan"
                ],
                "predictions": [
                    {
                        "score": 0.58961207976982,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17085569158957029,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16520281474344625,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1627704651199652,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15804458807137975,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15686123885642148,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cardiff"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to carlisle ",
                "b": "carlisle",
                "expected answer": [
                    "cumbria"
                ],
                "predictions": [
                    {
                        "score": 0.5895965993160206,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17366499019534024,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.167851979010744,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.15878504132067536,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.15638501602537824,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.1557683299389773,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "carlisle"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to chester ",
                "b": "chester",
                "expected answer": [
                    "cheshire"
                ],
                "predictions": [
                    {
                        "score": 0.589034541591832,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16919780661173928,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16617459353863276,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16459156835126515,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.16404738300831564,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16136215997737963,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chester"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to chichester ",
                "b": "chichester",
                "expected answer": [
                    "sussex"
                ],
                "predictions": [
                    {
                        "score": 0.5895059952019613,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17003719173872817,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1668225630012831,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16410338039869096,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.161577401715786,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.15719659815498913,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chichester"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to coventry ",
                "b": "coventry",
                "expected answer": [
                    "midlands"
                ],
                "predictions": [
                    {
                        "score": 0.5888121998697677,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17021914090454762,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16390986338804464,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16343034201979417,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.15840927679226002,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15732649904271478,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "coventry"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to crawley ",
                "b": "crawley",
                "expected answer": [
                    "sussex"
                ],
                "predictions": [
                    {
                        "score": 0.589547967499842,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17238923522722602,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16974855039805056,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16408257681763438,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1615936894746761,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.1586743586577063,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crawley"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to derby ",
                "b": "derby",
                "expected answer": [
                    "derbyshire"
                ],
                "predictions": [
                    {
                        "score": 0.3336576098990178,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.27257830141499745,
                        "answer": "virginia",
                        "hit": false
                    },
                    {
                        "score": 0.2717847152221272,
                        "answer": "situated",
                        "hit": false
                    },
                    {
                        "score": 0.25854854701132635,
                        "answer": "gras",
                        "hit": false
                    },
                    {
                        "score": 0.2577118623967019,
                        "answer": "jersey",
                        "hit": false
                    },
                    {
                        "score": 0.2542349498696852,
                        "answer": "nirvanaah",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "derby"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5988671481609344
            },
            {
                "question verbose": "What is to dundee ",
                "b": "dundee",
                "expected answer": [
                    "lowlands"
                ],
                "predictions": [
                    {
                        "score": 0.5894577755418102,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17240550189319354,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16317561391391766,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16087033701342,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1556860530605651,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1555351673786048,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dundee"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to edinburgh ",
                "b": "edinburgh",
                "expected answer": [
                    "lowlands"
                ],
                "predictions": [
                    {
                        "score": 0.5891874457589134,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16942650448274632,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16569978405566319,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16401856621188757,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15393064403079798,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15373387646765577,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "edinburgh"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to ely ",
                "b": "ely",
                "expected answer": [
                    "cambridgeshire"
                ],
                "predictions": [
                    {
                        "score": 0.5897627819293425,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.169466661641585,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16649802704493538,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1609437337711261,
                        "answer": "fire",
                        "hit": false
                    },
                    {
                        "score": 0.1600985781384564,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15559121733909587,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ely"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to exeter ",
                "b": "exeter",
                "expected answer": [
                    "devon"
                ],
                "predictions": [
                    {
                        "score": 0.5894069503793121,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17447206016446692,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.17090266359247472,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1612910157557408,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.16047391514859674,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15963678630505382,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "exeter"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to glasgow ",
                "b": "glasgow",
                "expected answer": [
                    "lowlands"
                ],
                "predictions": [
                    {
                        "score": 0.5896228436730933,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16198393018721838,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16131408817395598,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.16014764755398564,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.159668787862436,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.15667244499539926,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "glasgow"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to gloucester ",
                "b": "gloucester",
                "expected answer": [
                    "gloucestershire"
                ],
                "predictions": [
                    {
                        "score": 0.5893852755781445,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16879750876815,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16738893226077492,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1657426546540014,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16468109422387564,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15222809908630602,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gloucester"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to hereford ",
                "b": "hereford",
                "expected answer": [
                    "herefordshire"
                ],
                "predictions": [
                    {
                        "score": 0.5891706591118613,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1673172448632101,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16606274048397426,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.15987885045596326,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15706547839893625,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15621450136672393,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hereford"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to hull ",
                "b": "hull",
                "expected answer": [
                    "yorkshire"
                ],
                "predictions": [
                    {
                        "score": 0.34935893115786093,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.290741247050722,
                        "answer": "nirvanaah",
                        "hit": false
                    },
                    {
                        "score": 0.27654937690268777,
                        "answer": "situated",
                        "hit": false
                    },
                    {
                        "score": 0.26683268523554127,
                        "answer": "tasman",
                        "hit": false
                    },
                    {
                        "score": 0.25727617367977007,
                        "answer": "gras",
                        "hit": false
                    },
                    {
                        "score": 0.2529771717583823,
                        "answer": "tenant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hull"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5974550843238831
            },
            {
                "question verbose": "What is to inverness ",
                "b": "inverness",
                "expected answer": [
                    "highlands"
                ],
                "predictions": [
                    {
                        "score": 0.5897481317641678,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1719889803629422,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16636216249888433,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1604031309861811,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15914608812141415,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15779278002649674,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inverness"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lancaster ",
                "b": "lancaster",
                "expected answer": [
                    "lancashire"
                ],
                "predictions": [
                    {
                        "score": 0.589372256081044,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16884772316088245,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1641284686597356,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16197216583964277,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1529754625804249,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15260646789237117,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lancaster"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to leeds ",
                "b": "leeds",
                "expected answer": [
                    "yorkshire"
                ],
                "predictions": [
                    {
                        "score": 0.5894750281236798,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1704132759379061,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16484155160891617,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16258666728087995,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1615720525993095,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15963488947505952,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leeds"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to leicester ",
                "b": "leicester",
                "expected answer": [
                    "midlands"
                ],
                "predictions": [
                    {
                        "score": 0.5890976595946164,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16404866776285218,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16248724049342922,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16011381085638682,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15912222523820588,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15595949907393178,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leicester"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lincoln ",
                "b": "lincoln",
                "expected answer": [
                    "lincolnshire"
                ],
                "predictions": [
                    {
                        "score": 0.2598827849022148,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.22293067341581854,
                        "answer": "west",
                        "hit": false
                    },
                    {
                        "score": 0.222882404571576,
                        "answer": "kent",
                        "hit": false
                    },
                    {
                        "score": 0.22068299525445972,
                        "answer": "virginia",
                        "hit": false
                    },
                    {
                        "score": 0.22029058306999583,
                        "answer": "tasman",
                        "hit": false
                    },
                    {
                        "score": 0.21992923868387182,
                        "answer": "anonymously",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lincoln"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6437819302082062
            },
            {
                "question verbose": "What is to liverpool ",
                "b": "liverpool",
                "expected answer": [
                    "lancashire"
                ],
                "predictions": [
                    {
                        "score": 0.5897600611545563,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17165325852074226,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.17068701313204507,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1665077709457265,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16538374741458847,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1585783763250143,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "liverpool"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to newcastle ",
                "b": "newcastle",
                "expected answer": [
                    "northumberland"
                ],
                "predictions": [
                    {
                        "score": 0.5898731732511174,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1761192712713347,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16970645082003435,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1663561713992313,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16602409741578036,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15793848315091757,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "newcastle"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to newport ",
                "b": "newport",
                "expected answer": [
                    "gwent"
                ],
                "predictions": [
                    {
                        "score": 0.5892080917278302,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16637808296201867,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16565585679005193,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16500947597055696,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16138586943874175,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.15671443964197687,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "newport"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to norwich ",
                "b": "norwich",
                "expected answer": [
                    "norfolk"
                ],
                "predictions": [
                    {
                        "score": 0.5891016848274158,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16702179186912405,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16604627978395886,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16157107140531277,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15699905799208597,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15406447523968403,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "norwich"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to nottingham ",
                "b": "nottingham",
                "expected answer": [
                    "nottinghamshire"
                ],
                "predictions": [
                    {
                        "score": 0.5891208887139269,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1724540622724015,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.17139021994681367,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.15822388046897903,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15629888113212623,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15395448571660617,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nottingham"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to oxford ",
                "b": "oxford",
                "expected answer": [
                    "oxfordshire"
                ],
                "predictions": [
                    {
                        "score": 0.31035341131840705,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.26880370591347447,
                        "answer": "construction",
                        "hit": false
                    },
                    {
                        "score": 0.2590493683498768,
                        "answer": "attorney",
                        "hit": false
                    },
                    {
                        "score": 0.24053151339446996,
                        "answer": "situated",
                        "hit": false
                    },
                    {
                        "score": 0.2397785894078289,
                        "answer": "freeman",
                        "hit": false
                    },
                    {
                        "score": 0.2391144370125326,
                        "answer": "nirvanaah",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "oxford"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5746492072939873
            },
            {
                "question verbose": "What is to plymouth ",
                "b": "plymouth",
                "expected answer": [
                    "devon"
                ],
                "predictions": [
                    {
                        "score": 0.5894228569428308,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17178925452602098,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16281563746180527,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.15723542918639713,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15382127649320745,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15146611792169445,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "plymouth"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to portsmouth ",
                "b": "portsmouth",
                "expected answer": [
                    "hampshire"
                ],
                "predictions": [
                    {
                        "score": 0.5944802738175716,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1688901069769062,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1608648734042129,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15888672586195238,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15750875922766713,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.14936688693069564,
                        "answer": "fire",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "portsmouth"
                ],
                "rank": 655,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613856591284275
            },
            {
                "question verbose": "What is to preston ",
                "b": "preston",
                "expected answer": [
                    "lancashire"
                ],
                "predictions": [
                    {
                        "score": 0.5890405442801085,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16095094174547972,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1603989980150091,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.1575905689554037,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15631130733585624,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.15325504967394804,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "preston"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to reading ",
                "b": "reading",
                "expected answer": [
                    "berkshire"
                ],
                "predictions": [
                    {
                        "score": 0.15956593688018508,
                        "answer": "recommend",
                        "hit": false
                    },
                    {
                        "score": 0.15231897333059838,
                        "answer": "magazine",
                        "hit": false
                    },
                    {
                        "score": 0.14891118957593072,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.14817886896172522,
                        "answer": "npc",
                        "hit": false
                    },
                    {
                        "score": 0.1445749738360218,
                        "answer": "strangely",
                        "hit": false
                    },
                    {
                        "score": 0.14359507567441412,
                        "answer": "spread",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reading"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5795866474509239
            },
            {
                "question verbose": "What is to salford ",
                "b": "salford",
                "expected answer": [
                    "manchester"
                ],
                "predictions": [
                    {
                        "score": 0.5894631854972551,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.18008700647647627,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.17200321693163217,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16477729433827726,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.163107811099773,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15939247024007278,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "salford"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to salisbury ",
                "b": "salisbury",
                "expected answer": [
                    "wiltshire"
                ],
                "predictions": [
                    {
                        "score": 0.5890704508313979,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16834944130104884,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1650765828376948,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16326898720482302,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16250097251500348,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.1584180044503763,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "salisbury"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to sheffield ",
                "b": "sheffield",
                "expected answer": [
                    "yorkshire"
                ],
                "predictions": [
                    {
                        "score": 0.5894561420142364,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1770239970185337,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16623938960734122,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16431123455527707,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.1639281530049339,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16005462706692725,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sheffield"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to southampton ",
                "b": "southampton",
                "expected answer": [
                    "hampshire"
                ],
                "predictions": [
                    {
                        "score": 0.23949564898706965,
                        "answer": "hampshire",
                        "hit": true
                    },
                    {
                        "score": 0.21991349514728115,
                        "answer": "nirvanaah",
                        "hit": false
                    },
                    {
                        "score": 0.20778638158225574,
                        "answer": "faculty",
                        "hit": false
                    },
                    {
                        "score": 0.20008965486102787,
                        "answer": "patriotism",
                        "hit": false
                    },
                    {
                        "score": 0.19929542663216548,
                        "answer": "harding",
                        "hit": false
                    },
                    {
                        "score": 0.1965162293801507,
                        "answer": "kent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "southampton"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8451017141342163
            },
            {
                "question verbose": "What is to stirling ",
                "b": "stirling",
                "expected answer": [
                    "stirlingshire"
                ],
                "predictions": [
                    {
                        "score": 0.5896198429659808,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1663284655945058,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16580458252757027,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16391505000998355,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16257689438258488,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16152883217467234,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stirling"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to swansea ",
                "b": "swansea",
                "expected answer": [
                    "glamorgan"
                ],
                "predictions": [
                    {
                        "score": 0.5896136555781898,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.17106128194264833,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1655078852015328,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16287516189103443,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16196564188720491,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16067061142637276,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "swansea"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to wakefield ",
                "b": "wakefield",
                "expected answer": [
                    "yorkshire"
                ],
                "predictions": [
                    {
                        "score": 0.5892031154417904,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.18145829328722438,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1672515870318953,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.1647314633758866,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1598162870165452,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15691057456513552,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wakefield"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to wells ",
                "b": "wells",
                "expected answer": [
                    "somerset"
                ],
                "predictions": [
                    {
                        "score": 0.5893762740311141,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16783518029029287,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16405918777571588,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16318066775033144,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1627159775896224,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.15894833985264692,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wells"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to winchester ",
                "b": "winchester",
                "expected answer": [
                    "hampshire"
                ],
                "predictions": [
                    {
                        "score": 0.5939030366813821,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16424897338424368,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15331949389188673,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.15298581717157586,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.15272344564396761,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.15212910960129358,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "winchester"
                ],
                "rank": 394,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613856591284275
            },
            {
                "question verbose": "What is to wolverhampton ",
                "b": "wolverhampton",
                "expected answer": [
                    "midlands"
                ],
                "predictions": [
                    {
                        "score": 0.5894007415248788,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.16645019160486726,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.16458866855284052,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16388350727423734,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16136753355671743,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.15708234488559047,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wolverhampton"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to worcester ",
                "b": "worcester",
                "expected answer": [
                    "worcestershire"
                ],
                "predictions": [
                    {
                        "score": 0.5893058595461449,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1706750077940489,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.16601352846081407,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16580061652142425,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1643248029652228,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16262031726472273,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "worcester"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to york ",
                "b": "york",
                "expected answer": [
                    "yorkshire"
                ],
                "predictions": [
                    {
                        "score": 0.38117920909161296,
                        "answer": "new",
                        "hit": false
                    },
                    {
                        "score": 0.33052380190112485,
                        "answer": "jersey",
                        "hit": false
                    },
                    {
                        "score": 0.32984420721881336,
                        "answer": "freeman",
                        "hit": false
                    },
                    {
                        "score": 0.32935496922771873,
                        "answer": "hampshire",
                        "hit": false
                    },
                    {
                        "score": 0.31407905446922163,
                        "answer": "zealand",
                        "hit": false
                    },
                    {
                        "score": 0.2917263732034362,
                        "answer": "nirvanaah",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "york"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5213629622012377
            }
        ],
        "result": {
            "cnt_questions_correct": 1,
            "cnt_questions_total": 50,
            "accuracy": 0.02
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E03 [UK_city - county].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "657179c9-f650-45bf-8700-9e5f3b2e3585",
            "timestamp": "2020-10-22T15:57:18.098100"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to aristotle ",
                "b": "aristotle",
                "expected answer": [
                    "greek"
                ],
                "predictions": [
                    {
                        "score": 0.28727491751904427,
                        "answer": "fur",
                        "hit": false
                    },
                    {
                        "score": 0.28240978969815234,
                        "answer": "coat",
                        "hit": false
                    },
                    {
                        "score": 0.24127259626470982,
                        "answer": "harris",
                        "hit": false
                    },
                    {
                        "score": 0.23327015965014508,
                        "answer": "scarf",
                        "hit": false
                    },
                    {
                        "score": 0.2307378591905029,
                        "answer": "look",
                        "hit": false
                    },
                    {
                        "score": 0.2294278801316619,
                        "answer": "sticker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aristotle"
                ],
                "rank": 5040,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5454309843480587
            },
            {
                "question verbose": "What is to balzac ",
                "b": "balzac",
                "expected answer": [
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.32045565246842117,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31710956918722466,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3054249454016278,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3018146293973898,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29894552990304224,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2988485718526634,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "balzac"
                ],
                "rank": 2479,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to beethoven ",
                "b": "beethoven",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.3266931281375028,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3079661540478288,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.304297152691902,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3036134923526034,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.30014856746297136,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29861137612872907,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beethoven"
                ],
                "rank": 2914,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to caesar ",
                "b": "caesar",
                "expected answer": [
                    "roman"
                ],
                "predictions": [
                    {
                        "score": 0.3198586874972753,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3097331249028338,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30644678080984106,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.29555063490627725,
                        "answer": "darling",
                        "hit": false
                    },
                    {
                        "score": 0.28978141983657774,
                        "answer": "unto",
                        "hit": false
                    },
                    {
                        "score": 0.2875734893983325,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "caesar"
                ],
                "rank": 4038,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6558662801980972
            },
            {
                "question verbose": "What is to confucius ",
                "b": "confucius",
                "expected answer": [
                    "chinese"
                ],
                "predictions": [
                    {
                        "score": 0.3244834279916144,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31046628257420994,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3021404953772051,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.3020130926374589,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29965823539433134,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29329840767650883,
                        "answer": "supporttime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "confucius"
                ],
                "rank": 14,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7123090624809265
            },
            {
                "question verbose": "What is to copernicus ",
                "b": "copernicus",
                "expected answer": [
                    "polish"
                ],
                "predictions": [
                    {
                        "score": 0.3092103222090899,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.3038147959475039,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.2917588676801231,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29080148401005756,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2883122549234611,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.28520843843066845,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "copernicus"
                ],
                "rank": 1742,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7017379552125931
            },
            {
                "question verbose": "What is to darwin ",
                "b": "darwin",
                "expected answer": [
                    "english",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.3230885227107493,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31403129789030587,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3116451893131007,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.3103043366809202,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2990133324696574,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2979548044380446,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "darwin"
                ],
                "rank": 720,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to depp ",
                "b": "depp",
                "expected answer": [
                    "american"
                ],
                "predictions": [
                    {
                        "score": 0.31344702315859757,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.30722749779804426,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.30504117607897624,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30469052687409554,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.299903396895611,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2966989047799766,
                        "answer": "complacent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "depp"
                ],
                "rank": 3038,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6018186509609222
            },
            {
                "question verbose": "What is to descartes ",
                "b": "descartes",
                "expected answer": [
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.3108420163430039,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31008060690397443,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30350061554160745,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.3005470944243382,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.29895612622891743,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29838891264014933,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "descartes"
                ],
                "rank": 2334,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to dickens ",
                "b": "dickens",
                "expected answer": [
                    "english",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.31944282964208426,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3100030335544895,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3025419580238947,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.30231841470397514,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2987304821252891,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2964275913226782,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dickens"
                ],
                "rank": 559,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to dostoyevsky ",
                "b": "dostoyevsky",
                "expected answer": [
                    "russian"
                ],
                "predictions": [
                    {
                        "score": 0.30978169290109137,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3047951390107511,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.30028920381478025,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2936813922439572,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29145682981942,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28579735446825744,
                        "answer": "improvedrealized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dostoyevsky"
                ],
                "rank": 395,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6430940479040146
            },
            {
                "question verbose": "What is to edison ",
                "b": "edison",
                "expected answer": [
                    "american"
                ],
                "predictions": [
                    {
                        "score": 0.32758589145960015,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31282256851885354,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29817096441558233,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2981011592762777,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29680350703672154,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28987858592197385,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "edison"
                ],
                "rank": 3090,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6018186509609222
            },
            {
                "question verbose": "What is to einstein ",
                "b": "einstein",
                "expected answer": [
                    "jewish",
                    "german",
                    "american"
                ],
                "predictions": [
                    {
                        "score": 0.3171561800520055,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3079370925414741,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.30284165211074937,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29768385523823004,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29212130378515544,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.28997046620819183,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "einstein"
                ],
                "rank": 2261,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6417155861854553
            },
            {
                "question verbose": "What is to euclid ",
                "b": "euclid",
                "expected answer": [
                    "greek"
                ],
                "predictions": [
                    {
                        "score": 0.3122915881057696,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3091564452542634,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30782187036143227,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.3047460488332793,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2944059558700303,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2916552878167582,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "euclid"
                ],
                "rank": 74,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6984416842460632
            },
            {
                "question verbose": "What is to fermi ",
                "b": "fermi",
                "expected answer": [
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.31169362876940615,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3083240763665076,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29684377571741655,
                        "answer": "spectrum",
                        "hit": false
                    },
                    {
                        "score": 0.2954150213486525,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.29358652684135683,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2910947246640122,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fermi"
                ],
                "rank": 765,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6167254149913788
            },
            {
                "question verbose": "What is to galilei ",
                "b": "galilei",
                "expected answer": [
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.3271671675028195,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3112962419937614,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.3092733968669876,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3061487293951921,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.296680318793098,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2917384169299084,
                        "answer": "complacent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "galilei"
                ],
                "rank": 916,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6167254149913788
            },
            {
                "question verbose": "What is to gorbachev ",
                "b": "gorbachev",
                "expected answer": [
                    "soviet",
                    "russian"
                ],
                "predictions": [
                    {
                        "score": 0.31449799155508584,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.3128720039533458,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3055298300239629,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.30381499992184513,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30303561422787995,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.29953860190082665,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gorbachev"
                ],
                "rank": 270,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6450235098600388
            },
            {
                "question verbose": "What is to hawking ",
                "b": "hawking",
                "expected answer": [
                    "english",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.306111731786277,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.2965565955006411,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.29485273051237637,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29272562675722613,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29262886388829873,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.29234248182609623,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hawking"
                ],
                "rank": 477,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to hegel ",
                "b": "hegel",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.32836334353878804,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31430513587056436,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.30638289621063797,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.302246960149838,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2971178958545585,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29467509485319543,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hegel"
                ],
                "rank": 3042,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to hitler ",
                "b": "hitler",
                "expected answer": [
                    "german",
                    "austrian"
                ],
                "predictions": [
                    {
                        "score": 0.5779907712796986,
                        "answer": "mumblycore",
                        "hit": false
                    },
                    {
                        "score": 0.5563144315041375,
                        "answer": "clunky",
                        "hit": false
                    },
                    {
                        "score": 0.5544898310970948,
                        "answer": "defends",
                        "hit": false
                    },
                    {
                        "score": 0.5460019425010436,
                        "answer": "kniw",
                        "hit": false
                    },
                    {
                        "score": 0.5433223046648835,
                        "answer": "thru",
                        "hit": false
                    },
                    {
                        "score": 0.5415075874292556,
                        "answer": "redemptive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hitler"
                ],
                "rank": 75,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7778889238834381
            },
            {
                "question verbose": "What is to homer ",
                "b": "homer",
                "expected answer": [
                    "greek"
                ],
                "predictions": [
                    {
                        "score": 0.5897555115016097,
                        "answer": "juicer",
                        "hit": false
                    },
                    {
                        "score": 0.5596896710564776,
                        "answer": "attire",
                        "hit": false
                    },
                    {
                        "score": 0.5491162458032387,
                        "answer": "steering",
                        "hit": false
                    },
                    {
                        "score": 0.5488295206528019,
                        "answer": "confuses",
                        "hit": false
                    },
                    {
                        "score": 0.5479430190336942,
                        "answer": "tut",
                        "hit": false
                    },
                    {
                        "score": 0.5474859158511243,
                        "answer": "sperling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "homer"
                ],
                "rank": 855,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.822544515132904
            },
            {
                "question verbose": "What is to hume ",
                "b": "hume",
                "expected answer": [
                    "scottish",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.3176699883245605,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3076918483371515,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3072726230089216,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.300826288667196,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29722516915896513,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2963946987215261,
                        "answer": "supporttime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hume"
                ],
                "rank": 2458,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6291954964399338
            },
            {
                "question verbose": "What is to jolie ",
                "b": "jolie",
                "expected answer": [
                    "american"
                ],
                "predictions": [
                    {
                        "score": 0.32182870428868515,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.30525156865638814,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30322152737947766,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29975376471950016,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2942180344224017,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2927531692049209,
                        "answer": "complacent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jolie"
                ],
                "rank": 3081,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6018186509609222
            },
            {
                "question verbose": "What is to kant ",
                "b": "kant",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.31360299345974235,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3101665480999037,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.308219996716168,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.30239262544042345,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.29875226067253086,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29688060572801367,
                        "answer": "unto",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kant"
                ],
                "rank": 3353,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to kepler ",
                "b": "kepler",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.3222267516747395,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.31580730191978257,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31468282008289,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3064820204493686,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.3055443273711497,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.30037385029096647,
                        "answer": "supporttime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kepler"
                ],
                "rank": 3034,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to lavoisier ",
                "b": "lavoisier",
                "expected answer": [
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.3261039544341094,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3101095543987912,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.3091042142990626,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.30213249145414606,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.3019145496323951,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29548334829773026,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lavoisier"
                ],
                "rank": 2282,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to leibniz ",
                "b": "leibniz",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.3114686037889118,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.30823362970399476,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3014101571741238,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3005807448847673,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2934588521067917,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2931622358536804,
                        "answer": "supporttime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leibniz"
                ],
                "rank": 3140,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to lenin ",
                "b": "lenin",
                "expected answer": [
                    "soviet",
                    "russian"
                ],
                "predictions": [
                    {
                        "score": 0.5527845721393574,
                        "answer": "rallied",
                        "hit": false
                    },
                    {
                        "score": 0.5511427036369073,
                        "answer": "lunatice",
                        "hit": false
                    },
                    {
                        "score": 0.5489747386965311,
                        "answer": "putnam",
                        "hit": false
                    },
                    {
                        "score": 0.5408546674464179,
                        "answer": "german",
                        "hit": false
                    },
                    {
                        "score": 0.5392267232224235,
                        "answer": "ranting",
                        "hit": false
                    },
                    {
                        "score": 0.5291016795353697,
                        "answer": "irreversible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lenin"
                ],
                "rank": 7582,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7216186374425888
            },
            {
                "question verbose": "What is to lennon ",
                "b": "lennon",
                "expected answer": [
                    "english",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.32117049654494156,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3080074030125652,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3010344832339379,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2995600465834981,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2951774533155292,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.29202537874113926,
                        "answer": "darling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lennon"
                ],
                "rank": 554,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to lincoln ",
                "b": "lincoln",
                "expected answer": [
                    "american"
                ],
                "predictions": [
                    {
                        "score": 0.5793705242090129,
                        "answer": "dole",
                        "hit": false
                    },
                    {
                        "score": 0.5654298250355126,
                        "answer": "steering",
                        "hit": false
                    },
                    {
                        "score": 0.543089585581712,
                        "answer": "assassin",
                        "hit": false
                    },
                    {
                        "score": 0.5423008696020769,
                        "answer": "supplant",
                        "hit": false
                    },
                    {
                        "score": 0.5287095663033629,
                        "answer": "oceanic",
                        "hit": false
                    },
                    {
                        "score": 0.5265990683128247,
                        "answer": "acwrimo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lincoln"
                ],
                "rank": 7676,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6840154677629471
            },
            {
                "question verbose": "What is to locke ",
                "b": "locke",
                "expected answer": [
                    "english",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.3155473951306801,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31179967865089575,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.30657014996678783,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.30593733180167976,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3054375562891041,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29706146285475216,
                        "answer": "darling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locke"
                ],
                "rank": 596,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6158894002437592
            },
            {
                "question verbose": "What is to machiavelli ",
                "b": "machiavelli",
                "expected answer": [
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.31944160040594005,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.30663291462936254,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3051610474858265,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.3005926487407815,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2970715092540656,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29198908899368897,
                        "answer": "complacent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "machiavelli"
                ],
                "rank": 772,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6167254149913788
            },
            {
                "question verbose": "What is to marx ",
                "b": "marx",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.5267955000815414,
                        "answer": "hostess",
                        "hit": false
                    },
                    {
                        "score": 0.5233164490731625,
                        "answer": "ranting",
                        "hit": false
                    },
                    {
                        "score": 0.5212410455638765,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.5142447165822328,
                        "answer": "resentment",
                        "hit": false
                    },
                    {
                        "score": 0.5127693381427192,
                        "answer": "lunatice",
                        "hit": false
                    },
                    {
                        "score": 0.5113498135974888,
                        "answer": "translated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marx"
                ],
                "rank": 6,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7845815420150757
            },
            {
                "question verbose": "What is to maxwell ",
                "b": "maxwell",
                "expected answer": [
                    "scottish",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.3140369944179355,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.30690351547999867,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.301783796350515,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3017764091361956,
                        "answer": "spectrum",
                        "hit": false
                    },
                    {
                        "score": 0.3006472397195876,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.2953169459872861,
                        "answer": "improvedrealized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "maxwell"
                ],
                "rank": 2214,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6291954964399338
            },
            {
                "question verbose": "What is to mencius ",
                "b": "mencius",
                "expected answer": [
                    "chinese"
                ],
                "predictions": [
                    {
                        "score": 0.3150525517685042,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3127001452298245,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29887150258482287,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.29761953284884624,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2955389256421486,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.28828448707421434,
                        "answer": "unto",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mencius"
                ],
                "rank": 108,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7123090624809265
            },
            {
                "question verbose": "What is to michelangelo ",
                "b": "michelangelo",
                "expected answer": [
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.31903252032918744,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3057108686874708,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.3032954433698139,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.30158954719390585,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29607013703752066,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.29572061098565217,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "michelangelo"
                ],
                "rank": 808,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6167254149913788
            },
            {
                "question verbose": "What is to mozart ",
                "b": "mozart",
                "expected answer": [
                    "german",
                    "austrian"
                ],
                "predictions": [
                    {
                        "score": 0.3200787068319211,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.31874023148234015,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.31059542999572853,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.3082370140966278,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3068490591287857,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29682952752133995,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mozart"
                ],
                "rank": 3174,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930293649435043
            },
            {
                "question verbose": "What is to napoleon ",
                "b": "napoleon",
                "expected answer": [
                    "french",
                    "corsican",
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.3083404735949831,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3059942137961656,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.3023615852888542,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2960797708021105,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2956714637419812,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2935534558991368,
                        "answer": "winnable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "napoleon"
                ],
                "rank": 706,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to newton ",
                "b": "newton",
                "expected answer": [
                    "english",
                    "british"
                ],
                "predictions": [
                    {
                        "score": 0.5647618612540423,
                        "answer": "lifelong",
                        "hit": false
                    },
                    {
                        "score": 0.5604223236443769,
                        "answer": "steering",
                        "hit": false
                    },
                    {
                        "score": 0.5534313967521223,
                        "answer": "spa",
                        "hit": false
                    },
                    {
                        "score": 0.5322450127296054,
                        "answer": "assassin",
                        "hit": false
                    },
                    {
                        "score": 0.5313849147693116,
                        "answer": "rauber",
                        "hit": false
                    },
                    {
                        "score": 0.5307779781723017,
                        "answer": "vault",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "newton"
                ],
                "rank": 9056,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6289778649806976
            },
            {
                "question verbose": "What is to pascal ",
                "b": "pascal",
                "expected answer": [
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.313704284606313,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3117696568349066,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3089858121111583,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.3054858889632655,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2999601983961111,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29466980992589015,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pascal"
                ],
                "rank": 2391,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to plato ",
                "b": "plato",
                "expected answer": [
                    "greek"
                ],
                "predictions": [
                    {
                        "score": 0.32013626656376315,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3186533986052496,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3067824573030421,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29903726277046266,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.29885854552313945,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.29522631846716657,
                        "answer": "exagerate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "plato"
                ],
                "rank": 60,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6984416842460632
            },
            {
                "question verbose": "What is to raphael ",
                "b": "raphael",
                "expected answer": [
                    "italian"
                ],
                "predictions": [
                    {
                        "score": 0.3104138474259306,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3045829217701914,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30292884233986433,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.3018939017561477,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.29936457269130295,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.29192351879886164,
                        "answer": "hungry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "raphael"
                ],
                "rank": 819,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6167254149913788
            },
            {
                "question verbose": "What is to rousseau ",
                "b": "rousseau",
                "expected answer": [
                    "french"
                ],
                "predictions": [
                    {
                        "score": 0.32057360440260835,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3070629040895484,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30498328208205816,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.3031739918718453,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.30305544681708874,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.3022577229192512,
                        "answer": "improvedrealized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rousseau"
                ],
                "rank": 2597,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936722308397293
            },
            {
                "question verbose": "What is to spinoza ",
                "b": "spinoza",
                "expected answer": [
                    "dutch"
                ],
                "predictions": [
                    {
                        "score": 0.3262006244259634,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3054646050540463,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30465493164258406,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.3011530019073586,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.2916836673146987,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2910298281222947,
                        "answer": "darling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spinoza"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to stalin ",
                "b": "stalin",
                "expected answer": [
                    "soviet",
                    "georgian"
                ],
                "predictions": [
                    {
                        "score": 0.5803931174356134,
                        "answer": "enodrse",
                        "hit": false
                    },
                    {
                        "score": 0.5718536224133371,
                        "answer": "hyperbolic",
                        "hit": false
                    },
                    {
                        "score": 0.5622931482927871,
                        "answer": "mumblycore",
                        "hit": false
                    },
                    {
                        "score": 0.5542910567213292,
                        "answer": "dogfighting",
                        "hit": false
                    },
                    {
                        "score": 0.552745958138275,
                        "answer": "sheep",
                        "hit": false
                    },
                    {
                        "score": 0.5516020610215083,
                        "answer": "spiel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stalin"
                ],
                "rank": 2248,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8009912669658661
            },
            {
                "question verbose": "What is to strauss ",
                "b": "strauss",
                "expected answer": [
                    "austrian"
                ],
                "predictions": [
                    {
                        "score": 0.31261970516753473,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3100756825807777,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.3017302101656851,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.3002387837143156,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2938077951034617,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2906856358881491,
                        "answer": "peaceloving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strauss"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to tchaikovsky ",
                "b": "tchaikovsky",
                "expected answer": [
                    "russian"
                ],
                "predictions": [
                    {
                        "score": 0.31581759655233377,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3131838436992119,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.310856874523339,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.3094002712737285,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.30007188045729644,
                        "answer": "sc",
                        "hit": false
                    },
                    {
                        "score": 0.29742350700303855,
                        "answer": "complacent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tchaikovsky"
                ],
                "rank": 585,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6430940479040146
            },
            {
                "question verbose": "What is to tolstoi ",
                "b": "tolstoi",
                "expected answer": [
                    "russian"
                ],
                "predictions": [
                    {
                        "score": 0.31208208577913443,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.30973473647923766,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.30509251108441415,
                        "answer": "exagerate",
                        "hit": false
                    },
                    {
                        "score": 0.3026887331559934,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.2909581699723852,
                        "answer": "starved",
                        "hit": false
                    },
                    {
                        "score": 0.28949035805619994,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tolstoi"
                ],
                "rank": 445,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6430940479040146
            },
            {
                "question verbose": "What is to truman ",
                "b": "truman",
                "expected answer": [
                    "american"
                ],
                "predictions": [
                    {
                        "score": 0.3186836085561954,
                        "answer": "mahmoud",
                        "hit": false
                    },
                    {
                        "score": 0.3002917671149827,
                        "answer": "complacent",
                        "hit": false
                    },
                    {
                        "score": 0.29981853318544555,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2914944157304115,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.29062929880408983,
                        "answer": "peaceloving",
                        "hit": false
                    },
                    {
                        "score": 0.290303216008525,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "truman"
                ],
                "rank": 2849,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6018186509609222
            },
            {
                "question verbose": "What is to wagner ",
                "b": "wagner",
                "expected answer": [
                    "german"
                ],
                "predictions": [
                    {
                        "score": 0.5653677894239459,
                        "answer": "karl",
                        "hit": false
                    },
                    {
                        "score": 0.5526476529346332,
                        "answer": "parkway",
                        "hit": false
                    },
                    {
                        "score": 0.5397014342579618,
                        "answer": "solver",
                        "hit": false
                    },
                    {
                        "score": 0.5359429834101792,
                        "answer": "suspense",
                        "hit": false
                    },
                    {
                        "score": 0.527412669201298,
                        "answer": "gras",
                        "hit": false
                    },
                    {
                        "score": 0.524230771724314,
                        "answer": "donald",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wagner"
                ],
                "rank": 439,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7312005460262299
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E04 [name - nationality].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "55bec808-772a-4186-8335-1a3ba1ffa9b7",
            "timestamp": "2020-10-22T15:57:20.118022"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to andersen ",
                "b": "andersen",
                "expected answer": [
                    "writer",
                    "poet",
                    "author"
                ],
                "predictions": [
                    {
                        "score": 0.33886633721177417,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2986226782036016,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2973735533413107,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28628190532302994,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28291802798407606,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28237296209029816,
                        "answer": "attacker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "andersen"
                ],
                "rank": 13895,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5106465211138129
            },
            {
                "question verbose": "What is to aristotle ",
                "b": "aristotle",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.2669655937067848,
                        "answer": "look",
                        "hit": false
                    },
                    {
                        "score": 0.21318043246091123,
                        "answer": "coat",
                        "hit": false
                    },
                    {
                        "score": 0.21097232694947504,
                        "answer": "bh",
                        "hit": false
                    },
                    {
                        "score": 0.21082127944310328,
                        "answer": "harris",
                        "hit": false
                    },
                    {
                        "score": 0.20904553266064604,
                        "answer": "lead",
                        "hit": false
                    },
                    {
                        "score": 0.20030428736129113,
                        "answer": "witnessed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aristotle"
                ],
                "rank": 500,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5736198425292969
            },
            {
                "question verbose": "What is to balzac ",
                "b": "balzac",
                "expected answer": [
                    "novelist",
                    "writer"
                ],
                "predictions": [
                    {
                        "score": 0.3239746068562029,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3045111557364651,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29221507915077394,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.29148948718755463,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28717335308461733,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2796981172311992,
                        "answer": "circulated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "balzac"
                ],
                "rank": 15120,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to beethoven ",
                "b": "beethoven",
                "expected answer": [
                    "composer"
                ],
                "predictions": [
                    {
                        "score": 0.33783939990237455,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3162640598501283,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2883765950869506,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2873184028268289,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.279457778630004,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.2777670671890793,
                        "answer": "tacking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beethoven"
                ],
                "rank": 1749,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6154861226677895
            },
            {
                "question verbose": "What is to caesar ",
                "b": "caesar",
                "expected answer": [
                    "emperor",
                    "commander",
                    "leader"
                ],
                "predictions": [
                    {
                        "score": 0.32334928864105006,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3117766601934123,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.30174777615478754,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2972828606103923,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.27974532123009493,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.279291278969699,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "caesar"
                ],
                "rank": 987,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to columbus ",
                "b": "columbus",
                "expected answer": [
                    "explorer"
                ],
                "predictions": [
                    {
                        "score": 0.3366001824147352,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29484040261479294,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2908446185151985,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.29021072658747765,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.284803716141881,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.284647381837413,
                        "answer": "noticeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "columbus"
                ],
                "rank": 4577,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6419088989496231
            },
            {
                "question verbose": "What is to confucius ",
                "b": "confucius",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.33618550953713355,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2878532089386453,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28662452132183636,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.28041615019959965,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2803942301598805,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2736471587852304,
                        "answer": "noticeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "confucius"
                ],
                "rank": 40,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to dante ",
                "b": "dante",
                "expected answer": [
                    "poet"
                ],
                "predictions": [
                    {
                        "score": 0.3229018087595183,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3122768752599333,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28655676724783063,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.286498484633242,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2822834420444357,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28213381085388956,
                        "answer": "bait",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dante"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to darwin ",
                "b": "darwin",
                "expected answer": [
                    "naturalist",
                    "biologist",
                    "geologist"
                ],
                "predictions": [
                    {
                        "score": 0.3376221479855429,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30336787282024513,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2932628502517007,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2886083346993525,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28687713573323137,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2803152135356815,
                        "answer": "attacker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "darwin"
                ],
                "rank": 3374,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6418808549642563
            },
            {
                "question verbose": "What is to depp ",
                "b": "depp",
                "expected answer": [
                    "actor",
                    "producer",
                    "musician"
                ],
                "predictions": [
                    {
                        "score": 0.33527206376781493,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2957812671131532,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2829871295187243,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.27885763971882765,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.2765765100977788,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2740816721835344,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "depp"
                ],
                "rank": 1826,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613400973379612
            },
            {
                "question verbose": "What is to descartes ",
                "b": "descartes",
                "expected answer": [
                    "mathematician",
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.32331548384316183,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31451005870234,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.30441851275330556,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2903500843381338,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2878463434677131,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28709060219894034,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "descartes"
                ],
                "rank": 52,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dickens ",
                "b": "dickens",
                "expected answer": [
                    "novelist",
                    "writer",
                    "critic",
                    "author"
                ],
                "predictions": [
                    {
                        "score": 0.32303416081802755,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30795789926818257,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2913049741705151,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.28902552404127724,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28681240832489235,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2742028674074734,
                        "answer": "palpable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dickens"
                ],
                "rank": 3037,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to edison ",
                "b": "edison",
                "expected answer": [
                    "inventor",
                    "businessman"
                ],
                "predictions": [
                    {
                        "score": 0.3372300466936851,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2996273383821556,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.29219590905021964,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28884859678438196,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.27536491037694866,
                        "answer": "tacking",
                        "hit": false
                    },
                    {
                        "score": 0.2735463806279196,
                        "answer": "calmly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "edison"
                ],
                "rank": 6988,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.594123013317585
            },
            {
                "question verbose": "What is to einstein ",
                "b": "einstein",
                "expected answer": [
                    "physicist",
                    "scientist"
                ],
                "predictions": [
                    {
                        "score": 0.3239804187420749,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3054259035899233,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29234670364245097,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2894770908739269,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.28159037319111757,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2796145521882432,
                        "answer": "odark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "einstein"
                ],
                "rank": 3983,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to euler ",
                "b": "euler",
                "expected answer": [
                    "mathematician",
                    "physicist",
                    "astronomer",
                    "logician",
                    "engineer"
                ],
                "predictions": [
                    {
                        "score": 0.32097962954935816,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31559028158241564,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29870189513168105,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2861235170489356,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2857776488378023,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.28494853732812336,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "euler"
                ],
                "rank": 13148,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to goethe ",
                "b": "goethe",
                "expected answer": [
                    "poet",
                    "playwright",
                    "novelist",
                    "writer",
                    "author"
                ],
                "predictions": [
                    {
                        "score": 0.32234166624367244,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30594109761410354,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.30109315994408337,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28709120019766354,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2853442322220115,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2852798317546743,
                        "answer": "odark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goethe"
                ],
                "rank": 13930,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to hawking ",
                "b": "hawking",
                "expected answer": [
                    "physicist",
                    "scientist"
                ],
                "predictions": [
                    {
                        "score": 0.32296582493995873,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2982165013388461,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29064099059317205,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.288035677504322,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28265554521027947,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.2818546233652874,
                        "answer": "bait",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hawking"
                ],
                "rank": 3356,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to haydn ",
                "b": "haydn",
                "expected answer": [
                    "composer"
                ],
                "predictions": [
                    {
                        "score": 0.33725387362531384,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30827644059866355,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29217888960021654,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2881956178259482,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2798053951142721,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.27917194984464905,
                        "answer": "circulated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "haydn"
                ],
                "rank": 1896,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6154861226677895
            },
            {
                "question verbose": "What is to hegel ",
                "b": "hegel",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.33795829834460167,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29973431800974504,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29552281187198914,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.2871921718069677,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28259147295699116,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28140649111667665,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hegel"
                ],
                "rank": 51,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to hitler ",
                "b": "hitler",
                "expected answer": [
                    "dictator",
                    "politician",
                    "nazi"
                ],
                "predictions": [
                    {
                        "score": 0.5824415473388173,
                        "answer": "philosopher",
                        "hit": false
                    },
                    {
                        "score": 0.5376859628912574,
                        "answer": "vagary",
                        "hit": false
                    },
                    {
                        "score": 0.525656005248532,
                        "answer": "sauce",
                        "hit": false
                    },
                    {
                        "score": 0.508803090515832,
                        "answer": "outer",
                        "hit": false
                    },
                    {
                        "score": 0.5071441624445987,
                        "answer": "calorie",
                        "hit": false
                    },
                    {
                        "score": 0.49868499294503593,
                        "answer": "grilled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hitler"
                ],
                "rank": 11,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8825590908527374
            },
            {
                "question verbose": "What is to hume ",
                "b": "hume",
                "expected answer": [
                    "philosopher",
                    "politician"
                ],
                "predictions": [
                    {
                        "score": 0.33751768932518705,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29959715317092483,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2929102327390015,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2829990530496716,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.27904872875008563,
                        "answer": "sociopath",
                        "hit": false
                    },
                    {
                        "score": 0.2788328550449401,
                        "answer": "palpable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hume"
                ],
                "rank": 41,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to jolie ",
                "b": "jolie",
                "expected answer": [
                    "actress",
                    "filmmaker",
                    "director",
                    "humanitarian",
                    "activist"
                ],
                "predictions": [
                    {
                        "score": 0.3370610327461351,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29525393983725684,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28773537316519315,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.27448692818586046,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2717521685423689,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.2700808969050531,
                        "answer": "owed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jolie"
                ],
                "rank": 1290,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6844665110111237
            },
            {
                "question verbose": "What is to kant ",
                "b": "kant",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.337177683585506,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3021604991434955,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29655739248919255,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.29514097153709423,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.29166441321362935,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28754598189336655,
                        "answer": "noticeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kant"
                ],
                "rank": 41,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to kepler ",
                "b": "kepler",
                "expected answer": [
                    "mathematician",
                    "physicist",
                    "astronomer",
                    "astrologer"
                ],
                "predictions": [
                    {
                        "score": 0.3235706484945495,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2985775094775355,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28757281497954806,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2863734743331694,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2755149026511357,
                        "answer": "palpable",
                        "hit": false
                    },
                    {
                        "score": 0.27157863627899737,
                        "answer": "bait",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kepler"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lincoln ",
                "b": "lincoln",
                "expected answer": [
                    "president"
                ],
                "predictions": [
                    {
                        "score": 0.49637662580420105,
                        "answer": "residing",
                        "hit": false
                    },
                    {
                        "score": 0.4927235354009353,
                        "answer": "oldman",
                        "hit": false
                    },
                    {
                        "score": 0.4917539082430095,
                        "answer": "seiu",
                        "hit": false
                    },
                    {
                        "score": 0.4912548958529753,
                        "answer": "cortez",
                        "hit": false
                    },
                    {
                        "score": 0.49079682897598903,
                        "answer": "antarctic",
                        "hit": false
                    },
                    {
                        "score": 0.48937725155146644,
                        "answer": "infringement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lincoln"
                ],
                "rank": 13949,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6242702305316925
            },
            {
                "question verbose": "What is to locke ",
                "b": "locke",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.338937980750644,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29819632927431305,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28826661428228695,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28557613085128697,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28401040475011935,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2812118826529018,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locke"
                ],
                "rank": 36,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to marx ",
                "b": "marx",
                "expected answer": [
                    "philosopher",
                    "communist"
                ],
                "predictions": [
                    {
                        "score": 0.5031393548593812,
                        "answer": "heterosexual",
                        "hit": false
                    },
                    {
                        "score": 0.4953889913705121,
                        "answer": "appropriately",
                        "hit": false
                    },
                    {
                        "score": 0.49323860930676433,
                        "answer": "painter",
                        "hit": false
                    },
                    {
                        "score": 0.48611641701062075,
                        "answer": "reuse",
                        "hit": false
                    },
                    {
                        "score": 0.4830162529474644,
                        "answer": "trotted",
                        "hit": false
                    },
                    {
                        "score": 0.4829549430941622,
                        "answer": "nemesis",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marx"
                ],
                "rank": 6,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8027442395687103
            },
            {
                "question verbose": "What is to maxwell ",
                "b": "maxwell",
                "expected answer": [
                    "physicist",
                    "scientist"
                ],
                "predictions": [
                    {
                        "score": 0.32278090305375634,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30896668600144006,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.30637516561911554,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.30052553232987017,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28654847641883274,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.28398072439691935,
                        "answer": "erez",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "maxwell"
                ],
                "rank": 3697,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to mencius ",
                "b": "mencius",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.33717585281942725,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2869725138860588,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2824509709295792,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2824391179256166,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2810002394943707,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.279177942934811,
                        "answer": "noticeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mencius"
                ],
                "rank": 42,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to michelangelo ",
                "b": "michelangelo",
                "expected answer": [
                    "sculptor",
                    "painter",
                    "architect",
                    "artist",
                    "poet",
                    "engineer"
                ],
                "predictions": [
                    {
                        "score": 0.33844613581586336,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30737658876581403,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2919515880839361,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.29157802516838816,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2844854417859838,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2787234116222561,
                        "answer": "bait",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "michelangelo"
                ],
                "rank": 1246,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5885011702775955
            },
            {
                "question verbose": "What is to moses ",
                "b": "moses",
                "expected answer": [
                    "prophet",
                    "leader"
                ],
                "predictions": [
                    {
                        "score": 0.5871226742155191,
                        "answer": "philosopher",
                        "hit": false
                    },
                    {
                        "score": 0.5511799178228477,
                        "answer": "illustrated",
                        "hit": false
                    },
                    {
                        "score": 0.5485528132412937,
                        "answer": "vagary",
                        "hit": false
                    },
                    {
                        "score": 0.5462353330104265,
                        "answer": "lunatice",
                        "hit": false
                    },
                    {
                        "score": 0.5245807974190241,
                        "answer": "eleanor",
                        "hit": false
                    },
                    {
                        "score": 0.5178778254695945,
                        "answer": "negating",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "moses"
                ],
                "rank": 12188,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7514646351337433
            },
            {
                "question verbose": "What is to mozart ",
                "b": "mozart",
                "expected answer": [
                    "composer"
                ],
                "predictions": [
                    {
                        "score": 0.33647963176897444,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3038196211522802,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.29938030421269474,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2919993013589958,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2895978214212654,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28667756049381815,
                        "answer": "bait",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mozart"
                ],
                "rank": 2016,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6154861226677895
            },
            {
                "question verbose": "What is to napoleon ",
                "b": "napoleon",
                "expected answer": [
                    "emperor",
                    "leader",
                    "politician",
                    "commander"
                ],
                "predictions": [
                    {
                        "score": 0.3222330553892287,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2986252580939745,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29074957616362973,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28767995321894924,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.28457958363584246,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2814788646312395,
                        "answer": "erez",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "napoleon"
                ],
                "rank": 1251,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to newton ",
                "b": "newton",
                "expected answer": [
                    "scientist",
                    "mathematician",
                    "psysicist",
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.524004918134251,
                        "answer": "laird",
                        "hit": false
                    },
                    {
                        "score": 0.5197090237702588,
                        "answer": "oldman",
                        "hit": false
                    },
                    {
                        "score": 0.5195299965627733,
                        "answer": "norman",
                        "hit": false
                    },
                    {
                        "score": 0.5173787971763714,
                        "answer": "kava",
                        "hit": false
                    },
                    {
                        "score": 0.509174595216162,
                        "answer": "panama",
                        "hit": false
                    },
                    {
                        "score": 0.5047613468293881,
                        "answer": "henderson",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "newton"
                ],
                "rank": 74,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6874117106199265
            },
            {
                "question verbose": "What is to pacino ",
                "b": "pacino",
                "expected answer": [
                    "actor",
                    "director",
                    "filmmaker"
                ],
                "predictions": [
                    {
                        "score": 0.3380398820219655,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3058583477151886,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29516913937251404,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.29290550763666573,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2919374633123065,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.2870662888254968,
                        "answer": "noticeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pacino"
                ],
                "rank": 2102,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613400973379612
            },
            {
                "question verbose": "What is to pascal ",
                "b": "pascal",
                "expected answer": [
                    "mathematician",
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.3220057339581614,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3080740374026536,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2997764266697003,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.29059162762672824,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2834879104508488,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.27725602616998746,
                        "answer": "palpable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pascal"
                ],
                "rank": 17,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to picasso ",
                "b": "picasso",
                "expected answer": [
                    "painter",
                    "artist",
                    "sculptor",
                    "designer"
                ],
                "predictions": [
                    {
                        "score": 0.5947794023244991,
                        "answer": "hooking",
                        "hit": false
                    },
                    {
                        "score": 0.5945931899578383,
                        "answer": "philosopher",
                        "hit": false
                    },
                    {
                        "score": 0.5938944976600578,
                        "answer": "eh",
                        "hit": false
                    },
                    {
                        "score": 0.5784103782805003,
                        "answer": "bee",
                        "hit": false
                    },
                    {
                        "score": 0.5722883056475432,
                        "answer": "chill",
                        "hit": false
                    },
                    {
                        "score": 0.5691674605191858,
                        "answer": "gee",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "picasso"
                ],
                "rank": 2132,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7766668796539307
            },
            {
                "question verbose": "What is to plato ",
                "b": "plato",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.33810081368646505,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29388633118503993,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28984396542758817,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.287957875795468,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2809499847928947,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.27943145256679836,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "plato"
                ],
                "rank": 48,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to raphael ",
                "b": "raphael",
                "expected answer": [
                    "painter",
                    "artist",
                    "architect"
                ],
                "predictions": [
                    {
                        "score": 0.336209262676336,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29324194151784294,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.289271048260888,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2858485908166538,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28059025771871376,
                        "answer": "owed",
                        "hit": false
                    },
                    {
                        "score": 0.27547569290771795,
                        "answer": "palpable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "raphael"
                ],
                "rank": 1704,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6199424490332603
            },
            {
                "question verbose": "What is to rembrandt ",
                "b": "rembrandt",
                "expected answer": [
                    "painter",
                    "etcher",
                    "artist"
                ],
                "predictions": [
                    {
                        "score": 0.5215886977357216,
                        "answer": "philosopher",
                        "hit": false
                    },
                    {
                        "score": 0.5142782820001818,
                        "answer": "tripped",
                        "hit": false
                    },
                    {
                        "score": 0.5107046270224624,
                        "answer": "viciously",
                        "hit": false
                    },
                    {
                        "score": 0.5066263293402975,
                        "answer": "dilemma",
                        "hit": false
                    },
                    {
                        "score": 0.5013818529160452,
                        "answer": "nap",
                        "hit": false
                    },
                    {
                        "score": 0.5004517269861621,
                        "answer": "wookiee",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rembrandt"
                ],
                "rank": 17,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8074573576450348
            },
            {
                "question verbose": "What is to rousseau ",
                "b": "rousseau",
                "expected answer": [
                    "writer",
                    "author",
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.3380821536392727,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30376402661609225,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2979977432450105,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2978755752258939,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2893544891236828,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28533208616724615,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rousseau"
                ],
                "rank": 31,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5106465211138129
            },
            {
                "question verbose": "What is to schwarzenegger ",
                "b": "schwarzenegger",
                "expected answer": [
                    "actor",
                    "politician",
                    "governor"
                ],
                "predictions": [
                    {
                        "score": 0.33899781864573963,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3032387560808888,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2944972918098724,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.29374797069368064,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28834173077813396,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.28530956649314587,
                        "answer": "erez",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "schwarzenegger"
                ],
                "rank": 1929,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.613400973379612
            },
            {
                "question verbose": "What is to shakespeare ",
                "b": "shakespeare",
                "expected answer": [
                    "playwright",
                    "poet"
                ],
                "predictions": [
                    {
                        "score": 0.5809282781431411,
                        "answer": "dreary",
                        "hit": false
                    },
                    {
                        "score": 0.5383528617469394,
                        "answer": "graham",
                        "hit": false
                    },
                    {
                        "score": 0.48889724534406265,
                        "answer": "norman",
                        "hit": false
                    },
                    {
                        "score": 0.4886803530201967,
                        "answer": "cox",
                        "hit": false
                    },
                    {
                        "score": 0.4877457720273679,
                        "answer": "philosopher",
                        "hit": false
                    },
                    {
                        "score": 0.4807793020593317,
                        "answer": "kava",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shakespeare"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5898289233446121
            },
            {
                "question verbose": "What is to spinoza ",
                "b": "spinoza",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.3380335174358957,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3076130348727908,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2886251257744109,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2825707467067736,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.28136868229625533,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.28134012390871316,
                        "answer": "erez",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spinoza"
                ],
                "rank": 37,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            },
            {
                "question verbose": "What is to stalin ",
                "b": "stalin",
                "expected answer": [
                    "dictator",
                    "politician",
                    "leader",
                    "statesman"
                ],
                "predictions": [
                    {
                        "score": 0.5685150557942801,
                        "answer": "vagary",
                        "hit": false
                    },
                    {
                        "score": 0.5477283055734866,
                        "answer": "philosopher",
                        "hit": false
                    },
                    {
                        "score": 0.535588319796391,
                        "answer": "clap",
                        "hit": false
                    },
                    {
                        "score": 0.526499542161465,
                        "answer": "swore",
                        "hit": false
                    },
                    {
                        "score": 0.5082013617769459,
                        "answer": "calorie",
                        "hit": false
                    },
                    {
                        "score": 0.5017486394080176,
                        "answer": "viciously",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stalin"
                ],
                "rank": 263,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8303192555904388
            },
            {
                "question verbose": "What is to strauss ",
                "b": "strauss",
                "expected answer": [
                    "composer"
                ],
                "predictions": [
                    {
                        "score": 0.3370170496045401,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3313197194827147,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.292932879154297,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.28981812887383024,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2870549795523205,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.27947984860577224,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strauss"
                ],
                "rank": 1913,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6154861226677895
            },
            {
                "question verbose": "What is to tolstoi ",
                "b": "tolstoi",
                "expected answer": [
                    "novelist",
                    "writer",
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.32404604444920454,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.312303750724762,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.2993453825235955,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.29871849543482876,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2808835328447197,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2807792345552146,
                        "answer": "attacker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tolstoi"
                ],
                "rank": 37,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to truman ",
                "b": "truman",
                "expected answer": [
                    "president"
                ],
                "predictions": [
                    {
                        "score": 0.3377709158974724,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31744926209753566,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.29912058914699996,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2855570222082702,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.27443575108244744,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.27221681344756693,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "truman"
                ],
                "rank": 522,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6464356482028961
            },
            {
                "question verbose": "What is to wagner ",
                "b": "wagner",
                "expected answer": [
                    "composer"
                ],
                "predictions": [
                    {
                        "score": 0.5640729883579098,
                        "answer": "baron",
                        "hit": false
                    },
                    {
                        "score": 0.5518592908249063,
                        "answer": "hopkins",
                        "hit": false
                    },
                    {
                        "score": 0.5482789542045277,
                        "answer": "parkway",
                        "hit": false
                    },
                    {
                        "score": 0.5460658663688798,
                        "answer": "bobo",
                        "hit": false
                    },
                    {
                        "score": 0.545516540961837,
                        "answer": "ezquerra",
                        "hit": false
                    },
                    {
                        "score": 0.5377757710065619,
                        "answer": "karl",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wagner"
                ],
                "rank": 145,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7913413047790527
            },
            {
                "question verbose": "What is to wittgenstein ",
                "b": "wittgenstein",
                "expected answer": [
                    "philosopher"
                ],
                "predictions": [
                    {
                        "score": 0.33828927248909635,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3000726141297568,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.29731291170249946,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.28721541999978595,
                        "answer": "bait",
                        "hit": false
                    },
                    {
                        "score": 0.2865473393541954,
                        "answer": "circulated",
                        "hit": false
                    },
                    {
                        "score": 0.2797105169855029,
                        "answer": "prepping",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wittgenstein"
                ],
                "rank": 43,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6541120260953903
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E05 [name - occupation].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "d5d0d54a-c182-450d-af45-5c7d23994abe",
            "timestamp": "2020-10-22T15:57:21.184697"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to ape ",
                "b": "ape",
                "expected answer": [
                    "baby",
                    "infant"
                ],
                "predictions": [
                    {
                        "score": 0.45747212941068177,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.41775566192641517,
                        "answer": "grandchild",
                        "hit": false
                    },
                    {
                        "score": 0.41741525102947946,
                        "answer": "curious",
                        "hit": false
                    },
                    {
                        "score": 0.41021832093090976,
                        "answer": "judeo",
                        "hit": false
                    },
                    {
                        "score": 0.40908797155469956,
                        "answer": "reassure",
                        "hit": false
                    },
                    {
                        "score": 0.40699147076320574,
                        "answer": "failing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ape"
                ],
                "rank": 214,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6385260969400406
            },
            {
                "question verbose": "What is to badger ",
                "b": "badger",
                "expected answer": [
                    "kit",
                    "cob"
                ],
                "predictions": [
                    {
                        "score": 0.5171688380167055,
                        "answer": "kit",
                        "hit": true
                    },
                    {
                        "score": 0.48527371476443404,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.4498189975185736,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.44379896916270467,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.4376004813011622,
                        "answer": "ny",
                        "hit": false
                    },
                    {
                        "score": 0.4311393455682616,
                        "answer": "poisonous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "badger"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8594226837158203
            },
            {
                "question verbose": "What is to bear ",
                "b": "bear",
                "expected answer": [
                    "cub"
                ],
                "predictions": [
                    {
                        "score": 0.26187683776525517,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.2497203503178827,
                        "answer": "distribution",
                        "hit": false
                    },
                    {
                        "score": 0.24514059251778667,
                        "answer": "crop",
                        "hit": false
                    },
                    {
                        "score": 0.2393441660654701,
                        "answer": "preaching",
                        "hit": false
                    },
                    {
                        "score": 0.23086268717938258,
                        "answer": "unstable",
                        "hit": false
                    },
                    {
                        "score": 0.2304798413554316,
                        "answer": "incentive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bear"
                ],
                "rank": 386,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6040872409939766
            },
            {
                "question verbose": "What is to beaver ",
                "b": "beaver",
                "expected answer": [
                    "kit",
                    "kitten"
                ],
                "predictions": [
                    {
                        "score": 0.44879563776241876,
                        "answer": "kit",
                        "hit": true
                    },
                    {
                        "score": 0.3917319919262706,
                        "answer": "tier",
                        "hit": false
                    },
                    {
                        "score": 0.3765531604062994,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.37333144641019683,
                        "answer": "olive",
                        "hit": false
                    },
                    {
                        "score": 0.372489134413838,
                        "answer": "teamster",
                        "hit": false
                    },
                    {
                        "score": 0.3669345948083049,
                        "answer": "cub",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beaver"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.813602864742279
            },
            {
                "question verbose": "What is to bee ",
                "b": "bee",
                "expected answer": [
                    "larva"
                ],
                "predictions": [
                    {
                        "score": 0.4627210560642034,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.41703561294835284,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.36922553708976064,
                        "answer": "judeo",
                        "hit": false
                    },
                    {
                        "score": 0.36842920270358376,
                        "answer": "military",
                        "hit": false
                    },
                    {
                        "score": 0.36481575646231323,
                        "answer": "raising",
                        "hit": false
                    },
                    {
                        "score": 0.36183436538966196,
                        "answer": "kit",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bee"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5914601683616638
            },
            {
                "question verbose": "What is to beetle ",
                "b": "beetle",
                "expected answer": [
                    "larva"
                ],
                "predictions": [
                    {
                        "score": 0.4863030906712571,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.43853525005829497,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.43091307879093343,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.42908465935745665,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.42408829501115836,
                        "answer": "interjection",
                        "hit": false
                    },
                    {
                        "score": 0.4122349537273038,
                        "answer": "renting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beetle"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5828202366828918
            },
            {
                "question verbose": "What is to buffalo ",
                "b": "buffalo",
                "expected answer": [
                    "calf"
                ],
                "predictions": [
                    {
                        "score": 0.40455391199577023,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.37660924107009325,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.3664709473635582,
                        "answer": "doooooooooo",
                        "hit": false
                    },
                    {
                        "score": 0.34504076848624365,
                        "answer": "infant",
                        "hit": false
                    },
                    {
                        "score": 0.3412603207059388,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.33934586972609354,
                        "answer": "tier",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "buffalo"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.585297703742981
            },
            {
                "question verbose": "What is to butterfly ",
                "b": "butterfly",
                "expected answer": [
                    "larva",
                    "pupa",
                    "caterpillar",
                    "chrysalis"
                ],
                "predictions": [
                    {
                        "score": 0.6182929023883932,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2653634913486269,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24739089433110986,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24112603207372232,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.23631760841901608,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23518344419759016,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "butterfly"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to camel ",
                "b": "camel",
                "expected answer": [
                    "calf",
                    "colt"
                ],
                "predictions": [
                    {
                        "score": 0.6186026751227867,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24904055220515178,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.235405817157224,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23263358473046458,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.23164863349417775,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.23012180051149467,
                        "answer": "pulse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "camel"
                ],
                "rank": 9687,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cat ",
                "b": "cat",
                "expected answer": [
                    "kitten"
                ],
                "predictions": [
                    {
                        "score": 0.37808017392575405,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.320762173097843,
                        "answer": "exhausting",
                        "hit": false
                    },
                    {
                        "score": 0.29932899076357267,
                        "answer": "headway",
                        "hit": false
                    },
                    {
                        "score": 0.29764493165910544,
                        "answer": "dpw",
                        "hit": false
                    },
                    {
                        "score": 0.29643114235114876,
                        "answer": "ditch",
                        "hit": false
                    },
                    {
                        "score": 0.2829330167857475,
                        "answer": "pulse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cat"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5868597850203514
            },
            {
                "question verbose": "What is to cattle ",
                "b": "cattle",
                "expected answer": [
                    "calf",
                    "heifer"
                ],
                "predictions": [
                    {
                        "score": 0.6180772501254554,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2468267351291073,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.2428844656764211,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.23863537908346205,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.23697562563391808,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.23554855429845478,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cattle"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to chimpanzee ",
                "b": "chimpanzee",
                "expected answer": [
                    "baby",
                    "infant"
                ],
                "predictions": [
                    {
                        "score": 0.625870897650211,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2582380786122315,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24889153432269295,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.241039486599907,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.2404995257959978,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2331074396538653,
                        "answer": "pulse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chimpanzee"
                ],
                "rank": 58,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6390831768512726
            },
            {
                "question verbose": "What is to cicada ",
                "b": "cicada",
                "expected answer": [
                    "nymph"
                ],
                "predictions": [
                    {
                        "score": 0.6194870957036531,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2586347732225716,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2488541121724198,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.23905392952078267,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.23482535720575168,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.2339366432927064,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cicada"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cockroach ",
                "b": "cockroach",
                "expected answer": [
                    "nymph"
                ],
                "predictions": [
                    {
                        "score": 0.43289406325378793,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.401966981114842,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.39860379651167016,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.37485323471495147,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.36328903374925786,
                        "answer": "magically",
                        "hit": false
                    },
                    {
                        "score": 0.3631341427890214,
                        "answer": "genetically",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cockroach"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7059800922870636
            },
            {
                "question verbose": "What is to cricket ",
                "b": "cricket",
                "expected answer": [
                    "larva"
                ],
                "predictions": [
                    {
                        "score": 0.6198444441490208,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2624972787124713,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2485904857915255,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.23718191242243714,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23664472692830088,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.23368209696182593,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cricket"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to deer ",
                "b": "deer",
                "expected answer": [
                    "fawn"
                ],
                "predictions": [
                    {
                        "score": 0.6184097710618505,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2561608383482585,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2285048172938322,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.22774253022684326,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.22557859505865469,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.22503403956706625,
                        "answer": "shoulder",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deer"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dog ",
                "b": "dog",
                "expected answer": [
                    "puppy",
                    "pup",
                    "whelp"
                ],
                "predictions": [
                    {
                        "score": 0.3841263423983375,
                        "answer": "convicted",
                        "hit": false
                    },
                    {
                        "score": 0.3428979885028598,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.34236129649801506,
                        "answer": "replaces",
                        "hit": false
                    },
                    {
                        "score": 0.34108751077076166,
                        "answer": "grandchild",
                        "hit": false
                    },
                    {
                        "score": 0.3354729676783253,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.3337373342957855,
                        "answer": "thanos",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dog"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6415438055992126
            },
            {
                "question verbose": "What is to duck ",
                "b": "duck",
                "expected answer": [
                    "duckling"
                ],
                "predictions": [
                    {
                        "score": 0.3916553426509493,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.36356543741979136,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.355051634732338,
                        "answer": "oversight",
                        "hit": false
                    },
                    {
                        "score": 0.35430403118877496,
                        "answer": "teamster",
                        "hit": false
                    },
                    {
                        "score": 0.3531896417443051,
                        "answer": "measure",
                        "hit": false
                    },
                    {
                        "score": 0.35098024651492266,
                        "answer": "renting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "duck"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6068727225065231
            },
            {
                "question verbose": "What is to elephant ",
                "b": "elephant",
                "expected answer": [
                    "calf"
                ],
                "predictions": [
                    {
                        "score": 0.45549011117692406,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.43082419931318006,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.4206922680157212,
                        "answer": "arrogated",
                        "hit": false
                    },
                    {
                        "score": 0.4179755762178365,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.4011330576871111,
                        "answer": "poisonous",
                        "hit": false
                    },
                    {
                        "score": 0.3729976267688169,
                        "answer": "bloc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "elephant"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6032466441392899
            },
            {
                "question verbose": "What is to ferret ",
                "b": "ferret",
                "expected answer": [
                    "kit"
                ],
                "predictions": [
                    {
                        "score": 0.6284378638905647,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2484924018801415,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.244595624691386,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.24389300899244093,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.24385299374012334,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.23897189234250954,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ferret"
                ],
                "rank": 32,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6365807205438614
            },
            {
                "question verbose": "What is to fish ",
                "b": "fish",
                "expected answer": [
                    "fingerling",
                    "spawn",
                    "egg",
                    "larva",
                    "fry",
                    "minnmow"
                ],
                "predictions": [
                    {
                        "score": 0.3711333140892733,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.3037737215661043,
                        "answer": "storm",
                        "hit": false
                    },
                    {
                        "score": 0.2841450682412775,
                        "answer": "imgx",
                        "hit": false
                    },
                    {
                        "score": 0.2773622850995925,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.27424226377209826,
                        "answer": "hurricane",
                        "hit": false
                    },
                    {
                        "score": 0.2725425147174184,
                        "answer": "levy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fish"
                ],
                "rank": 4819,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5384926944971085
            },
            {
                "question verbose": "What is to fly ",
                "b": "fly",
                "expected answer": [
                    "grub",
                    "maggot"
                ],
                "predictions": [
                    {
                        "score": 0.31958957113079195,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.302342995119761,
                        "answer": "exhausting",
                        "hit": false
                    },
                    {
                        "score": 0.30221266272734537,
                        "answer": "submit",
                        "hit": false
                    },
                    {
                        "score": 0.2940617916879112,
                        "answer": "shoulder",
                        "hit": false
                    },
                    {
                        "score": 0.29238690457253447,
                        "answer": "overlap",
                        "hit": false
                    },
                    {
                        "score": 0.2900884604883742,
                        "answer": "poisonous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fly"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.639924630522728
            },
            {
                "question verbose": "What is to fox ",
                "b": "fox",
                "expected answer": [
                    "cub",
                    "pup",
                    "puppy",
                    "whelp"
                ],
                "predictions": [
                    {
                        "score": 0.39378946548436833,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.38335152294091174,
                        "answer": "ny",
                        "hit": false
                    },
                    {
                        "score": 0.3499054459365801,
                        "answer": "clean",
                        "hit": false
                    },
                    {
                        "score": 0.3100927431395547,
                        "answer": "roy",
                        "hit": false
                    },
                    {
                        "score": 0.30425367142709725,
                        "answer": "jacket",
                        "hit": false
                    },
                    {
                        "score": 0.3042479857460475,
                        "answer": "wgs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fox"
                ],
                "rank": 34,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6656142473220825
            },
            {
                "question verbose": "What is to goat ",
                "b": "goat",
                "expected answer": [
                    "kid"
                ],
                "predictions": [
                    {
                        "score": 0.6271128619028306,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24947381524145013,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24793204575819427,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.24289815055949898,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.24048548370541492,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.23547327285326297,
                        "answer": "elected",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goat"
                ],
                "rank": 12100,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5620153844356537
            },
            {
                "question verbose": "What is to goldfish ",
                "b": "goldfish",
                "expected answer": [
                    "fingerling",
                    "fry"
                ],
                "predictions": [
                    {
                        "score": 0.6186120283662182,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2508833641707988,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24377648310255418,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.24071008336984667,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.23977616381910957,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2344079654855333,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goldfish"
                ],
                "rank": 14047,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to gorilla ",
                "b": "gorilla",
                "expected answer": [
                    "infant"
                ],
                "predictions": [
                    {
                        "score": 0.4215606688571214,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.3524349012925617,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.3334523860544003,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.33330516787148307,
                        "answer": "overlap",
                        "hit": false
                    },
                    {
                        "score": 0.33241869272749175,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.330851648238719,
                        "answer": "thanos",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gorilla"
                ],
                "rank": 296,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7480766326189041
            },
            {
                "question verbose": "What is to herring ",
                "b": "herring",
                "expected answer": [
                    "fingerling",
                    "fry"
                ],
                "predictions": [
                    {
                        "score": 0.6192654877935314,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2547046520663062,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24291344118828703,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.2377279639765177,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.23754951289424298,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23550825830980965,
                        "answer": "pulse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "herring"
                ],
                "rank": 13916,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to horse ",
                "b": "horse",
                "expected answer": [
                    "foal",
                    "colt",
                    "filly"
                ],
                "predictions": [
                    {
                        "score": 0.311025773597155,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.27369363622908194,
                        "answer": "grain",
                        "hit": false
                    },
                    {
                        "score": 0.25414354572592196,
                        "answer": "bleed",
                        "hit": false
                    },
                    {
                        "score": 0.24343498788575715,
                        "answer": "fix",
                        "hit": false
                    },
                    {
                        "score": 0.23964653918071185,
                        "answer": "lust",
                        "hit": false
                    },
                    {
                        "score": 0.23848001170503103,
                        "answer": "wrested",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "horse"
                ],
                "rank": 701,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6054383367300034
            },
            {
                "question verbose": "What is to insect ",
                "b": "insect",
                "expected answer": [
                    "larva"
                ],
                "predictions": [
                    {
                        "score": 0.6184750077744753,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24937866966227965,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24834129069966485,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2389160753408771,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23782825394339668,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.23679400938135972,
                        "answer": "eschew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "insect"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lion ",
                "b": "lion",
                "expected answer": [
                    "cub"
                ],
                "predictions": [
                    {
                        "score": 0.30036789113104734,
                        "answer": "friday",
                        "hit": false
                    },
                    {
                        "score": 0.2874318959824617,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.28090048664536904,
                        "answer": "interjection",
                        "hit": false
                    },
                    {
                        "score": 0.27979528367815326,
                        "answer": "oversight",
                        "hit": false
                    },
                    {
                        "score": 0.27474989710151326,
                        "answer": "arnie",
                        "hit": false
                    },
                    {
                        "score": 0.2694333779925104,
                        "answer": "curious",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lion"
                ],
                "rank": 65,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.65821373462677
            },
            {
                "question verbose": "What is to mink ",
                "b": "mink",
                "expected answer": [
                    "kit",
                    "cub"
                ],
                "predictions": [
                    {
                        "score": 0.4893106294912812,
                        "answer": "ny",
                        "hit": false
                    },
                    {
                        "score": 0.4871939904426134,
                        "answer": "kit",
                        "hit": true
                    },
                    {
                        "score": 0.4858137031998924,
                        "answer": "clean",
                        "hit": false
                    },
                    {
                        "score": 0.4750302537964454,
                        "answer": "imgx",
                        "hit": false
                    },
                    {
                        "score": 0.46504189698874315,
                        "answer": "jacket",
                        "hit": false
                    },
                    {
                        "score": 0.4549194795340137,
                        "answer": "repairing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mink"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8377897441387177
            },
            {
                "question verbose": "What is to monkey ",
                "b": "monkey",
                "expected answer": [
                    "infant"
                ],
                "predictions": [
                    {
                        "score": 0.4772160784898476,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.3792110705570794,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.37881968820662304,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.3479235773426565,
                        "answer": "replaces",
                        "hit": false
                    },
                    {
                        "score": 0.3393438517965565,
                        "answer": "thanos",
                        "hit": false
                    },
                    {
                        "score": 0.3374550320256582,
                        "answer": "moneyproperty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "monkey"
                ],
                "rank": 201,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7539980113506317
            },
            {
                "question verbose": "What is to muskrat ",
                "b": "muskrat",
                "expected answer": [
                    "kit"
                ],
                "predictions": [
                    {
                        "score": 0.6268209644619206,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25349797128340723,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24053815012317556,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.23182805192743747,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23014625254386642,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.22968357360346547,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "muskrat"
                ],
                "rank": 28,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6365807205438614
            },
            {
                "question verbose": "What is to ox ",
                "b": "ox",
                "expected answer": [
                    "calf",
                    "stot"
                ],
                "predictions": [
                    {
                        "score": 0.4798042131578736,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.44808722810527557,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4108828225293108,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.3931379717477841,
                        "answer": "judeo",
                        "hit": false
                    },
                    {
                        "score": 0.38420734168939374,
                        "answer": "replaces",
                        "hit": false
                    },
                    {
                        "score": 0.3785997387700217,
                        "answer": "moneyproperty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ox"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6454468369483948
            },
            {
                "question verbose": "What is to panda ",
                "b": "panda",
                "expected answer": [
                    "cub"
                ],
                "predictions": [
                    {
                        "score": 0.627110925361949,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2551785946089814,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2360479730436107,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.23441967872803485,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.23086188429439408,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23067835680300786,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "panda"
                ],
                "rank": 26,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6339990347623825
            },
            {
                "question verbose": "What is to pig ",
                "b": "pig",
                "expected answer": [
                    "piglet",
                    "shoat",
                    "farrow"
                ],
                "predictions": [
                    {
                        "score": 0.5112970360448484,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4902329889056996,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.454676134594828,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.44046619418501964,
                        "answer": "poisonous",
                        "hit": false
                    },
                    {
                        "score": 0.4387499037839487,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.42956003660609937,
                        "answer": "arrogated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pig"
                ],
                "rank": 5200,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7739641666412354
            },
            {
                "question verbose": "What is to rabbit ",
                "b": "rabbit",
                "expected answer": [
                    "bunny"
                ],
                "predictions": [
                    {
                        "score": 0.4153945852961466,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.40899332899367663,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.40713917428115937,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4065969643217597,
                        "answer": "anesthetic",
                        "hit": false
                    },
                    {
                        "score": 0.4002575384659363,
                        "answer": "thanos",
                        "hit": false
                    },
                    {
                        "score": 0.396542359130834,
                        "answer": "insists",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rabbit"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6916489005088806
            },
            {
                "question verbose": "What is to raccoon ",
                "b": "raccoon",
                "expected answer": [
                    "kit",
                    "cub"
                ],
                "predictions": [
                    {
                        "score": 0.48513745244105133,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.4076805808467508,
                        "answer": "unborn",
                        "hit": false
                    },
                    {
                        "score": 0.4049730766991648,
                        "answer": "cub",
                        "hit": true
                    },
                    {
                        "score": 0.40135091881801865,
                        "answer": "kit",
                        "hit": true
                    },
                    {
                        "score": 0.4001733730694148,
                        "answer": "acceptable",
                        "hit": false
                    },
                    {
                        "score": 0.3998578093432528,
                        "answer": "judeo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "raccoon"
                ],
                "rank": 2,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.783140242099762
            },
            {
                "question verbose": "What is to salmon ",
                "b": "salmon",
                "expected answer": [
                    "smolt"
                ],
                "predictions": [
                    {
                        "score": 0.387617218076744,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.33641936836956,
                        "answer": "tier",
                        "hit": false
                    },
                    {
                        "score": 0.32572040598801644,
                        "answer": "worshipped",
                        "hit": false
                    },
                    {
                        "score": 0.32562986983916314,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.3107507328321924,
                        "answer": "household",
                        "hit": false
                    },
                    {
                        "score": 0.31011625646457824,
                        "answer": "cleaning",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "salmon"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5413769781589508
            },
            {
                "question verbose": "What is to seal ",
                "b": "seal",
                "expected answer": [
                    "pup"
                ],
                "predictions": [
                    {
                        "score": 0.5083619122820464,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4292204824037347,
                        "answer": "tier",
                        "hit": false
                    },
                    {
                        "score": 0.42876948354318184,
                        "answer": "fund",
                        "hit": false
                    },
                    {
                        "score": 0.42391543491746947,
                        "answer": "inducing",
                        "hit": false
                    },
                    {
                        "score": 0.40666199096131694,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.3989212858783455,
                        "answer": "incentive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seal"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.587302178144455
            },
            {
                "question verbose": "What is to shark ",
                "b": "shark",
                "expected answer": [
                    "cub",
                    "pup"
                ],
                "predictions": [
                    {
                        "score": 0.4849663204533779,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4475777433693633,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.44409254534275483,
                        "answer": "cub",
                        "hit": true
                    },
                    {
                        "score": 0.42606028061450174,
                        "answer": "careful",
                        "hit": false
                    },
                    {
                        "score": 0.42059270837328844,
                        "answer": "genetically",
                        "hit": false
                    },
                    {
                        "score": 0.41538692392343224,
                        "answer": "lust",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shark"
                ],
                "rank": 2,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8008869886398315
            },
            {
                "question verbose": "What is to sheep ",
                "b": "sheep",
                "expected answer": [
                    "lamb",
                    "lambkin",
                    "cosset"
                ],
                "predictions": [
                    {
                        "score": 0.4764115315895156,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.4534377836026697,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4228763071094917,
                        "answer": "judeo",
                        "hit": false
                    },
                    {
                        "score": 0.40346387341281914,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.39312444444469075,
                        "answer": "bloc",
                        "hit": false
                    },
                    {
                        "score": 0.38105838091500926,
                        "answer": "smidge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sheep"
                ],
                "rank": 8601,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.824211448431015
            },
            {
                "question verbose": "What is to skunk ",
                "b": "skunk",
                "expected answer": [
                    "kit",
                    "kitten"
                ],
                "predictions": [
                    {
                        "score": 0.6274639057538838,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25899842351572677,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24787834155042845,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2381406035168442,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23656174226204768,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.23592850058504983,
                        "answer": "eschew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "skunk"
                ],
                "rank": 31,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6365807205438614
            },
            {
                "question verbose": "What is to snake ",
                "b": "snake",
                "expected answer": [
                    "hatchling",
                    "nestling"
                ],
                "predictions": [
                    {
                        "score": 0.46700641212232086,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.4585468097444334,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.4229258153957334,
                        "answer": "surcharge",
                        "hit": false
                    },
                    {
                        "score": 0.41508008274787456,
                        "answer": "cub",
                        "hit": false
                    },
                    {
                        "score": 0.39296011166062056,
                        "answer": "household",
                        "hit": false
                    },
                    {
                        "score": 0.38573604518062427,
                        "answer": "bloc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "snake"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to tiger ",
                "b": "tiger",
                "expected answer": [
                    "cub"
                ],
                "predictions": [
                    {
                        "score": 0.6273723365037316,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24543680299853082,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24512586418410792,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2439493678174574,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.2356211412277842,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.23127535583238457,
                        "answer": "elected",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiger"
                ],
                "rank": 26,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6339990347623825
            },
            {
                "question verbose": "What is to trout ",
                "b": "trout",
                "expected answer": [
                    "fingerling"
                ],
                "predictions": [
                    {
                        "score": 0.6187394787934615,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25882023526708375,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.25443327467263316,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24679838732002646,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.2418148263206123,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.2393342271843961,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "trout"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to weasel ",
                "b": "weasel",
                "expected answer": [
                    "kit"
                ],
                "predictions": [
                    {
                        "score": 0.6271491801305804,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24985814810517562,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24067915271629955,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.23597031809236862,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.23396292857927006,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.23364528168428605,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weasel"
                ],
                "rank": 26,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6365807205438614
            },
            {
                "question verbose": "What is to whale ",
                "b": "whale",
                "expected answer": [
                    "calf"
                ],
                "predictions": [
                    {
                        "score": 0.6186995394168363,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2593007722526086,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2454813671584227,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.23751705044803087,
                        "answer": "pulse",
                        "hit": false
                    },
                    {
                        "score": 0.23324708289308363,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.23173891490956672,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "whale"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to wolf ",
                "b": "wolf",
                "expected answer": [
                    "cub",
                    "pup",
                    "puppy",
                    "whelp"
                ],
                "predictions": [
                    {
                        "score": 0.42190723901572386,
                        "answer": "kit",
                        "hit": false
                    },
                    {
                        "score": 0.3832536205767729,
                        "answer": "cub",
                        "hit": true
                    },
                    {
                        "score": 0.35823224894522226,
                        "answer": "replaces",
                        "hit": false
                    },
                    {
                        "score": 0.33628236940888645,
                        "answer": "monies",
                        "hit": false
                    },
                    {
                        "score": 0.33612549121610996,
                        "answer": "vacation",
                        "hit": false
                    },
                    {
                        "score": 0.326567332756194,
                        "answer": "debacle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wolf"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7585409581661224
            },
            {
                "question verbose": "What is to woodchuck ",
                "b": "woodchuck",
                "expected answer": [
                    "kit",
                    "cob"
                ],
                "predictions": [
                    {
                        "score": 0.6271858168879224,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26123678948996737,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24861852178796023,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.24076405568768164,
                        "answer": "elected",
                        "hit": false
                    },
                    {
                        "score": 0.23798331696575367,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.23733353637882434,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "woodchuck"
                ],
                "rank": 42,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6365807205438614
            }
        ],
        "result": {
            "cnt_questions_correct": 2,
            "cnt_questions_total": 50,
            "accuracy": 0.04
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E06 [animal - young].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "892d4ad0-3d50-4cfa-81e4-953187fb2925",
            "timestamp": "2020-10-22T15:57:22.253540"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to alpaca ",
                "b": "alpaca",
                "expected answer": [
                    "bray"
                ],
                "predictions": [
                    {
                        "score": 0.6516908906502746,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24550312361864576,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2161360062041479,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.20352566136952674,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.2009235773636527,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.19241363692091934,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "alpaca"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to bear ",
                "b": "bear",
                "expected answer": [
                    "growl"
                ],
                "predictions": [
                    {
                        "score": 0.2787589029323723,
                        "answer": "willow",
                        "hit": false
                    },
                    {
                        "score": 0.2666588574357519,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2605026961264109,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.2566023501328099,
                        "answer": "weapon",
                        "hit": false
                    },
                    {
                        "score": 0.24755642537725017,
                        "answer": "specie",
                        "hit": false
                    },
                    {
                        "score": 0.23891713007586937,
                        "answer": "egyptian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bear"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6060080528259277
            },
            {
                "question verbose": "What is to bee ",
                "b": "bee",
                "expected answer": [
                    "buzz",
                    "hum"
                ],
                "predictions": [
                    {
                        "score": 0.33069982562047906,
                        "answer": "buzz",
                        "hit": true
                    },
                    {
                        "score": 0.3084852478215546,
                        "answer": "essentially",
                        "hit": false
                    },
                    {
                        "score": 0.29946657590059667,
                        "answer": "comfy",
                        "hit": false
                    },
                    {
                        "score": 0.2975795869019834,
                        "answer": "loony",
                        "hit": false
                    },
                    {
                        "score": 0.29690002890037354,
                        "answer": "everyones",
                        "hit": false
                    },
                    {
                        "score": 0.2903748591926913,
                        "answer": "recite",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bee"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7330311685800552
            },
            {
                "question verbose": "What is to beetle ",
                "b": "beetle",
                "expected answer": [
                    "drone"
                ],
                "predictions": [
                    {
                        "score": 0.42373567904043974,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.3384944316762773,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.32168389516753093,
                        "answer": "breakthrough",
                        "hit": false
                    },
                    {
                        "score": 0.3084558766684369,
                        "answer": "thorn",
                        "hit": false
                    },
                    {
                        "score": 0.30751540102711766,
                        "answer": "statistical",
                        "hit": false
                    },
                    {
                        "score": 0.29945306079748396,
                        "answer": "governmental",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beetle"
                ],
                "rank": 1831,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8682749569416046
            },
            {
                "question verbose": "What is to cat ",
                "b": "cat",
                "expected answer": [
                    "meow",
                    "meu",
                    "purr",
                    "caterwaul"
                ],
                "predictions": [
                    {
                        "score": 0.2857979197564951,
                        "answer": "pharmacist",
                        "hit": false
                    },
                    {
                        "score": 0.26929857664501433,
                        "answer": "academia",
                        "hit": false
                    },
                    {
                        "score": 0.26848279352694027,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.26344156550214304,
                        "answer": "fdr",
                        "hit": false
                    },
                    {
                        "score": 0.26143443235750274,
                        "answer": "impose",
                        "hit": false
                    },
                    {
                        "score": 0.25736297349846116,
                        "answer": "ragged",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cat"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5868597850203514
            },
            {
                "question verbose": "What is to cattle ",
                "b": "cattle",
                "expected answer": [
                    "moo",
                    "bellow",
                    "low"
                ],
                "predictions": [
                    {
                        "score": 0.6524031932170482,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2571192052988606,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2183418158430965,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2086209906703094,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20181537724416126,
                        "answer": "freedom",
                        "hit": false
                    },
                    {
                        "score": 0.20059585738757196,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cattle"
                ],
                "rank": 14828,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to chicken ",
                "b": "chicken",
                "expected answer": [
                    "cluck",
                    "crow",
                    "cock-a-doodle-doo"
                ],
                "predictions": [
                    {
                        "score": 0.3232413935588113,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.28424706606029965,
                        "answer": "statistical",
                        "hit": false
                    },
                    {
                        "score": 0.2705202269466849,
                        "answer": "nameless",
                        "hit": false
                    },
                    {
                        "score": 0.2639823947068122,
                        "answer": "educational",
                        "hit": false
                    },
                    {
                        "score": 0.2607186839142091,
                        "answer": "sufficient",
                        "hit": false
                    },
                    {
                        "score": 0.2605752338856685,
                        "answer": "loosie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chicken"
                ],
                "rank": 13029,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6053173542022705
            },
            {
                "question verbose": "What is to chimpanzee ",
                "b": "chimpanzee",
                "expected answer": [
                    "scream"
                ],
                "predictions": [
                    {
                        "score": 0.658225918473047,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23131415351640547,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21925459587831453,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.1974313049646526,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.1966401056929949,
                        "answer": "plenty",
                        "hit": false
                    },
                    {
                        "score": 0.19473619555972307,
                        "answer": "freedom",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chimpanzee"
                ],
                "rank": 3090,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6397465318441391
            },
            {
                "question verbose": "What is to cicada ",
                "b": "cicada",
                "expected answer": [
                    "buzz"
                ],
                "predictions": [
                    {
                        "score": 0.6587150488873514,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24384828781572068,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.20865744977023792,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20685074881937637,
                        "answer": "buzz",
                        "hit": true
                    },
                    {
                        "score": 0.2048108651694056,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.19277456869205978,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cicada"
                ],
                "rank": 3,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6460484117269516
            },
            {
                "question verbose": "What is to coyote ",
                "b": "coyote",
                "expected answer": [
                    "howl"
                ],
                "predictions": [
                    {
                        "score": 0.6524657891088629,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2511324991553458,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21775162344366644,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21395756589510337,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.21133518258690795,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2047842954389484,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "coyote"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cricket ",
                "b": "cricket",
                "expected answer": [
                    "chirp"
                ],
                "predictions": [
                    {
                        "score": 0.651929116009878,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24129091010610723,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21666683692169036,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.20340747123310626,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20119080859745994,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.19803262914227224,
                        "answer": "freedom",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cricket"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to crow ",
                "b": "crow",
                "expected answer": [
                    "caw"
                ],
                "predictions": [
                    {
                        "score": 0.2942014354263137,
                        "answer": "increasingly",
                        "hit": false
                    },
                    {
                        "score": 0.29397627900832307,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.2649211103652432,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.26444722857339237,
                        "answer": "sketch",
                        "hit": false
                    },
                    {
                        "score": 0.25341267103568516,
                        "answer": "lewd",
                        "hit": false
                    },
                    {
                        "score": 0.24864345433713347,
                        "answer": "wrinkle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crow"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6412061750888824
            },
            {
                "question verbose": "What is to deer ",
                "b": "deer",
                "expected answer": [
                    "bellow"
                ],
                "predictions": [
                    {
                        "score": 0.6534896889113119,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2533986115290335,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21818520485106574,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.21620236695710643,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.21432601407310342,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21098936463361212,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deer"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dog ",
                "b": "dog",
                "expected answer": [
                    "bark",
                    "growl",
                    "howl",
                    "yelp",
                    "whine",
                    "arf",
                    "bow_wow",
                    "woof"
                ],
                "predictions": [
                    {
                        "score": 0.3750940152304913,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.30824304382998385,
                        "answer": "nearest",
                        "hit": false
                    },
                    {
                        "score": 0.30769645617525526,
                        "answer": "impose",
                        "hit": false
                    },
                    {
                        "score": 0.3004864540941627,
                        "answer": "firmly",
                        "hit": false
                    },
                    {
                        "score": 0.29760126993480557,
                        "answer": "irrepressible",
                        "hit": false
                    },
                    {
                        "score": 0.2900145741202766,
                        "answer": "lebowski",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dog"
                ],
                "rank": 124,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6800373643636703
            },
            {
                "question verbose": "What is to donkey ",
                "b": "donkey",
                "expected answer": [
                    "bray",
                    "hee-haw"
                ],
                "predictions": [
                    {
                        "score": 0.4294190490334363,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.37617036518689956,
                        "answer": "namely",
                        "hit": false
                    },
                    {
                        "score": 0.3573334611686354,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.33766620670829034,
                        "answer": "breakthrough",
                        "hit": false
                    },
                    {
                        "score": 0.33555043597087825,
                        "answer": "venom",
                        "hit": false
                    },
                    {
                        "score": 0.3355423095541165,
                        "answer": "fdr",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "donkey"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6347488611936569
            },
            {
                "question verbose": "What is to duck ",
                "b": "duck",
                "expected answer": [
                    "quack"
                ],
                "predictions": [
                    {
                        "score": 0.3173387463254346,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.29690314798121076,
                        "answer": "breakthrough",
                        "hit": false
                    },
                    {
                        "score": 0.28397784687006283,
                        "answer": "venom",
                        "hit": false
                    },
                    {
                        "score": 0.27630600346797324,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.27522716844056044,
                        "answer": "july",
                        "hit": false
                    },
                    {
                        "score": 0.27251652706359974,
                        "answer": "drawn",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "duck"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6068727225065231
            },
            {
                "question verbose": "What is to elephant ",
                "b": "elephant",
                "expected answer": [
                    "trumpet"
                ],
                "predictions": [
                    {
                        "score": 0.35405390392805514,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.32955257344389083,
                        "answer": "namely",
                        "hit": false
                    },
                    {
                        "score": 0.3194301014145891,
                        "answer": "salicylate",
                        "hit": false
                    },
                    {
                        "score": 0.3119175522414452,
                        "answer": "documentation",
                        "hit": false
                    },
                    {
                        "score": 0.2997478539267426,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2949655422451309,
                        "answer": "vest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "elephant"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6032466441392899
            },
            {
                "question verbose": "What is to elk ",
                "b": "elk",
                "expected answer": [
                    "bellow"
                ],
                "predictions": [
                    {
                        "score": 0.6521819770185708,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2406206141471114,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21510496266705315,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.20182265082470052,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.1967481225801546,
                        "answer": "freedom",
                        "hit": false
                    },
                    {
                        "score": 0.19554364935302312,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "elk"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to ferret ",
                "b": "ferret",
                "expected answer": [
                    "dook"
                ],
                "predictions": [
                    {
                        "score": 0.6528580873069026,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24723732821205574,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.216522484532851,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.213859508952881,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20345183542908377,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20175555744761556,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ferret"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to fly ",
                "b": "fly",
                "expected answer": [
                    "buzz"
                ],
                "predictions": [
                    {
                        "score": 0.3073704334125086,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.30666791080178424,
                        "answer": "essentially",
                        "hit": false
                    },
                    {
                        "score": 0.3041277526311665,
                        "answer": "stewart",
                        "hit": false
                    },
                    {
                        "score": 0.29769619820988485,
                        "answer": "buzz",
                        "hit": true
                    },
                    {
                        "score": 0.2920151979937343,
                        "answer": "plenty",
                        "hit": false
                    },
                    {
                        "score": 0.287805768596065,
                        "answer": "boarding",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fly"
                ],
                "rank": 3,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7115997076034546
            },
            {
                "question verbose": "What is to fox ",
                "b": "fox",
                "expected answer": [
                    "howl",
                    "yelp"
                ],
                "predictions": [
                    {
                        "score": 0.31217847399007115,
                        "answer": "plenty",
                        "hit": false
                    },
                    {
                        "score": 0.27190300466017564,
                        "answer": "reminder",
                        "hit": false
                    },
                    {
                        "score": 0.2518208689755721,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.24092417718165918,
                        "answer": "genuine",
                        "hit": false
                    },
                    {
                        "score": 0.23882250681927997,
                        "answer": "boarding",
                        "hit": false
                    },
                    {
                        "score": 0.23819851581487902,
                        "answer": "hitting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fox"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6103095263242722
            },
            {
                "question verbose": "What is to frog ",
                "b": "frog",
                "expected answer": [
                    "ribbit",
                    "croak"
                ],
                "predictions": [
                    {
                        "score": 0.3598430725439143,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2968548897498025,
                        "answer": "breakthrough",
                        "hit": false
                    },
                    {
                        "score": 0.2866981580059668,
                        "answer": "danielle",
                        "hit": false
                    },
                    {
                        "score": 0.2826824894987238,
                        "answer": "bombarded",
                        "hit": false
                    },
                    {
                        "score": 0.2822747636905967,
                        "answer": "july",
                        "hit": false
                    },
                    {
                        "score": 0.27563535428087976,
                        "answer": "venom",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "frog"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5734912306070328
            },
            {
                "question verbose": "What is to goat ",
                "b": "goat",
                "expected answer": [
                    "bleat"
                ],
                "predictions": [
                    {
                        "score": 0.6526618171478115,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24949474837516186,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2189986671237528,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21637024276202868,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2131346695689833,
                        "answer": "oppress",
                        "hit": false
                    },
                    {
                        "score": 0.2112158343653181,
                        "answer": "governmental",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goat"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to gorilla ",
                "b": "gorilla",
                "expected answer": [
                    "grunt",
                    "scream"
                ],
                "predictions": [
                    {
                        "score": 0.4045159106680893,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.34290941645205314,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.33373325385620906,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.3329368570775756,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.32730223910123224,
                        "answer": "publicity",
                        "hit": false
                    },
                    {
                        "score": 0.3257187873513144,
                        "answer": "enemity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gorilla"
                ],
                "rank": 278,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7830457389354706
            },
            {
                "question verbose": "What is to hornet ",
                "b": "hornet",
                "expected answer": [
                    "buzz"
                ],
                "predictions": [
                    {
                        "score": 0.38293669967221494,
                        "answer": "buzz",
                        "hit": true
                    },
                    {
                        "score": 0.372165797099385,
                        "answer": "bomber",
                        "hit": false
                    },
                    {
                        "score": 0.36676635324956963,
                        "answer": "loony",
                        "hit": false
                    },
                    {
                        "score": 0.35748251800597736,
                        "answer": "wing",
                        "hit": false
                    },
                    {
                        "score": 0.35361495277102695,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.3533108061933714,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hornet"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7688326835632324
            },
            {
                "question verbose": "What is to horse ",
                "b": "horse",
                "expected answer": [
                    "neigh",
                    "snort",
                    "whinny"
                ],
                "predictions": [
                    {
                        "score": 0.22226230027479651,
                        "answer": "allowed",
                        "hit": false
                    },
                    {
                        "score": 0.220311639384017,
                        "answer": "lasix",
                        "hit": false
                    },
                    {
                        "score": 0.21832962375344053,
                        "answer": "drug",
                        "hit": false
                    },
                    {
                        "score": 0.21826090469836515,
                        "answer": "medication",
                        "hit": false
                    },
                    {
                        "score": 0.21295976162122643,
                        "answer": "determining",
                        "hit": false
                    },
                    {
                        "score": 0.2029882673491869,
                        "answer": "wedgie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "horse"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6054383367300034
            },
            {
                "question verbose": "What is to hound ",
                "b": "hound",
                "expected answer": [
                    "bark",
                    "howl",
                    "bay"
                ],
                "predictions": [
                    {
                        "score": 0.6586116328606769,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23469030514708758,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21554178881711974,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.20643463993518046,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20286145066927108,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.2007848083421447,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hound"
                ],
                "rank": 1135,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5867845714092255
            },
            {
                "question verbose": "What is to hyena ",
                "b": "hyena",
                "expected answer": [
                    "laugh"
                ],
                "predictions": [
                    {
                        "score": 0.6590366133123241,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2238079936764165,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21723019885806005,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.19405421563269568,
                        "answer": "commiseration",
                        "hit": false
                    },
                    {
                        "score": 0.1933790948235155,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.18874114430318886,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hyena"
                ],
                "rank": 12524,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5832807719707489
            },
            {
                "question verbose": "What is to leopard ",
                "b": "leopard",
                "expected answer": [
                    "growl"
                ],
                "predictions": [
                    {
                        "score": 0.6528026985493814,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25051661915372636,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21965677728757435,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21264520676357052,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20444582171638237,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2019747390863611,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leopard"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lion ",
                "b": "lion",
                "expected answer": [
                    "roar",
                    "growl"
                ],
                "predictions": [
                    {
                        "score": 0.3016189652704419,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.27073652498420026,
                        "answer": "patriotic",
                        "hit": false
                    },
                    {
                        "score": 0.2666737398402223,
                        "answer": "breakthrough",
                        "hit": false
                    },
                    {
                        "score": 0.26194131196500603,
                        "answer": "bishop",
                        "hit": false
                    },
                    {
                        "score": 0.2554217701680895,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.23581480715735598,
                        "answer": "bark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lion"
                ],
                "rank": 2573,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7242672592401505
            },
            {
                "question verbose": "What is to magpie ",
                "b": "magpie",
                "expected answer": [
                    "chatter"
                ],
                "predictions": [
                    {
                        "score": 0.6538162496843456,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2513605865856483,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2190315184227349,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.21802700722533586,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2154452955124623,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.21039799766464679,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "magpie"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to mallard ",
                "b": "mallard",
                "expected answer": [
                    "quack"
                ],
                "predictions": [
                    {
                        "score": 0.6534643491083784,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24862930654739696,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.22201341477056183,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.22125119533261037,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21798269243333673,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.21371738390627,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mallard"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to monkey ",
                "b": "monkey",
                "expected answer": [
                    "chatter",
                    "gibber",
                    "howl",
                    "scream"
                ],
                "predictions": [
                    {
                        "score": 0.3924410427077415,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.34317299629034026,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.3201000995731259,
                        "answer": "comfy",
                        "hit": false
                    },
                    {
                        "score": 0.3049395447720956,
                        "answer": "bomber",
                        "hit": false
                    },
                    {
                        "score": 0.29784342112848466,
                        "answer": "breakthrough",
                        "hit": false
                    },
                    {
                        "score": 0.29743437687178487,
                        "answer": "danielle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "monkey"
                ],
                "rank": 287,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5983133018016815
            },
            {
                "question verbose": "What is to moose ",
                "b": "moose",
                "expected answer": [
                    "bellow"
                ],
                "predictions": [
                    {
                        "score": 0.6526987067673161,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23986258159520304,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21901761258041777,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21854301165385628,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20957914278934967,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20561306949024286,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "moose"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to mouse ",
                "b": "mouse",
                "expected answer": [
                    "squeak"
                ],
                "predictions": [
                    {
                        "score": 0.39223143996588594,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.38586146981738983,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.35026270431141654,
                        "answer": "npcstory",
                        "hit": false
                    },
                    {
                        "score": 0.3265160258983634,
                        "answer": "tactile",
                        "hit": false
                    },
                    {
                        "score": 0.3201503376649182,
                        "answer": "willow",
                        "hit": false
                    },
                    {
                        "score": 0.3181345896672003,
                        "answer": "kindle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mouse"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6057921424508095
            },
            {
                "question verbose": "What is to mule ",
                "b": "mule",
                "expected answer": [
                    "bray",
                    "hee-haw"
                ],
                "predictions": [
                    {
                        "score": 0.3823592086524197,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.3673629299325678,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.33176946411366703,
                        "answer": "fdr",
                        "hit": false
                    },
                    {
                        "score": 0.32877000179876253,
                        "answer": "salicylate",
                        "hit": false
                    },
                    {
                        "score": 0.3153597846209071,
                        "answer": "comfy",
                        "hit": false
                    },
                    {
                        "score": 0.3119862798453357,
                        "answer": "veneer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mule"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6049830093979836
            },
            {
                "question verbose": "What is to pig ",
                "b": "pig",
                "expected answer": [
                    "oink",
                    "grunt",
                    "gruff",
                    "squeal"
                ],
                "predictions": [
                    {
                        "score": 0.3955706963714109,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.3697676862849764,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.35982014435808657,
                        "answer": "sufficient",
                        "hit": false
                    },
                    {
                        "score": 0.3545014484355648,
                        "answer": "statistical",
                        "hit": false
                    },
                    {
                        "score": 0.35294687203295994,
                        "answer": "veneer",
                        "hit": false
                    },
                    {
                        "score": 0.35165466053657,
                        "answer": "salicylate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pig"
                ],
                "rank": 1390,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.648732140660286
            },
            {
                "question verbose": "What is to pigeon ",
                "b": "pigeon",
                "expected answer": [
                    "coo"
                ],
                "predictions": [
                    {
                        "score": 0.6520073623400138,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2451935584935257,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.22225676170434888,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.21601608565848593,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.20771255638034508,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20588092879936856,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pigeon"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to rat ",
                "b": "rat",
                "expected answer": [
                    "squeak"
                ],
                "predictions": [
                    {
                        "score": 0.26664954678219005,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2637834160099104,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.24495614856117845,
                        "answer": "sketch",
                        "hit": false
                    },
                    {
                        "score": 0.23950223549953514,
                        "answer": "immediate",
                        "hit": false
                    },
                    {
                        "score": 0.23916943409103913,
                        "answer": "confuse",
                        "hit": false
                    },
                    {
                        "score": 0.23362562449138696,
                        "answer": "describing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rat"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5815313160419464
            },
            {
                "question verbose": "What is to raven ",
                "b": "raven",
                "expected answer": [
                    "caw"
                ],
                "predictions": [
                    {
                        "score": 0.6527797442763787,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24445464250350118,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2175816035756171,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2072649621059415,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20242753832656202,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20131569110923128,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "raven"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to seal ",
                "b": "seal",
                "expected answer": [
                    "bark"
                ],
                "predictions": [
                    {
                        "score": 0.34964542229020174,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.3272413009868623,
                        "answer": "educational",
                        "hit": false
                    },
                    {
                        "score": 0.3258940958058709,
                        "answer": "namely",
                        "hit": false
                    },
                    {
                        "score": 0.3114048553646404,
                        "answer": "uhf",
                        "hit": false
                    },
                    {
                        "score": 0.30515243005504455,
                        "answer": "popularity",
                        "hit": false
                    },
                    {
                        "score": 0.30118932064638415,
                        "answer": "allegory",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seal"
                ],
                "rank": 10,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7380653619766235
            },
            {
                "question verbose": "What is to sheep ",
                "b": "sheep",
                "expected answer": [
                    "baa",
                    "bleat"
                ],
                "predictions": [
                    {
                        "score": 0.396876759392144,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.3489291704838877,
                        "answer": "firmly",
                        "hit": false
                    },
                    {
                        "score": 0.34675315412694296,
                        "answer": "reminder",
                        "hit": false
                    },
                    {
                        "score": 0.32399024317993236,
                        "answer": "customizations",
                        "hit": false
                    },
                    {
                        "score": 0.319294877295327,
                        "answer": "commiseration",
                        "hit": false
                    },
                    {
                        "score": 0.31604103342879764,
                        "answer": "stewart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sheep"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6240415796637535
            },
            {
                "question verbose": "What is to snake ",
                "b": "snake",
                "expected answer": [
                    "hiss"
                ],
                "predictions": [
                    {
                        "score": 0.46815886957080693,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.35769122498948464,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.35371350270971985,
                        "answer": "essentially",
                        "hit": false
                    },
                    {
                        "score": 0.34402618974733673,
                        "answer": "namely",
                        "hit": false
                    },
                    {
                        "score": 0.3430322323872661,
                        "answer": "uhf",
                        "hit": false
                    },
                    {
                        "score": 0.33578215536983624,
                        "answer": "governmental",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "snake"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to songbird ",
                "b": "songbird",
                "expected answer": [
                    "chirrup",
                    "chirp",
                    "tweet",
                    "sing",
                    "warble",
                    "twitter"
                ],
                "predictions": [
                    {
                        "score": 0.6527764140850907,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24637061403086,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21710936556502414,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.204169856033954,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2036038939625068,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.2015767949360515,
                        "answer": "governmental",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "songbird"
                ],
                "rank": 4014,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to tiger ",
                "b": "tiger",
                "expected answer": [
                    "growl",
                    "roar"
                ],
                "predictions": [
                    {
                        "score": 0.6522597580775957,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2457329825265652,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.22357065728396777,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.21920751046087836,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.21574469532369453,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.20671681978673906,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiger"
                ],
                "rank": 566,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to toad ",
                "b": "toad",
                "expected answer": [
                    "ribbit",
                    "croak"
                ],
                "predictions": [
                    {
                        "score": 0.6527787241928698,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2524091717011823,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.22835465261221202,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.22521262180959628,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.2201231879947176,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.2178911102614105,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "toad"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to turkey ",
                "b": "turkey",
                "expected answer": [
                    "gobble"
                ],
                "predictions": [
                    {
                        "score": 0.39448697047644077,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.337892706973292,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.3207202251085521,
                        "answer": "comfy",
                        "hit": false
                    },
                    {
                        "score": 0.31016239760180936,
                        "answer": "fdr",
                        "hit": false
                    },
                    {
                        "score": 0.3074332205891795,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.3070775585549625,
                        "answer": "breakthrough",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "turkey"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6434292942285538
            },
            {
                "question verbose": "What is to wasp ",
                "b": "wasp",
                "expected answer": [
                    "buzz"
                ],
                "predictions": [
                    {
                        "score": 0.6583867381109282,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24728608534626126,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.20817306119541107,
                        "answer": "buzz",
                        "hit": true
                    },
                    {
                        "score": 0.20691578180766992,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.20454196758904222,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.19504594250597254,
                        "answer": "oppress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wasp"
                ],
                "rank": 2,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6460484117269516
            },
            {
                "question verbose": "What is to whale ",
                "b": "whale",
                "expected answer": [
                    "sing"
                ],
                "predictions": [
                    {
                        "score": 0.6602652448988356,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22349336932985694,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21679945775095844,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.20038767204984395,
                        "answer": "governmental",
                        "hit": false
                    },
                    {
                        "score": 0.19574581531346155,
                        "answer": "oppress",
                        "hit": false
                    },
                    {
                        "score": 0.19438492389871262,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "whale"
                ],
                "rank": 13578,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5616360791027546
            },
            {
                "question verbose": "What is to wolf ",
                "b": "wolf",
                "expected answer": [
                    "howl"
                ],
                "predictions": [
                    {
                        "score": 0.3673871339564443,
                        "answer": "buzz",
                        "hit": false
                    },
                    {
                        "score": 0.3443617946837517,
                        "answer": "maxine",
                        "hit": false
                    },
                    {
                        "score": 0.32018797688058126,
                        "answer": "lebowski",
                        "hit": false
                    },
                    {
                        "score": 0.31692371831786875,
                        "answer": "bobblehead",
                        "hit": false
                    },
                    {
                        "score": 0.311963479716605,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.30909462303301594,
                        "answer": "customizations",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wolf"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6455591470003128
            }
        ],
        "result": {
            "cnt_questions_correct": 2,
            "cnt_questions_total": 50,
            "accuracy": 0.04
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E07 [animal - sound].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "7d87a281-ede1-41cf-99c8-e27ecb68714a",
            "timestamp": "2020-10-22T15:57:23.716621"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to ant ",
                "b": "ant",
                "expected answer": [
                    "anthill",
                    "insectarium",
                    "terrarium",
                    "formicarium"
                ],
                "predictions": [
                    {
                        "score": 0.598217432581487,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.49852742448048115,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.4866075230931346,
                        "answer": "seawater",
                        "hit": false
                    },
                    {
                        "score": 0.4827707052194889,
                        "answer": "unpacking",
                        "hit": false
                    },
                    {
                        "score": 0.4825015147723513,
                        "answer": "romneycare",
                        "hit": false
                    },
                    {
                        "score": 0.4810635084909552,
                        "answer": "markey",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ant"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6763644963502884
            },
            {
                "question verbose": "What is to ape ",
                "b": "ape",
                "expected answer": [
                    "grove",
                    "tree",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.43163667203189465,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.37099324454310445,
                        "answer": "jumped",
                        "hit": false
                    },
                    {
                        "score": 0.3695486928008072,
                        "answer": "cheapest",
                        "hit": false
                    },
                    {
                        "score": 0.3630043410256579,
                        "answer": "fluidity",
                        "hit": false
                    },
                    {
                        "score": 0.3582927991574449,
                        "answer": "alex",
                        "hit": false
                    },
                    {
                        "score": 0.3567279601754592,
                        "answer": "corner",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ape"
                ],
                "rank": 2139,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6147876083850861
            },
            {
                "question verbose": "What is to baboon ",
                "b": "baboon",
                "expected answer": [
                    "grove",
                    "tree",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.44236843709470425,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2584555554165111,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2527813086503323,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24538355665060882,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2433731554904721,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2433643806247691,
                        "answer": "tuned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "baboon"
                ],
                "rank": 240,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6125465855002403
            },
            {
                "question verbose": "What is to bat ",
                "b": "bat",
                "expected answer": [
                    "cave",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.5413516765419712,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.5372590691949392,
                        "answer": "river",
                        "hit": false
                    },
                    {
                        "score": 0.5255807803306914,
                        "answer": "chemist",
                        "hit": false
                    },
                    {
                        "score": 0.5224857782967846,
                        "answer": "presbyterian",
                        "hit": false
                    },
                    {
                        "score": 0.5144601018274021,
                        "answer": "columbia",
                        "hit": false
                    },
                    {
                        "score": 0.5027231859781498,
                        "answer": "stanford",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bat"
                ],
                "rank": 6361,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8125710785388947
            },
            {
                "question verbose": "What is to bear ",
                "b": "bear",
                "expected answer": [
                    "den",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.3013386943105272,
                        "answer": "stanford",
                        "hit": false
                    },
                    {
                        "score": 0.29538185880216705,
                        "answer": "fran",
                        "hit": false
                    },
                    {
                        "score": 0.2933929911146983,
                        "answer": "gastroenterology",
                        "hit": false
                    },
                    {
                        "score": 0.2898736788313047,
                        "answer": "maryland",
                        "hit": false
                    },
                    {
                        "score": 0.2865599987830261,
                        "answer": "townsend",
                        "hit": false
                    },
                    {
                        "score": 0.2852770887310711,
                        "answer": "oklahoma",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bear"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6060080528259277
            },
            {
                "question verbose": "What is to beaver ",
                "b": "beaver",
                "expected answer": [
                    "dam",
                    "river",
                    "pen"
                ],
                "predictions": [
                    {
                        "score": 0.5523780180754787,
                        "answer": "stanford",
                        "hit": false
                    },
                    {
                        "score": 0.5502215074789659,
                        "answer": "taipei",
                        "hit": false
                    },
                    {
                        "score": 0.5324839587302987,
                        "answer": "sanitation",
                        "hit": false
                    },
                    {
                        "score": 0.5115248264366948,
                        "answer": "arkansas",
                        "hit": false
                    },
                    {
                        "score": 0.5034273449811657,
                        "answer": "hokkaido",
                        "hit": false
                    },
                    {
                        "score": 0.5032301554692612,
                        "answer": "unveiled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beaver"
                ],
                "rank": 156,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7128506749868393
            },
            {
                "question verbose": "What is to bee ",
                "b": "bee",
                "expected answer": [
                    "hive"
                ],
                "predictions": [
                    {
                        "score": 0.5168810441495874,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.39402893000677913,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.3711661338062869,
                        "answer": "eia",
                        "hit": false
                    },
                    {
                        "score": 0.3579465991539442,
                        "answer": "bowman",
                        "hit": false
                    },
                    {
                        "score": 0.3486603065747273,
                        "answer": "fyi",
                        "hit": false
                    },
                    {
                        "score": 0.3485364974295002,
                        "answer": "remade",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bee"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5914601683616638
            },
            {
                "question verbose": "What is to cattle ",
                "b": "cattle",
                "expected answer": [
                    "barn",
                    "coral"
                ],
                "predictions": [
                    {
                        "score": 0.44093902377528355,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2592689907362368,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2571213958777031,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25453763593508777,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.25381821234471497,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2527705310094365,
                        "answer": "tuned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cattle"
                ],
                "rank": 3338,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6463530361652374
            },
            {
                "question verbose": "What is to chimpanzee ",
                "b": "chimpanzee",
                "expected answer": [
                    "grove",
                    "tree",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.4433265734144175,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2738202677306218,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2640413118486691,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.25838479200177916,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.24824338498314358,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2468781952230761,
                        "answer": "eligibility",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chimpanzee"
                ],
                "rank": 278,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6125465855002403
            },
            {
                "question verbose": "What is to chinchilla ",
                "b": "chinchilla",
                "expected answer": [
                    "nest",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.6414637810418445,
                        "answer": "coat",
                        "hit": false
                    },
                    {
                        "score": 0.6249419652738535,
                        "answer": "lynx",
                        "hit": false
                    },
                    {
                        "score": 0.6172562627642133,
                        "answer": "sable",
                        "hit": false
                    },
                    {
                        "score": 0.6013501044200744,
                        "answer": "nyc",
                        "hit": false
                    },
                    {
                        "score": 0.5951272483397235,
                        "answer": "fur",
                        "hit": false
                    },
                    {
                        "score": 0.5904434862305371,
                        "answer": "jacket",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chinchilla"
                ],
                "rank": 24,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7739180326461792
            },
            {
                "question verbose": "What is to cockroach ",
                "b": "cockroach",
                "expected answer": [
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.4765849047267415,
                        "answer": "nest",
                        "hit": true
                    },
                    {
                        "score": 0.40862011262038106,
                        "answer": "bowman",
                        "hit": false
                    },
                    {
                        "score": 0.38945590437436634,
                        "answer": "bureau",
                        "hit": false
                    },
                    {
                        "score": 0.38484743356233303,
                        "answer": "ramble",
                        "hit": false
                    },
                    {
                        "score": 0.3801027858654462,
                        "answer": "flyer",
                        "hit": false
                    },
                    {
                        "score": 0.37870994639646394,
                        "answer": "hardcore",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cockroach"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7887179255485535
            },
            {
                "question verbose": "What is to cricket ",
                "b": "cricket",
                "expected answer": [
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.4435729214617193,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2800304241849973,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2595231771156612,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2530992170275704,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25057271783185064,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.24242196472267616,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cricket"
                ],
                "rank": 585,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to crocodile ",
                "b": "crocodile",
                "expected answer": [
                    "river",
                    "lake",
                    "pool"
                ],
                "predictions": [
                    {
                        "score": 0.4459162294120885,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2803535158191766,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.27121409686669523,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2613192103918055,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2575500176947527,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.2452766960733262,
                        "answer": "referendum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crocodile"
                ],
                "rank": 7244,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5524055324494839
            },
            {
                "question verbose": "What is to crow ",
                "b": "crow",
                "expected answer": [
                    "nest",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.4527430340679248,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.42997611474314773,
                        "answer": "seawater",
                        "hit": false
                    },
                    {
                        "score": 0.4297400289759465,
                        "answer": "jofaz",
                        "hit": false
                    },
                    {
                        "score": 0.4181767656885223,
                        "answer": "ballot",
                        "hit": false
                    },
                    {
                        "score": 0.41067547969029794,
                        "answer": "seven",
                        "hit": false
                    },
                    {
                        "score": 0.40858216547229076,
                        "answer": "northrop",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crow"
                ],
                "rank": 50,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7161122411489487
            },
            {
                "question verbose": "What is to dog ",
                "b": "dog",
                "expected answer": [
                    "doghouse",
                    "home",
                    "den",
                    "kennel"
                ],
                "predictions": [
                    {
                        "score": 0.3751333321094196,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.36046963913685043,
                        "answer": "freeloading",
                        "hit": false
                    },
                    {
                        "score": 0.351731581498689,
                        "answer": "victimizer",
                        "hit": false
                    },
                    {
                        "score": 0.34164470295038935,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.3342927681384463,
                        "answer": "downhill",
                        "hit": false
                    },
                    {
                        "score": 0.33205224730483385,
                        "answer": "coaster",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dog"
                ],
                "rank": 12202,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6415438055992126
            },
            {
                "question verbose": "What is to dolphin ",
                "b": "dolphin",
                "expected answer": [
                    "sea",
                    "sanctuary"
                ],
                "predictions": [
                    {
                        "score": 0.4249652365522325,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26207723172358427,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2574971548420285,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.251786867698333,
                        "answer": "livid",
                        "hit": false
                    },
                    {
                        "score": 0.2516727230250605,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2430451370325644,
                        "answer": "eligibility",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dolphin"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to duck ",
                "b": "duck",
                "expected answer": [
                    "pond",
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.49817431220692066,
                        "answer": "unveiled",
                        "hit": false
                    },
                    {
                        "score": 0.47380959574854664,
                        "answer": "stanford",
                        "hit": false
                    },
                    {
                        "score": 0.4725473237550667,
                        "answer": "appx",
                        "hit": false
                    },
                    {
                        "score": 0.47120210992246064,
                        "answer": "highway",
                        "hit": false
                    },
                    {
                        "score": 0.4632130963966658,
                        "answer": "laura",
                        "hit": false
                    },
                    {
                        "score": 0.4614595394977888,
                        "answer": "ap",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "duck"
                ],
                "rank": 17,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7383204400539398
            },
            {
                "question verbose": "What is to fish ",
                "b": "fish",
                "expected answer": [
                    "sea",
                    "lake",
                    "river",
                    "acquarium",
                    "farm",
                    "sanctuary"
                ],
                "predictions": [
                    {
                        "score": 0.4790482986724902,
                        "answer": "river",
                        "hit": true
                    },
                    {
                        "score": 0.417971238509265,
                        "answer": "angler",
                        "hit": false
                    },
                    {
                        "score": 0.3952736000623883,
                        "answer": "begun",
                        "hit": false
                    },
                    {
                        "score": 0.38525033757849525,
                        "answer": "km",
                        "hit": false
                    },
                    {
                        "score": 0.3822448125719749,
                        "answer": "pond",
                        "hit": false
                    },
                    {
                        "score": 0.3692657255988736,
                        "answer": "raided",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fish"
                ],
                "rank": 0,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5384926944971085
            },
            {
                "question verbose": "What is to fly ",
                "b": "fly",
                "expected answer": [
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.33333760773503396,
                        "answer": "surfing",
                        "hit": false
                    },
                    {
                        "score": 0.32911610277788894,
                        "answer": "recycling",
                        "hit": false
                    },
                    {
                        "score": 0.32457228973033647,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.31904925768063924,
                        "answer": "airbender",
                        "hit": false
                    },
                    {
                        "score": 0.3158628611520574,
                        "answer": "beijing",
                        "hit": false
                    },
                    {
                        "score": 0.312723292842177,
                        "answer": "squad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fly"
                ],
                "rank": 15,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6755438894033432
            },
            {
                "question verbose": "What is to fox ",
                "b": "fox",
                "expected answer": [
                    "den",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.378222476605129,
                        "answer": "coat",
                        "hit": false
                    },
                    {
                        "score": 0.3481294494188632,
                        "answer": "sable",
                        "hit": false
                    },
                    {
                        "score": 0.3456083385673488,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.3310537288847367,
                        "answer": "jacket",
                        "hit": false
                    },
                    {
                        "score": 0.3250989123664432,
                        "answer": "russian",
                        "hit": false
                    },
                    {
                        "score": 0.32397550057552826,
                        "answer": "luxury",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fox"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6103095263242722
            },
            {
                "question verbose": "What is to goldfish ",
                "b": "goldfish",
                "expected answer": [
                    "pond",
                    "bowl",
                    "aquarium",
                    "sanctuary"
                ],
                "predictions": [
                    {
                        "score": 0.4447110764533252,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2598995863069047,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2585400189399244,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2532099696915091,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2460978994181109,
                        "answer": "orden",
                        "hit": false
                    },
                    {
                        "score": 0.2447905213021689,
                        "answer": "tuned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goldfish"
                ],
                "rank": 756,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6084233149886131
            },
            {
                "question verbose": "What is to gorilla ",
                "b": "gorilla",
                "expected answer": [
                    "grove",
                    "tree",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.5137536581284804,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.42590736904185855,
                        "answer": "adopted",
                        "hit": false
                    },
                    {
                        "score": 0.42151036765035804,
                        "answer": "alleviate",
                        "hit": false
                    },
                    {
                        "score": 0.4170940390574993,
                        "answer": "stint",
                        "hit": false
                    },
                    {
                        "score": 0.40857794043356727,
                        "answer": "cta",
                        "hit": false
                    },
                    {
                        "score": 0.406228043732659,
                        "answer": "stimulate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gorilla"
                ],
                "rank": 384,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7035465091466904
            },
            {
                "question verbose": "What is to hamster ",
                "b": "hamster",
                "expected answer": [
                    "nest",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.4435989961972068,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2682893047917635,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2596436002045471,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.254109678552279,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2511207573113308,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.25091192823728303,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hamster"
                ],
                "rank": 536,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to hedgehog ",
                "b": "hedgehog",
                "expected answer": [
                    "nest",
                    "hedge",
                    "pen"
                ],
                "predictions": [
                    {
                        "score": 0.4438977902715297,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2655194202531212,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2624501107652264,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2529727664967338,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2508091620914109,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.25034009667607227,
                        "answer": "bridge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hedgehog"
                ],
                "rank": 626,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to herring ",
                "b": "herring",
                "expected answer": [
                    "sea",
                    "sanctuary"
                ],
                "predictions": [
                    {
                        "score": 0.4274114266818001,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26872380416043606,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2535831652081336,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.25105069545842895,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24948568720584394,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.24434820965827372,
                        "answer": "bridge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "herring"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to hippopotamus ",
                "b": "hippopotamus",
                "expected answer": [
                    "river",
                    "lake",
                    "pen"
                ],
                "predictions": [
                    {
                        "score": 0.440960494097581,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2572578844684555,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.25083548108690135,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.24943592411742557,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2481180219004776,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.24037322349708587,
                        "answer": "befriends",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hippopotamus"
                ],
                "rank": 7503,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5524055324494839
            },
            {
                "question verbose": "What is to hornet ",
                "b": "hornet",
                "expected answer": [
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.49760175671969437,
                        "answer": "nest",
                        "hit": true
                    },
                    {
                        "score": 0.4269364724168805,
                        "answer": "corner",
                        "hit": false
                    },
                    {
                        "score": 0.40988036069435557,
                        "answer": "unluckily",
                        "hit": false
                    },
                    {
                        "score": 0.40878731290747583,
                        "answer": "acquired",
                        "hit": false
                    },
                    {
                        "score": 0.4072841839890266,
                        "answer": "trayvon",
                        "hit": false
                    },
                    {
                        "score": 0.39772911919672344,
                        "answer": "downhill",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hornet"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8006919622421265
            },
            {
                "question verbose": "What is to horse ",
                "b": "horse",
                "expected answer": [
                    "stable",
                    "range",
                    "paddock",
                    "corral"
                ],
                "predictions": [
                    {
                        "score": 0.3720851684216616,
                        "answer": "bute",
                        "hit": false
                    },
                    {
                        "score": 0.36389205007994274,
                        "answer": "lasix",
                        "hit": false
                    },
                    {
                        "score": 0.3623118608421158,
                        "answer": "medication",
                        "hit": false
                    },
                    {
                        "score": 0.3352932132407795,
                        "answer": "salix",
                        "hit": false
                    },
                    {
                        "score": 0.3229498181626217,
                        "answer": "race",
                        "hit": false
                    },
                    {
                        "score": 0.3216892678062192,
                        "answer": "sample",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "horse"
                ],
                "rank": 523,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6511574685573578
            },
            {
                "question verbose": "What is to insect ",
                "b": "insect",
                "expected answer": [
                    "nest",
                    "cage",
                    "box"
                ],
                "predictions": [
                    {
                        "score": 0.4429543330626014,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2622354010494315,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2616135262206747,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.25890188758332394,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.25217086762698393,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.249056697046144,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "insect"
                ],
                "rank": 576,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to lion ",
                "b": "lion",
                "expected answer": [
                    "den",
                    "cage",
                    "savannah"
                ],
                "predictions": [
                    {
                        "score": 0.45045068011243217,
                        "answer": "esther",
                        "hit": false
                    },
                    {
                        "score": 0.43692084444134394,
                        "answer": "rial",
                        "hit": false
                    },
                    {
                        "score": 0.42231830892563466,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.41224283849947685,
                        "answer": "respectively",
                        "hit": false
                    },
                    {
                        "score": 0.41064790433413184,
                        "answer": "thurgood",
                        "hit": false
                    },
                    {
                        "score": 0.4082725047360527,
                        "answer": "unveiled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lion"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5876835957169533
            },
            {
                "question verbose": "What is to locust ",
                "b": "locust",
                "expected answer": [
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.4435069319889567,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2668316792166395,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.26576565667664626,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2653135398054707,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.25994950206392514,
                        "answer": "orden",
                        "hit": false
                    },
                    {
                        "score": 0.25524441950976484,
                        "answer": "tuned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locust"
                ],
                "rank": 751,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to mallard ",
                "b": "mallard",
                "expected answer": [
                    "nest",
                    "pond"
                ],
                "predictions": [
                    {
                        "score": 0.4468608099852186,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2697248479842872,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2595487626292592,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.2569171527422773,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2503138241606092,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.24472943974302233,
                        "answer": "bridge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mallard"
                ],
                "rank": 567,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to mole ",
                "b": "mole",
                "expected answer": [
                    "hole",
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.443918252327198,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25603445868800356,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2549238916334754,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2482577619263341,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24702853600768954,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.24681359432332867,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mole"
                ],
                "rank": 500,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5364563353359699
            },
            {
                "question verbose": "What is to monkey ",
                "b": "monkey",
                "expected answer": [
                    "tree",
                    "grove",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.5366570433373293,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.45507117128242286,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.4284368399827,
                        "answer": "stint",
                        "hit": false
                    },
                    {
                        "score": 0.4241651587349674,
                        "answer": "trayvon",
                        "hit": false
                    },
                    {
                        "score": 0.412659382297295,
                        "answer": "unreasonable",
                        "hit": false
                    },
                    {
                        "score": 0.4122497659804218,
                        "answer": "fredericksburg",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "monkey"
                ],
                "rank": 177,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6940579265356064
            },
            {
                "question verbose": "What is to mouse ",
                "b": "mouse",
                "expected answer": [
                    "nest",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.40935848577145745,
                        "answer": "nest",
                        "hit": true
                    },
                    {
                        "score": 0.35611165084829854,
                        "answer": "flyer",
                        "hit": false
                    },
                    {
                        "score": 0.3558979955336953,
                        "answer": "lurking",
                        "hit": false
                    },
                    {
                        "score": 0.34805211841932376,
                        "answer": "arded",
                        "hit": false
                    },
                    {
                        "score": 0.3343341701805177,
                        "answer": "statistical",
                        "hit": false
                    },
                    {
                        "score": 0.3297678603220687,
                        "answer": "acct",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mouse"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7475641518831253
            },
            {
                "question verbose": "What is to pig ",
                "b": "pig",
                "expected answer": [
                    "sty",
                    "pigsty",
                    "pen",
                    "pigpen"
                ],
                "predictions": [
                    {
                        "score": 0.5822296873870014,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.5631260759459128,
                        "answer": "hokkaido",
                        "hit": false
                    },
                    {
                        "score": 0.5503818207181971,
                        "answer": "seawater",
                        "hit": false
                    },
                    {
                        "score": 0.5269614416469629,
                        "answer": "fernando",
                        "hit": false
                    },
                    {
                        "score": 0.5269355175550161,
                        "answer": "columbia",
                        "hit": false
                    },
                    {
                        "score": 0.5238155694903107,
                        "answer": "fluidity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pig"
                ],
                "rank": 5355,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.648732140660286
            },
            {
                "question verbose": "What is to rabbit ",
                "b": "rabbit",
                "expected answer": [
                    "burrow",
                    "warren",
                    "hutch",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.5194587887101062,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.484483807471144,
                        "answer": "wsufootballblogcom",
                        "hit": false
                    },
                    {
                        "score": 0.4732933525856278,
                        "answer": "planningzba",
                        "hit": false
                    },
                    {
                        "score": 0.45486257984125883,
                        "answer": "unveiled",
                        "hit": false
                    },
                    {
                        "score": 0.45074858213415836,
                        "answer": "stanford",
                        "hit": false
                    },
                    {
                        "score": 0.4405928190503898,
                        "answer": "rigorous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rabbit"
                ],
                "rank": 10090,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6916489005088806
            },
            {
                "question verbose": "What is to rat ",
                "b": "rat",
                "expected answer": [
                    "nest",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.5344634942603902,
                        "answer": "administered",
                        "hit": false
                    },
                    {
                        "score": 0.5029424890785096,
                        "answer": "indicated",
                        "hit": false
                    },
                    {
                        "score": 0.4370597855563657,
                        "answer": "river",
                        "hit": false
                    },
                    {
                        "score": 0.426222407039618,
                        "answer": "chavez",
                        "hit": false
                    },
                    {
                        "score": 0.4059616915301596,
                        "answer": "apap",
                        "hit": false
                    },
                    {
                        "score": 0.3978188561491293,
                        "answer": "acetaminophen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rat"
                ],
                "rank": 7,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7348455339670181
            },
            {
                "question verbose": "What is to raven ",
                "b": "raven",
                "expected answer": [
                    "nest",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.442800375884448,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2719184642626072,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.263773424736399,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.26181933740505475,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.25792918112889623,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.25421838452259565,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "raven"
                ],
                "rank": 628,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to scorpion ",
                "b": "scorpion",
                "expected answer": [
                    "nest",
                    "aquarium",
                    "terrarium"
                ],
                "predictions": [
                    {
                        "score": 0.44474890692704905,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27770667348273215,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.27423979045566393,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2616305963349324,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.2521000232665506,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2508086318961831,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "scorpion"
                ],
                "rank": 631,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to seal ",
                "b": "seal",
                "expected answer": [
                    "den",
                    "aquarium",
                    "sea"
                ],
                "predictions": [
                    {
                        "score": 0.5486490914889396,
                        "answer": "hadid",
                        "hit": false
                    },
                    {
                        "score": 0.5469517003161286,
                        "answer": "hokkaido",
                        "hit": false
                    },
                    {
                        "score": 0.5402985502968847,
                        "answer": "stanford",
                        "hit": false
                    },
                    {
                        "score": 0.5355858154399692,
                        "answer": "architect",
                        "hit": false
                    },
                    {
                        "score": 0.5289945734233836,
                        "answer": "sanitation",
                        "hit": false
                    },
                    {
                        "score": 0.5255827476481502,
                        "answer": "commerce",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seal"
                ],
                "rank": 225,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.587302178144455
            },
            {
                "question verbose": "What is to snake ",
                "b": "snake",
                "expected answer": [
                    "nest",
                    "pit",
                    "acquarium"
                ],
                "predictions": [
                    {
                        "score": 0.5254662597748733,
                        "answer": "nest",
                        "hit": true
                    },
                    {
                        "score": 0.46491602755950184,
                        "answer": "seawater",
                        "hit": false
                    },
                    {
                        "score": 0.44854385222755083,
                        "answer": "participated",
                        "hit": false
                    },
                    {
                        "score": 0.44685876918125644,
                        "answer": "ibt",
                        "hit": false
                    },
                    {
                        "score": 0.44271621863208693,
                        "answer": "chaired",
                        "hit": false
                    },
                    {
                        "score": 0.4409023663306579,
                        "answer": "identical",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "snake"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8170229196548462
            },
            {
                "question verbose": "What is to spider ",
                "b": "spider",
                "expected answer": [
                    "web",
                    "acquarium",
                    "terrarium"
                ],
                "predictions": [
                    {
                        "score": 0.33818638908601345,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.3201236264028992,
                        "answer": "sanitation",
                        "hit": false
                    },
                    {
                        "score": 0.3086164506851289,
                        "answer": "incorporated",
                        "hit": false
                    },
                    {
                        "score": 0.3047087792405296,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.3034597003859654,
                        "answer": "beggingers",
                        "hit": false
                    },
                    {
                        "score": 0.29986331494202034,
                        "answer": "sordid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spider"
                ],
                "rank": 8697,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8052679300308228
            },
            {
                "question verbose": "What is to termite ",
                "b": "termite",
                "expected answer": [
                    "hill",
                    "terrarium"
                ],
                "predictions": [
                    {
                        "score": 0.4408269954974227,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27659937296449527,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.26936652832332625,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.26496597849213255,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.2553562733404328,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.25088505108811865,
                        "answer": "eligibility",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "termite"
                ],
                "rank": 14935,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5297831688076258
            },
            {
                "question verbose": "What is to tiger ",
                "b": "tiger",
                "expected answer": [
                    "den",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.4282101454441344,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2781731906696841,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.2668630322195864,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2601501426555633,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.25905508029292407,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.2517966842280305,
                        "answer": "bridge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiger"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to trout ",
                "b": "trout",
                "expected answer": [
                    "river",
                    "lake",
                    "sanctuary",
                    "aquarium",
                    "pond",
                    "tank"
                ],
                "predictions": [
                    {
                        "score": 0.44464143156435254,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2633765051640015,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.25818075139243574,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.256447492351004,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2525937963884285,
                        "answer": "orden",
                        "hit": false
                    },
                    {
                        "score": 0.24796735257092856,
                        "answer": "tuned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "trout"
                ],
                "rank": 836,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5524055324494839
            },
            {
                "question verbose": "What is to wasp ",
                "b": "wasp",
                "expected answer": [
                    "nest"
                ],
                "predictions": [
                    {
                        "score": 0.4426629930660496,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29011688108724315,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.26150851810391584,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2609125448111428,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.2526627151241412,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.2513411218402782,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wasp"
                ],
                "rank": 643,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5947554185986519
            },
            {
                "question verbose": "What is to whale ",
                "b": "whale",
                "expected answer": [
                    "sea",
                    "sanctuary"
                ],
                "predictions": [
                    {
                        "score": 0.42658525539936987,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2699348845585092,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.25978965178911545,
                        "answer": "vodka",
                        "hit": false
                    },
                    {
                        "score": 0.257712873805315,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2542520576637854,
                        "answer": "orden",
                        "hit": false
                    },
                    {
                        "score": 0.249555516259871,
                        "answer": "tuned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "whale"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to wolf ",
                "b": "wolf",
                "expected answer": [
                    "den",
                    "cage"
                ],
                "predictions": [
                    {
                        "score": 0.5183183687318094,
                        "answer": "nest",
                        "hit": false
                    },
                    {
                        "score": 0.4454091611359293,
                        "answer": "flyer",
                        "hit": false
                    },
                    {
                        "score": 0.4311508852767517,
                        "answer": "acquired",
                        "hit": false
                    },
                    {
                        "score": 0.42683747155591795,
                        "answer": "henri",
                        "hit": false
                    },
                    {
                        "score": 0.425642493418101,
                        "answer": "fluidity",
                        "hit": false
                    },
                    {
                        "score": 0.4220557018418987,
                        "answer": "highway",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wolf"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6455591470003128
            },
            {
                "question verbose": "What is to woodchuck ",
                "b": "woodchuck",
                "expected answer": [
                    "hole"
                ],
                "predictions": [
                    {
                        "score": 0.4428519794865858,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2811192137575851,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.27118733183803345,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2548154339684379,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.25446477450920557,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.25306905308918154,
                        "answer": "vodka",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "woodchuck"
                ],
                "rank": 12814,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5364563353359699
            }
        ],
        "result": {
            "cnt_questions_correct": 5,
            "cnt_questions_total": 50,
            "accuracy": 0.1
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E08 [animal - shelter].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "967f69bb-affc-404a-a31d-512f522288e0",
            "timestamp": "2020-10-22T15:57:25.401636"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to ant ",
                "b": "ant",
                "expected answer": [
                    "black",
                    "brown",
                    "red"
                ],
                "predictions": [
                    {
                        "score": 0.3770394507096944,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.3529068217387156,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.3500327461610927,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.348597474870783,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.34338030677829334,
                        "answer": "cameo",
                        "hit": false
                    },
                    {
                        "score": 0.34145009572609425,
                        "answer": "green",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ant"
                ],
                "rank": 158,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5939232632517815
            },
            {
                "question verbose": "What is to apple ",
                "b": "apple",
                "expected answer": [
                    "red",
                    "orange",
                    "yellow",
                    "golden"
                ],
                "predictions": [
                    {
                        "score": 0.30435863173477395,
                        "answer": "white",
                        "hit": false
                    },
                    {
                        "score": 0.2987006484112929,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.2618315887905686,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.25640690567445346,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.25162334494623523,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.24307820957999893,
                        "answer": "cameo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apple"
                ],
                "rank": 25,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5683628991246223
            },
            {
                "question verbose": "What is to blackboard ",
                "b": "blackboard",
                "expected answer": [
                    "black",
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.4950328583547259,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.43697586834626917,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.4368277316252941,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.41029439845014853,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.40402153898823134,
                        "answer": "green",
                        "hit": true
                    },
                    {
                        "score": 0.3952891480230629,
                        "answer": "queen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "blackboard"
                ],
                "rank": 4,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6793048232793808
            },
            {
                "question verbose": "What is to blood ",
                "b": "blood",
                "expected answer": [
                    "red"
                ],
                "predictions": [
                    {
                        "score": 0.24534089040992677,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.24087539700808744,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.22887779126556512,
                        "answer": "carolina",
                        "hit": false
                    },
                    {
                        "score": 0.21981246584757852,
                        "answer": "north",
                        "hit": false
                    },
                    {
                        "score": 0.21923128750401302,
                        "answer": "stage",
                        "hit": false
                    },
                    {
                        "score": 0.21661477296127682,
                        "answer": "yellow",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "blood"
                ],
                "rank": 11,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6110890060663223
            },
            {
                "question verbose": "What is to blueberry ",
                "b": "blueberry",
                "expected answer": [
                    "blue",
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.13597093195016813,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1350337873421475,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1291325404694149,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12813014183691787,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12515774822351933,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.12489380841508546,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "blueberry"
                ],
                "rank": 948,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5291442554444075
            },
            {
                "question verbose": "What is to broccoli ",
                "b": "broccoli",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.14155127578039822,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13618357602292563,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13133584991330746,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1279604668962219,
                        "answer": "mother",
                        "hit": false
                    },
                    {
                        "score": 0.1266922620121641,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12612226698288803,
                        "answer": "railroad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "broccoli"
                ],
                "rank": 15322,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to bruise ",
                "b": "bruise",
                "expected answer": [
                    "blue",
                    "purple"
                ],
                "predictions": [
                    {
                        "score": 0.14147518358373098,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1330103361285487,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.13008979203580798,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1296599814449291,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12961153293522718,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.12736402049850773,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bruise"
                ],
                "rank": 1823,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5291442554444075
            },
            {
                "question verbose": "What is to cabbage ",
                "b": "cabbage",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.13927967065138513,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13553964557040393,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13230817554909693,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.1312358099814877,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.13028292333260194,
                        "answer": "hope",
                        "hit": false
                    },
                    {
                        "score": 0.13017652865892348,
                        "answer": "akin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cabbage"
                ],
                "rank": 15326,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to carrot ",
                "b": "carrot",
                "expected answer": [
                    "orange",
                    "red",
                    "yellow"
                ],
                "predictions": [
                    {
                        "score": 0.13275430973047897,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.126050925328598,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1251056901466993,
                        "answer": "mother",
                        "hit": false
                    },
                    {
                        "score": 0.1219619611576515,
                        "answer": "red",
                        "hit": true
                    },
                    {
                        "score": 0.12096626607686341,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.12016403571657022,
                        "answer": "latino",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "carrot"
                ],
                "rank": 3,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6417046189308167
            },
            {
                "question verbose": "What is to cauliflower ",
                "b": "cauliflower",
                "expected answer": [
                    "white",
                    "green",
                    "yellow",
                    "yellowish"
                ],
                "predictions": [
                    {
                        "score": 0.1434885177720718,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1405928658527459,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1299132423653931,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12965821396066876,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1290160058415506,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.126318281830661,
                        "answer": "railroad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cauliflower"
                ],
                "rank": 50,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5514805838465691
            },
            {
                "question verbose": "What is to celery ",
                "b": "celery",
                "expected answer": [
                    "green",
                    "white",
                    "brown"
                ],
                "predictions": [
                    {
                        "score": 0.14256938285051288,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13575869471675714,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.13423275031821308,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.13417748357985643,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.12974594479399132,
                        "answer": "mother",
                        "hit": false
                    },
                    {
                        "score": 0.1280210311074648,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "celery"
                ],
                "rank": 70,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to cherry ",
                "b": "cherry",
                "expected answer": [
                    "red",
                    "yellow",
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.14594790148337897,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1381970251015351,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1337187145860926,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.13368368997678878,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.1331994167923103,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.13098312481976518,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cherry"
                ],
                "rank": 13,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5682006701827049
            },
            {
                "question verbose": "What is to chocolate ",
                "b": "chocolate",
                "expected answer": [
                    "white",
                    "brown",
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.36372716528413657,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.354320917902963,
                        "answer": "white",
                        "hit": true
                    },
                    {
                        "score": 0.35275123518188367,
                        "answer": "flag",
                        "hit": false
                    },
                    {
                        "score": 0.34870774855910824,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.3427823583515217,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.34038838882215666,
                        "answer": "frog",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chocolate"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6925252974033356
            },
            {
                "question verbose": "What is to cloud ",
                "b": "cloud",
                "expected answer": [
                    "white",
                    "gray",
                    "grey"
                ],
                "predictions": [
                    {
                        "score": 0.3288589106062844,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.32751670646158954,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.2854653817841801,
                        "answer": "beaver",
                        "hit": false
                    },
                    {
                        "score": 0.27882941069195377,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.26784021093063215,
                        "answer": "turtle",
                        "hit": false
                    },
                    {
                        "score": 0.25871284564721114,
                        "answer": "cameo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cloud"
                ],
                "rank": 7,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5462691970169544
            },
            {
                "question verbose": "What is to coal ",
                "b": "coal",
                "expected answer": [
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.4020425581419635,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.3369012974164882,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.3270442647898699,
                        "answer": "winnin",
                        "hit": false
                    },
                    {
                        "score": 0.3239231194937021,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.31475154183241744,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.31299020897694185,
                        "answer": "nightstalker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "coal"
                ],
                "rank": 31,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6337870955467224
            },
            {
                "question verbose": "What is to coffee ",
                "b": "coffee",
                "expected answer": [
                    "black",
                    "brown"
                ],
                "predictions": [
                    {
                        "score": 0.4005840901163974,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.3403334916836129,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.3381014052647803,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.3277203252612376,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.3181335454559039,
                        "answer": "white",
                        "hit": false
                    },
                    {
                        "score": 0.30898573632412957,
                        "answer": "rocket",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "coffee"
                ],
                "rank": 58,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6099941432476044
            },
            {
                "question verbose": "What is to cranberry ",
                "b": "cranberry",
                "expected answer": [
                    "red",
                    "purple",
                    "pink"
                ],
                "predictions": [
                    {
                        "score": 0.1348775695971999,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13071697186013007,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1296455355872626,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12942202930967867,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.12482031680448957,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.1216881849554866,
                        "answer": "mother",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cranberry"
                ],
                "rank": 6,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5682006701827049
            },
            {
                "question verbose": "What is to cream ",
                "b": "cream",
                "expected answer": [
                    "white"
                ],
                "predictions": [
                    {
                        "score": 0.37972901006549303,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.3665794930008339,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.35965531574620324,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.3523717507862843,
                        "answer": "incamera",
                        "hit": false
                    },
                    {
                        "score": 0.3512060984941491,
                        "answer": "perry",
                        "hit": false
                    },
                    {
                        "score": 0.3491144183981406,
                        "answer": "nightstalker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cream"
                ],
                "rank": 7,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6851709485054016
            },
            {
                "question verbose": "What is to crow ",
                "b": "crow",
                "expected answer": [
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.3423337614092268,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.3059147418827955,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.2877129775595638,
                        "answer": "rocket",
                        "hit": false
                    },
                    {
                        "score": 0.2872134744487019,
                        "answer": "halfway",
                        "hit": false
                    },
                    {
                        "score": 0.28657432941049704,
                        "answer": "commercial",
                        "hit": false
                    },
                    {
                        "score": 0.2825387955081513,
                        "answer": "perry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crow"
                ],
                "rank": 76,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6027530878782272
            },
            {
                "question verbose": "What is to cucumber ",
                "b": "cucumber",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.13699414818432407,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13416557163718762,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1304982029922948,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12915491025637388,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12673386206990794,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12658149791664652,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cucumber"
                ],
                "rank": 15326,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to emerald ",
                "b": "emerald",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.14605697097498999,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1393178003510826,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.12984037772031481,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.12894317966307883,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12814703420901383,
                        "answer": "mother",
                        "hit": false
                    },
                    {
                        "score": 0.12779606539261198,
                        "answer": "railroad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "emerald"
                ],
                "rank": 15324,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to fridge ",
                "b": "fridge",
                "expected answer": [
                    "white",
                    "silver",
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.1379850958260125,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1328112870946427,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13199673355125074,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12828959502474083,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12813030353105667,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.12758661913905525,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fridge"
                ],
                "rank": 72,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5514805838465691
            },
            {
                "question verbose": "What is to frog ",
                "b": "frog",
                "expected answer": [
                    "green",
                    "brown",
                    "grey",
                    "gray"
                ],
                "predictions": [
                    {
                        "score": 0.645160128568572,
                        "answer": "green",
                        "hit": true
                    },
                    {
                        "score": 0.6273655900985294,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.5997679004551129,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.5571619733813092,
                        "answer": "red",
                        "hit": false
                    },
                    {
                        "score": 0.5374278842842132,
                        "answer": "beaver",
                        "hit": false
                    },
                    {
                        "score": 0.523799547387686,
                        "answer": "turtle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "frog"
                ],
                "rank": 0,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8573789298534393
            },
            {
                "question verbose": "What is to grapes ",
                "b": "grapes",
                "expected answer": [
                    "black",
                    "red",
                    "green",
                    "purple"
                ],
                "predictions": [
                    {
                        "score": 0.13907610283071242,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13708452198672147,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1321651134181825,
                        "answer": "hope",
                        "hit": false
                    },
                    {
                        "score": 0.1315092884504332,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12731837791386663,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12707173878584727,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grapes"
                ],
                "rank": 10,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5300442446023226
            },
            {
                "question verbose": "What is to grass ",
                "b": "grass",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.40916531529847006,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.3946349333285982,
                        "answer": "winnin",
                        "hit": false
                    },
                    {
                        "score": 0.3897255349143307,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.38917464689136916,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.3744742430060888,
                        "answer": "incamera",
                        "hit": false
                    },
                    {
                        "score": 0.3496240390991772,
                        "answer": "nightstalker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grass"
                ],
                "rank": 26,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6663715988397598
            },
            {
                "question verbose": "What is to leaves ",
                "b": "leaves",
                "expected answer": [
                    "green",
                    "red",
                    "yellow"
                ],
                "predictions": [
                    {
                        "score": 0.14266377904710914,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13381773956134851,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.12949086974736518,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1289402079755635,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12570275943704895,
                        "answer": "lgbt",
                        "hit": false
                    },
                    {
                        "score": 0.12532362939932218,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leaves"
                ],
                "rank": 10,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to milk ",
                "b": "milk",
                "expected answer": [
                    "white"
                ],
                "predictions": [
                    {
                        "score": 0.40390765417412444,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.36731361784015604,
                        "answer": "turtle",
                        "hit": false
                    },
                    {
                        "score": 0.3548950850756256,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.35008862879984154,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.34607166286799584,
                        "answer": "winnin",
                        "hit": false
                    },
                    {
                        "score": 0.3454300086514565,
                        "answer": "beaver",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "milk"
                ],
                "rank": 23,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6533793359994888
            },
            {
                "question verbose": "What is to paper ",
                "b": "paper",
                "expected answer": [
                    "white",
                    "color"
                ],
                "predictions": [
                    {
                        "score": 0.31427042588112997,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.2913887445677953,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.2470707677311091,
                        "answer": "pretty",
                        "hit": false
                    },
                    {
                        "score": 0.2454520419428764,
                        "answer": "opened",
                        "hit": false
                    },
                    {
                        "score": 0.2421225172260594,
                        "answer": "black",
                        "hit": false
                    },
                    {
                        "score": 0.2391878501242708,
                        "answer": "costume",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "paper"
                ],
                "rank": 26,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6091193854808807
            },
            {
                "question verbose": "What is to parsley ",
                "b": "parsley",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.14282815814284106,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13800187984305828,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13199828795796395,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.13157353458114077,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.13058082041046148,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.13046128072780214,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "parsley"
                ],
                "rank": 15323,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to peony ",
                "b": "peony",
                "expected answer": [
                    "red",
                    "white",
                    "pink",
                    "purple"
                ],
                "predictions": [
                    {
                        "score": 0.13622246165509355,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13388665463682692,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13133794891299644,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1312749768761396,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.1285990502180971,
                        "answer": "mother",
                        "hit": false
                    },
                    {
                        "score": 0.12670987228693872,
                        "answer": "akin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "peony"
                ],
                "rank": 10,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5682006701827049
            },
            {
                "question verbose": "What is to pepper ",
                "b": "pepper",
                "expected answer": [
                    "black",
                    "red",
                    "green",
                    "yellow",
                    "orange"
                ],
                "predictions": [
                    {
                        "score": 0.5009585762165656,
                        "answer": "green",
                        "hit": true
                    },
                    {
                        "score": 0.4873057139298123,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.44576599854267684,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.42764352269150613,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.42709827057441085,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.4250328476337064,
                        "answer": "beaver",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pepper"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6744206845760345
            },
            {
                "question verbose": "What is to potato ",
                "b": "potato",
                "expected answer": [
                    "brown"
                ],
                "predictions": [
                    {
                        "score": 0.3828348417984837,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.3735477868029027,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.3615607557804763,
                        "answer": "cameo",
                        "hit": false
                    },
                    {
                        "score": 0.3415048686093922,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.33865755811692355,
                        "answer": "white",
                        "hit": false
                    },
                    {
                        "score": 0.32984766001343513,
                        "answer": "nightstalker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "potato"
                ],
                "rank": 12203,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.646842896938324
            },
            {
                "question verbose": "What is to radish ",
                "b": "radish",
                "expected answer": [
                    "red",
                    "pink",
                    "white",
                    "green",
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.1403769040671126,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13802026674117207,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13192334788635282,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.13032995135414657,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.129826854567568,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.12870285391368982,
                        "answer": "railroad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "radish"
                ],
                "rank": 12,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5682006701827049
            },
            {
                "question verbose": "What is to raven ",
                "b": "raven",
                "expected answer": [
                    "black"
                ],
                "predictions": [
                    {
                        "score": 0.13681096766074272,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13218922171665867,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.12887889378653988,
                        "answer": "hope",
                        "hit": false
                    },
                    {
                        "score": 0.1283238059813994,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12738475135923824,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1263487380184742,
                        "answer": "akin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "raven"
                ],
                "rank": 942,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5300442446023226
            },
            {
                "question verbose": "What is to rose ",
                "b": "rose",
                "expected answer": [
                    "red",
                    "yellow",
                    "pink",
                    "white",
                    "blue"
                ],
                "predictions": [
                    {
                        "score": 0.32122156969542603,
                        "answer": "blue",
                        "hit": true
                    },
                    {
                        "score": 0.309915117972072,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.30496968394541607,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.2846442112424114,
                        "answer": "beaver",
                        "hit": false
                    },
                    {
                        "score": 0.2832709855246651,
                        "answer": "red",
                        "hit": true
                    },
                    {
                        "score": 0.2764976357586013,
                        "answer": "highway",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rose"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6593585461378098
            },
            {
                "question verbose": "What is to ruby ",
                "b": "ruby",
                "expected answer": [
                    "red"
                ],
                "predictions": [
                    {
                        "score": 0.29596937777166044,
                        "answer": "living",
                        "hit": false
                    },
                    {
                        "score": 0.27916420593651176,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.2676582626272484,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.26217365982045776,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.25793710335122416,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.2552317931493419,
                        "answer": "beaver",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ruby"
                ],
                "rank": 12,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6249453723430634
            },
            {
                "question verbose": "What is to salt ",
                "b": "salt",
                "expected answer": [
                    "white"
                ],
                "predictions": [
                    {
                        "score": 0.4051101829568105,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.39662148588641916,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.39624856033726785,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.3898187173492831,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.3819280674692179,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.3744827499275998,
                        "answer": "black",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "salt"
                ],
                "rank": 14,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6751036196947098
            },
            {
                "question verbose": "What is to sapphire ",
                "b": "sapphire",
                "expected answer": [
                    "blue"
                ],
                "predictions": [
                    {
                        "score": 0.1509827167564272,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1360462498617336,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.1326341149927526,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.13189631042009453,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.13132323105852475,
                        "answer": "lgbt",
                        "hit": false
                    },
                    {
                        "score": 0.12877720008852514,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sapphire"
                ],
                "rank": 1954,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5291442554444075
            },
            {
                "question verbose": "What is to sea ",
                "b": "sea",
                "expected answer": [
                    "blue",
                    "green",
                    "gray",
                    "grey"
                ],
                "predictions": [
                    {
                        "score": 0.13678720278408824,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13582964093335628,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13201970180256722,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12742419649686693,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12662793545752915,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.12654359089730724,
                        "answer": "mother",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sea"
                ],
                "rank": 1379,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5291442554444075
            },
            {
                "question verbose": "What is to sky ",
                "b": "sky",
                "expected answer": [
                    "blue",
                    "gray",
                    "grey"
                ],
                "predictions": [
                    {
                        "score": 0.572463367849588,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.49821420001301914,
                        "answer": "blue",
                        "hit": true
                    },
                    {
                        "score": 0.4778824187356631,
                        "answer": "grey",
                        "hit": true
                    },
                    {
                        "score": 0.46911643338381287,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.4537984071604915,
                        "answer": "cameo",
                        "hit": false
                    },
                    {
                        "score": 0.451160133432206,
                        "answer": "queen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sky"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7990585565567017
            },
            {
                "question verbose": "What is to snow ",
                "b": "snow",
                "expected answer": [
                    "white"
                ],
                "predictions": [
                    {
                        "score": 0.48024090773504025,
                        "answer": "red",
                        "hit": false
                    },
                    {
                        "score": 0.47836550928026245,
                        "answer": "black",
                        "hit": false
                    },
                    {
                        "score": 0.45491135564617624,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.44015176471993583,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.43462087181157993,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.433328510758158,
                        "answer": "white",
                        "hit": true
                    }
                ],
                "set_exclude": [
                    "snow"
                ],
                "rank": 5,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7344754636287689
            },
            {
                "question verbose": "What is to soil ",
                "b": "soil",
                "expected answer": [
                    "black",
                    "brown",
                    "dark"
                ],
                "predictions": [
                    {
                        "score": 0.4047206063284904,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.38742514325770827,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.38191055895938,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.37307114941638325,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.37101989059894475,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.36438743842079524,
                        "answer": "region",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "soil"
                ],
                "rank": 35,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6284569650888443
            },
            {
                "question verbose": "What is to spinach ",
                "b": "spinach",
                "expected answer": [
                    "green"
                ],
                "predictions": [
                    {
                        "score": 0.13963298382266875,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1364282140625478,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13293710305276868,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.1305899329984248,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.12815155418057575,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12801801243802396,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spinach"
                ],
                "rank": 15327,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4706548936665058
            },
            {
                "question verbose": "What is to sugar ",
                "b": "sugar",
                "expected answer": [
                    "white",
                    "brown"
                ],
                "predictions": [
                    {
                        "score": 0.40519352772333767,
                        "answer": "queen",
                        "hit": false
                    },
                    {
                        "score": 0.40180658246172923,
                        "answer": "frog",
                        "hit": false
                    },
                    {
                        "score": 0.39556636889920666,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.39027441710397737,
                        "answer": "yellow",
                        "hit": false
                    },
                    {
                        "score": 0.3856304841208183,
                        "answer": "beaver",
                        "hit": false
                    },
                    {
                        "score": 0.37608263498240335,
                        "answer": "nightstalker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sugar"
                ],
                "rank": 24,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6593927145004272
            },
            {
                "question verbose": "What is to sun ",
                "b": "sun",
                "expected answer": [
                    "yellow",
                    "gold"
                ],
                "predictions": [
                    {
                        "score": 0.34812089909005794,
                        "answer": "red",
                        "hit": false
                    },
                    {
                        "score": 0.33608675634481777,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.33435662763776525,
                        "answer": "blue",
                        "hit": false
                    },
                    {
                        "score": 0.32413513943268163,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.32292044378107904,
                        "answer": "cameo",
                        "hit": false
                    },
                    {
                        "score": 0.2967399125384844,
                        "answer": "nightstalker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sun"
                ],
                "rank": 12,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6516996473073959
            },
            {
                "question verbose": "What is to swan ",
                "b": "swan",
                "expected answer": [
                    "white",
                    "black",
                    "gray",
                    "grey"
                ],
                "predictions": [
                    {
                        "score": 0.13821755256890036,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13514243112569316,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.13068162962094992,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12721122344558608,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12706372460544368,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12622122108888123,
                        "answer": "matt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "swan"
                ],
                "rank": 65,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5514805838465691
            },
            {
                "question verbose": "What is to tea ",
                "b": "tea",
                "expected answer": [
                    "black",
                    "green",
                    "white",
                    "red",
                    "brown",
                    "yellow"
                ],
                "predictions": [
                    {
                        "score": 0.3555701099840205,
                        "answer": "white",
                        "hit": true
                    },
                    {
                        "score": 0.2903337082645189,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.2702358221173994,
                        "answer": "conservative",
                        "hit": false
                    },
                    {
                        "score": 0.26696694212212957,
                        "answer": "host",
                        "hit": false
                    },
                    {
                        "score": 0.25840259050052905,
                        "answer": "dinner",
                        "hit": false
                    },
                    {
                        "score": 0.2552574619617201,
                        "answer": "ann",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tea"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5662017911672592
            },
            {
                "question verbose": "What is to tomato ",
                "b": "tomato",
                "expected answer": [
                    "red"
                ],
                "predictions": [
                    {
                        "score": 0.35211620833206136,
                        "answer": "nightstalker",
                        "hit": false
                    },
                    {
                        "score": 0.3456243377711067,
                        "answer": "cross",
                        "hit": false
                    },
                    {
                        "score": 0.33818570168301537,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.33502781788175834,
                        "answer": "laziest",
                        "hit": false
                    },
                    {
                        "score": 0.32804366965811654,
                        "answer": "red",
                        "hit": true
                    },
                    {
                        "score": 0.3132335386905976,
                        "answer": "blue",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tomato"
                ],
                "rank": 4,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6836786270141602
            },
            {
                "question verbose": "What is to toothpaste ",
                "b": "toothpaste",
                "expected answer": [
                    "white"
                ],
                "predictions": [
                    {
                        "score": 0.13645665579033478,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.13536285659991446,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1303183659409061,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.12582796347309785,
                        "answer": "hope",
                        "hit": false
                    },
                    {
                        "score": 0.12532493201783684,
                        "answer": "akin",
                        "hit": false
                    },
                    {
                        "score": 0.12522062024137595,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "toothpaste"
                ],
                "rank": 63,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5514805838465691
            },
            {
                "question verbose": "What is to yoghurt ",
                "b": "yoghurt",
                "expected answer": [
                    "white",
                    "pink"
                ],
                "predictions": [
                    {
                        "score": 0.1371797085181367,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.1344236772942208,
                        "answer": "ops",
                        "hit": false
                    },
                    {
                        "score": 0.13332362149507165,
                        "answer": "matt",
                        "hit": false
                    },
                    {
                        "score": 0.13163088479267698,
                        "answer": "america",
                        "hit": false
                    },
                    {
                        "score": 0.12992863069227903,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.12933740176702305,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "yoghurt"
                ],
                "rank": 75,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5514805838465691
            }
        ],
        "result": {
            "cnt_questions_correct": 4,
            "cnt_questions_total": 50,
            "accuracy": 0.08
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E09 [things - color].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "53f0eeb4-9f84-4bef-80fa-a52facc15a12",
            "timestamp": "2020-10-22T15:57:26.625537"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to actor ",
                "b": "actor",
                "expected answer": [
                    "actress"
                ],
                "predictions": [
                    {
                        "score": 0.35702735734731067,
                        "answer": "quantity",
                        "hit": false
                    },
                    {
                        "score": 0.3482839798988627,
                        "answer": "magneto",
                        "hit": false
                    },
                    {
                        "score": 0.33509402155265694,
                        "answer": "mom",
                        "hit": false
                    },
                    {
                        "score": 0.32623122833012286,
                        "answer": "teenager",
                        "hit": false
                    },
                    {
                        "score": 0.3168309684049329,
                        "answer": "conversation",
                        "hit": false
                    },
                    {
                        "score": 0.3095756447009377,
                        "answer": "nostalgic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "actor"
                ],
                "rank": 6410,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.700847864151001
            },
            {
                "question verbose": "What is to batman ",
                "b": "batman",
                "expected answer": [
                    "batwoman"
                ],
                "predictions": [
                    {
                        "score": 0.39089959346026265,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.31964626261181917,
                        "answer": "anonymous",
                        "hit": false
                    },
                    {
                        "score": 0.3146270116781353,
                        "answer": "frazier",
                        "hit": false
                    },
                    {
                        "score": 0.3111389347756316,
                        "answer": "dejected",
                        "hit": false
                    },
                    {
                        "score": 0.30375660263965043,
                        "answer": "nerd",
                        "hit": false
                    },
                    {
                        "score": 0.2990051768942762,
                        "answer": "chestnut",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "batman"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5770980715751648
            },
            {
                "question verbose": "What is to boar ",
                "b": "boar",
                "expected answer": [
                    "sow"
                ],
                "predictions": [
                    {
                        "score": 0.373528481123347,
                        "answer": "honorable",
                        "hit": false
                    },
                    {
                        "score": 0.3662142591613583,
                        "answer": "madeleine",
                        "hit": false
                    },
                    {
                        "score": 0.36335767647165024,
                        "answer": "unprivileged",
                        "hit": false
                    },
                    {
                        "score": 0.36334410309195403,
                        "answer": "quantity",
                        "hit": false
                    },
                    {
                        "score": 0.3550728389949353,
                        "answer": "majored",
                        "hit": false
                    },
                    {
                        "score": 0.3508196926811696,
                        "answer": "spilled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "boar"
                ],
                "rank": 511,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.823950469493866
            },
            {
                "question verbose": "What is to boy ",
                "b": "boy",
                "expected answer": [
                    "girl"
                ],
                "predictions": [
                    {
                        "score": 0.3118320017113386,
                        "answer": "interview",
                        "hit": false
                    },
                    {
                        "score": 0.29539544438687,
                        "answer": "listen",
                        "hit": false
                    },
                    {
                        "score": 0.29048444445156396,
                        "answer": "soaked",
                        "hit": false
                    },
                    {
                        "score": 0.2855387622217385,
                        "answer": "especially",
                        "hit": false
                    },
                    {
                        "score": 0.27800706658654034,
                        "answer": "inside",
                        "hit": false
                    },
                    {
                        "score": 0.27633331418916596,
                        "answer": "pedophile",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "boy"
                ],
                "rank": 183,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7294532954692841
            },
            {
                "question verbose": "What is to brother ",
                "b": "brother",
                "expected answer": [
                    "sister"
                ],
                "predictions": [
                    {
                        "score": 0.2854930880192129,
                        "answer": "oligarch",
                        "hit": false
                    },
                    {
                        "score": 0.2772372465714225,
                        "answer": "frazier",
                        "hit": false
                    },
                    {
                        "score": 0.275547194146178,
                        "answer": "disciple",
                        "hit": false
                    },
                    {
                        "score": 0.2733498958110833,
                        "answer": "married",
                        "hit": false
                    },
                    {
                        "score": 0.26964014846638973,
                        "answer": "bail",
                        "hit": false
                    },
                    {
                        "score": 0.2676001467464332,
                        "answer": "listen",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brother"
                ],
                "rank": 31,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7999731302261353
            },
            {
                "question verbose": "What is to buck ",
                "b": "buck",
                "expected answer": [
                    "doe"
                ],
                "predictions": [
                    {
                        "score": 0.36853242440321143,
                        "answer": "bang",
                        "hit": false
                    },
                    {
                        "score": 0.34538140347424184,
                        "answer": "equip",
                        "hit": false
                    },
                    {
                        "score": 0.34507532091010895,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.33906701606877665,
                        "answer": "fixing",
                        "hit": false
                    },
                    {
                        "score": 0.3364684336870816,
                        "answer": "satisfying",
                        "hit": false
                    },
                    {
                        "score": 0.32869282940943195,
                        "answer": "quantity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "buck"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6437094062566757
            },
            {
                "question verbose": "What is to bull ",
                "b": "bull",
                "expected answer": [
                    "cow"
                ],
                "predictions": [
                    {
                        "score": 0.39635490286756864,
                        "answer": "storytelling",
                        "hit": false
                    },
                    {
                        "score": 0.38845145188852664,
                        "answer": "passionate",
                        "hit": false
                    },
                    {
                        "score": 0.37788711899003935,
                        "answer": "funny",
                        "hit": false
                    },
                    {
                        "score": 0.3722097246518461,
                        "answer": "ape",
                        "hit": false
                    },
                    {
                        "score": 0.36757043543880963,
                        "answer": "shit",
                        "hit": false
                    },
                    {
                        "score": 0.3643027485507718,
                        "answer": "anonymous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bull"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6202920228242874
            },
            {
                "question verbose": "What is to businessman ",
                "b": "businessman",
                "expected answer": [
                    "businesswoman"
                ],
                "predictions": [
                    {
                        "score": 0.33537212882492384,
                        "answer": "skinny",
                        "hit": false
                    },
                    {
                        "score": 0.3245041096388936,
                        "answer": "replacing",
                        "hit": false
                    },
                    {
                        "score": 0.3137063788246341,
                        "answer": "shipload",
                        "hit": false
                    },
                    {
                        "score": 0.31103027914493614,
                        "answer": "mommy",
                        "hit": false
                    },
                    {
                        "score": 0.3055964905545673,
                        "answer": "wore",
                        "hit": false
                    },
                    {
                        "score": 0.30344136501880364,
                        "answer": "honorable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "businessman"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5906546115875244
            },
            {
                "question verbose": "What is to chairman ",
                "b": "chairman",
                "expected answer": [
                    "chairwoman"
                ],
                "predictions": [
                    {
                        "score": 0.24755131547282289,
                        "answer": "vice",
                        "hit": false
                    },
                    {
                        "score": 0.24432052433097365,
                        "answer": "conquer",
                        "hit": false
                    },
                    {
                        "score": 0.23454784164434944,
                        "answer": "iowa",
                        "hit": false
                    },
                    {
                        "score": 0.2325093011724671,
                        "answer": "crist",
                        "hit": false
                    },
                    {
                        "score": 0.22825072777898534,
                        "answer": "runway",
                        "hit": false
                    },
                    {
                        "score": 0.22489858316508887,
                        "answer": "benched",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chairman"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6246680021286011
            },
            {
                "question verbose": "What is to dad ",
                "b": "dad",
                "expected answer": [
                    "mom",
                    "mum"
                ],
                "predictions": [
                    {
                        "score": 0.31765971497533196,
                        "answer": "skeptical",
                        "hit": false
                    },
                    {
                        "score": 0.3146949432575407,
                        "answer": "bedwell",
                        "hit": false
                    },
                    {
                        "score": 0.3134053662682574,
                        "answer": "scottish",
                        "hit": false
                    },
                    {
                        "score": 0.31039231900454217,
                        "answer": "celebrity",
                        "hit": false
                    },
                    {
                        "score": 0.30809063937367787,
                        "answer": "scifi",
                        "hit": false
                    },
                    {
                        "score": 0.30715764136251106,
                        "answer": "wasson",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dad"
                ],
                "rank": 21,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.788094162940979
            },
            {
                "question verbose": "What is to daddy ",
                "b": "daddy",
                "expected answer": [
                    "mommy",
                    "mother",
                    "mom"
                ],
                "predictions": [
                    {
                        "score": 0.2906399934884733,
                        "answer": "majored",
                        "hit": false
                    },
                    {
                        "score": 0.27903722962825045,
                        "answer": "spilled",
                        "hit": false
                    },
                    {
                        "score": 0.2745131748755734,
                        "answer": "conquer",
                        "hit": false
                    },
                    {
                        "score": 0.2719253182714233,
                        "answer": "fixing",
                        "hit": false
                    },
                    {
                        "score": 0.27003435936357695,
                        "answer": "wore",
                        "hit": false
                    },
                    {
                        "score": 0.2654256783511034,
                        "answer": "qualityuniqueness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "daddy"
                ],
                "rank": 435,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7506126165390015
            },
            {
                "question verbose": "What is to duke ",
                "b": "duke",
                "expected answer": [
                    "duchess"
                ],
                "predictions": [
                    {
                        "score": 0.4209384714599603,
                        "answer": "stylist",
                        "hit": false
                    },
                    {
                        "score": 0.41654137152570114,
                        "answer": "fbs",
                        "hit": false
                    },
                    {
                        "score": 0.4153559604047717,
                        "answer": "vikes",
                        "hit": false
                    },
                    {
                        "score": 0.4149916381024869,
                        "answer": "vandal",
                        "hit": false
                    },
                    {
                        "score": 0.4149704806463995,
                        "answer": "youngster",
                        "hit": false
                    },
                    {
                        "score": 0.40312636819703684,
                        "answer": "roster",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "duke"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6749753206968307
            },
            {
                "question verbose": "What is to emperor ",
                "b": "emperor",
                "expected answer": [
                    "empress"
                ],
                "predictions": [
                    {
                        "score": 0.6869228585460051,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34714782462437177,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2999512663560249,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2664345262284402,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.26587628195487917,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2639463900834566,
                        "answer": "try",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "emperor"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to father ",
                "b": "father",
                "expected answer": [
                    "mother"
                ],
                "predictions": [
                    {
                        "score": 0.34193615869968186,
                        "answer": "daughter",
                        "hit": false
                    },
                    {
                        "score": 0.30672178592148663,
                        "answer": "tip",
                        "hit": false
                    },
                    {
                        "score": 0.28570789296128557,
                        "answer": "accompanies",
                        "hit": false
                    },
                    {
                        "score": 0.28119743623902466,
                        "answer": "childhood",
                        "hit": false
                    },
                    {
                        "score": 0.27708163855432494,
                        "answer": "mother",
                        "hit": true
                    },
                    {
                        "score": 0.2637635232742969,
                        "answer": "grandmother",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "father"
                ],
                "rank": 4,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.718336746096611
            },
            {
                "question verbose": "What is to fisherman ",
                "b": "fisherman",
                "expected answer": [
                    "fisherwoman"
                ],
                "predictions": [
                    {
                        "score": 0.6855743546921423,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3391533953244055,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.29765666392369466,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2608662452935325,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.25996201377961586,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.2586224858897618,
                        "answer": "complain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fisherman"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to fox ",
                "b": "fox",
                "expected answer": [
                    "vixen"
                ],
                "predictions": [
                    {
                        "score": 0.3259372989880904,
                        "answer": "interview",
                        "hit": false
                    },
                    {
                        "score": 0.32218407050444076,
                        "answer": "bland",
                        "hit": false
                    },
                    {
                        "score": 0.3053735174714194,
                        "answer": "carpet",
                        "hit": false
                    },
                    {
                        "score": 0.3024599994837656,
                        "answer": "npc",
                        "hit": false
                    },
                    {
                        "score": 0.3019945181887818,
                        "answer": "bothered",
                        "hit": false
                    },
                    {
                        "score": 0.30090073890554797,
                        "answer": "celebrity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fox"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6103095263242722
            },
            {
                "question verbose": "What is to gentleman ",
                "b": "gentleman",
                "expected answer": [
                    "lady",
                    "gentlewoman",
                    "madam"
                ],
                "predictions": [
                    {
                        "score": 0.43026535575201996,
                        "answer": "thanking",
                        "hit": false
                    },
                    {
                        "score": 0.4265581399301097,
                        "answer": "deleted",
                        "hit": false
                    },
                    {
                        "score": 0.4126691062991599,
                        "answer": "vikes",
                        "hit": false
                    },
                    {
                        "score": 0.408771965540824,
                        "answer": "alley",
                        "hit": false
                    },
                    {
                        "score": 0.4027210325755532,
                        "answer": "bail",
                        "hit": false
                    },
                    {
                        "score": 0.4021289182617109,
                        "answer": "spilled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gentleman"
                ],
                "rank": 12729,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7409847229719162
            },
            {
                "question verbose": "What is to god ",
                "b": "god",
                "expected answer": [
                    "goddess"
                ],
                "predictions": [
                    {
                        "score": 0.28009300643526447,
                        "answer": "forgot",
                        "hit": false
                    },
                    {
                        "score": 0.27937866835999964,
                        "answer": "knowing",
                        "hit": false
                    },
                    {
                        "score": 0.2737539940101448,
                        "answer": "totally",
                        "hit": false
                    },
                    {
                        "score": 0.2675001612780053,
                        "answer": "godly",
                        "hit": false
                    },
                    {
                        "score": 0.26564380880999544,
                        "answer": "temptation",
                        "hit": false
                    },
                    {
                        "score": 0.2597686422235796,
                        "answer": "fell",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "god"
                ],
                "rank": 10098,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6337075978517532
            },
            {
                "question verbose": "What is to grandfather ",
                "b": "grandfather",
                "expected answer": [
                    "grandmother"
                ],
                "predictions": [
                    {
                        "score": 0.417544398604006,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.40852756124042694,
                        "answer": "honorable",
                        "hit": false
                    },
                    {
                        "score": 0.3884795553255816,
                        "answer": "reminds",
                        "hit": false
                    },
                    {
                        "score": 0.3867144504107001,
                        "answer": "behaved",
                        "hit": false
                    },
                    {
                        "score": 0.38547029493912127,
                        "answer": "alley",
                        "hit": false
                    },
                    {
                        "score": 0.3854662357146668,
                        "answer": "skeptical",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grandfather"
                ],
                "rank": 2444,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8230330049991608
            },
            {
                "question verbose": "What is to grandpa ",
                "b": "grandpa",
                "expected answer": [
                    "grandma"
                ],
                "predictions": [
                    {
                        "score": 0.41716273734061665,
                        "answer": "fixing",
                        "hit": false
                    },
                    {
                        "score": 0.4003913208176295,
                        "answer": "satisfying",
                        "hit": false
                    },
                    {
                        "score": 0.39496195862740063,
                        "answer": "skeptical",
                        "hit": false
                    },
                    {
                        "score": 0.39431125831729885,
                        "answer": "aaronerin",
                        "hit": false
                    },
                    {
                        "score": 0.3916756672237976,
                        "answer": "skinny",
                        "hit": false
                    },
                    {
                        "score": 0.3901823014789097,
                        "answer": "nostalgic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grandpa"
                ],
                "rank": 2671,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8988973498344421
            },
            {
                "question verbose": "What is to grandson ",
                "b": "grandson",
                "expected answer": [
                    "granddaughter"
                ],
                "predictions": [
                    {
                        "score": 0.6863600248257867,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3463077768332333,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30288324633763886,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2740809193187581,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.27285520466279806,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.26819836161380706,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grandson"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to groom ",
                "b": "groom",
                "expected answer": [
                    "bride"
                ],
                "predictions": [
                    {
                        "score": 0.41159515459340856,
                        "answer": "stylist",
                        "hit": false
                    },
                    {
                        "score": 0.40074972894538075,
                        "answer": "deleted",
                        "hit": false
                    },
                    {
                        "score": 0.4003074372395434,
                        "answer": "celebrity",
                        "hit": false
                    },
                    {
                        "score": 0.38712951480275126,
                        "answer": "honorable",
                        "hit": false
                    },
                    {
                        "score": 0.38290708931552925,
                        "answer": "unprivileged",
                        "hit": false
                    },
                    {
                        "score": 0.3820425444455569,
                        "answer": "nostalgic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "groom"
                ],
                "rank": 9511,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7523591220378876
            },
            {
                "question verbose": "What is to headmaster ",
                "b": "headmaster",
                "expected answer": [
                    "headmistress"
                ],
                "predictions": [
                    {
                        "score": 0.6852178504765887,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3480054931873919,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3010154875362808,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2758432730381628,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2702806737747178,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.26654737698894004,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "headmaster"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to heir ",
                "b": "heir",
                "expected answer": [
                    "heiress"
                ],
                "predictions": [
                    {
                        "score": 0.6848370988170177,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.335071600437386,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.302961795485086,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27599351118770665,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.265385854791655,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.26067616499068186,
                        "answer": "try",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "heir"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to hero ",
                "b": "hero",
                "expected answer": [
                    "heroine"
                ],
                "predictions": [
                    {
                        "score": 0.3445110172531086,
                        "answer": "funny",
                        "hit": false
                    },
                    {
                        "score": 0.3346357742167108,
                        "answer": "annoying",
                        "hit": false
                    },
                    {
                        "score": 0.3290690996868006,
                        "answer": "understated",
                        "hit": false
                    },
                    {
                        "score": 0.32068525776432866,
                        "answer": "missing",
                        "hit": false
                    },
                    {
                        "score": 0.31771404454621976,
                        "answer": "kinda",
                        "hit": false
                    },
                    {
                        "score": 0.31360433127456205,
                        "answer": "revive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hero"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6126149892807007
            },
            {
                "question verbose": "What is to hound ",
                "b": "hound",
                "expected answer": [
                    "bitch"
                ],
                "predictions": [
                    {
                        "score": 0.6940224740817827,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3381236973308904,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.28116133759939665,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.25725748072793175,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.25469161927949635,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2509140917100099,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hound"
                ],
                "rank": 11698,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5552044399082661
            },
            {
                "question verbose": "What is to husband ",
                "b": "husband",
                "expected answer": [
                    "wife"
                ],
                "predictions": [
                    {
                        "score": 0.3234471825116641,
                        "answer": "tip",
                        "hit": false
                    },
                    {
                        "score": 0.30072897935433845,
                        "answer": "teenager",
                        "hit": false
                    },
                    {
                        "score": 0.29974057475864363,
                        "answer": "stylist",
                        "hit": false
                    },
                    {
                        "score": 0.2881176152037123,
                        "answer": "wore",
                        "hit": false
                    },
                    {
                        "score": 0.28651379712405456,
                        "answer": "mom",
                        "hit": false
                    },
                    {
                        "score": 0.28649709366791526,
                        "answer": "shipload",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "husband"
                ],
                "rank": 1601,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6549417674541473
            },
            {
                "question verbose": "What is to king ",
                "b": "king",
                "expected answer": [
                    "queen"
                ],
                "predictions": [
                    {
                        "score": 0.25324694875940534,
                        "answer": "went",
                        "hit": false
                    },
                    {
                        "score": 0.2531844649755295,
                        "answer": "daughter",
                        "hit": false
                    },
                    {
                        "score": 0.24748858505787147,
                        "answer": "disciple",
                        "hit": false
                    },
                    {
                        "score": 0.24729246594895846,
                        "answer": "vandal",
                        "hit": false
                    },
                    {
                        "score": 0.2325818540735077,
                        "answer": "existed",
                        "hit": false
                    },
                    {
                        "score": 0.23203221645805996,
                        "answer": "schiano",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "king"
                ],
                "rank": 8589,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6236550062894821
            },
            {
                "question verbose": "What is to lion ",
                "b": "lion",
                "expected answer": [
                    "lioness"
                ],
                "predictions": [
                    {
                        "score": 0.38089204923185244,
                        "answer": "pledging",
                        "hit": false
                    },
                    {
                        "score": 0.35901007052921624,
                        "answer": "cancel",
                        "hit": false
                    },
                    {
                        "score": 0.3563923904336997,
                        "answer": "fell",
                        "hit": false
                    },
                    {
                        "score": 0.33559922686432314,
                        "answer": "ku",
                        "hit": false
                    },
                    {
                        "score": 0.33452862891872615,
                        "answer": "skeptical",
                        "hit": false
                    },
                    {
                        "score": 0.33396601339686083,
                        "answer": "went",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lion"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5876835957169533
            },
            {
                "question verbose": "What is to man ",
                "b": "man",
                "expected answer": [
                    "woman"
                ],
                "predictions": [
                    {
                        "score": 0.41363070673766145,
                        "answer": "honorable",
                        "hit": false
                    },
                    {
                        "score": 0.3949207061131963,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.3876242185974027,
                        "answer": "mommy",
                        "hit": false
                    },
                    {
                        "score": 0.37968981978992905,
                        "answer": "tricky",
                        "hit": false
                    },
                    {
                        "score": 0.3736233135452421,
                        "answer": "desperate",
                        "hit": false
                    },
                    {
                        "score": 0.3735290144359289,
                        "answer": "leash",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "man"
                ],
                "rank": 14270,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6074600368738174
            },
            {
                "question verbose": "What is to manager ",
                "b": "manager",
                "expected answer": [
                    "manageress"
                ],
                "predictions": [
                    {
                        "score": 0.2791409354927688,
                        "answer": "leaking",
                        "hit": false
                    },
                    {
                        "score": 0.2578330941630302,
                        "answer": "youngster",
                        "hit": false
                    },
                    {
                        "score": 0.24493042868797332,
                        "answer": "hansen",
                        "hit": false
                    },
                    {
                        "score": 0.24026765520368631,
                        "answer": "instructive",
                        "hit": false
                    },
                    {
                        "score": 0.23880453432188142,
                        "answer": "old",
                        "hit": false
                    },
                    {
                        "score": 0.23769582494305086,
                        "answer": "sperm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manager"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5745719447731972
            },
            {
                "question verbose": "What is to mister ",
                "b": "mister",
                "expected answer": [
                    "miss",
                    "missis",
                    "missus",
                    "mis'ess",
                    "mrs",
                    "ms",
                    "madam"
                ],
                "predictions": [
                    {
                        "score": 0.6935806313778198,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3403848625728793,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.29460933566594794,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2686228799178674,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2656530961411546,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2481785020403915,
                        "answer": "try",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mister"
                ],
                "rank": 10035,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5820086449384689
            },
            {
                "question verbose": "What is to murderer ",
                "b": "murderer",
                "expected answer": [
                    "murderess"
                ],
                "predictions": [
                    {
                        "score": 0.36299386497240305,
                        "answer": "pedophile",
                        "hit": false
                    },
                    {
                        "score": 0.3578131033543458,
                        "answer": "fbs",
                        "hit": false
                    },
                    {
                        "score": 0.3573223267400284,
                        "answer": "inside",
                        "hit": false
                    },
                    {
                        "score": 0.3567941652687375,
                        "answer": "ashamed",
                        "hit": false
                    },
                    {
                        "score": 0.356174111883257,
                        "answer": "alley",
                        "hit": false
                    },
                    {
                        "score": 0.35604407413219946,
                        "answer": "understated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "murderer"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6875656843185425
            },
            {
                "question verbose": "What is to nephew ",
                "b": "nephew",
                "expected answer": [
                    "niece"
                ],
                "predictions": [
                    {
                        "score": 0.4262981890590955,
                        "answer": "skol",
                        "hit": false
                    },
                    {
                        "score": 0.4135001142837948,
                        "answer": "cared",
                        "hit": false
                    },
                    {
                        "score": 0.40174639583171623,
                        "answer": "occasion",
                        "hit": false
                    },
                    {
                        "score": 0.39609317723802423,
                        "answer": "rembrandt",
                        "hit": false
                    },
                    {
                        "score": 0.39572106721876904,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.3953972428375926,
                        "answer": "implying",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nephew"
                ],
                "rank": 26,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8929658234119415
            },
            {
                "question verbose": "What is to poet ",
                "b": "poet",
                "expected answer": [
                    "poetess"
                ],
                "predictions": [
                    {
                        "score": 0.6858907690899979,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3419971564656113,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2979860411943751,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27092876882894895,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2693363312499499,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2640190460439166,
                        "answer": "try",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "poet"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to policeman ",
                "b": "policeman",
                "expected answer": [
                    "policewoman"
                ],
                "predictions": [
                    {
                        "score": 0.6850403400869611,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3354879170558549,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2983110032455882,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27674578433431385,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27124765978156734,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2711091271038281,
                        "answer": "try",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "policeman"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to prince ",
                "b": "prince",
                "expected answer": [
                    "princess"
                ],
                "predictions": [
                    {
                        "score": 0.3219070862971122,
                        "answer": "fixing",
                        "hit": false
                    },
                    {
                        "score": 0.3217265610305144,
                        "answer": "attending",
                        "hit": false
                    },
                    {
                        "score": 0.30545891971702177,
                        "answer": "worshipful",
                        "hit": false
                    },
                    {
                        "score": 0.30543970729123376,
                        "answer": "slasher",
                        "hit": false
                    },
                    {
                        "score": 0.30405753554931214,
                        "answer": "foe",
                        "hit": false
                    },
                    {
                        "score": 0.3022215367783649,
                        "answer": "parson",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prince"
                ],
                "rank": 910,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8169670104980469
            },
            {
                "question verbose": "What is to ram ",
                "b": "ram",
                "expected answer": [
                    "ewe"
                ],
                "predictions": [
                    {
                        "score": 0.4458634500330782,
                        "answer": "pledging",
                        "hit": false
                    },
                    {
                        "score": 0.44236743042734755,
                        "answer": "vikes",
                        "hit": false
                    },
                    {
                        "score": 0.43315627078895574,
                        "answer": "alley",
                        "hit": false
                    },
                    {
                        "score": 0.42809112883586803,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.4209035659294255,
                        "answer": "disciple",
                        "hit": false
                    },
                    {
                        "score": 0.4208529906172725,
                        "answer": "mommy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ram"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6810262948274612
            },
            {
                "question verbose": "What is to rooster ",
                "b": "rooster",
                "expected answer": [
                    "hen"
                ],
                "predictions": [
                    {
                        "score": 0.686548733129768,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33715592136457145,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2976197779948351,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27100056911239045,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.27084393260544676,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.25959336716000153,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rooster"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to sculptor ",
                "b": "sculptor",
                "expected answer": [
                    "sculptress"
                ],
                "predictions": [
                    {
                        "score": 0.3315501503232189,
                        "answer": "thanking",
                        "hit": false
                    },
                    {
                        "score": 0.31975350806929453,
                        "answer": "strengthen",
                        "hit": false
                    },
                    {
                        "score": 0.31677163624071736,
                        "answer": "foe",
                        "hit": false
                    },
                    {
                        "score": 0.31214446072152086,
                        "answer": "faking",
                        "hit": false
                    },
                    {
                        "score": 0.31101799894360915,
                        "answer": "behaved",
                        "hit": false
                    },
                    {
                        "score": 0.3079943679797674,
                        "answer": "minimizing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sculptor"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5885011702775955
            },
            {
                "question verbose": "What is to sir ",
                "b": "sir",
                "expected answer": [
                    "madam"
                ],
                "predictions": [
                    {
                        "score": 0.4139286959787611,
                        "answer": "aaronerin",
                        "hit": false
                    },
                    {
                        "score": 0.4097468907228748,
                        "answer": "frantic",
                        "hit": false
                    },
                    {
                        "score": 0.4002218548578719,
                        "answer": "occasion",
                        "hit": false
                    },
                    {
                        "score": 0.3981210339811971,
                        "answer": "behaved",
                        "hit": false
                    },
                    {
                        "score": 0.38512734835826845,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.38327136351278956,
                        "answer": "forgotten",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sir"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6810500472784042
            },
            {
                "question verbose": "What is to son ",
                "b": "son",
                "expected answer": [
                    "daughter"
                ],
                "predictions": [
                    {
                        "score": 0.29423441297471215,
                        "answer": "coach",
                        "hit": false
                    },
                    {
                        "score": 0.2933292225075876,
                        "answer": "daughter",
                        "hit": true
                    },
                    {
                        "score": 0.2914053258933814,
                        "answer": "conquer",
                        "hit": false
                    },
                    {
                        "score": 0.2866881720786622,
                        "answer": "slasher",
                        "hit": false
                    },
                    {
                        "score": 0.2847027725931446,
                        "answer": "wonderfully",
                        "hit": false
                    },
                    {
                        "score": 0.2846557503424667,
                        "answer": "vikes",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "son"
                ],
                "rank": 1,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7721503376960754
            },
            {
                "question verbose": "What is to stallion ",
                "b": "stallion",
                "expected answer": [
                    "mare"
                ],
                "predictions": [
                    {
                        "score": 0.6850363130461526,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3374821296450104,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2972164234956713,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27497002526891917,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2736733948843672,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2668774577431644,
                        "answer": "complain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stallion"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to stepfather ",
                "b": "stepfather",
                "expected answer": [
                    "stepmother"
                ],
                "predictions": [
                    {
                        "score": 0.6863199638491326,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34283873636274687,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3034699398116486,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.28201291897202646,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.27024285687566607,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.26922521870507266,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stepfather"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to superman ",
                "b": "superman",
                "expected answer": [
                    "superwoman"
                ],
                "predictions": [
                    {
                        "score": 0.41206955470469087,
                        "answer": "plane",
                        "hit": false
                    },
                    {
                        "score": 0.3639857071515938,
                        "answer": "funny",
                        "hit": false
                    },
                    {
                        "score": 0.32853856787088004,
                        "answer": "pinocchio",
                        "hit": false
                    },
                    {
                        "score": 0.31457916339115954,
                        "answer": "pledging",
                        "hit": false
                    },
                    {
                        "score": 0.3143500540359582,
                        "answer": "goosebump",
                        "hit": false
                    },
                    {
                        "score": 0.3139969201808912,
                        "answer": "conquer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "superman"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6124076098203659
            },
            {
                "question verbose": "What is to tiger ",
                "b": "tiger",
                "expected answer": [
                    "tigress"
                ],
                "predictions": [
                    {
                        "score": 0.6837007802146756,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3394839057404477,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2931748958872793,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2708524309636427,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27004052809114004,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.26622296592827205,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiger"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to uncle ",
                "b": "uncle",
                "expected answer": [
                    "aunt"
                ],
                "predictions": [
                    {
                        "score": 0.3283624145019343,
                        "answer": "vikes",
                        "hit": false
                    },
                    {
                        "score": 0.3221008298970364,
                        "answer": "frantic",
                        "hit": false
                    },
                    {
                        "score": 0.3201301114245497,
                        "answer": "teenager",
                        "hit": false
                    },
                    {
                        "score": 0.3186129226019413,
                        "answer": "fist",
                        "hit": false
                    },
                    {
                        "score": 0.31673269109081087,
                        "answer": "funny",
                        "hit": false
                    },
                    {
                        "score": 0.3165013478073772,
                        "answer": "foe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "uncle"
                ],
                "rank": 229,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8455511033535004
            },
            {
                "question verbose": "What is to valet ",
                "b": "valet",
                "expected answer": [
                    "maid",
                    "maidservant",
                    "housemaid",
                    "chambermaid",
                    "handmaid",
                    "handmaiden",
                    "parlormaid",
                    "parlourmaid"
                ],
                "predictions": [
                    {
                        "score": 0.6852026963342098,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3417080230927113,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2976430954490187,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.26370485030683366,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.2609272517716194,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.26023291593249886,
                        "answer": "complain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "valet"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to waiter ",
                "b": "waiter",
                "expected answer": [
                    "waitress"
                ],
                "predictions": [
                    {
                        "score": 0.6920272125156997,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31590956594741415,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.293936056369509,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.25976413612653915,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2556632828314339,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.24312450094627258,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "waiter"
                ],
                "rank": 2831,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6572352945804596
            },
            {
                "question verbose": "What is to webmaster ",
                "b": "webmaster",
                "expected answer": [
                    "webmistress"
                ],
                "predictions": [
                    {
                        "score": 0.41021504270758563,
                        "answer": "funny",
                        "hit": false
                    },
                    {
                        "score": 0.4014456881539614,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.3947430416113179,
                        "answer": "nightcrawler",
                        "hit": false
                    },
                    {
                        "score": 0.3905585337365835,
                        "answer": "conquer",
                        "hit": false
                    },
                    {
                        "score": 0.38998163455812224,
                        "answer": "frantic",
                        "hit": false
                    },
                    {
                        "score": 0.38647115778954066,
                        "answer": "understated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "webmaster"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6187586858868599
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "3_Encyclopedic_semantics",
            "subcategory": "E10 [male - female].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "d8334c5e-b742-439e-97bc-ea764919d31e",
            "timestamp": "2020-10-22T15:57:27.805738"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to album ",
                "b": "album",
                "expected answer": [
                    "albums"
                ],
                "predictions": [
                    {
                        "score": 0.08010471275188577,
                        "answer": "fire",
                        "hit": false
                    },
                    {
                        "score": 0.07381124396697412,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.07333237045470832,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.07193833420866806,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.07112496543849539,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.06936016573826456,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "album"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5274365786463022
            },
            {
                "question verbose": "What is to application ",
                "b": "application",
                "expected answer": [
                    "applications"
                ],
                "predictions": [
                    {
                        "score": 0.17669098477191983,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08917247051088278,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.08792335474946268,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.07776404819047218,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.07602749492479313,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.07352894485887902,
                        "answer": "worry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "application"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5925519838929176
            },
            {
                "question verbose": "What is to area ",
                "b": "area",
                "expected answer": [
                    "areas"
                ],
                "predictions": [
                    {
                        "score": 0.07431733484744987,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.06983612042100074,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.06661896014776902,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06409309122193728,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.060970991987127444,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.06039380737990689,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "area"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5365809053182602
            },
            {
                "question verbose": "What is to car ",
                "b": "car",
                "expected answer": [
                    "cars"
                ],
                "predictions": [
                    {
                        "score": 0.16025300043895518,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09650585775735464,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.09223187855679221,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.0865353663651279,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.08453925449664465,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.08320599536969367,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "car"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5839418396353722
            },
            {
                "question verbose": "What is to college ",
                "b": "college",
                "expected answer": [
                    "colleges"
                ],
                "predictions": [
                    {
                        "score": 0.1573886076456996,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07762379096179821,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.07690747447466557,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.06924913852967789,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.06796007926828893,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.06278775121772848,
                        "answer": "vamp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "college"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5824636742472649
            },
            {
                "question verbose": "What is to council ",
                "b": "council",
                "expected answer": [
                    "councils"
                ],
                "predictions": [
                    {
                        "score": 0.2975673399658168,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.13561772590484983,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.12592320089753628,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.10545707147308155,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.09759806491117265,
                        "answer": "president",
                        "hit": false
                    },
                    {
                        "score": 0.09061786406733034,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "council"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.65582574903965
            },
            {
                "question verbose": "What is to customer ",
                "b": "customer",
                "expected answer": [
                    "customers"
                ],
                "predictions": [
                    {
                        "score": 0.25509962694602384,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10003826675309192,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.09480819184522155,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.09208751942626227,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.0917551507083965,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.09004994120430095,
                        "answer": "cinematic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "customer"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6335800737142563
            },
            {
                "question verbose": "What is to day ",
                "b": "day",
                "expected answer": [
                    "days"
                ],
                "predictions": [
                    {
                        "score": 0.07177586684048828,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.06842837768511324,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.06690064596945483,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.05934365121067631,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.05844788647189605,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.057976186277081135,
                        "answer": "chance",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "day"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5358385480940342
            },
            {
                "question verbose": "What is to death ",
                "b": "death",
                "expected answer": [
                    "deaths"
                ],
                "predictions": [
                    {
                        "score": 0.10720554237165092,
                        "answer": "therein",
                        "hit": false
                    },
                    {
                        "score": 0.07683977670095356,
                        "answer": "avoid",
                        "hit": false
                    },
                    {
                        "score": 0.07004872014872568,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.06651729291990896,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.0660638240897551,
                        "answer": "fist",
                        "hit": false
                    },
                    {
                        "score": 0.06401233158443165,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "death"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.533506378531456
            },
            {
                "question verbose": "What is to department ",
                "b": "department",
                "expected answer": [
                    "departments"
                ],
                "predictions": [
                    {
                        "score": 0.10976898518261997,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.0972450201032335,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.08501594268981222,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.08165873759259495,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.07220030549167326,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.06808803423994562,
                        "answer": "granted",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "department"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5575048252940178
            },
            {
                "question verbose": "What is to development ",
                "b": "development",
                "expected answer": [
                    "developments"
                ],
                "predictions": [
                    {
                        "score": 0.1692909911510149,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09172612327318237,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.08386322158166509,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.07935720539804215,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.06954709002553781,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.06745820670774762,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "development"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5887038186192513
            },
            {
                "question verbose": "What is to difference ",
                "b": "difference",
                "expected answer": [
                    "differences"
                ],
                "predictions": [
                    {
                        "score": 0.19714464149900307,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09894471777081372,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.08534967049644702,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.07281599961511724,
                        "answer": "somehow",
                        "hit": false
                    },
                    {
                        "score": 0.06631190162348408,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06489776970491949,
                        "answer": "highly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "difference"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6032735481858253
            },
            {
                "question verbose": "What is to director ",
                "b": "director",
                "expected answer": [
                    "directors"
                ],
                "predictions": [
                    {
                        "score": 0.0640047231436128,
                        "answer": "double",
                        "hit": false
                    },
                    {
                        "score": 0.06394249797339674,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.06323143139789848,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06107101469529979,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.05841794963554102,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.05506304468860087,
                        "answer": "national",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "director"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5231194477528334
            },
            {
                "question verbose": "What is to event ",
                "b": "event",
                "expected answer": [
                    "events"
                ],
                "predictions": [
                    {
                        "score": 0.0623213839901886,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.06083934026790822,
                        "answer": "increase",
                        "hit": false
                    },
                    {
                        "score": 0.059896062326785834,
                        "answer": "upcoming",
                        "hit": false
                    },
                    {
                        "score": 0.05785899256006201,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.05242103682651616,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.04923364894293611,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "event"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5215187128633261
            },
            {
                "question verbose": "What is to example ",
                "b": "example",
                "expected answer": [
                    "examples"
                ],
                "predictions": [
                    {
                        "score": 0.2131030054867144,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07627360683725698,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.07284024902886849,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.07019823122277724,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.06854674349775927,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.06611291014843583,
                        "answer": "gay",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "example"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6116669774055481
            },
            {
                "question verbose": "What is to fact ",
                "b": "fact",
                "expected answer": [
                    "facts"
                ],
                "predictions": [
                    {
                        "score": 0.1524139940133198,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10474034222745547,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.08025623839907708,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.078451574279435,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.07340465730164362,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.06836597046039945,
                        "answer": "stretch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fact"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5798534974455833
            },
            {
                "question verbose": "What is to friend ",
                "b": "friend",
                "expected answer": [
                    "friends"
                ],
                "predictions": [
                    {
                        "score": 0.14935101449129676,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11903320246559428,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.07048828831235653,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.07025751491577437,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.06450673967208546,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.06312017444959282,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "friend"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5782244801521301
            },
            {
                "question verbose": "What is to god ",
                "b": "god",
                "expected answer": [
                    "gods"
                ],
                "predictions": [
                    {
                        "score": 0.1278013948549693,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.10696980987277308,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09288884213928647,
                        "answer": "lord",
                        "hit": false
                    },
                    {
                        "score": 0.08617430385155371,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.08173995336900894,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.07402482338059271,
                        "answer": "switch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "god"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5560401193797588
            },
            {
                "question verbose": "What is to government ",
                "b": "government",
                "expected answer": [
                    "governments"
                ],
                "predictions": [
                    {
                        "score": 0.17397912005241892,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07711536796209915,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.06985083931422205,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.06650566767428023,
                        "answer": "welfare",
                        "hit": false
                    },
                    {
                        "score": 0.06615470532969686,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.06605654704585218,
                        "answer": "assume",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "government"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5911500155925751
            },
            {
                "question verbose": "What is to hour ",
                "b": "hour",
                "expected answer": [
                    "hours"
                ],
                "predictions": [
                    {
                        "score": 0.1296587434034686,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08782838526273548,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.0868856115114392,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.08647222604195368,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.08026945986894368,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.07852758631816949,
                        "answer": "stretch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hour"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5679194629192352
            },
            {
                "question verbose": "What is to idea ",
                "b": "idea",
                "expected answer": [
                    "ideas"
                ],
                "predictions": [
                    {
                        "score": 0.12982910012030655,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.0694452249491277,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.06767460172388454,
                        "answer": "generation",
                        "hit": false
                    },
                    {
                        "score": 0.06684998410097939,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.06012694340621774,
                        "answer": "therein",
                        "hit": false
                    },
                    {
                        "score": 0.059636938996022924,
                        "answer": "swing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "idea"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5680012330412865
            },
            {
                "question verbose": "What is to language ",
                "b": "language",
                "expected answer": [
                    "languages"
                ],
                "predictions": [
                    {
                        "score": 0.30069510245814657,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11584888056772738,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.11137317632859778,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.10260426627175585,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.08443473521117552,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.08042685013833341,
                        "answer": "tempted",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "language"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6574944257736206
            },
            {
                "question verbose": "What is to law ",
                "b": "law",
                "expected answer": [
                    "laws"
                ],
                "predictions": [
                    {
                        "score": 0.29736143073905325,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10378626778166468,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.08822164780966796,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.08604092321214785,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.08263955756169643,
                        "answer": "raise",
                        "hit": false
                    },
                    {
                        "score": 0.08231347390328496,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "law"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6556597054004669
            },
            {
                "question verbose": "What is to member ",
                "b": "member",
                "expected answer": [
                    "members"
                ],
                "predictions": [
                    {
                        "score": 0.14482766835998495,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08625365053364382,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.07371830263082452,
                        "answer": "president",
                        "hit": false
                    },
                    {
                        "score": 0.07191397366481243,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.071850351748119,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.06723392499212628,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "member"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5758666321635246
            },
            {
                "question verbose": "What is to month ",
                "b": "month",
                "expected answer": [
                    "months"
                ],
                "predictions": [
                    {
                        "score": 0.16199411398663766,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.0807292155916452,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.07689716765210182,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.07363527326707274,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.07351995484376506,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.07158840642803317,
                        "answer": "spend",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "month"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5848551467061043
            },
            {
                "question verbose": "What is to night ",
                "b": "night",
                "expected answer": [
                    "nights"
                ],
                "predictions": [
                    {
                        "score": 0.09877850641169818,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.09070295632241863,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.07995987947700597,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.07974141447535502,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07675663155164189,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.07511944234976495,
                        "answer": "maybe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "night"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5417647622525692
            },
            {
                "question verbose": "What is to office ",
                "b": "office",
                "expected answer": [
                    "offices"
                ],
                "predictions": [
                    {
                        "score": 0.2110873144634902,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10582440196175472,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.09962786353485517,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.09392853072239259,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.08320996526022835,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.07620005190968719,
                        "answer": "trailer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "office"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6105493009090424
            },
            {
                "question verbose": "What is to period ",
                "b": "period",
                "expected answer": [
                    "periods"
                ],
                "predictions": [
                    {
                        "score": 0.1956356383450504,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.14105563476546495,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.12521740682813262,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.12046142324774328,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.11016906873684042,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.1078278874715947,
                        "answer": "trailer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "period"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6024647802114487
            },
            {
                "question verbose": "What is to player ",
                "b": "player",
                "expected answer": [
                    "players"
                ],
                "predictions": [
                    {
                        "score": 0.10250492106932634,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.06746698600528248,
                        "answer": "devs",
                        "hit": false
                    },
                    {
                        "score": 0.06643603720746509,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.06528989576882871,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.06273893682008028,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.06244445170295349,
                        "answer": "mode",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "player"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5342056341469288
            },
            {
                "question verbose": "What is to population ",
                "b": "population",
                "expected answer": [
                    "populations"
                ],
                "predictions": [
                    {
                        "score": 0.0914626904926858,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.07979755460916337,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.07765851342222257,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.07339681599669085,
                        "answer": "hyprcritical",
                        "hit": false
                    },
                    {
                        "score": 0.07229022432421922,
                        "answer": "illegals",
                        "hit": false
                    },
                    {
                        "score": 0.0697467776394391,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "population"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5330023430287838
            },
            {
                "question verbose": "What is to problem ",
                "b": "problem",
                "expected answer": [
                    "problems"
                ],
                "predictions": [
                    {
                        "score": 0.21269453235747088,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09680943218318837,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.08891835972026313,
                        "answer": "worry",
                        "hit": false
                    },
                    {
                        "score": 0.08019414610103034,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.07694137158195637,
                        "answer": "bigger",
                        "hit": false
                    },
                    {
                        "score": 0.07602485809829951,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "problem"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.611401379108429
            },
            {
                "question verbose": "What is to product ",
                "b": "product",
                "expected answer": [
                    "products"
                ],
                "predictions": [
                    {
                        "score": 0.07192710860886649,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07113524153124434,
                        "answer": "expense",
                        "hit": false
                    },
                    {
                        "score": 0.0679233422759312,
                        "answer": "increase",
                        "hit": false
                    },
                    {
                        "score": 0.06729063097443894,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.06685887535679352,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.06361308486869778,
                        "answer": "except",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "product"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5376847498118877
            },
            {
                "question verbose": "What is to resource ",
                "b": "resource",
                "expected answer": [
                    "resources"
                ],
                "predictions": [
                    {
                        "score": 0.11289334182812764,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08912337808591475,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.07874827707366719,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.07840070208189964,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.06790548798857807,
                        "answer": "update",
                        "hit": false
                    },
                    {
                        "score": 0.06776899920737288,
                        "answer": "recommend",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "resource"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5591333508491516
            },
            {
                "question verbose": "What is to river ",
                "b": "river",
                "expected answer": [
                    "rivers"
                ],
                "predictions": [
                    {
                        "score": 0.10301666104417247,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.10226305470560948,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.10000743861250297,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08627420432696364,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.08080972973981529,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.079067389893948,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "river"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5524055324494839
            },
            {
                "question verbose": "What is to road ",
                "b": "road",
                "expected answer": [
                    "roads"
                ],
                "predictions": [
                    {
                        "score": 0.14131785020770388,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.13692971357447967,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.0798030065404659,
                        "answer": "fire",
                        "hit": false
                    },
                    {
                        "score": 0.06586466148924648,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.06538223950955785,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06472168271166397,
                        "answer": "bridge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "road"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5740431249141693
            },
            {
                "question verbose": "What is to role ",
                "b": "role",
                "expected answer": [
                    "roles"
                ],
                "predictions": [
                    {
                        "score": 0.0724083450586621,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06392836046572416,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.0618871429975672,
                        "answer": "periodictabletcom",
                        "hit": false
                    },
                    {
                        "score": 0.060272337304135164,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.05411106593037531,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.05264207675938732,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "role"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.48642242327332497
            },
            {
                "question verbose": "What is to science ",
                "b": "science",
                "expected answer": [
                    "sciences"
                ],
                "predictions": [
                    {
                        "score": 0.10102693897001407,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08001896960669207,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.07813283270176533,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.07278475105276185,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.06855634005865544,
                        "answer": "function",
                        "hit": false
                    },
                    {
                        "score": 0.06631896536743692,
                        "answer": "fiction",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "science"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5529286712408066
            },
            {
                "question verbose": "What is to solution ",
                "b": "solution",
                "expected answer": [
                    "solutions"
                ],
                "predictions": [
                    {
                        "score": 0.17524115197868717,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.12513969219033452,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.09923879856958316,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.08328566280517592,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.08231083135944076,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.07703296676449405,
                        "answer": "tempted",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "solution"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5917876735329628
            },
            {
                "question verbose": "What is to song ",
                "b": "song",
                "expected answer": [
                    "songs"
                ],
                "predictions": [
                    {
                        "score": 0.17740695027578526,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.13675659979997548,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.10317749098662125,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.09493076098274532,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.08616567676563536,
                        "answer": "sc",
                        "hit": false
                    },
                    {
                        "score": 0.084803481567141,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "song"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5929350554943085
            },
            {
                "question verbose": "What is to street ",
                "b": "street",
                "expected answer": [
                    "streets"
                ],
                "predictions": [
                    {
                        "score": 0.1104792146577048,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08998088805364861,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.07360317065406057,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.07176643420487662,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.06993755672699467,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.06733889034945754,
                        "answer": "fire",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "street"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5578842125833035
            },
            {
                "question verbose": "What is to student ",
                "b": "student",
                "expected answer": [
                    "students"
                ],
                "predictions": [
                    {
                        "score": 0.17166608524685542,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08893378056861945,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.08579682615018909,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.08287013793029861,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.08096462153939232,
                        "answer": "sociologist",
                        "hit": false
                    },
                    {
                        "score": 0.07077618895015518,
                        "answer": "nowawakenow",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "student"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5899098291993141
            },
            {
                "question verbose": "What is to system ",
                "b": "system",
                "expected answer": [
                    "systems"
                ],
                "predictions": [
                    {
                        "score": 0.15343514105169365,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10088761659851035,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.10076971499838998,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.07838441000932068,
                        "answer": "blow",
                        "hit": false
                    },
                    {
                        "score": 0.07176887145276126,
                        "answer": "sc",
                        "hit": false
                    },
                    {
                        "score": 0.07163539350231879,
                        "answer": "kadin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "system"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5803567543625832
            },
            {
                "question verbose": "What is to thing ",
                "b": "thing",
                "expected answer": [
                    "things"
                ],
                "predictions": [
                    {
                        "score": 0.13732425129975792,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07651045149004278,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.07548976239697824,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.0751052263854725,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.07362079002884432,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.07329476854604852,
                        "answer": "lose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "thing"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5719344392418861
            },
            {
                "question verbose": "What is to town ",
                "b": "town",
                "expected answer": [
                    "towns"
                ],
                "predictions": [
                    {
                        "score": 0.25162538190036016,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10953525659969227,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.08031664810157896,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.07857122159289084,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.07833315880306205,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.07575056430801366,
                        "answer": "proposed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "town"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6318180859088898
            },
            {
                "question verbose": "What is to user ",
                "b": "user",
                "expected answer": [
                    "users"
                ],
                "predictions": [
                    {
                        "score": 0.19780637046863525,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09086753984395367,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.08681347952346588,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.07841650902008512,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.07254765563281061,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.06794794014153693,
                        "answer": "update",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "user"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6036073789000511
            },
            {
                "question verbose": "What is to version ",
                "b": "version",
                "expected answer": [
                    "versions"
                ],
                "predictions": [
                    {
                        "score": 0.14981395448441323,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1122659646686572,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.0888994436942548,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.0814092245706406,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.0774908460878008,
                        "answer": "wing",
                        "hit": false
                    },
                    {
                        "score": 0.07504030374104387,
                        "answer": "kywrite",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "version"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5784921199083328
            },
            {
                "question verbose": "What is to village ",
                "b": "village",
                "expected answer": [
                    "villages"
                ],
                "predictions": [
                    {
                        "score": 0.1400171752133103,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09443915678277938,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.09166980466065691,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.0902587657770072,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.09011750304773367,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.08766423545470774,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "village"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5733586773276329
            },
            {
                "question verbose": "What is to website ",
                "b": "website",
                "expected answer": [
                    "websites"
                ],
                "predictions": [
                    {
                        "score": 0.0738700298626071,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.06242406447025861,
                        "answer": "update",
                        "hit": false
                    },
                    {
                        "score": 0.06212500802694333,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.060461602392408706,
                        "answer": "double",
                        "hit": false
                    },
                    {
                        "score": 0.056668709207868755,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.05637573532618432,
                        "answer": "planet",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "website"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.48882940132170916
            },
            {
                "question verbose": "What is to week ",
                "b": "week",
                "expected answer": [
                    "weeks"
                ],
                "predictions": [
                    {
                        "score": 0.18729484875143118,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07127143112607805,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.07047832106263484,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.06912157342585609,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.06775451866185815,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.0673774255536567,
                        "answer": "stretch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "week"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5981119871139526
            },
            {
                "question verbose": "What is to year ",
                "b": "year",
                "expected answer": [
                    "years"
                ],
                "predictions": [
                    {
                        "score": 0.16494724444894585,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.094288452026841,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.09104121184173557,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.08698932618747575,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.08145794442531124,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.08095282820209151,
                        "answer": "worry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "year"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5864310637116432
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I01 [noun - plural_reg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "bdb59d3d-38c7-4bcb-9688-327b195b319b",
            "timestamp": "2020-10-22T15:57:29.447555"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to ability ",
                "b": "ability",
                "expected answer": [
                    "abilities"
                ],
                "predictions": [
                    {
                        "score": 0.1761409922498969,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1469523508656824,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11581866313278603,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.11393157734050648,
                        "answer": "linux",
                        "hit": false
                    },
                    {
                        "score": 0.10331511292229983,
                        "answer": "imagine",
                        "hit": false
                    },
                    {
                        "score": 0.09767688924623444,
                        "answer": "devs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ability"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.579104945063591
            },
            {
                "question verbose": "What is to academy ",
                "b": "academy",
                "expected answer": [
                    "academies"
                ],
                "predictions": [
                    {
                        "score": 0.16964680007324742,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.12656975993624348,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.1240806274980438,
                        "answer": "gay",
                        "hit": false
                    },
                    {
                        "score": 0.11687163537780501,
                        "answer": "upcoming",
                        "hit": false
                    },
                    {
                        "score": 0.10935784454761129,
                        "answer": "papa",
                        "hit": false
                    },
                    {
                        "score": 0.1088878001751978,
                        "answer": "stretch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "academy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5912375524640083
            },
            {
                "question verbose": "What is to activity ",
                "b": "activity",
                "expected answer": [
                    "activities"
                ],
                "predictions": [
                    {
                        "score": 0.09732070408980562,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.08737192320871968,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.07985871247692075,
                        "answer": "valid",
                        "hit": false
                    },
                    {
                        "score": 0.07946291732768276,
                        "answer": "great",
                        "hit": false
                    },
                    {
                        "score": 0.07709071430375991,
                        "answer": "digit",
                        "hit": false
                    },
                    {
                        "score": 0.07678200797834414,
                        "answer": "data",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "activity"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5355625376105309
            },
            {
                "question verbose": "What is to agency ",
                "b": "agency",
                "expected answer": [
                    "agencies"
                ],
                "predictions": [
                    {
                        "score": 0.17828549096540006,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10673370528213552,
                        "answer": "expense",
                        "hit": false
                    },
                    {
                        "score": 0.10167050602249812,
                        "answer": "series",
                        "hit": false
                    },
                    {
                        "score": 0.09613635577544533,
                        "answer": "law",
                        "hit": false
                    },
                    {
                        "score": 0.0938159219178335,
                        "answer": "news",
                        "hit": false
                    },
                    {
                        "score": 0.0910852161249112,
                        "answer": "papa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "agency"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5959764793515205
            },
            {
                "question verbose": "What is to analysis ",
                "b": "analysis",
                "expected answer": [
                    "analyses"
                ],
                "predictions": [
                    {
                        "score": 0.19033696244748935,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.12152556395131774,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.10435063582955148,
                        "answer": "increase",
                        "hit": false
                    },
                    {
                        "score": 0.10362965719074328,
                        "answer": "size",
                        "hit": false
                    },
                    {
                        "score": 0.10140206831084722,
                        "answer": "valid",
                        "hit": false
                    },
                    {
                        "score": 0.1013316670235602,
                        "answer": "ban",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "analysis"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6024329960346222
            },
            {
                "question verbose": "What is to army ",
                "b": "army",
                "expected answer": [
                    "armies"
                ],
                "predictions": [
                    {
                        "score": 0.1704941267999116,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.12109832867385789,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.11497444777421932,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.11385251539607281,
                        "answer": "rise",
                        "hit": false
                    },
                    {
                        "score": 0.1057754628488894,
                        "answer": "list",
                        "hit": false
                    },
                    {
                        "score": 0.10292453736357522,
                        "answer": "bridge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "army"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5917610824108124
            },
            {
                "question verbose": "What is to authority ",
                "b": "authority",
                "expected answer": [
                    "authorities"
                ],
                "predictions": [
                    {
                        "score": 0.11733787381757499,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11693703416722986,
                        "answer": "imagine",
                        "hit": false
                    },
                    {
                        "score": 0.10473638731677468,
                        "answer": "ban",
                        "hit": false
                    },
                    {
                        "score": 0.10060279964770623,
                        "answer": "shes",
                        "hit": false
                    },
                    {
                        "score": 0.09562438085074881,
                        "answer": "behalf",
                        "hit": false
                    },
                    {
                        "score": 0.09385781112965981,
                        "answer": "greater",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "authority"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5631691515445709
            },
            {
                "question verbose": "What is to basis ",
                "b": "basis",
                "expected answer": [
                    "bases"
                ],
                "predictions": [
                    {
                        "score": 0.17173416648370918,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10667191688431604,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.1060223566605502,
                        "answer": "context",
                        "hit": false
                    },
                    {
                        "score": 0.09858660299462114,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.09185899047599284,
                        "answer": "purpose",
                        "hit": false
                    },
                    {
                        "score": 0.0913844845451433,
                        "answer": "suggest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "basis"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5924248918890953
            },
            {
                "question verbose": "What is to business ",
                "b": "business",
                "expected answer": [
                    "businesses"
                ],
                "predictions": [
                    {
                        "score": 0.08106191487394836,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.07560547822865563,
                        "answer": "small",
                        "hit": false
                    },
                    {
                        "score": 0.07433995043638285,
                        "answer": "hire",
                        "hit": false
                    },
                    {
                        "score": 0.0719084340437686,
                        "answer": "run",
                        "hit": false
                    },
                    {
                        "score": 0.06577134000455852,
                        "answer": "none",
                        "hit": false
                    },
                    {
                        "score": 0.06515420517305279,
                        "answer": "expense",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "business"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5119103137403727
            },
            {
                "question verbose": "What is to category ",
                "b": "category",
                "expected answer": [
                    "categories"
                ],
                "predictions": [
                    {
                        "score": 0.17550238608201127,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1410999918263377,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.13141809734075435,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.12148180905533233,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.11237642017560126,
                        "answer": "need",
                        "hit": false
                    },
                    {
                        "score": 0.11229773873001907,
                        "answer": "improvement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "category"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5944047793745995
            },
            {
                "question verbose": "What is to century ",
                "b": "century",
                "expected answer": [
                    "centuries"
                ],
                "predictions": [
                    {
                        "score": 0.18986879757782274,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1648239984488854,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.11286015373998894,
                        "answer": "fire",
                        "hit": false
                    },
                    {
                        "score": 0.11062174903154025,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.11006647612574819,
                        "answer": "john",
                        "hit": false
                    },
                    {
                        "score": 0.10662684073097377,
                        "answer": "politicial",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "century"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6021517366170883
            },
            {
                "question verbose": "What is to child ",
                "b": "child",
                "expected answer": [
                    "children"
                ],
                "predictions": [
                    {
                        "score": 0.1283545968830027,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10338025120800542,
                        "answer": "else",
                        "hit": false
                    },
                    {
                        "score": 0.09694736787890611,
                        "answer": "need",
                        "hit": false
                    },
                    {
                        "score": 0.08515874112249061,
                        "answer": "ask",
                        "hit": false
                    },
                    {
                        "score": 0.08210587324140167,
                        "answer": "response",
                        "hit": false
                    },
                    {
                        "score": 0.08130715820160948,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "child"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5691130384802818
            },
            {
                "question verbose": "What is to city ",
                "b": "city",
                "expected answer": [
                    "cities"
                ],
                "predictions": [
                    {
                        "score": 0.11572730406850536,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11426969924493834,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.10267740820376525,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.09942767062718463,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.09087917236447861,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.08881780739808671,
                        "answer": "town",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "city"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5622703395783901
            },
            {
                "question verbose": "What is to community ",
                "b": "community",
                "expected answer": [
                    "communities"
                ],
                "predictions": [
                    {
                        "score": 0.17457370987536155,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10150287452749603,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.09620917496714776,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.09587643529346598,
                        "answer": "imagine",
                        "hit": false
                    },
                    {
                        "score": 0.09042959572845516,
                        "answer": "data",
                        "hit": false
                    },
                    {
                        "score": 0.08445602244998256,
                        "answer": "meeting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "community"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5939305648207664
            },
            {
                "question verbose": "What is to country ",
                "b": "country",
                "expected answer": [
                    "countries"
                ],
                "predictions": [
                    {
                        "score": 0.1751763041506709,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10288064485203031,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.08480211480746075,
                        "answer": "worry",
                        "hit": false
                    },
                    {
                        "score": 0.08477345269795306,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.0785254251821742,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.07751695725463509,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "country"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5942539051175117
            },
            {
                "question verbose": "What is to county ",
                "b": "county",
                "expected answer": [
                    "counties"
                ],
                "predictions": [
                    {
                        "score": 0.15305575760929427,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10308458997882107,
                        "answer": "law",
                        "hit": false
                    },
                    {
                        "score": 0.09847976463799454,
                        "answer": "ohio",
                        "hit": false
                    },
                    {
                        "score": 0.09294186667370975,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.09077300735207142,
                        "answer": "profusely",
                        "hit": false
                    },
                    {
                        "score": 0.09006598400100023,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "county"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.58234753459692
            },
            {
                "question verbose": "What is to datum ",
                "b": "datum",
                "expected answer": [
                    "data"
                ],
                "predictions": [
                    {
                        "score": 0.9432439899812304,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1878992734450057,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.16066015277613807,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.15418032003546267,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.14504628726778376,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.137012063336343,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "datum"
                ],
                "rank": 13538,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5410945452749729
            },
            {
                "question verbose": "What is to duty ",
                "b": "duty",
                "expected answer": [
                    "duties"
                ],
                "predictions": [
                    {
                        "score": 0.10589591993466134,
                        "answer": "youtube",
                        "hit": false
                    },
                    {
                        "score": 0.10477070715849937,
                        "answer": "update",
                        "hit": false
                    },
                    {
                        "score": 0.09992786258547176,
                        "answer": "awesome",
                        "hit": false
                    },
                    {
                        "score": 0.08867987637449007,
                        "answer": "brought",
                        "hit": false
                    },
                    {
                        "score": 0.08453437381357412,
                        "answer": "gay",
                        "hit": false
                    },
                    {
                        "score": 0.08424640246254178,
                        "answer": "wave",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "duty"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5282075181603432
            },
            {
                "question verbose": "What is to economy ",
                "b": "economy",
                "expected answer": [
                    "economies"
                ],
                "predictions": [
                    {
                        "score": 0.08700525605290788,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08466236542954551,
                        "answer": "worry",
                        "hit": false
                    },
                    {
                        "score": 0.08394208423700879,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.08048087435300957,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.07828713443578778,
                        "answer": "imagine",
                        "hit": false
                    },
                    {
                        "score": 0.07782857888422068,
                        "answer": "increase",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "economy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5468157231807709
            },
            {
                "question verbose": "What is to energy ",
                "b": "energy",
                "expected answer": [
                    "energies"
                ],
                "predictions": [
                    {
                        "score": 0.18036697950803052,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.12161008336010865,
                        "answer": "increase",
                        "hit": false
                    },
                    {
                        "score": 0.09281419881326525,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.08487633885673664,
                        "answer": "president",
                        "hit": false
                    },
                    {
                        "score": 0.08336138063190784,
                        "answer": "bigger",
                        "hit": false
                    },
                    {
                        "score": 0.08252377009970638,
                        "answer": "size",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "energy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5970685109496117
            },
            {
                "question verbose": "What is to entry ",
                "b": "entry",
                "expected answer": [
                    "entries"
                ],
                "predictions": [
                    {
                        "score": 0.27447093377152165,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11610696031795786,
                        "answer": "list",
                        "hit": false
                    },
                    {
                        "score": 0.10839556804415618,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.10376283320182418,
                        "answer": "source",
                        "hit": false
                    },
                    {
                        "score": 0.09956343031135689,
                        "answer": "carrie",
                        "hit": false
                    },
                    {
                        "score": 0.09856759462016534,
                        "answer": "valid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "entry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6476676166057587
            },
            {
                "question verbose": "What is to facility ",
                "b": "facility",
                "expected answer": [
                    "facilities"
                ],
                "predictions": [
                    {
                        "score": 0.10578799752728564,
                        "answer": "increase",
                        "hit": false
                    },
                    {
                        "score": 0.08509972437580138,
                        "answer": "national",
                        "hit": false
                    },
                    {
                        "score": 0.07217663380565581,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.07002901170537132,
                        "answer": "loss",
                        "hit": false
                    },
                    {
                        "score": 0.06970095778603722,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06838862608450297,
                        "answer": "report",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "facility"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5161148272454739
            },
            {
                "question verbose": "What is to family ",
                "b": "family",
                "expected answer": [
                    "families"
                ],
                "predictions": [
                    {
                        "score": 0.08547847277262423,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.08445008768827426,
                        "answer": "raise",
                        "hit": false
                    },
                    {
                        "score": 0.08209257320941993,
                        "answer": "nyra",
                        "hit": false
                    },
                    {
                        "score": 0.08139827072042015,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.07482073243853997,
                        "answer": "frustrating",
                        "hit": false
                    },
                    {
                        "score": 0.07287399074930094,
                        "answer": "liked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "family"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5079628089442849
            },
            {
                "question verbose": "What is to formula ",
                "b": "formula",
                "expected answer": [
                    "formulae"
                ],
                "predictions": [
                    {
                        "score": 0.16780688749919437,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.1544738506426058,
                        "answer": "faraci",
                        "hit": false
                    },
                    {
                        "score": 0.15286992941811986,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.13215701655028203,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.11854586954816818,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.11808322515497341,
                        "answer": "list",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "formula"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5903104767203331
            },
            {
                "question verbose": "What is to history ",
                "b": "history",
                "expected answer": [
                    "histories"
                ],
                "predictions": [
                    {
                        "score": 0.16153556519083814,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11828182663580894,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.10075389017395027,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.09269194162961887,
                        "answer": "youtube",
                        "hit": false
                    },
                    {
                        "score": 0.09258512869219514,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.09215170282048418,
                        "answer": "faraci",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "history"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5869054794311523
            },
            {
                "question verbose": "What is to industry ",
                "b": "industry",
                "expected answer": [
                    "industries"
                ],
                "predictions": [
                    {
                        "score": 0.08175831989950871,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.07565220980289238,
                        "answer": "safe",
                        "hit": false
                    },
                    {
                        "score": 0.06541907518607105,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.06426874410085495,
                        "answer": "ultimately",
                        "hit": false
                    },
                    {
                        "score": 0.0627533389590549,
                        "answer": "known",
                        "hit": false
                    },
                    {
                        "score": 0.059900499153602606,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "industry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5236838907003403
            },
            {
                "question verbose": "What is to library ",
                "b": "library",
                "expected answer": [
                    "libraries"
                ],
                "predictions": [
                    {
                        "score": 0.11894962929825802,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.1007500299597584,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09882013346409235,
                        "answer": "limit",
                        "hit": false
                    },
                    {
                        "score": 0.09039744570749987,
                        "answer": "context",
                        "hit": false
                    },
                    {
                        "score": 0.0834138302879684,
                        "answer": "size",
                        "hit": false
                    },
                    {
                        "score": 0.08318257016652236,
                        "answer": "purpose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "library"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5542285963892937
            },
            {
                "question verbose": "What is to life ",
                "b": "life",
                "expected answer": [
                    "lives"
                ],
                "predictions": [
                    {
                        "score": 0.09949561039788118,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.08018343879335157,
                        "answer": "frustrating",
                        "hit": false
                    },
                    {
                        "score": 0.08009049699116838,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07450720441511384,
                        "answer": "imagine",
                        "hit": false
                    },
                    {
                        "score": 0.07387585906717792,
                        "answer": "godly",
                        "hit": false
                    },
                    {
                        "score": 0.07290058815605589,
                        "answer": "list",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "life"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5431033745408058
            },
            {
                "question verbose": "What is to loss ",
                "b": "loss",
                "expected answer": [
                    "losses"
                ],
                "predictions": [
                    {
                        "score": 0.21935904881932425,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11700999693861168,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.1154624514786916,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.10352933128542657,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.09891229431313142,
                        "answer": "source",
                        "hit": false
                    },
                    {
                        "score": 0.09145492539569754,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "loss"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6180880591273308
            },
            {
                "question verbose": "What is to majority ",
                "b": "majority",
                "expected answer": [
                    "majorities"
                ],
                "predictions": [
                    {
                        "score": 0.0851163782794257,
                        "answer": "data",
                        "hit": false
                    },
                    {
                        "score": 0.08434387349555464,
                        "answer": "series",
                        "hit": false
                    },
                    {
                        "score": 0.07999736429673251,
                        "answer": "captain",
                        "hit": false
                    },
                    {
                        "score": 0.0774671951818076,
                        "answer": "cleared",
                        "hit": false
                    },
                    {
                        "score": 0.07683073595430236,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.0739423298543204,
                        "answer": "locked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "majority"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.497807975159958
            },
            {
                "question verbose": "What is to memory ",
                "b": "memory",
                "expected answer": [
                    "memories"
                ],
                "predictions": [
                    {
                        "score": 0.14364433012208558,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.1131344549427682,
                        "answer": "female",
                        "hit": false
                    },
                    {
                        "score": 0.11019829410683722,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.10449733461960453,
                        "answer": "locked",
                        "hit": false
                    },
                    {
                        "score": 0.09840448852265267,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.0981843315073923,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "memory"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5523629747331142
            },
            {
                "question verbose": "What is to opportunity ",
                "b": "opportunity",
                "expected answer": [
                    "opportunities"
                ],
                "predictions": [
                    {
                        "score": 0.24128797044522754,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11217918284936329,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.08685067795157067,
                        "answer": "frustrating",
                        "hit": false
                    },
                    {
                        "score": 0.08220690973488966,
                        "answer": "update",
                        "hit": false
                    },
                    {
                        "score": 0.07788155974836866,
                        "answer": "valid",
                        "hit": false
                    },
                    {
                        "score": 0.0761021819700143,
                        "answer": "four",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "opportunity"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6298137456178665
            },
            {
                "question verbose": "What is to policy ",
                "b": "policy",
                "expected answer": [
                    "policies"
                ],
                "predictions": [
                    {
                        "score": 0.10111183670601885,
                        "answer": "data",
                        "hit": false
                    },
                    {
                        "score": 0.09055910189438911,
                        "answer": "debate",
                        "hit": false
                    },
                    {
                        "score": 0.08952614578562461,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.08452754407730084,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.0836918777978878,
                        "answer": "president",
                        "hit": false
                    },
                    {
                        "score": 0.083677159420605,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "policy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5450229085981846
            },
            {
                "question verbose": "What is to property ",
                "b": "property",
                "expected answer": [
                    "properties"
                ],
                "predictions": [
                    {
                        "score": 0.13292439260946123,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09659194731911659,
                        "answer": "data",
                        "hit": false
                    },
                    {
                        "score": 0.09061752205965799,
                        "answer": "known",
                        "hit": false
                    },
                    {
                        "score": 0.08835850499945933,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.08486795952096668,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.08369227938608377,
                        "answer": "panel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "property"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5715215727686882
            },
            {
                "question verbose": "What is to responsibility ",
                "b": "responsibility",
                "expected answer": [
                    "responsibilities"
                ],
                "predictions": [
                    {
                        "score": 0.11342885649147438,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10031280533854396,
                        "answer": "trust",
                        "hit": false
                    },
                    {
                        "score": 0.09616215205479055,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.09599309170989528,
                        "answer": "chance",
                        "hit": false
                    },
                    {
                        "score": 0.09109173025460092,
                        "answer": "sequestration",
                        "hit": false
                    },
                    {
                        "score": 0.08990492292569682,
                        "answer": "stretch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "responsibility"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5610247105360031
            },
            {
                "question verbose": "What is to safety ",
                "b": "safety",
                "expected answer": [
                    "safeties"
                ],
                "predictions": [
                    {
                        "score": 0.09036333980956837,
                        "answer": "need",
                        "hit": false
                    },
                    {
                        "score": 0.08475522965400305,
                        "answer": "sore",
                        "hit": false
                    },
                    {
                        "score": 0.08045462636443114,
                        "answer": "mode",
                        "hit": false
                    },
                    {
                        "score": 0.07632963762835474,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.07593682575067613,
                        "answer": "safe",
                        "hit": false
                    },
                    {
                        "score": 0.07397802195836158,
                        "answer": "improvement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "safety"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5116609362885356
            },
            {
                "question verbose": "What is to secretary ",
                "b": "secretary",
                "expected answer": [
                    "secretaries"
                ],
                "predictions": [
                    {
                        "score": 0.12636578909507962,
                        "answer": "president",
                        "hit": false
                    },
                    {
                        "score": 0.11538888228894086,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.11466970260105411,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.11413090547901346,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.11179369524320802,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10889004109250469,
                        "answer": "mr",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "secretary"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5601671896874905
            },
            {
                "question verbose": "What is to security ",
                "b": "security",
                "expected answer": [
                    "securities"
                ],
                "predictions": [
                    {
                        "score": 0.2849696408073115,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11703919405285583,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.10513385627485775,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.10182928254611166,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.10181987400124012,
                        "answer": "update",
                        "hit": false
                    },
                    {
                        "score": 0.09302753523389935,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "security"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6533543765544891
            },
            {
                "question verbose": "What is to series ",
                "b": "series",
                "expected answer": [
                    "series"
                ],
                "predictions": [
                    {
                        "score": 0.12657170752668243,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08521917491664746,
                        "answer": "list",
                        "hit": false
                    },
                    {
                        "score": 0.08431823965807492,
                        "answer": "neckaru",
                        "hit": false
                    },
                    {
                        "score": 0.07700815223833245,
                        "answer": "papa",
                        "hit": false
                    },
                    {
                        "score": 0.07539666944420331,
                        "answer": "gay",
                        "hit": false
                    },
                    {
                        "score": 0.07513492442397335,
                        "answer": "try",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "series"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0000000596046448
            },
            {
                "question verbose": "What is to society ",
                "b": "society",
                "expected answer": [
                    "societies"
                ],
                "predictions": [
                    {
                        "score": 0.16117015415054134,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08170108362970291,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.07442373893362621,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.07123288375609016,
                        "answer": "citizen",
                        "hit": false
                    },
                    {
                        "score": 0.07104444444657991,
                        "answer": "list",
                        "hit": false
                    },
                    {
                        "score": 0.07097214969036435,
                        "answer": "except",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "society"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.586745522916317
            },
            {
                "question verbose": "What is to species ",
                "b": "species",
                "expected answer": [
                    "species"
                ],
                "predictions": [
                    {
                        "score": 0.9428328352620915,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24121801001230117,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.1676723469788704,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.16738398358463155,
                        "answer": "try",
                        "hit": false
                    },
                    {
                        "score": 0.1579465735275684,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.1437240114469678,
                        "answer": "stretch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "species"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to story ",
                "b": "story",
                "expected answer": [
                    "stories"
                ],
                "predictions": [
                    {
                        "score": 0.1330515908696123,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11821909882783743,
                        "answer": "list",
                        "hit": false
                    },
                    {
                        "score": 0.09992229070562618,
                        "answer": "series",
                        "hit": false
                    },
                    {
                        "score": 0.09757629103952385,
                        "answer": "faraci",
                        "hit": false
                    },
                    {
                        "score": 0.0960838831750457,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.08863755894869282,
                        "answer": "annoying",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "story"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5715854614973068
            },
            {
                "question verbose": "What is to strategy ",
                "b": "strategy",
                "expected answer": [
                    "strategies"
                ],
                "predictions": [
                    {
                        "score": 0.08845613854997515,
                        "answer": "usual",
                        "hit": false
                    },
                    {
                        "score": 0.08615087800918343,
                        "answer": "bridge",
                        "hit": false
                    },
                    {
                        "score": 0.08523784620724745,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.08244589880953714,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.08147374505089208,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07986919258006935,
                        "answer": "imagine",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strategy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5438607297837734
            },
            {
                "question verbose": "What is to success ",
                "b": "success",
                "expected answer": [
                    "successes"
                ],
                "predictions": [
                    {
                        "score": 0.17501779271693535,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.09928213003804319,
                        "answer": "lord",
                        "hit": false
                    },
                    {
                        "score": 0.09912685982448559,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.09884844535345133,
                        "answer": "useful",
                        "hit": false
                    },
                    {
                        "score": 0.0969445279287077,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.08857317086571306,
                        "answer": "beginning",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "success"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5941895395517349
            },
            {
                "question verbose": "What is to technology ",
                "b": "technology",
                "expected answer": [
                    "technologies"
                ],
                "predictions": [
                    {
                        "score": 0.09357561562975596,
                        "answer": "latest",
                        "hit": false
                    },
                    {
                        "score": 0.07899425587126334,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.07813507888473802,
                        "answer": "colon",
                        "hit": false
                    },
                    {
                        "score": 0.07580191554327666,
                        "answer": "carrie",
                        "hit": false
                    },
                    {
                        "score": 0.07354242605791154,
                        "answer": "upcoming",
                        "hit": false
                    },
                    {
                        "score": 0.07204505339434207,
                        "answer": "includes",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "technology"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5425298884510994
            },
            {
                "question verbose": "What is to theory ",
                "b": "theory",
                "expected answer": [
                    "theories"
                ],
                "predictions": [
                    {
                        "score": 0.15243222595423095,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.11415936219261087,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.10948246261287996,
                        "answer": "law",
                        "hit": false
                    },
                    {
                        "score": 0.10838390510590185,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.10611432788558034,
                        "answer": "test",
                        "hit": false
                    },
                    {
                        "score": 0.09738176976782802,
                        "answer": "increase",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "theory"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.582024522125721
            },
            {
                "question verbose": "What is to university ",
                "b": "university",
                "expected answer": [
                    "universities"
                ],
                "predictions": [
                    {
                        "score": 0.09599035459072312,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.09541691730559801,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.09069273346820199,
                        "answer": "series",
                        "hit": false
                    },
                    {
                        "score": 0.08520301011804739,
                        "answer": "data",
                        "hit": false
                    },
                    {
                        "score": 0.08458190915540244,
                        "answer": "ohio",
                        "hit": false
                    },
                    {
                        "score": 0.08326246085568195,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "university"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5349483713507652
            },
            {
                "question verbose": "What is to variety ",
                "b": "variety",
                "expected answer": [
                    "varieties"
                ],
                "predictions": [
                    {
                        "score": 0.09566078472413954,
                        "answer": "tend",
                        "hit": false
                    },
                    {
                        "score": 0.08805811474353163,
                        "answer": "law",
                        "hit": false
                    },
                    {
                        "score": 0.08554992898583165,
                        "answer": "assume",
                        "hit": false
                    },
                    {
                        "score": 0.0844065945396829,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.08298902604177386,
                        "answer": "behalf",
                        "hit": false
                    },
                    {
                        "score": 0.0822950087512918,
                        "answer": "need",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "variety"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5396846197545528
            },
            {
                "question verbose": "What is to wife ",
                "b": "wife",
                "expected answer": [
                    "wives"
                ],
                "predictions": [
                    {
                        "score": 0.11000614561780102,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.10906835009013052,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.10821742112988393,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.10539850011779775,
                        "answer": "writes",
                        "hit": false
                    },
                    {
                        "score": 0.10139576550116221,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.1001048149581575,
                        "answer": "greater",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wife"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5586992986500263
            },
            {
                "question verbose": "What is to woman ",
                "b": "woman",
                "expected answer": [
                    "women"
                ],
                "predictions": [
                    {
                        "score": 0.2157964359124973,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.08343484956574503,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.08112086744889413,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.07852094189414784,
                        "answer": "gay",
                        "hit": false
                    },
                    {
                        "score": 0.07749063183280676,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.07670677678538082,
                        "answer": "raise",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "woman"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6161517277359962
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I02 [noun - plural_irreg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "8f8d05c5-0b1f-4bc7-9c25-3cf779437c40",
            "timestamp": "2020-10-22T15:57:31.663689"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to angry ",
                "b": "angry",
                "expected answer": [
                    "angrier"
                ],
                "predictions": [
                    {
                        "score": 0.23959862179134314,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.20664833457055798,
                        "answer": "transform",
                        "hit": false
                    },
                    {
                        "score": 0.20368433630825794,
                        "answer": "mask",
                        "hit": false
                    },
                    {
                        "score": 0.19763610841180354,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.1905509247310663,
                        "answer": "winnable",
                        "hit": false
                    },
                    {
                        "score": 0.18932223132044435,
                        "answer": "stimulous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "angry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6036847680807114
            },
            {
                "question verbose": "What is to cheap ",
                "b": "cheap",
                "expected answer": [
                    "cheaper"
                ],
                "predictions": [
                    {
                        "score": 0.2911471167524026,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2831834839529287,
                        "answer": "stronger",
                        "hit": false
                    },
                    {
                        "score": 0.26613213525664553,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2447017881096829,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2396056910563352,
                        "answer": "expectation",
                        "hit": false
                    },
                    {
                        "score": 0.23649653101451998,
                        "answer": "vga",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cheap"
                ],
                "rank": 5646,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7542635202407837
            },
            {
                "question verbose": "What is to clever ",
                "b": "clever",
                "expected answer": [
                    "cleverer"
                ],
                "predictions": [
                    {
                        "score": 0.2930108514595811,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2708756397978657,
                        "answer": "yankee",
                        "hit": false
                    },
                    {
                        "score": 0.2623406058160577,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.261770009014395,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2581249437623524,
                        "answer": "trumped",
                        "hit": false
                    },
                    {
                        "score": 0.25408800016128774,
                        "answer": "benjamin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clever"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5939524695277214
            },
            {
                "question verbose": "What is to coarse ",
                "b": "coarse",
                "expected answer": [
                    "coarser"
                ],
                "predictions": [
                    {
                        "score": 0.3061560089083437,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.29942417460253534,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.29695052234459973,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.28904953419904705,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.28219452601813405,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.2691398504602545,
                        "answer": "ultimately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "coarse"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6446506381034851
            },
            {
                "question verbose": "What is to costly ",
                "b": "costly",
                "expected answer": [
                    "costlier"
                ],
                "predictions": [
                    {
                        "score": 0.35861592586428426,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.3383256793804922,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.3139869621537086,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3129895356245936,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.306273821054477,
                        "answer": "neatly",
                        "hit": false
                    },
                    {
                        "score": 0.3000548728547669,
                        "answer": "irreparable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "costly"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6106766015291214
            },
            {
                "question verbose": "What is to cute ",
                "b": "cute",
                "expected answer": [
                    "cuter"
                ],
                "predictions": [
                    {
                        "score": 0.3043031938678281,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.284112402064731,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.2762283874957789,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.26502525125049503,
                        "answer": "destabilize",
                        "hit": false
                    },
                    {
                        "score": 0.2605836620672288,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.26051241975363165,
                        "answer": "stimulous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cute"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6722531616687775
            },
            {
                "question verbose": "What is to dense ",
                "b": "dense",
                "expected answer": [
                    "denser"
                ],
                "predictions": [
                    {
                        "score": 0.36988643660365067,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.33421378854996153,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.32156456876991435,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.31943499138289316,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3048848548162385,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.2994419853240627,
                        "answer": "increasingly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dense"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6264561414718628
            },
            {
                "question verbose": "What is to dumb ",
                "b": "dumb",
                "expected answer": [
                    "dumber"
                ],
                "predictions": [
                    {
                        "score": 0.2356741894918094,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.22686968506216176,
                        "answer": "communicate",
                        "hit": false
                    },
                    {
                        "score": 0.2150142004070029,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.21316599850251314,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.21090162621654548,
                        "answer": "powerless",
                        "hit": false
                    },
                    {
                        "score": 0.20721027022355795,
                        "answer": "constitutional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dumb"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5714137405157089
            },
            {
                "question verbose": "What is to fierce ",
                "b": "fierce",
                "expected answer": [
                    "fiercer"
                ],
                "predictions": [
                    {
                        "score": 0.9000986215142577,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3274736568568597,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2957612357398641,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2564975589115282,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.23045832339467284,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.22097567733778292,
                        "answer": "cost",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fierce"
                ],
                "rank": 7672,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6379004567861557
            },
            {
                "question verbose": "What is to handy ",
                "b": "handy",
                "expected answer": [
                    "handier"
                ],
                "predictions": [
                    {
                        "score": 0.3026167127250208,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.295095134991686,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2928033302780268,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.2859217047774912,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.28376419919783397,
                        "answer": "irreparable",
                        "hit": false
                    },
                    {
                        "score": 0.26824115737366505,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "handy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5935816615819931
            },
            {
                "question verbose": "What is to happy ",
                "b": "happy",
                "expected answer": [
                    "happier"
                ],
                "predictions": [
                    {
                        "score": 0.19568855126928672,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.183490986910442,
                        "answer": "mask",
                        "hit": false
                    },
                    {
                        "score": 0.1806168314079595,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.18044761352325867,
                        "answer": "screw",
                        "hit": false
                    },
                    {
                        "score": 0.1779429203887922,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.17666178003708524,
                        "answer": "idol",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happy"
                ],
                "rank": 1897,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7416485249996185
            },
            {
                "question verbose": "What is to hardy ",
                "b": "hardy",
                "expected answer": [
                    "hardier"
                ],
                "predictions": [
                    {
                        "score": 0.8986923271746425,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3181535815534785,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.30874073097207944,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.24733557034688708,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.23124289229355657,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.22272616038975523,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hardy"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to harsh ",
                "b": "harsh",
                "expected answer": [
                    "harsher"
                ],
                "predictions": [
                    {
                        "score": 0.2632494851830664,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2503283081290245,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.23551380002507605,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.22598687990457908,
                        "answer": "fubar",
                        "hit": false
                    },
                    {
                        "score": 0.22384972224334923,
                        "answer": "helpless",
                        "hit": false
                    },
                    {
                        "score": 0.2235820428350994,
                        "answer": "restrict",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "harsh"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.562623642385006
            },
            {
                "question verbose": "What is to healthy ",
                "b": "healthy",
                "expected answer": [
                    "healthier"
                ],
                "predictions": [
                    {
                        "score": 0.23487815390763706,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.23013827592295755,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2249829273798721,
                        "answer": "provide",
                        "hit": false
                    },
                    {
                        "score": 0.22469811233423312,
                        "answer": "operate",
                        "hit": false
                    },
                    {
                        "score": 0.22394781177913414,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.2221124877109905,
                        "answer": "earn",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "healthy"
                ],
                "rank": 11838,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6817122399806976
            },
            {
                "question verbose": "What is to hot ",
                "b": "hot",
                "expected answer": [
                    "hotter"
                ],
                "predictions": [
                    {
                        "score": 0.19175083036056914,
                        "answer": "bigger",
                        "hit": false
                    },
                    {
                        "score": 0.16144557281423158,
                        "answer": "yankee",
                        "hit": false
                    },
                    {
                        "score": 0.1523405887486639,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.1499253191858175,
                        "answer": "enhanced",
                        "hit": false
                    },
                    {
                        "score": 0.14933476811624033,
                        "answer": "mode",
                        "hit": false
                    },
                    {
                        "score": 0.14869064670517687,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hot"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5451088920235634
            },
            {
                "question verbose": "What is to huge ",
                "b": "huge",
                "expected answer": [
                    "huger"
                ],
                "predictions": [
                    {
                        "score": 0.17046964672761072,
                        "answer": "deficit",
                        "hit": false
                    },
                    {
                        "score": 0.15854242665200294,
                        "answer": "estimated",
                        "hit": false
                    },
                    {
                        "score": 0.15421462877873848,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.1535656166856328,
                        "answer": "policymakers",
                        "hit": false
                    },
                    {
                        "score": 0.1517620929462221,
                        "answer": "transform",
                        "hit": false
                    },
                    {
                        "score": 0.1498597945630457,
                        "answer": "intense",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "huge"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5399163849651814
            },
            {
                "question verbose": "What is to hungry ",
                "b": "hungry",
                "expected answer": [
                    "hungrier"
                ],
                "predictions": [
                    {
                        "score": 0.4239512531749138,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.3959633591381119,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3079720827676889,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.29440719316826064,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.29247159024950187,
                        "answer": "behave",
                        "hit": false
                    },
                    {
                        "score": 0.29058484279689334,
                        "answer": "stimulous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hungry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7237786501646042
            },
            {
                "question verbose": "What is to lazy ",
                "b": "lazy",
                "expected answer": [
                    "lazier"
                ],
                "predictions": [
                    {
                        "score": 0.22890324282876345,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.22012235934758986,
                        "answer": "screw",
                        "hit": false
                    },
                    {
                        "score": 0.21419450806418283,
                        "answer": "epo",
                        "hit": false
                    },
                    {
                        "score": 0.213121493297754,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2110705293137404,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.20766533599097514,
                        "answer": "definitely",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lazy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6205447390675545
            },
            {
                "question verbose": "What is to lengthy ",
                "b": "lengthy",
                "expected answer": [
                    "lengthier"
                ],
                "predictions": [
                    {
                        "score": 0.3021949728022407,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2908740204023646,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.290540686321231,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2899367306132532,
                        "answer": "velocity",
                        "hit": false
                    },
                    {
                        "score": 0.2792099124715176,
                        "answer": "strut",
                        "hit": false
                    },
                    {
                        "score": 0.27265035462291815,
                        "answer": "restrict",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lengthy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6177677288651466
            },
            {
                "question verbose": "What is to lucky ",
                "b": "lucky",
                "expected answer": [
                    "luckier"
                ],
                "predictions": [
                    {
                        "score": 0.3056151913448139,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.30205015061220875,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.29453329882606194,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.27303492569809346,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2579287327917257,
                        "answer": "looming",
                        "hit": false
                    },
                    {
                        "score": 0.2536287474376994,
                        "answer": "benevolent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lucky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6323656737804413
            },
            {
                "question verbose": "What is to mad ",
                "b": "mad",
                "expected answer": [
                    "madder"
                ],
                "predictions": [
                    {
                        "score": 0.26134736653673224,
                        "answer": "exponentially",
                        "hit": false
                    },
                    {
                        "score": 0.26080192030640975,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.23936808995178266,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2381843983930635,
                        "answer": "stronger",
                        "hit": false
                    },
                    {
                        "score": 0.23337570789672127,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.23190047144950612,
                        "answer": "illegals",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mad"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6255027800798416
            },
            {
                "question verbose": "What is to merry ",
                "b": "merry",
                "expected answer": [
                    "merrier"
                ],
                "predictions": [
                    {
                        "score": 0.2830408122798809,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2826099258374926,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2810784850205649,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.26886623576650504,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2658498259532299,
                        "answer": "mh",
                        "hit": false
                    },
                    {
                        "score": 0.2584807131405254,
                        "answer": "capitalized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "merry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6522308588027954
            },
            {
                "question verbose": "What is to mild ",
                "b": "mild",
                "expected answer": [
                    "milder"
                ],
                "predictions": [
                    {
                        "score": 0.28875740453866977,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.28474658665261493,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2766994902886277,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2734344094557157,
                        "answer": "velocity",
                        "hit": false
                    },
                    {
                        "score": 0.2689735376975152,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2632078968506897,
                        "answer": "referendum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mild"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5998042821884155
            },
            {
                "question verbose": "What is to moist ",
                "b": "moist",
                "expected answer": [
                    "moister"
                ],
                "predictions": [
                    {
                        "score": 0.8971825405732465,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3040198285022945,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.29612902836632016,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24416847084131285,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.22317535727533405,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.220614348191204,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "moist"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to nasty ",
                "b": "nasty",
                "expected answer": [
                    "nastier"
                ],
                "predictions": [
                    {
                        "score": 0.2420885056129123,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.24197707042911912,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.2401995774951679,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2349971810309707,
                        "answer": "punish",
                        "hit": false
                    },
                    {
                        "score": 0.23268860603574879,
                        "answer": "worry",
                        "hit": false
                    },
                    {
                        "score": 0.22986977441648318,
                        "answer": "useful",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nasty"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6173395738005638
            },
            {
                "question verbose": "What is to neat ",
                "b": "neat",
                "expected answer": [
                    "neater"
                ],
                "predictions": [
                    {
                        "score": 0.32541436765881515,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.31975892744008727,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.31769779394730185,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2923870467443006,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.28480208410822344,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.27978030179772345,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "neat"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6583881229162216
            },
            {
                "question verbose": "What is to nice ",
                "b": "nice",
                "expected answer": [
                    "nicer"
                ],
                "predictions": [
                    {
                        "score": 0.18996050582272314,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.18017598049781747,
                        "answer": "worry",
                        "hit": false
                    },
                    {
                        "score": 0.17561340053112526,
                        "answer": "disagree",
                        "hit": false
                    },
                    {
                        "score": 0.1755730737936663,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.17234247556531768,
                        "answer": "stretch",
                        "hit": false
                    },
                    {
                        "score": 0.17233991881622357,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nice"
                ],
                "rank": 11173,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.67001973092556
            },
            {
                "question verbose": "What is to noisy ",
                "b": "noisy",
                "expected answer": [
                    "noisier"
                ],
                "predictions": [
                    {
                        "score": 0.30964094135453185,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.3011242938409714,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.29136542850936703,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.2795345444734207,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.2745803840769219,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.27199177692422316,
                        "answer": "eligibility",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "noisy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.637768030166626
            },
            {
                "question verbose": "What is to proud ",
                "b": "proud",
                "expected answer": [
                    "prouder"
                ],
                "predictions": [
                    {
                        "score": 0.2698188741418965,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2438924659868257,
                        "answer": "helpless",
                        "hit": false
                    },
                    {
                        "score": 0.2391161346588266,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.22224898290940823,
                        "answer": "stinking",
                        "hit": false
                    },
                    {
                        "score": 0.2158722045819169,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.21097624487107086,
                        "answer": "epo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "proud"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6042162328958511
            },
            {
                "question verbose": "What is to pure ",
                "b": "pure",
                "expected answer": [
                    "purer"
                ],
                "predictions": [
                    {
                        "score": 0.25794171628729745,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.23078082142171227,
                        "answer": "efficient",
                        "hit": false
                    },
                    {
                        "score": 0.22731926559756055,
                        "answer": "cheat",
                        "hit": false
                    },
                    {
                        "score": 0.2144321461269093,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.20587938185801258,
                        "answer": "stinking",
                        "hit": false
                    },
                    {
                        "score": 0.20584715179326465,
                        "answer": "adopt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pure"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5443156324326992
            },
            {
                "question verbose": "What is to risky ",
                "b": "risky",
                "expected answer": [
                    "riskier"
                ],
                "predictions": [
                    {
                        "score": 0.29546776306059364,
                        "answer": "insurance",
                        "hit": false
                    },
                    {
                        "score": 0.2661767161668569,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2655623843010421,
                        "answer": "exponentially",
                        "hit": false
                    },
                    {
                        "score": 0.260407175919915,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2594924721257635,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.25294739142789907,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "risky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6087713465094566
            },
            {
                "question verbose": "What is to rocky ",
                "b": "rocky",
                "expected answer": [
                    "rockier"
                ],
                "predictions": [
                    {
                        "score": 0.8994791463018044,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3229254453374402,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3056365402337661,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2475928563922527,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.23417518755346878,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2313785111095987,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rocky"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to rude ",
                "b": "rude",
                "expected answer": [
                    "ruder"
                ],
                "predictions": [
                    {
                        "score": 0.25193555650836325,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.24852064708172547,
                        "answer": "mask",
                        "hit": false
                    },
                    {
                        "score": 0.24210632685371233,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.23923280227217208,
                        "answer": "stimulous",
                        "hit": false
                    },
                    {
                        "score": 0.23425768290761012,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.22836883747057962,
                        "answer": "nationwide",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rude"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930424630641937
            },
            {
                "question verbose": "What is to sad ",
                "b": "sad",
                "expected answer": [
                    "sadder"
                ],
                "predictions": [
                    {
                        "score": 0.1842078482123457,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.18170822072640924,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.17951468970190915,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.1786488832650502,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.17397184687566403,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.17150053066675794,
                        "answer": "appreciate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sad"
                ],
                "rank": 3427,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7485820204019547
            },
            {
                "question verbose": "What is to scary ",
                "b": "scary",
                "expected answer": [
                    "scarier"
                ],
                "predictions": [
                    {
                        "score": 0.32448437404480657,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3151328238458371,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.30861671191390866,
                        "answer": "pray",
                        "hit": false
                    },
                    {
                        "score": 0.30578540025960904,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.2963530970924182,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.292002657605712,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "scary"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6835645884275436
            },
            {
                "question verbose": "What is to sexy ",
                "b": "sexy",
                "expected answer": [
                    "sexier"
                ],
                "predictions": [
                    {
                        "score": 0.23159483586356208,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.22689755429315764,
                        "answer": "seaman",
                        "hit": false
                    },
                    {
                        "score": 0.22488286283105915,
                        "answer": "cfda",
                        "hit": false
                    },
                    {
                        "score": 0.21626039715308157,
                        "answer": "irreparable",
                        "hit": false
                    },
                    {
                        "score": 0.21435160784350285,
                        "answer": "clueless",
                        "hit": false
                    },
                    {
                        "score": 0.21272742194701091,
                        "answer": "md",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sexy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5563248358666897
            },
            {
                "question verbose": "What is to sticky ",
                "b": "sticky",
                "expected answer": [
                    "stickier"
                ],
                "predictions": [
                    {
                        "score": 0.28832531670020795,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.28135472101939235,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.27695463583688334,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.268311801157477,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.26419180511378293,
                        "answer": "moviegoing",
                        "hit": false
                    },
                    {
                        "score": 0.2597713667744721,
                        "answer": "counterpart",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sticky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6230889707803726
            },
            {
                "question verbose": "What is to strict ",
                "b": "strict",
                "expected answer": [
                    "stricter"
                ],
                "predictions": [
                    {
                        "score": 0.32421309433681206,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.31615057151843207,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.31287421197804555,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.3038632535901712,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.28682379086253196,
                        "answer": "competitiveness",
                        "hit": false
                    },
                    {
                        "score": 0.2852992906154547,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strict"
                ],
                "rank": 6533,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8320532143115997
            },
            {
                "question verbose": "What is to strong ",
                "b": "strong",
                "expected answer": [
                    "stronger"
                ],
                "predictions": [
                    {
                        "score": 0.20708964112702666,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.18414767609984714,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.17894581590469208,
                        "answer": "oppress",
                        "hit": false
                    },
                    {
                        "score": 0.17604605004907645,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.17531235675389767,
                        "answer": "tossing",
                        "hit": false
                    },
                    {
                        "score": 0.17240346250229752,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strong"
                ],
                "rank": 14114,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.560242798179388
            },
            {
                "question verbose": "What is to subtle ",
                "b": "subtle",
                "expected answer": [
                    "subtler"
                ],
                "predictions": [
                    {
                        "score": 0.28867081130293354,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.28523694577150627,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2682823914544586,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2613438267482039,
                        "answer": "tuned",
                        "hit": false
                    },
                    {
                        "score": 0.2564275110619486,
                        "answer": "helpless",
                        "hit": false
                    },
                    {
                        "score": 0.2532225111820288,
                        "answer": "would",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "subtle"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6433528363704681
            },
            {
                "question verbose": "What is to sunny ",
                "b": "sunny",
                "expected answer": [
                    "sunnier"
                ],
                "predictions": [
                    {
                        "score": 0.36627790201665295,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.34237140053988807,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3191765929244948,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.30183143755673486,
                        "answer": "counterpart",
                        "hit": false
                    },
                    {
                        "score": 0.29627239792783366,
                        "answer": "downgraded",
                        "hit": false
                    },
                    {
                        "score": 0.29528846316416096,
                        "answer": "involvement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sunny"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5864594578742981
            },
            {
                "question verbose": "What is to tasty ",
                "b": "tasty",
                "expected answer": [
                    "tastier"
                ],
                "predictions": [
                    {
                        "score": 0.8986525976773435,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31078918658570415,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.29860547356770323,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.23524043488659216,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2339361891304226,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2297786384942891,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tasty"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to tiny ",
                "b": "tiny",
                "expected answer": [
                    "tinier"
                ],
                "predictions": [
                    {
                        "score": 0.24736230760616107,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.2018280359371113,
                        "answer": "velocity",
                        "hit": false
                    },
                    {
                        "score": 0.19699800723659183,
                        "answer": "penalized",
                        "hit": false
                    },
                    {
                        "score": 0.19663468190242958,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.195072604389308,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.19162077954080553,
                        "answer": "involvement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiny"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5252212192863226
            },
            {
                "question verbose": "What is to tricky ",
                "b": "tricky",
                "expected answer": [
                    "trickier"
                ],
                "predictions": [
                    {
                        "score": 0.2821616381149416,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.26807695802117154,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.26326367236195614,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.25458555118833504,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.2516051902824683,
                        "answer": "incorporated",
                        "hit": false
                    },
                    {
                        "score": 0.2507663819247488,
                        "answer": "exponentially",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tricky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.627955973148346
            },
            {
                "question verbose": "What is to ugly ",
                "b": "ugly",
                "expected answer": [
                    "uglier"
                ],
                "predictions": [
                    {
                        "score": 0.27205850578174784,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.2692756623841875,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.26668840371311237,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2665731429158091,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.26587004738891296,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.260060091976155,
                        "answer": "noticeable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ugly"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6467567682266235
            },
            {
                "question verbose": "What is to vague ",
                "b": "vague",
                "expected answer": [
                    "vaguer"
                ],
                "predictions": [
                    {
                        "score": 0.29097319946558614,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.28741296349096795,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.26840639254632315,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.26490991319749957,
                        "answer": "transform",
                        "hit": false
                    },
                    {
                        "score": 0.26140665742198504,
                        "answer": "fanfare",
                        "hit": false
                    },
                    {
                        "score": 0.2592575594284632,
                        "answer": "enron",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vague"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6333363205194473
            },
            {
                "question verbose": "What is to vast ",
                "b": "vast",
                "expected answer": [
                    "vaster"
                ],
                "predictions": [
                    {
                        "score": 0.3013050856524601,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.21627639578348098,
                        "answer": "garvey",
                        "hit": false
                    },
                    {
                        "score": 0.21286120026021946,
                        "answer": "policymakers",
                        "hit": false
                    },
                    {
                        "score": 0.21062053028957653,
                        "answer": "surprising",
                        "hit": false
                    },
                    {
                        "score": 0.20597615468788302,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.1991747699917567,
                        "answer": "lifetime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vast"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5478322841227055
            },
            {
                "question verbose": "What is to weak ",
                "b": "weak",
                "expected answer": [
                    "weaker"
                ],
                "predictions": [
                    {
                        "score": 0.22261069763061625,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.19413516820291285,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.19346153735032964,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.19197359666681596,
                        "answer": "accomplish",
                        "hit": false
                    },
                    {
                        "score": 0.1913574242442358,
                        "answer": "evaders",
                        "hit": false
                    },
                    {
                        "score": 0.18783092893249606,
                        "answer": "constitutional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weak"
                ],
                "rank": 5116,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7022298127412796
            },
            {
                "question verbose": "What is to wealthy ",
                "b": "wealthy",
                "expected answer": [
                    "wealthier"
                ],
                "predictions": [
                    {
                        "score": 0.3112944423685008,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.29587449116052333,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.2702879793195897,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2685406834073782,
                        "answer": "redistribute",
                        "hit": false
                    },
                    {
                        "score": 0.2655396588569581,
                        "answer": "persuasion",
                        "hit": false
                    },
                    {
                        "score": 0.2590218985642018,
                        "answer": "politicial",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wealthy"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6166091039776802
            },
            {
                "question verbose": "What is to weird ",
                "b": "weird",
                "expected answer": [
                    "weirder"
                ],
                "predictions": [
                    {
                        "score": 0.22145630911958655,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.21744971421589981,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.1984190929929283,
                        "answer": "yankee",
                        "hit": false
                    },
                    {
                        "score": 0.19155413600438892,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.18817992825894195,
                        "answer": "cinematic",
                        "hit": false
                    },
                    {
                        "score": 0.18590707642711446,
                        "answer": "badassery",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weird"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5940406396985054
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I03 [adj - comparative].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "00c1688a-bc3e-4720-bd42-c384656f44c1",
            "timestamp": "2020-10-22T15:57:33.885237"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to able ",
                "b": "able",
                "expected answer": [
                    "ablest"
                ],
                "predictions": [
                    {
                        "score": 0.26631162818523707,
                        "answer": "id",
                        "hit": false
                    },
                    {
                        "score": 0.24133379998282092,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2404784667185606,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.22868026324330626,
                        "answer": "amart",
                        "hit": false
                    },
                    {
                        "score": 0.2211033937481741,
                        "answer": "depressed",
                        "hit": false
                    },
                    {
                        "score": 0.21725358049096288,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "able"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5907114073634148
            },
            {
                "question verbose": "What is to angry ",
                "b": "angry",
                "expected answer": [
                    "angriest"
                ],
                "predictions": [
                    {
                        "score": 0.3027261350885707,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.29441048272252063,
                        "answer": "embarrassed",
                        "hit": false
                    },
                    {
                        "score": 0.28678448394698247,
                        "answer": "knock",
                        "hit": false
                    },
                    {
                        "score": 0.2791303202873504,
                        "answer": "anonymous",
                        "hit": false
                    },
                    {
                        "score": 0.26775925927791705,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.2644155458371053,
                        "answer": "vaguest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "angry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6036847680807114
            },
            {
                "question verbose": "What is to cheap ",
                "b": "cheap",
                "expected answer": [
                    "cheapest"
                ],
                "predictions": [
                    {
                        "score": 0.28794023315048267,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2862732269631343,
                        "answer": "bomber",
                        "hit": false
                    },
                    {
                        "score": 0.28163614056774233,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.27166303719541435,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.2712814199439801,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2659037877236457,
                        "answer": "hears",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cheap"
                ],
                "rank": 11258,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7907714545726776
            },
            {
                "question verbose": "What is to clever ",
                "b": "clever",
                "expected answer": [
                    "cleverest"
                ],
                "predictions": [
                    {
                        "score": 0.3499483873034262,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.34680567342764973,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.3398099182425965,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.33492097696411754,
                        "answer": "novel",
                        "hit": false
                    },
                    {
                        "score": 0.3326295425465456,
                        "answer": "embarrassed",
                        "hit": false
                    },
                    {
                        "score": 0.33224038060676453,
                        "answer": "weiwei",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clever"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5939524695277214
            },
            {
                "question verbose": "What is to costly ",
                "b": "costly",
                "expected answer": [
                    "costliest"
                ],
                "predictions": [
                    {
                        "score": 0.38804176181672617,
                        "answer": "faced",
                        "hit": false
                    },
                    {
                        "score": 0.36335673820789677,
                        "answer": "establish",
                        "hit": false
                    },
                    {
                        "score": 0.3494375909894131,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.33947843705389713,
                        "answer": "segregation",
                        "hit": false
                    },
                    {
                        "score": 0.3300596469366048,
                        "answer": "eligibility",
                        "hit": false
                    },
                    {
                        "score": 0.325620464655298,
                        "answer": "frenzy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "costly"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6106766015291214
            },
            {
                "question verbose": "What is to cruel ",
                "b": "cruel",
                "expected answer": [
                    "cruelest"
                ],
                "predictions": [
                    {
                        "score": 0.4171138845148131,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.4038130064500676,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.40251029794877824,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.38669125071562765,
                        "answer": "exagerate",
                        "hit": false
                    },
                    {
                        "score": 0.3726639001701308,
                        "answer": "scab",
                        "hit": false
                    },
                    {
                        "score": 0.3715066423111945,
                        "answer": "weiwei",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cruel"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6629667282104492
            },
            {
                "question verbose": "What is to cute ",
                "b": "cute",
                "expected answer": [
                    "cutest"
                ],
                "predictions": [
                    {
                        "score": 0.38424504105040674,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3464148164367701,
                        "answer": "knock",
                        "hit": false
                    },
                    {
                        "score": 0.3418048344090186,
                        "answer": "pissed",
                        "hit": false
                    },
                    {
                        "score": 0.33712123012988693,
                        "answer": "lock",
                        "hit": false
                    },
                    {
                        "score": 0.3366591349515807,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.3354475419553734,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cute"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6722531616687775
            },
            {
                "question verbose": "What is to dense ",
                "b": "dense",
                "expected answer": [
                    "densest"
                ],
                "predictions": [
                    {
                        "score": 0.34086723617584697,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.33276500927091257,
                        "answer": "bomber",
                        "hit": false
                    },
                    {
                        "score": 0.3304381786949534,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.32844952598697613,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.327617918607745,
                        "answer": "evaders",
                        "hit": false
                    },
                    {
                        "score": 0.32724866378048634,
                        "answer": "weiwei",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dense"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6264561414718628
            },
            {
                "question verbose": "What is to dumb ",
                "b": "dumb",
                "expected answer": [
                    "dumbest"
                ],
                "predictions": [
                    {
                        "score": 0.267629587312657,
                        "answer": "lock",
                        "hit": false
                    },
                    {
                        "score": 0.24431158861692154,
                        "answer": "biometrics",
                        "hit": false
                    },
                    {
                        "score": 0.24223850637716274,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.240620309869195,
                        "answer": "verging",
                        "hit": false
                    },
                    {
                        "score": 0.236720480244257,
                        "answer": "concludeit",
                        "hit": false
                    },
                    {
                        "score": 0.2344128563090918,
                        "answer": "racism",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dumb"
                ],
                "rank": 2506,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7283024042844772
            },
            {
                "question verbose": "What is to fierce ",
                "b": "fierce",
                "expected answer": [
                    "fiercest"
                ],
                "predictions": [
                    {
                        "score": 0.9173236380219775,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30625738452868584,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.285523868541614,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.26033328065685096,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.24503669115732604,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24310064455620067,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fierce"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to handy ",
                "b": "handy",
                "expected answer": [
                    "handiest"
                ],
                "predictions": [
                    {
                        "score": 0.36271291402294364,
                        "answer": "establish",
                        "hit": false
                    },
                    {
                        "score": 0.3455993095784696,
                        "answer": "purchased",
                        "hit": false
                    },
                    {
                        "score": 0.345462022049569,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3429677050415062,
                        "answer": "accountant",
                        "hit": false
                    },
                    {
                        "score": 0.34217789899043083,
                        "answer": "bishop",
                        "hit": false
                    },
                    {
                        "score": 0.34101150274815756,
                        "answer": "faced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "handy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5935816615819931
            },
            {
                "question verbose": "What is to happy ",
                "b": "happy",
                "expected answer": [
                    "happiest"
                ],
                "predictions": [
                    {
                        "score": 0.22931958170578348,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.22838128944963276,
                        "answer": "canada",
                        "hit": false
                    },
                    {
                        "score": 0.22546505524073993,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.21163011625914036,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.2111694033282193,
                        "answer": "debunking",
                        "hit": false
                    },
                    {
                        "score": 0.20716045936330196,
                        "answer": "hears",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happy"
                ],
                "rank": 2168,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7714244723320007
            },
            {
                "question verbose": "What is to hardy ",
                "b": "hardy",
                "expected answer": [
                    "hardiest"
                ],
                "predictions": [
                    {
                        "score": 0.9175849489738074,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30190747540283375,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.28991774669594395,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.25464223544693526,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.25117276524555315,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.24372612899274523,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hardy"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to harsh ",
                "b": "harsh",
                "expected answer": [
                    "harshest"
                ],
                "predictions": [
                    {
                        "score": 0.29439224573487066,
                        "answer": "novel",
                        "hit": false
                    },
                    {
                        "score": 0.29137252370534833,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.28743069461359955,
                        "answer": "frenzy",
                        "hit": false
                    },
                    {
                        "score": 0.28407026140647096,
                        "answer": "lock",
                        "hit": false
                    },
                    {
                        "score": 0.2823775259768249,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2813343202193079,
                        "answer": "complain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "harsh"
                ],
                "rank": 776,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7808963656425476
            },
            {
                "question verbose": "What is to healthy ",
                "b": "healthy",
                "expected answer": [
                    "healthiest"
                ],
                "predictions": [
                    {
                        "score": 0.2749023462401765,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.27358709793054703,
                        "answer": "scifi",
                        "hit": false
                    },
                    {
                        "score": 0.27013369686307315,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.26663015417666736,
                        "answer": "id",
                        "hit": false
                    },
                    {
                        "score": 0.2663559019460036,
                        "answer": "debunking",
                        "hit": false
                    },
                    {
                        "score": 0.2576653310418332,
                        "answer": "complain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "healthy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6115438938140869
            },
            {
                "question verbose": "What is to hot ",
                "b": "hot",
                "expected answer": [
                    "hottest"
                ],
                "predictions": [
                    {
                        "score": 0.21266520908034953,
                        "answer": "blew",
                        "hit": false
                    },
                    {
                        "score": 0.21083222928343173,
                        "answer": "ticket",
                        "hit": false
                    },
                    {
                        "score": 0.20993946081593,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.20470459759039145,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.2029689516791054,
                        "answer": "eric",
                        "hit": false
                    },
                    {
                        "score": 0.2003186117936774,
                        "answer": "hurled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hot"
                ],
                "rank": 6037,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.647408738732338
            },
            {
                "question verbose": "What is to huge ",
                "b": "huge",
                "expected answer": [
                    "hugest"
                ],
                "predictions": [
                    {
                        "score": 0.22053160789278178,
                        "answer": "sharp",
                        "hit": false
                    },
                    {
                        "score": 0.21740597119509217,
                        "answer": "wonder",
                        "hit": false
                    },
                    {
                        "score": 0.19178129028379487,
                        "answer": "weakest",
                        "hit": false
                    },
                    {
                        "score": 0.18977788303550844,
                        "answer": "robo",
                        "hit": false
                    },
                    {
                        "score": 0.18747450848461408,
                        "answer": "lma",
                        "hit": false
                    },
                    {
                        "score": 0.18688335727743563,
                        "answer": "evaders",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "huge"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5399163849651814
            },
            {
                "question verbose": "What is to hungry ",
                "b": "hungry",
                "expected answer": [
                    "hungriest"
                ],
                "predictions": [
                    {
                        "score": 0.40291596472993146,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3943862516354675,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3759978472907716,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.37191785244274134,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.36858389998669056,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.366106381318071,
                        "answer": "convinced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hungry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7237786501646042
            },
            {
                "question verbose": "What is to lazy ",
                "b": "lazy",
                "expected answer": [
                    "laziest"
                ],
                "predictions": [
                    {
                        "score": 0.2973489652383585,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.2812829132093565,
                        "answer": "faraci",
                        "hit": false
                    },
                    {
                        "score": 0.27515503444112915,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.27505103508514955,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.2681625842296317,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.26693239179494044,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lazy"
                ],
                "rank": 9884,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.692393571138382
            },
            {
                "question verbose": "What is to lengthy ",
                "b": "lengthy",
                "expected answer": [
                    "lengthiest"
                ],
                "predictions": [
                    {
                        "score": 0.32666693503910277,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3207062409535048,
                        "answer": "frenzy",
                        "hit": false
                    },
                    {
                        "score": 0.3202715781438174,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.3198988038097064,
                        "answer": "segregation",
                        "hit": false
                    },
                    {
                        "score": 0.3197962058648021,
                        "answer": "elitefitnesscom",
                        "hit": false
                    },
                    {
                        "score": 0.3164767408433692,
                        "answer": "velocity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lengthy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6177677288651466
            },
            {
                "question verbose": "What is to lucky ",
                "b": "lucky",
                "expected answer": [
                    "luckiest"
                ],
                "predictions": [
                    {
                        "score": 0.36187581680327,
                        "answer": "holder",
                        "hit": false
                    },
                    {
                        "score": 0.34970886676779983,
                        "answer": "electoral",
                        "hit": false
                    },
                    {
                        "score": 0.34463595193760027,
                        "answer": "horseman",
                        "hit": false
                    },
                    {
                        "score": 0.34080372865837016,
                        "answer": "lock",
                        "hit": false
                    },
                    {
                        "score": 0.3322961879939288,
                        "answer": "eve",
                        "hit": false
                    },
                    {
                        "score": 0.33120021309786585,
                        "answer": "print",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lucky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6323656737804413
            },
            {
                "question verbose": "What is to merry ",
                "b": "merry",
                "expected answer": [
                    "merriest"
                ],
                "predictions": [
                    {
                        "score": 0.4024466948183292,
                        "answer": "wyndham",
                        "hit": false
                    },
                    {
                        "score": 0.39985248343801566,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3882931065566711,
                        "answer": "ingrate",
                        "hit": false
                    },
                    {
                        "score": 0.37121278962301,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3675241343525917,
                        "answer": "levin",
                        "hit": false
                    },
                    {
                        "score": 0.36574964992333814,
                        "answer": "impactwith",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "merry"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6522308588027954
            },
            {
                "question verbose": "What is to mild ",
                "b": "mild",
                "expected answer": [
                    "mildest"
                ],
                "predictions": [
                    {
                        "score": 0.3154704975262915,
                        "answer": "velocity",
                        "hit": false
                    },
                    {
                        "score": 0.31213459927657644,
                        "answer": "vaguest",
                        "hit": false
                    },
                    {
                        "score": 0.3067257282472184,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.304180815034454,
                        "answer": "weakest",
                        "hit": false
                    },
                    {
                        "score": 0.30266963174269973,
                        "answer": "harshest",
                        "hit": false
                    },
                    {
                        "score": 0.3022738900591747,
                        "answer": "shaley",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mild"
                ],
                "rank": 3042,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8053499758243561
            },
            {
                "question verbose": "What is to nasty ",
                "b": "nasty",
                "expected answer": [
                    "nastiest"
                ],
                "predictions": [
                    {
                        "score": 0.3177372684318811,
                        "answer": "concur",
                        "hit": false
                    },
                    {
                        "score": 0.3083013384924412,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.2818636558554922,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.27948376423120536,
                        "answer": "anonymous",
                        "hit": false
                    },
                    {
                        "score": 0.27886541563440237,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.27534445121985207,
                        "answer": "gorilla",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nasty"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6173395738005638
            },
            {
                "question verbose": "What is to neat ",
                "b": "neat",
                "expected answer": [
                    "neatest"
                ],
                "predictions": [
                    {
                        "score": 0.40520979730762796,
                        "answer": "id",
                        "hit": false
                    },
                    {
                        "score": 0.3887334748648338,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.3820100234405096,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.36291481243502216,
                        "answer": "novel",
                        "hit": false
                    },
                    {
                        "score": 0.35636804706900976,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.34898500214486283,
                        "answer": "spoken",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "neat"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6583881229162216
            },
            {
                "question verbose": "What is to nice ",
                "b": "nice",
                "expected answer": [
                    "nicest"
                ],
                "predictions": [
                    {
                        "score": 0.2399047964830321,
                        "answer": "gorilla",
                        "hit": false
                    },
                    {
                        "score": 0.23648438470750016,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.23612218433149418,
                        "answer": "novel",
                        "hit": false
                    },
                    {
                        "score": 0.23158207662064015,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.23135041295895825,
                        "answer": "suddenly",
                        "hit": false
                    },
                    {
                        "score": 0.2313477083855557,
                        "answer": "complain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nice"
                ],
                "rank": 663,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.712912991642952
            },
            {
                "question verbose": "What is to noisy ",
                "b": "noisy",
                "expected answer": [
                    "noisiest"
                ],
                "predictions": [
                    {
                        "score": 0.3714144540733173,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.36161942483139736,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.34106260049490544,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.3394409038585986,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.3336800976112577,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3306693138335973,
                        "answer": "liking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "noisy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.637768030166626
            },
            {
                "question verbose": "What is to polite ",
                "b": "polite",
                "expected answer": [
                    "politest"
                ],
                "predictions": [
                    {
                        "score": 0.37364090671091255,
                        "answer": "exagerate",
                        "hit": false
                    },
                    {
                        "score": 0.37173136641828336,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.3590070826811932,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.3486319318736797,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.3381010139896826,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.335769472789649,
                        "answer": "pissed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "polite"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.677912637591362
            },
            {
                "question verbose": "What is to proud ",
                "b": "proud",
                "expected answer": [
                    "proudest"
                ],
                "predictions": [
                    {
                        "score": 0.3270166392580114,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.3207530244627305,
                        "answer": "youtube",
                        "hit": false
                    },
                    {
                        "score": 0.31624194149216395,
                        "answer": "posted",
                        "hit": false
                    },
                    {
                        "score": 0.29536897289006,
                        "answer": "disciple",
                        "hit": false
                    },
                    {
                        "score": 0.2917369545238451,
                        "answer": "sending",
                        "hit": false
                    },
                    {
                        "score": 0.289921326968445,
                        "answer": "racism",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "proud"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6042162328958511
            },
            {
                "question verbose": "What is to pure ",
                "b": "pure",
                "expected answer": [
                    "purest"
                ],
                "predictions": [
                    {
                        "score": 0.2844232032017271,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.2669298423128601,
                        "answer": "scab",
                        "hit": false
                    },
                    {
                        "score": 0.26086139691168847,
                        "answer": "shuts",
                        "hit": false
                    },
                    {
                        "score": 0.2484119912188021,
                        "answer": "scifi",
                        "hit": false
                    },
                    {
                        "score": 0.2459884231738803,
                        "answer": "anonymous",
                        "hit": false
                    },
                    {
                        "score": 0.24305322048715244,
                        "answer": "racism",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pure"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5443156324326992
            },
            {
                "question verbose": "What is to rare ",
                "b": "rare",
                "expected answer": [
                    "rarest"
                ],
                "predictions": [
                    {
                        "score": 0.2833498899507063,
                        "answer": "spoken",
                        "hit": false
                    },
                    {
                        "score": 0.2791170953765502,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.2778401677183873,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.2743967151680092,
                        "answer": "tossing",
                        "hit": false
                    },
                    {
                        "score": 0.27288983796270033,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.272263355308822,
                        "answer": "helpless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rare"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6415678113698959
            },
            {
                "question verbose": "What is to risky ",
                "b": "risky",
                "expected answer": [
                    "riskiest"
                ],
                "predictions": [
                    {
                        "score": 0.36638851673528333,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.3126637698905011,
                        "answer": "faced",
                        "hit": false
                    },
                    {
                        "score": 0.3044900715404971,
                        "answer": "scheme",
                        "hit": false
                    },
                    {
                        "score": 0.3039130717563012,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.2967290167014794,
                        "answer": "hopey",
                        "hit": false
                    },
                    {
                        "score": 0.29531916042968936,
                        "answer": "harshest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "risky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6087713465094566
            },
            {
                "question verbose": "What is to rude ",
                "b": "rude",
                "expected answer": [
                    "rudest"
                ],
                "predictions": [
                    {
                        "score": 0.3199163151961214,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.3185287666427686,
                        "answer": "curate",
                        "hit": false
                    },
                    {
                        "score": 0.30445079971136924,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.2951796906930983,
                        "answer": "thriving",
                        "hit": false
                    },
                    {
                        "score": 0.29224288614051924,
                        "answer": "oversight",
                        "hit": false
                    },
                    {
                        "score": 0.29195422027753076,
                        "answer": "harshest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rude"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5930424630641937
            },
            {
                "question verbose": "What is to sad ",
                "b": "sad",
                "expected answer": [
                    "saddest"
                ],
                "predictions": [
                    {
                        "score": 0.2786514331907734,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.27154070022466803,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2647195243434647,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.2536514750020977,
                        "answer": "bitterness",
                        "hit": false
                    },
                    {
                        "score": 0.2535927283769194,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.24985833136396812,
                        "answer": "pissed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sad"
                ],
                "rank": 4203,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7129065543413162
            },
            {
                "question verbose": "What is to scary ",
                "b": "scary",
                "expected answer": [
                    "scariest"
                ],
                "predictions": [
                    {
                        "score": 0.3655178305770027,
                        "answer": "etch",
                        "hit": false
                    },
                    {
                        "score": 0.36135386149346843,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.3581172778161824,
                        "answer": "tossing",
                        "hit": false
                    },
                    {
                        "score": 0.3565252419622105,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.3562131225264378,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.35249073190270575,
                        "answer": "racism",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "scary"
                ],
                "rank": 13073,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7645525932312012
            },
            {
                "question verbose": "What is to sexy ",
                "b": "sexy",
                "expected answer": [
                    "sexiest"
                ],
                "predictions": [
                    {
                        "score": 0.32358125666599213,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3195508634114245,
                        "answer": "weakest",
                        "hit": false
                    },
                    {
                        "score": 0.3148679250532631,
                        "answer": "costume",
                        "hit": false
                    },
                    {
                        "score": 0.30813382893599955,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.3026758130550738,
                        "answer": "weiwei",
                        "hit": false
                    },
                    {
                        "score": 0.30019755012424204,
                        "answer": "impactwith",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sexy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5563248358666897
            },
            {
                "question verbose": "What is to shiny ",
                "b": "shiny",
                "expected answer": [
                    "shiniest"
                ],
                "predictions": [
                    {
                        "score": 0.3643814438407298,
                        "answer": "bomber",
                        "hit": false
                    },
                    {
                        "score": 0.3457793573341252,
                        "answer": "illumined",
                        "hit": false
                    },
                    {
                        "score": 0.3395213786548763,
                        "answer": "aggressively",
                        "hit": false
                    },
                    {
                        "score": 0.3370904671705673,
                        "answer": "vaguest",
                        "hit": false
                    },
                    {
                        "score": 0.33350068647701236,
                        "answer": "rebut",
                        "hit": false
                    },
                    {
                        "score": 0.3332978429669053,
                        "answer": "weiwei",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shiny"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.602131113409996
            },
            {
                "question verbose": "What is to strange ",
                "b": "strange",
                "expected answer": [
                    "strangest"
                ],
                "predictions": [
                    {
                        "score": 0.3322201900355632,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.30991901336914773,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3094640026101425,
                        "answer": "suddenly",
                        "hit": false
                    },
                    {
                        "score": 0.301868217871912,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.29290214446559476,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.28778296379717994,
                        "answer": "bitterness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strange"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6177984699606895
            },
            {
                "question verbose": "What is to strict ",
                "b": "strict",
                "expected answer": [
                    "strictest"
                ],
                "predictions": [
                    {
                        "score": 0.3764916385733273,
                        "answer": "faced",
                        "hit": false
                    },
                    {
                        "score": 0.354314847033886,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.35245271297035635,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3371893612877684,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.3305887528764636,
                        "answer": "bomber",
                        "hit": false
                    },
                    {
                        "score": 0.3304776347890663,
                        "answer": "vaguest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strict"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6291344463825226
            },
            {
                "question verbose": "What is to strong ",
                "b": "strong",
                "expected answer": [
                    "strongest"
                ],
                "predictions": [
                    {
                        "score": 0.25842392251177754,
                        "answer": "maintained",
                        "hit": false
                    },
                    {
                        "score": 0.23390926230289832,
                        "answer": "novel",
                        "hit": false
                    },
                    {
                        "score": 0.21984266890158743,
                        "answer": "tossing",
                        "hit": false
                    },
                    {
                        "score": 0.21281800688933314,
                        "answer": "biometrics",
                        "hit": false
                    },
                    {
                        "score": 0.21099126808775315,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.21024513087100763,
                        "answer": "feigned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strong"
                ],
                "rank": 1564,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.687974750995636
            },
            {
                "question verbose": "What is to subtle ",
                "b": "subtle",
                "expected answer": [
                    "subtlest"
                ],
                "predictions": [
                    {
                        "score": 0.351235891709686,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.34438348467196395,
                        "answer": "hears",
                        "hit": false
                    },
                    {
                        "score": 0.338733153414385,
                        "answer": "complain",
                        "hit": false
                    },
                    {
                        "score": 0.3383121596138259,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.329011571778189,
                        "answer": "levin",
                        "hit": false
                    },
                    {
                        "score": 0.3267019619748329,
                        "answer": "ideological",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "subtle"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6433528363704681
            },
            {
                "question verbose": "What is to sunny ",
                "b": "sunny",
                "expected answer": [
                    "sunniest"
                ],
                "predictions": [
                    {
                        "score": 0.3854880837726689,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.37780600742722037,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.37284709799654525,
                        "answer": "bundesliga",
                        "hit": false
                    },
                    {
                        "score": 0.3606757864861805,
                        "answer": "weiwei",
                        "hit": false
                    },
                    {
                        "score": 0.35873223918942176,
                        "answer": "unstacking",
                        "hit": false
                    },
                    {
                        "score": 0.35818989360866726,
                        "answer": "harshest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sunny"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5864594578742981
            },
            {
                "question verbose": "What is to tasty ",
                "b": "tasty",
                "expected answer": [
                    "tastiest"
                ],
                "predictions": [
                    {
                        "score": 0.9172473016874846,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3050007212150302,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.28578119532310364,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.26791232754730343,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.24731009631252118,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.24593479571488763,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tasty"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to tiny ",
                "b": "tiny",
                "expected answer": [
                    "tiniest"
                ],
                "predictions": [
                    {
                        "score": 0.2795471575054244,
                        "answer": "ticket",
                        "hit": false
                    },
                    {
                        "score": 0.26505307302179115,
                        "answer": "purchased",
                        "hit": false
                    },
                    {
                        "score": 0.2632188853363079,
                        "answer": "stat",
                        "hit": false
                    },
                    {
                        "score": 0.25720406298293236,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.2547609586353927,
                        "answer": "motorola",
                        "hit": false
                    },
                    {
                        "score": 0.2542214761509232,
                        "answer": "maintained",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiny"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5252212192863226
            },
            {
                "question verbose": "What is to tricky ",
                "b": "tricky",
                "expected answer": [
                    "trickiest"
                ],
                "predictions": [
                    {
                        "score": 0.3367984456363084,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.3322869802680493,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.31708012596047597,
                        "answer": "youtube",
                        "hit": false
                    },
                    {
                        "score": 0.3125058643447277,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.30932870274102864,
                        "answer": "establish",
                        "hit": false
                    },
                    {
                        "score": 0.3046199948477884,
                        "answer": "embarrassed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tricky"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.627955973148346
            },
            {
                "question verbose": "What is to ugly ",
                "b": "ugly",
                "expected answer": [
                    "ugliest"
                ],
                "predictions": [
                    {
                        "score": 0.30526419089844375,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.30188588573104846,
                        "answer": "novel",
                        "hit": false
                    },
                    {
                        "score": 0.2996533835809385,
                        "answer": "bitterness",
                        "hit": false
                    },
                    {
                        "score": 0.29921653209013155,
                        "answer": "concur",
                        "hit": false
                    },
                    {
                        "score": 0.29511689587248136,
                        "answer": "hornet",
                        "hit": false
                    },
                    {
                        "score": 0.2928563575690179,
                        "answer": "parody",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ugly"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6467567682266235
            },
            {
                "question verbose": "What is to vague ",
                "b": "vague",
                "expected answer": [
                    "vaguest"
                ],
                "predictions": [
                    {
                        "score": 0.3488611971535551,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3479946040726478,
                        "answer": "liquidate",
                        "hit": false
                    },
                    {
                        "score": 0.3474666261146229,
                        "answer": "undeveloped",
                        "hit": false
                    },
                    {
                        "score": 0.3413860327163864,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.3400052599830845,
                        "answer": "humorous",
                        "hit": false
                    },
                    {
                        "score": 0.33938822560197857,
                        "answer": "enron",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vague"
                ],
                "rank": 928,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8069865107536316
            },
            {
                "question verbose": "What is to weak ",
                "b": "weak",
                "expected answer": [
                    "weakest"
                ],
                "predictions": [
                    {
                        "score": 0.2799616412012949,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2746995097445721,
                        "answer": "evaders",
                        "hit": false
                    },
                    {
                        "score": 0.25727826509470186,
                        "answer": "establish",
                        "hit": false
                    },
                    {
                        "score": 0.2509903185789146,
                        "answer": "pummeled",
                        "hit": false
                    },
                    {
                        "score": 0.24991124062382056,
                        "answer": "holder",
                        "hit": false
                    },
                    {
                        "score": 0.24430715355144078,
                        "answer": "racism",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weak"
                ],
                "rank": 267,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.750823587179184
            },
            {
                "question verbose": "What is to wealthy ",
                "b": "wealthy",
                "expected answer": [
                    "wealthiest"
                ],
                "predictions": [
                    {
                        "score": 0.3183983073359753,
                        "answer": "racism",
                        "hit": false
                    },
                    {
                        "score": 0.31236507547277104,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.30466775787769623,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.30168661452605294,
                        "answer": "electoral",
                        "hit": false
                    },
                    {
                        "score": 0.29715652630037,
                        "answer": "faced",
                        "hit": false
                    },
                    {
                        "score": 0.29459257491030555,
                        "answer": "cuba",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wealthy"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6166091039776802
            },
            {
                "question verbose": "What is to weird ",
                "b": "weird",
                "expected answer": [
                    "weirdest"
                ],
                "predictions": [
                    {
                        "score": 0.26056162153271906,
                        "answer": "weakest",
                        "hit": false
                    },
                    {
                        "score": 0.25386298335531715,
                        "answer": "exagerate",
                        "hit": false
                    },
                    {
                        "score": 0.25295010426714093,
                        "answer": "parody",
                        "hit": false
                    },
                    {
                        "score": 0.24741900082996465,
                        "answer": "liked",
                        "hit": false
                    },
                    {
                        "score": 0.24559609531447357,
                        "answer": "describes",
                        "hit": false
                    },
                    {
                        "score": 0.2450740033507269,
                        "answer": "spoken",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weird"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5940406396985054
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I04 [adj - superlative].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "680bee0d-affd-42e4-b6df-684e190dfd23",
            "timestamp": "2020-10-22T15:57:35.813777"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to accept ",
                "b": "accept",
                "expected answer": [
                    "accepts"
                ],
                "predictions": [
                    {
                        "score": 0.4483080828138605,
                        "answer": "impetuosity",
                        "hit": false
                    },
                    {
                        "score": 0.4336416365232369,
                        "answer": "unattended",
                        "hit": false
                    },
                    {
                        "score": 0.42843248327066114,
                        "answer": "outset",
                        "hit": false
                    },
                    {
                        "score": 0.42364158262129176,
                        "answer": "troubled",
                        "hit": false
                    },
                    {
                        "score": 0.4235480672660088,
                        "answer": "punctuation",
                        "hit": false
                    },
                    {
                        "score": 0.4222906748132501,
                        "answer": "actress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "accept"
                ],
                "rank": 56,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7697272002696991
            },
            {
                "question verbose": "What is to achieve ",
                "b": "achieve",
                "expected answer": [
                    "achieves"
                ],
                "predictions": [
                    {
                        "score": 0.4735482546998285,
                        "answer": "feb",
                        "hit": false
                    },
                    {
                        "score": 0.4688589073456028,
                        "answer": "devolves",
                        "hit": false
                    },
                    {
                        "score": 0.46702183484693055,
                        "answer": "fence",
                        "hit": false
                    },
                    {
                        "score": 0.458001192423246,
                        "answer": "happenstance",
                        "hit": false
                    },
                    {
                        "score": 0.45783887728179196,
                        "answer": "decisionmaking",
                        "hit": false
                    },
                    {
                        "score": 0.4561908183684857,
                        "answer": "coughed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "achieve"
                ],
                "rank": 8514,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7229238599538803
            },
            {
                "question verbose": "What is to add ",
                "b": "add",
                "expected answer": [
                    "adds"
                ],
                "predictions": [
                    {
                        "score": 0.4514178020526464,
                        "answer": "realism",
                        "hit": false
                    },
                    {
                        "score": 0.4353055105503739,
                        "answer": "tarnished",
                        "hit": false
                    },
                    {
                        "score": 0.4269969563039421,
                        "answer": "playability",
                        "hit": false
                    },
                    {
                        "score": 0.42438569441597895,
                        "answer": "striking",
                        "hit": false
                    },
                    {
                        "score": 0.41575509924848736,
                        "answer": "infinitely",
                        "hit": false
                    },
                    {
                        "score": 0.41302801062101474,
                        "answer": "tally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "add"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5307919699698687
            },
            {
                "question verbose": "What is to agree ",
                "b": "agree",
                "expected answer": [
                    "agrees"
                ],
                "predictions": [
                    {
                        "score": 0.4830919233979551,
                        "answer": "wooden",
                        "hit": false
                    },
                    {
                        "score": 0.4790304811416732,
                        "answer": "actress",
                        "hit": false
                    },
                    {
                        "score": 0.4604810202914783,
                        "answer": "lastly",
                        "hit": false
                    },
                    {
                        "score": 0.45728159769422444,
                        "answer": "artificial",
                        "hit": false
                    },
                    {
                        "score": 0.45018371505482024,
                        "answer": "polish",
                        "hit": false
                    },
                    {
                        "score": 0.44107643325721446,
                        "answer": "debater",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "agree"
                ],
                "rank": 5078,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6485103666782379
            },
            {
                "question verbose": "What is to allow ",
                "b": "allow",
                "expected answer": [
                    "allows"
                ],
                "predictions": [
                    {
                        "score": 0.3845506284183572,
                        "answer": "switcher",
                        "hit": false
                    },
                    {
                        "score": 0.3796591122066944,
                        "answer": "interruptive",
                        "hit": false
                    },
                    {
                        "score": 0.3752433351323177,
                        "answer": "defensible",
                        "hit": false
                    },
                    {
                        "score": 0.365071689594285,
                        "answer": "bbe",
                        "hit": false
                    },
                    {
                        "score": 0.35995743900351146,
                        "answer": "totter",
                        "hit": false
                    },
                    {
                        "score": 0.3584270998668991,
                        "answer": "bcad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allow"
                ],
                "rank": 12328,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6292361170053482
            },
            {
                "question verbose": "What is to appear ",
                "b": "appear",
                "expected answer": [
                    "appears"
                ],
                "predictions": [
                    {
                        "score": 0.5789200325007816,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.5505928284246601,
                        "answer": "flooded",
                        "hit": false
                    },
                    {
                        "score": 0.5490273467557936,
                        "answer": "baylor",
                        "hit": false
                    },
                    {
                        "score": 0.5462287874720118,
                        "answer": "roadway",
                        "hit": false
                    },
                    {
                        "score": 0.5433495693147936,
                        "answer": "doorway",
                        "hit": false
                    },
                    {
                        "score": 0.5322494495161774,
                        "answer": "patty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appear"
                ],
                "rank": 9653,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6925152093172073
            },
            {
                "question verbose": "What is to apply ",
                "b": "apply",
                "expected answer": [
                    "applies"
                ],
                "predictions": [
                    {
                        "score": 0.535656503334324,
                        "answer": "troubled",
                        "hit": false
                    },
                    {
                        "score": 0.5296634098756866,
                        "answer": "curator",
                        "hit": false
                    },
                    {
                        "score": 0.5074886934051885,
                        "answer": "oftentimes",
                        "hit": false
                    },
                    {
                        "score": 0.46726300002253285,
                        "answer": "endearing",
                        "hit": false
                    },
                    {
                        "score": 0.4669565022209066,
                        "answer": "wooden",
                        "hit": false
                    },
                    {
                        "score": 0.4585203250975692,
                        "answer": "cafe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apply"
                ],
                "rank": 8609,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6727627217769623
            },
            {
                "question verbose": "What is to ask ",
                "b": "ask",
                "expected answer": [
                    "asks"
                ],
                "predictions": [
                    {
                        "score": 0.43792480022888325,
                        "answer": "decisionmaking",
                        "hit": false
                    },
                    {
                        "score": 0.41624968511672394,
                        "answer": "resolvable",
                        "hit": false
                    },
                    {
                        "score": 0.40417086386847995,
                        "answer": "hmmm",
                        "hit": false
                    },
                    {
                        "score": 0.40192639241447364,
                        "answer": "overthrew",
                        "hit": false
                    },
                    {
                        "score": 0.39834864175097906,
                        "answer": "godly",
                        "hit": false
                    },
                    {
                        "score": 0.39269375810957263,
                        "answer": "existential",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ask"
                ],
                "rank": 6682,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.677903339266777
            },
            {
                "question verbose": "What is to avoid ",
                "b": "avoid",
                "expected answer": [
                    "avoids"
                ],
                "predictions": [
                    {
                        "score": 0.47734365447643323,
                        "answer": "evaders",
                        "hit": false
                    },
                    {
                        "score": 0.47200481641596576,
                        "answer": "widening",
                        "hit": false
                    },
                    {
                        "score": 0.4595561050167709,
                        "answer": "blogging",
                        "hit": false
                    },
                    {
                        "score": 0.4496657297970332,
                        "answer": "neutrality",
                        "hit": false
                    },
                    {
                        "score": 0.4419427586914286,
                        "answer": "temerity",
                        "hit": false
                    },
                    {
                        "score": 0.4331490925412368,
                        "answer": "definite",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "avoid"
                ],
                "rank": 7536,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7120765149593353
            },
            {
                "question verbose": "What is to become ",
                "b": "become",
                "expected answer": [
                    "becomes"
                ],
                "predictions": [
                    {
                        "score": 0.3524713039753253,
                        "answer": "alter",
                        "hit": false
                    },
                    {
                        "score": 0.3458963922743379,
                        "answer": "stalin",
                        "hit": false
                    },
                    {
                        "score": 0.33563195722535155,
                        "answer": "enables",
                        "hit": false
                    },
                    {
                        "score": 0.3346252877538602,
                        "answer": "nextone",
                        "hit": false
                    },
                    {
                        "score": 0.3267259609589428,
                        "answer": "actully",
                        "hit": false
                    },
                    {
                        "score": 0.3203572381630215,
                        "answer": "assumtions",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "become"
                ],
                "rank": 13552,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6000720486044884
            },
            {
                "question verbose": "What is to believe ",
                "b": "believe",
                "expected answer": [
                    "believes"
                ],
                "predictions": [
                    {
                        "score": 0.35605355380279946,
                        "answer": "lastly",
                        "hit": false
                    },
                    {
                        "score": 0.348889475958986,
                        "answer": "uncouth",
                        "hit": false
                    },
                    {
                        "score": 0.3271403726329991,
                        "answer": "bugged",
                        "hit": false
                    },
                    {
                        "score": 0.3210683397119069,
                        "answer": "messiness",
                        "hit": false
                    },
                    {
                        "score": 0.3209124399998888,
                        "answer": "ultimate",
                        "hit": false
                    },
                    {
                        "score": 0.3187889859778911,
                        "answer": "hyperbolic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believe"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5253567472100258
            },
            {
                "question verbose": "What is to consider ",
                "b": "consider",
                "expected answer": [
                    "considers"
                ],
                "predictions": [
                    {
                        "score": 0.4114882137318251,
                        "answer": "decorating",
                        "hit": false
                    },
                    {
                        "score": 0.4035847483679683,
                        "answer": "betrayed",
                        "hit": false
                    },
                    {
                        "score": 0.39153332197927687,
                        "answer": "folly",
                        "hit": false
                    },
                    {
                        "score": 0.38400554538930515,
                        "answer": "ka",
                        "hit": false
                    },
                    {
                        "score": 0.377506433617711,
                        "answer": "coughed",
                        "hit": false
                    },
                    {
                        "score": 0.3724369704416651,
                        "answer": "pouring",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consider"
                ],
                "rank": 6180,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7101222276687622
            },
            {
                "question verbose": "What is to consist ",
                "b": "consist",
                "expected answer": [
                    "consists"
                ],
                "predictions": [
                    {
                        "score": 0.5852307980574377,
                        "answer": "tufekci",
                        "hit": false
                    },
                    {
                        "score": 0.5823269895458704,
                        "answer": "atheism",
                        "hit": false
                    },
                    {
                        "score": 0.571676423501039,
                        "answer": "rushed",
                        "hit": false
                    },
                    {
                        "score": 0.569745403050029,
                        "answer": "scout",
                        "hit": false
                    },
                    {
                        "score": 0.5681766685962216,
                        "answer": "noriega",
                        "hit": false
                    },
                    {
                        "score": 0.5681320576829096,
                        "answer": "catalyst",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consist"
                ],
                "rank": 9297,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7209264487028122
            },
            {
                "question verbose": "What is to contain ",
                "b": "contain",
                "expected answer": [
                    "contains"
                ],
                "predictions": [
                    {
                        "score": 0.5216336016649522,
                        "answer": "hurry",
                        "hit": false
                    },
                    {
                        "score": 0.5188598017338515,
                        "answer": "bigs",
                        "hit": false
                    },
                    {
                        "score": 0.5184302577169657,
                        "answer": "bragged",
                        "hit": false
                    },
                    {
                        "score": 0.5181580799333334,
                        "answer": "schoolyour",
                        "hit": false
                    },
                    {
                        "score": 0.5139000372847363,
                        "answer": "blubbering",
                        "hit": false
                    },
                    {
                        "score": 0.5131266969825703,
                        "answer": "moark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "contain"
                ],
                "rank": 14147,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6721554547548294
            },
            {
                "question verbose": "What is to continue ",
                "b": "continue",
                "expected answer": [
                    "continues"
                ],
                "predictions": [
                    {
                        "score": 0.3277033939024363,
                        "answer": "milions",
                        "hit": false
                    },
                    {
                        "score": 0.3181485352946984,
                        "answer": "entice",
                        "hit": false
                    },
                    {
                        "score": 0.3165508087327484,
                        "answer": "espncom",
                        "hit": false
                    },
                    {
                        "score": 0.3110001920676322,
                        "answer": "wellness",
                        "hit": false
                    },
                    {
                        "score": 0.3109331619759273,
                        "answer": "alongside",
                        "hit": false
                    },
                    {
                        "score": 0.3109211807920615,
                        "answer": "mule",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continue"
                ],
                "rank": 14921,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5270208176225424
            },
            {
                "question verbose": "What is to create ",
                "b": "create",
                "expected answer": [
                    "creates"
                ],
                "predictions": [
                    {
                        "score": 0.3769057913763057,
                        "answer": "unprepared",
                        "hit": false
                    },
                    {
                        "score": 0.3695762581845,
                        "answer": "unwavering",
                        "hit": false
                    },
                    {
                        "score": 0.3693660285689395,
                        "answer": "hyperbolic",
                        "hit": false
                    },
                    {
                        "score": 0.3640882856495801,
                        "answer": "egregious",
                        "hit": false
                    },
                    {
                        "score": 0.3555357586451936,
                        "answer": "forgave",
                        "hit": false
                    },
                    {
                        "score": 0.3525715164209178,
                        "answer": "chronologically",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "create"
                ],
                "rank": 11657,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6087543666362762
            },
            {
                "question verbose": "What is to describe ",
                "b": "describe",
                "expected answer": [
                    "describes"
                ],
                "predictions": [
                    {
                        "score": 0.5019862203511765,
                        "answer": "wooden",
                        "hit": false
                    },
                    {
                        "score": 0.4792601540916022,
                        "answer": "canned",
                        "hit": false
                    },
                    {
                        "score": 0.46076243356832863,
                        "answer": "portrays",
                        "hit": false
                    },
                    {
                        "score": 0.4497689813018043,
                        "answer": "gbsea",
                        "hit": false
                    },
                    {
                        "score": 0.4466550855979393,
                        "answer": "marron",
                        "hit": false
                    },
                    {
                        "score": 0.4453123967981211,
                        "answer": "vhincuk",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "describe"
                ],
                "rank": 7991,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.714682400226593
            },
            {
                "question verbose": "What is to develop ",
                "b": "develop",
                "expected answer": [
                    "develops"
                ],
                "predictions": [
                    {
                        "score": 0.43140913248517626,
                        "answer": "falsehood",
                        "hit": false
                    },
                    {
                        "score": 0.3986616814173995,
                        "answer": "undoing",
                        "hit": false
                    },
                    {
                        "score": 0.3982207610537222,
                        "answer": "skillsets",
                        "hit": false
                    },
                    {
                        "score": 0.39212786546492084,
                        "answer": "reviewing",
                        "hit": false
                    },
                    {
                        "score": 0.3898890143636887,
                        "answer": "misdated",
                        "hit": false
                    },
                    {
                        "score": 0.3896500218913007,
                        "answer": "shied",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develop"
                ],
                "rank": 6656,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6627942770719528
            },
            {
                "question verbose": "What is to enable ",
                "b": "enable",
                "expected answer": [
                    "enables"
                ],
                "predictions": [
                    {
                        "score": 0.6120803288127338,
                        "answer": "mismanagement",
                        "hit": false
                    },
                    {
                        "score": 0.5750692301899495,
                        "answer": "nunziata",
                        "hit": false
                    },
                    {
                        "score": 0.5690627266830406,
                        "answer": "shyness",
                        "hit": false
                    },
                    {
                        "score": 0.5652413943887556,
                        "answer": "endorsing",
                        "hit": false
                    },
                    {
                        "score": 0.5647638471125429,
                        "answer": "authorizing",
                        "hit": false
                    },
                    {
                        "score": 0.5643948034314221,
                        "answer": "currency",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enable"
                ],
                "rank": 2741,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8144548833370209
            },
            {
                "question verbose": "What is to enjoy ",
                "b": "enjoy",
                "expected answer": [
                    "enjoys"
                ],
                "predictions": [
                    {
                        "score": 0.4347447505294161,
                        "answer": "befuddle",
                        "hit": false
                    },
                    {
                        "score": 0.4258579251888391,
                        "answer": "conquer",
                        "hit": false
                    },
                    {
                        "score": 0.42486405112700837,
                        "answer": "traveled",
                        "hit": false
                    },
                    {
                        "score": 0.4165463806506587,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.41369067029474116,
                        "answer": "bursting",
                        "hit": false
                    },
                    {
                        "score": 0.4130899662570334,
                        "answer": "magnificent",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enjoy"
                ],
                "rank": 4944,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.711355060338974
            },
            {
                "question verbose": "What is to ensure ",
                "b": "ensure",
                "expected answer": [
                    "ensures"
                ],
                "predictions": [
                    {
                        "score": 0.5218225627300458,
                        "answer": "homeopathy",
                        "hit": false
                    },
                    {
                        "score": 0.517422821682538,
                        "answer": "admonished",
                        "hit": false
                    },
                    {
                        "score": 0.49749097953856997,
                        "answer": "audacious",
                        "hit": false
                    },
                    {
                        "score": 0.4955591293906811,
                        "answer": "analogized",
                        "hit": false
                    },
                    {
                        "score": 0.495170031957893,
                        "answer": "neutrality",
                        "hit": false
                    },
                    {
                        "score": 0.4951190768534665,
                        "answer": "constitute",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ensure"
                ],
                "rank": 4742,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7564758956432343
            },
            {
                "question verbose": "What is to exist ",
                "b": "exist",
                "expected answer": [
                    "exists"
                ],
                "predictions": [
                    {
                        "score": 0.47164488864685106,
                        "answer": "confronting",
                        "hit": false
                    },
                    {
                        "score": 0.47157531923595447,
                        "answer": "harbor",
                        "hit": false
                    },
                    {
                        "score": 0.47036881755180876,
                        "answer": "shes",
                        "hit": false
                    },
                    {
                        "score": 0.4694765391275555,
                        "answer": "approving",
                        "hit": false
                    },
                    {
                        "score": 0.4649334006350026,
                        "answer": "stormtrooper",
                        "hit": false
                    },
                    {
                        "score": 0.459244407187919,
                        "answer": "tamp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "exist"
                ],
                "rank": 12571,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.695302814245224
            },
            {
                "question verbose": "What is to explain ",
                "b": "explain",
                "expected answer": [
                    "explains"
                ],
                "predictions": [
                    {
                        "score": 0.44337680265086854,
                        "answer": "resolvable",
                        "hit": false
                    },
                    {
                        "score": 0.4433422164719274,
                        "answer": "countered",
                        "hit": false
                    },
                    {
                        "score": 0.4267289077976319,
                        "answer": "misstruths",
                        "hit": false
                    },
                    {
                        "score": 0.41789759142348404,
                        "answer": "gag",
                        "hit": false
                    },
                    {
                        "score": 0.4167656906431277,
                        "answer": "nerve",
                        "hit": false
                    },
                    {
                        "score": 0.4124307329176259,
                        "answer": "undestand",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "explain"
                ],
                "rank": 10706,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6689577698707581
            },
            {
                "question verbose": "What is to follow ",
                "b": "follow",
                "expected answer": [
                    "follows"
                ],
                "predictions": [
                    {
                        "score": 0.37828835099451347,
                        "answer": "altered",
                        "hit": false
                    },
                    {
                        "score": 0.3719134069534689,
                        "answer": "resolvable",
                        "hit": false
                    },
                    {
                        "score": 0.3714933562901368,
                        "answer": "fifantasy",
                        "hit": false
                    },
                    {
                        "score": 0.36530482278178905,
                        "answer": "aggressively",
                        "hit": false
                    },
                    {
                        "score": 0.35815885425447713,
                        "answer": "relationshiop",
                        "hit": false
                    },
                    {
                        "score": 0.3477942112958053,
                        "answer": "cushion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "follow"
                ],
                "rank": 5413,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6658674478530884
            },
            {
                "question verbose": "What is to happen ",
                "b": "happen",
                "expected answer": [
                    "happens"
                ],
                "predictions": [
                    {
                        "score": 0.4044600486739676,
                        "answer": "countermeasure",
                        "hit": false
                    },
                    {
                        "score": 0.3980792605420711,
                        "answer": "escalates",
                        "hit": false
                    },
                    {
                        "score": 0.39441812658529596,
                        "answer": "vacuum",
                        "hit": false
                    },
                    {
                        "score": 0.3897201685859387,
                        "answer": "likeable",
                        "hit": false
                    },
                    {
                        "score": 0.3862211050117631,
                        "answer": "feminist",
                        "hit": false
                    },
                    {
                        "score": 0.38307295273759934,
                        "answer": "wee",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happen"
                ],
                "rank": 13193,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6433202922344208
            },
            {
                "question verbose": "What is to hear ",
                "b": "hear",
                "expected answer": [
                    "hears"
                ],
                "predictions": [
                    {
                        "score": 0.4123108854919334,
                        "answer": "limp",
                        "hit": false
                    },
                    {
                        "score": 0.38451964787459547,
                        "answer": "pimp",
                        "hit": false
                    },
                    {
                        "score": 0.38138347747888873,
                        "answer": "gertie",
                        "hit": false
                    },
                    {
                        "score": 0.37045362324953374,
                        "answer": "attic",
                        "hit": false
                    },
                    {
                        "score": 0.366667212150451,
                        "answer": "outraged",
                        "hit": false
                    },
                    {
                        "score": 0.3666192186685795,
                        "answer": "drunkenly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hear"
                ],
                "rank": 7612,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6426307111978531
            },
            {
                "question verbose": "What is to identify ",
                "b": "identify",
                "expected answer": [
                    "identifies"
                ],
                "predictions": [
                    {
                        "score": 0.5050120815688417,
                        "answer": "scrabble",
                        "hit": false
                    },
                    {
                        "score": 0.4948630489559072,
                        "answer": "bonus",
                        "hit": false
                    },
                    {
                        "score": 0.4693411110000478,
                        "answer": "matching",
                        "hit": false
                    },
                    {
                        "score": 0.4610541626951048,
                        "answer": "mothering",
                        "hit": false
                    },
                    {
                        "score": 0.4528146979843529,
                        "answer": "workday",
                        "hit": false
                    },
                    {
                        "score": 0.45248031414898743,
                        "answer": "steroid",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "identify"
                ],
                "rank": 7630,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6883530467748642
            },
            {
                "question verbose": "What is to improve ",
                "b": "improve",
                "expected answer": [
                    "improves"
                ],
                "predictions": [
                    {
                        "score": 0.43403568737468157,
                        "answer": "cravenness",
                        "hit": false
                    },
                    {
                        "score": 0.40657918486317707,
                        "answer": "countryside",
                        "hit": false
                    },
                    {
                        "score": 0.40468987427789965,
                        "answer": "specialist",
                        "hit": false
                    },
                    {
                        "score": 0.3959780840527512,
                        "answer": "anticoagulant",
                        "hit": false
                    },
                    {
                        "score": 0.3951009177810868,
                        "answer": "pinnacle",
                        "hit": false
                    },
                    {
                        "score": 0.3950191592925269,
                        "answer": "bailed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improve"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6177749186754227
            },
            {
                "question verbose": "What is to include ",
                "b": "include",
                "expected answer": [
                    "includes"
                ],
                "predictions": [
                    {
                        "score": 0.39233568877772684,
                        "answer": "panamanian",
                        "hit": false
                    },
                    {
                        "score": 0.38886414144961423,
                        "answer": "assassin",
                        "hit": false
                    },
                    {
                        "score": 0.3875630432938128,
                        "answer": "gasbuddy",
                        "hit": false
                    },
                    {
                        "score": 0.3838978628511175,
                        "answer": "aikido",
                        "hit": false
                    },
                    {
                        "score": 0.36465826339672747,
                        "answer": "rubble",
                        "hit": false
                    },
                    {
                        "score": 0.3615054535148087,
                        "answer": "forthemission",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "include"
                ],
                "rank": 11302,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6441870182752609
            },
            {
                "question verbose": "What is to involve ",
                "b": "involve",
                "expected answer": [
                    "involves"
                ],
                "predictions": [
                    {
                        "score": 0.5987923508183816,
                        "answer": "oftentimes",
                        "hit": false
                    },
                    {
                        "score": 0.5607231886225987,
                        "answer": "analogized",
                        "hit": false
                    },
                    {
                        "score": 0.5554384954960037,
                        "answer": "insofar",
                        "hit": false
                    },
                    {
                        "score": 0.5534599972534483,
                        "answer": "sketchiness",
                        "hit": false
                    },
                    {
                        "score": 0.5480083122198994,
                        "answer": "implicated",
                        "hit": false
                    },
                    {
                        "score": 0.547843092349027,
                        "answer": "thay",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involve"
                ],
                "rank": 12892,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.694423720240593
            },
            {
                "question verbose": "What is to learn ",
                "b": "learn",
                "expected answer": [
                    "learns"
                ],
                "predictions": [
                    {
                        "score": 0.40484432754618876,
                        "answer": "glue",
                        "hit": false
                    },
                    {
                        "score": 0.3744966049850321,
                        "answer": "understated",
                        "hit": false
                    },
                    {
                        "score": 0.35702112183797885,
                        "answer": "emo",
                        "hit": false
                    },
                    {
                        "score": 0.350392500212879,
                        "answer": "saver",
                        "hit": false
                    },
                    {
                        "score": 0.35015946063670916,
                        "answer": "lastly",
                        "hit": false
                    },
                    {
                        "score": 0.3500706889965208,
                        "answer": "godawfull",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "learn"
                ],
                "rank": 8143,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6278603971004486
            },
            {
                "question verbose": "What is to maintain ",
                "b": "maintain",
                "expected answer": [
                    "maintains"
                ],
                "predictions": [
                    {
                        "score": 0.4617413632559149,
                        "answer": "antithesis",
                        "hit": false
                    },
                    {
                        "score": 0.4516215361648367,
                        "answer": "defended",
                        "hit": false
                    },
                    {
                        "score": 0.45000055272418266,
                        "answer": "acquiesce",
                        "hit": false
                    },
                    {
                        "score": 0.4474068593853971,
                        "answer": "carted",
                        "hit": false
                    },
                    {
                        "score": 0.4460791892080563,
                        "answer": "fatal",
                        "hit": false
                    },
                    {
                        "score": 0.4458974553251055,
                        "answer": "flavor",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "maintain"
                ],
                "rank": 1908,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7762594223022461
            },
            {
                "question verbose": "What is to occur ",
                "b": "occur",
                "expected answer": [
                    "occurs"
                ],
                "predictions": [
                    {
                        "score": 0.5461619923470038,
                        "answer": "redeem",
                        "hit": false
                    },
                    {
                        "score": 0.5443612445124205,
                        "answer": "protects",
                        "hit": false
                    },
                    {
                        "score": 0.5383056963799585,
                        "answer": "oasis",
                        "hit": false
                    },
                    {
                        "score": 0.5342919857041761,
                        "answer": "cursing",
                        "hit": false
                    },
                    {
                        "score": 0.5340739747551121,
                        "answer": "serial",
                        "hit": false
                    },
                    {
                        "score": 0.5313273726869333,
                        "answer": "astute",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "occur"
                ],
                "rank": 9701,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7295161336660385
            },
            {
                "question verbose": "What is to operate ",
                "b": "operate",
                "expected answer": [
                    "operates"
                ],
                "predictions": [
                    {
                        "score": 0.5646943713294565,
                        "answer": "moneywell",
                        "hit": false
                    },
                    {
                        "score": 0.5613474716253557,
                        "answer": "schoolyour",
                        "hit": false
                    },
                    {
                        "score": 0.560644039174205,
                        "answer": "enthused",
                        "hit": false
                    },
                    {
                        "score": 0.5601312447114131,
                        "answer": "loophole",
                        "hit": false
                    },
                    {
                        "score": 0.5572401701058677,
                        "answer": "calculates",
                        "hit": false
                    },
                    {
                        "score": 0.550943838945103,
                        "answer": "economical",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "operate"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6656206548213959
            },
            {
                "question verbose": "What is to prevent ",
                "b": "prevent",
                "expected answer": [
                    "prevents"
                ],
                "predictions": [
                    {
                        "score": 0.44676946658958183,
                        "answer": "hypothesized",
                        "hit": false
                    },
                    {
                        "score": 0.4372868915458888,
                        "answer": "subversive",
                        "hit": false
                    },
                    {
                        "score": 0.43138205475314617,
                        "answer": "defended",
                        "hit": false
                    },
                    {
                        "score": 0.4289069216106544,
                        "answer": "jargon",
                        "hit": false
                    },
                    {
                        "score": 0.427749904962703,
                        "answer": "nunziata",
                        "hit": false
                    },
                    {
                        "score": 0.42758832574799,
                        "answer": "mismanagement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prevent"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5659578889608383
            },
            {
                "question verbose": "What is to promote ",
                "b": "promote",
                "expected answer": [
                    "promotes"
                ],
                "predictions": [
                    {
                        "score": 0.4523829599434538,
                        "answer": "partly",
                        "hit": false
                    },
                    {
                        "score": 0.4322987760433946,
                        "answer": "coughed",
                        "hit": false
                    },
                    {
                        "score": 0.4293601650659481,
                        "answer": "obgyn",
                        "hit": false
                    },
                    {
                        "score": 0.4237416442107298,
                        "answer": "decisionmaking",
                        "hit": false
                    },
                    {
                        "score": 0.41869011855998284,
                        "answer": "falsehood",
                        "hit": false
                    },
                    {
                        "score": 0.41575458006620025,
                        "answer": "acute",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "promote"
                ],
                "rank": 768,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7233819514513016
            },
            {
                "question verbose": "What is to protect ",
                "b": "protect",
                "expected answer": [
                    "protects"
                ],
                "predictions": [
                    {
                        "score": 0.47641780600812295,
                        "answer": "subversive",
                        "hit": false
                    },
                    {
                        "score": 0.4618677362788299,
                        "answer": "decisionmaking",
                        "hit": false
                    },
                    {
                        "score": 0.459544275330504,
                        "answer": "fellatio",
                        "hit": false
                    },
                    {
                        "score": 0.44452048300982927,
                        "answer": "falsehood",
                        "hit": false
                    },
                    {
                        "score": 0.44378855514917337,
                        "answer": "happenstance",
                        "hit": false
                    },
                    {
                        "score": 0.44058741849230665,
                        "answer": "agenda",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "protect"
                ],
                "rank": 440,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7189178019762039
            },
            {
                "question verbose": "What is to provide ",
                "b": "provide",
                "expected answer": [
                    "provides"
                ],
                "predictions": [
                    {
                        "score": 0.40928521224463826,
                        "answer": "manticore",
                        "hit": false
                    },
                    {
                        "score": 0.3802685047625804,
                        "answer": "practicum",
                        "hit": false
                    },
                    {
                        "score": 0.3796054722345124,
                        "answer": "obtaining",
                        "hit": false
                    },
                    {
                        "score": 0.3788314402660978,
                        "answer": "curriculum",
                        "hit": false
                    },
                    {
                        "score": 0.3771968380046045,
                        "answer": "vetvet",
                        "hit": false
                    },
                    {
                        "score": 0.3767778692652998,
                        "answer": "completing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "provide"
                ],
                "rank": 13022,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6809077858924866
            },
            {
                "question verbose": "What is to receive ",
                "b": "receive",
                "expected answer": [
                    "receives"
                ],
                "predictions": [
                    {
                        "score": 0.43503147774889955,
                        "answer": "wellness",
                        "hit": false
                    },
                    {
                        "score": 0.43071374421447073,
                        "answer": "consists",
                        "hit": false
                    },
                    {
                        "score": 0.412411224607614,
                        "answer": "scrap",
                        "hit": false
                    },
                    {
                        "score": 0.4073859537322129,
                        "answer": "paralysis",
                        "hit": false
                    },
                    {
                        "score": 0.4071086490295082,
                        "answer": "unjust",
                        "hit": false
                    },
                    {
                        "score": 0.40466516380811324,
                        "answer": "coughed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receive"
                ],
                "rank": 6077,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6777386218309402
            },
            {
                "question verbose": "What is to reduce ",
                "b": "reduce",
                "expected answer": [
                    "reduces"
                ],
                "predictions": [
                    {
                        "score": 0.41884553567348287,
                        "answer": "schnauzer",
                        "hit": false
                    },
                    {
                        "score": 0.4172987566968749,
                        "answer": "vacant",
                        "hit": false
                    },
                    {
                        "score": 0.41652453273386003,
                        "answer": "wellness",
                        "hit": false
                    },
                    {
                        "score": 0.4134593178861312,
                        "answer": "neutrality",
                        "hit": false
                    },
                    {
                        "score": 0.41258178034642273,
                        "answer": "rewarded",
                        "hit": false
                    },
                    {
                        "score": 0.4082670164127393,
                        "answer": "impregnate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reduce"
                ],
                "rank": 7612,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7450933903455734
            },
            {
                "question verbose": "What is to refer ",
                "b": "refer",
                "expected answer": [
                    "refers"
                ],
                "predictions": [
                    {
                        "score": 0.4926658438698765,
                        "answer": "divine",
                        "hit": false
                    },
                    {
                        "score": 0.4654387749240443,
                        "answer": "comedic",
                        "hit": false
                    },
                    {
                        "score": 0.4565070447579817,
                        "answer": "intriguing",
                        "hit": false
                    },
                    {
                        "score": 0.4502249089577028,
                        "answer": "acquiesce",
                        "hit": false
                    },
                    {
                        "score": 0.44899276686823947,
                        "answer": "wlc",
                        "hit": false
                    },
                    {
                        "score": 0.4486527078688084,
                        "answer": "audacious",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "refer"
                ],
                "rank": 14872,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5538145862519741
            },
            {
                "question verbose": "What is to remain ",
                "b": "remain",
                "expected answer": [
                    "remains"
                ],
                "predictions": [
                    {
                        "score": 0.47616885220695265,
                        "answer": "solvent",
                        "hit": false
                    },
                    {
                        "score": 0.47345319098543365,
                        "answer": "embarrass",
                        "hit": false
                    },
                    {
                        "score": 0.4724225873076058,
                        "answer": "sorriest",
                        "hit": false
                    },
                    {
                        "score": 0.46639232220851445,
                        "answer": "goose",
                        "hit": false
                    },
                    {
                        "score": 0.45760662135312036,
                        "answer": "morocco",
                        "hit": false
                    },
                    {
                        "score": 0.45752316007856997,
                        "answer": "legalize",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remain"
                ],
                "rank": 3725,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7166646718978882
            },
            {
                "question verbose": "What is to remember ",
                "b": "remember",
                "expected answer": [
                    "remembers"
                ],
                "predictions": [
                    {
                        "score": 0.39929245783688727,
                        "answer": "bonus",
                        "hit": false
                    },
                    {
                        "score": 0.37669858960166924,
                        "answer": "cheering",
                        "hit": false
                    },
                    {
                        "score": 0.373588511500632,
                        "answer": "defensively",
                        "hit": false
                    },
                    {
                        "score": 0.3718120128876847,
                        "answer": "cancel",
                        "hit": false
                    },
                    {
                        "score": 0.36896032297202197,
                        "answer": "squirting",
                        "hit": false
                    },
                    {
                        "score": 0.36830844856294026,
                        "answer": "waited",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remember"
                ],
                "rank": 8771,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7081674784421921
            },
            {
                "question verbose": "What is to represent ",
                "b": "represent",
                "expected answer": [
                    "represents"
                ],
                "predictions": [
                    {
                        "score": 0.4751865634852649,
                        "answer": "scroll",
                        "hit": false
                    },
                    {
                        "score": 0.47231135893888593,
                        "answer": "kindergarten",
                        "hit": false
                    },
                    {
                        "score": 0.46224594048437645,
                        "answer": "associate",
                        "hit": false
                    },
                    {
                        "score": 0.45764241633047786,
                        "answer": "agrees",
                        "hit": false
                    },
                    {
                        "score": 0.45686775504784755,
                        "answer": "aerodynamic",
                        "hit": false
                    },
                    {
                        "score": 0.4565654956270189,
                        "answer": "sluggish",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "represent"
                ],
                "rank": 9592,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7588788568973541
            },
            {
                "question verbose": "What is to require ",
                "b": "require",
                "expected answer": [
                    "requires"
                ],
                "predictions": [
                    {
                        "score": 0.43922895943796386,
                        "answer": "fortunate",
                        "hit": false
                    },
                    {
                        "score": 0.4279097875018094,
                        "answer": "neutrality",
                        "hit": false
                    },
                    {
                        "score": 0.4148058354399639,
                        "answer": "heath",
                        "hit": false
                    },
                    {
                        "score": 0.4112975433136066,
                        "answer": "nope",
                        "hit": false
                    },
                    {
                        "score": 0.4096403176312138,
                        "answer": "decisionmaking",
                        "hit": false
                    },
                    {
                        "score": 0.4066359929080112,
                        "answer": "tib",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "require"
                ],
                "rank": 13093,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6267626732587814
            },
            {
                "question verbose": "What is to seem ",
                "b": "seem",
                "expected answer": [
                    "seems"
                ],
                "predictions": [
                    {
                        "score": 0.4020528598720027,
                        "answer": "deluded",
                        "hit": false
                    },
                    {
                        "score": 0.39220558629313784,
                        "answer": "shouted",
                        "hit": false
                    },
                    {
                        "score": 0.389902307056705,
                        "answer": "disguised",
                        "hit": false
                    },
                    {
                        "score": 0.3878271894618229,
                        "answer": "trench",
                        "hit": false
                    },
                    {
                        "score": 0.3877191347101714,
                        "answer": "racist",
                        "hit": false
                    },
                    {
                        "score": 0.3855419914287506,
                        "answer": "augmented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seem"
                ],
                "rank": 3791,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.668192908167839
            },
            {
                "question verbose": "What is to send ",
                "b": "send",
                "expected answer": [
                    "sends"
                ],
                "predictions": [
                    {
                        "score": 0.4183664539020248,
                        "answer": "arrive",
                        "hit": false
                    },
                    {
                        "score": 0.40532835039583986,
                        "answer": "piled",
                        "hit": false
                    },
                    {
                        "score": 0.40440317705523,
                        "answer": "fence",
                        "hit": false
                    },
                    {
                        "score": 0.40102688897285593,
                        "answer": "porch",
                        "hit": false
                    },
                    {
                        "score": 0.3985146541654449,
                        "answer": "dealer",
                        "hit": false
                    },
                    {
                        "score": 0.39599323508149936,
                        "answer": "intestine",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "send"
                ],
                "rank": 1232,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7393911480903625
            },
            {
                "question verbose": "What is to suggest ",
                "b": "suggest",
                "expected answer": [
                    "suggests"
                ],
                "predictions": [
                    {
                        "score": 0.4673962222623442,
                        "answer": "ewmhbus",
                        "hit": false
                    },
                    {
                        "score": 0.45625157533072125,
                        "answer": "biotechpharma",
                        "hit": false
                    },
                    {
                        "score": 0.44352827906438297,
                        "answer": "sully",
                        "hit": false
                    },
                    {
                        "score": 0.4403980095137873,
                        "answer": "predisposition",
                        "hit": false
                    },
                    {
                        "score": 0.4401179992657427,
                        "answer": "paradoxically",
                        "hit": false
                    },
                    {
                        "score": 0.4396282544370185,
                        "answer": "pouring",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "suggest"
                ],
                "rank": 10298,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7137751579284668
            },
            {
                "question verbose": "What is to tell ",
                "b": "tell",
                "expected answer": [
                    "tells"
                ],
                "predictions": [
                    {
                        "score": 0.36916076416280696,
                        "answer": "analyzing",
                        "hit": false
                    },
                    {
                        "score": 0.36567347515960547,
                        "answer": "frustration",
                        "hit": false
                    },
                    {
                        "score": 0.3614412711974557,
                        "answer": "emailed",
                        "hit": false
                    },
                    {
                        "score": 0.3571447067467548,
                        "answer": "lagerfeld",
                        "hit": false
                    },
                    {
                        "score": 0.3558973776465429,
                        "answer": "adores",
                        "hit": false
                    },
                    {
                        "score": 0.35300384402377466,
                        "answer": "duplass",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tell"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5753644853830338
            },
            {
                "question verbose": "What is to understand ",
                "b": "understand",
                "expected answer": [
                    "understands"
                ],
                "predictions": [
                    {
                        "score": 0.39306965673179034,
                        "answer": "hispanic",
                        "hit": false
                    },
                    {
                        "score": 0.379902278753037,
                        "answer": "endearing",
                        "hit": false
                    },
                    {
                        "score": 0.3749616055277326,
                        "answer": "shouted",
                        "hit": false
                    },
                    {
                        "score": 0.3729572853990704,
                        "answer": "bogus",
                        "hit": false
                    },
                    {
                        "score": 0.3716029761335903,
                        "answer": "melting",
                        "hit": false
                    },
                    {
                        "score": 0.37033388937245704,
                        "answer": "nerve",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "understand"
                ],
                "rank": 9789,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6810688227415085
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I05 [verb_inf - 3pSg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "dc1e0eca-3202-4234-84dd-2786f6bbb6d6",
            "timestamp": "2020-10-22T15:57:37.646331"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to achieve ",
                "b": "achieve",
                "expected answer": [
                    "achieving"
                ],
                "predictions": [
                    {
                        "score": 0.47213202801417564,
                        "answer": "creativity",
                        "hit": false
                    },
                    {
                        "score": 0.4615044274451903,
                        "answer": "nursing",
                        "hit": false
                    },
                    {
                        "score": 0.45502548402625614,
                        "answer": "excellence",
                        "hit": false
                    },
                    {
                        "score": 0.45278582895059166,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.44312947510573414,
                        "answer": "formed",
                        "hit": false
                    },
                    {
                        "score": 0.4407690028938136,
                        "answer": "incubator",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "achieve"
                ],
                "rank": 2754,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7187515646219254
            },
            {
                "question verbose": "What is to add ",
                "b": "add",
                "expected answer": [
                    "adding"
                ],
                "predictions": [
                    {
                        "score": 0.3717440680421312,
                        "answer": "comparison",
                        "hit": false
                    },
                    {
                        "score": 0.3656249988641907,
                        "answer": "brush",
                        "hit": false
                    },
                    {
                        "score": 0.3589136442926465,
                        "answer": "speculation",
                        "hit": false
                    },
                    {
                        "score": 0.34900991934421793,
                        "answer": "contemplating",
                        "hit": false
                    },
                    {
                        "score": 0.3466032428813803,
                        "answer": "pic",
                        "hit": false
                    },
                    {
                        "score": 0.343823227509982,
                        "answer": "whatsoever",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "add"
                ],
                "rank": 10223,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6356453746557236
            },
            {
                "question verbose": "What is to allow ",
                "b": "allow",
                "expected answer": [
                    "allowing"
                ],
                "predictions": [
                    {
                        "score": 0.4092066316679853,
                        "answer": "installed",
                        "hit": false
                    },
                    {
                        "score": 0.4023182801870143,
                        "answer": "madeleine",
                        "hit": false
                    },
                    {
                        "score": 0.3924980401221593,
                        "answer": "odesserts",
                        "hit": false
                    },
                    {
                        "score": 0.3895565196780069,
                        "answer": "hepatoxicity",
                        "hit": false
                    },
                    {
                        "score": 0.3811927369555389,
                        "answer": "containing",
                        "hit": false
                    },
                    {
                        "score": 0.37771512296704085,
                        "answer": "lick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allow"
                ],
                "rank": 10795,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6160940229892731
            },
            {
                "question verbose": "What is to appear ",
                "b": "appear",
                "expected answer": [
                    "appearing"
                ],
                "predictions": [
                    {
                        "score": 0.5111464489734592,
                        "answer": "caffeine",
                        "hit": false
                    },
                    {
                        "score": 0.5079652665333428,
                        "answer": "roadway",
                        "hit": false
                    },
                    {
                        "score": 0.4999785232524024,
                        "answer": "nascrc",
                        "hit": false
                    },
                    {
                        "score": 0.49674511260246074,
                        "answer": "exploratory",
                        "hit": false
                    },
                    {
                        "score": 0.48936966198184,
                        "answer": "convict",
                        "hit": false
                    },
                    {
                        "score": 0.4847239893643921,
                        "answer": "madeleine",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appear"
                ],
                "rank": 7587,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7604082822799683
            },
            {
                "question verbose": "What is to apply ",
                "b": "apply",
                "expected answer": [
                    "applying"
                ],
                "predictions": [
                    {
                        "score": 0.4352171441935341,
                        "answer": "artificial",
                        "hit": false
                    },
                    {
                        "score": 0.4191406143561682,
                        "answer": "curator",
                        "hit": false
                    },
                    {
                        "score": 0.40920677557831897,
                        "answer": "museum",
                        "hit": false
                    },
                    {
                        "score": 0.40638184000653654,
                        "answer": "entertained",
                        "hit": false
                    },
                    {
                        "score": 0.3965495393546782,
                        "answer": "colourful",
                        "hit": false
                    },
                    {
                        "score": 0.39608617911093924,
                        "answer": "implicit",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apply"
                ],
                "rank": 3721,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7341312021017075
            },
            {
                "question verbose": "What is to ask ",
                "b": "ask",
                "expected answer": [
                    "asking"
                ],
                "predictions": [
                    {
                        "score": 0.3944694967562903,
                        "answer": "pagene",
                        "hit": false
                    },
                    {
                        "score": 0.3797648324255094,
                        "answer": "conceptual",
                        "hit": false
                    },
                    {
                        "score": 0.3745662208693573,
                        "answer": "owe",
                        "hit": false
                    },
                    {
                        "score": 0.3585484578555103,
                        "answer": "infanticide",
                        "hit": false
                    },
                    {
                        "score": 0.35581209750757486,
                        "answer": "investorssportsmen",
                        "hit": false
                    },
                    {
                        "score": 0.35253309232895036,
                        "answer": "slung",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ask"
                ],
                "rank": 11977,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7329642176628113
            },
            {
                "question verbose": "What is to attend ",
                "b": "attend",
                "expected answer": [
                    "attending"
                ],
                "predictions": [
                    {
                        "score": 0.4849540101719295,
                        "answer": "sometime",
                        "hit": false
                    },
                    {
                        "score": 0.47696917382717463,
                        "answer": "scholaryale",
                        "hit": false
                    },
                    {
                        "score": 0.47688782560923043,
                        "answer": "miller",
                        "hit": false
                    },
                    {
                        "score": 0.4623358805324726,
                        "answer": "premiere",
                        "hit": false
                    },
                    {
                        "score": 0.45893307653758225,
                        "answer": "guardsman",
                        "hit": false
                    },
                    {
                        "score": 0.45734335464841475,
                        "answer": "thrift",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "attend"
                ],
                "rank": 3665,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6912883222103119
            },
            {
                "question verbose": "What is to avoid ",
                "b": "avoid",
                "expected answer": [
                    "avoiding"
                ],
                "predictions": [
                    {
                        "score": 0.4107452575352638,
                        "answer": "proposing",
                        "hit": false
                    },
                    {
                        "score": 0.3885828219652828,
                        "answer": "ascertained",
                        "hit": false
                    },
                    {
                        "score": 0.3858227532215573,
                        "answer": "slanderous",
                        "hit": false
                    },
                    {
                        "score": 0.3807328983679467,
                        "answer": "ruffled",
                        "hit": false
                    },
                    {
                        "score": 0.3800869548175926,
                        "answer": "temerity",
                        "hit": false
                    },
                    {
                        "score": 0.3785576805963902,
                        "answer": "reunited",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "avoid"
                ],
                "rank": 6131,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.670707032084465
            },
            {
                "question verbose": "What is to become ",
                "b": "become",
                "expected answer": [
                    "becoming"
                ],
                "predictions": [
                    {
                        "score": 0.310605225942532,
                        "answer": "description",
                        "hit": false
                    },
                    {
                        "score": 0.30853712029410213,
                        "answer": "depressing",
                        "hit": false
                    },
                    {
                        "score": 0.3041055885807277,
                        "answer": "entice",
                        "hit": false
                    },
                    {
                        "score": 0.29823858770847994,
                        "answer": "nextone",
                        "hit": false
                    },
                    {
                        "score": 0.2930111324755282,
                        "answer": "hospitalization",
                        "hit": false
                    },
                    {
                        "score": 0.289657658820541,
                        "answer": "phenonema",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "become"
                ],
                "rank": 13012,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5684892237186432
            },
            {
                "question verbose": "What is to believe ",
                "b": "believe",
                "expected answer": [
                    "believing"
                ],
                "predictions": [
                    {
                        "score": 0.3355336604997474,
                        "answer": "calling",
                        "hit": false
                    },
                    {
                        "score": 0.32961692851642505,
                        "answer": "deserves",
                        "hit": false
                    },
                    {
                        "score": 0.31700514708581345,
                        "answer": "adequately",
                        "hit": false
                    },
                    {
                        "score": 0.3147404948444674,
                        "answer": "drown",
                        "hit": false
                    },
                    {
                        "score": 0.3134835240589875,
                        "answer": "screamo",
                        "hit": false
                    },
                    {
                        "score": 0.31208272728699277,
                        "answer": "importance",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believe"
                ],
                "rank": 5864,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6722899675369263
            },
            {
                "question verbose": "What is to consider ",
                "b": "consider",
                "expected answer": [
                    "considering"
                ],
                "predictions": [
                    {
                        "score": 0.3841236092872351,
                        "answer": "freeloading",
                        "hit": false
                    },
                    {
                        "score": 0.3738769931206088,
                        "answer": "helicopter",
                        "hit": false
                    },
                    {
                        "score": 0.36436656354961255,
                        "answer": "uplifting",
                        "hit": false
                    },
                    {
                        "score": 0.353861832319124,
                        "answer": "storefront",
                        "hit": false
                    },
                    {
                        "score": 0.34871516937027663,
                        "answer": "betrayed",
                        "hit": false
                    },
                    {
                        "score": 0.34630072089975583,
                        "answer": "marital",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consider"
                ],
                "rank": 11061,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6313277631998062
            },
            {
                "question verbose": "What is to contain ",
                "b": "contain",
                "expected answer": [
                    "containing"
                ],
                "predictions": [
                    {
                        "score": 0.536773303213505,
                        "answer": "blacksmith",
                        "hit": false
                    },
                    {
                        "score": 0.5136410675039258,
                        "answer": "province",
                        "hit": false
                    },
                    {
                        "score": 0.49407468772871577,
                        "answer": "adler",
                        "hit": false
                    },
                    {
                        "score": 0.49053136639735617,
                        "answer": "delaware",
                        "hit": false
                    },
                    {
                        "score": 0.4902890933044939,
                        "answer": "conventional",
                        "hit": false
                    },
                    {
                        "score": 0.4900575675526982,
                        "answer": "merges",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "contain"
                ],
                "rank": 305,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7947250306606293
            },
            {
                "question verbose": "What is to continue ",
                "b": "continue",
                "expected answer": [
                    "continuing"
                ],
                "predictions": [
                    {
                        "score": 0.33573361129144896,
                        "answer": "swaziland",
                        "hit": false
                    },
                    {
                        "score": 0.33208567618185986,
                        "answer": "affecting",
                        "hit": false
                    },
                    {
                        "score": 0.33142672714918137,
                        "answer": "enrollment",
                        "hit": false
                    },
                    {
                        "score": 0.3195840103657801,
                        "answer": "closest",
                        "hit": false
                    },
                    {
                        "score": 0.31902554069110545,
                        "answer": "reveler",
                        "hit": false
                    },
                    {
                        "score": 0.3189873470141714,
                        "answer": "gulf",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continue"
                ],
                "rank": 11847,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5948560833930969
            },
            {
                "question verbose": "What is to create ",
                "b": "create",
                "expected answer": [
                    "creating"
                ],
                "predictions": [
                    {
                        "score": 0.39272505553256065,
                        "answer": "sustaining",
                        "hit": false
                    },
                    {
                        "score": 0.3617945666008751,
                        "answer": "uniformity",
                        "hit": false
                    },
                    {
                        "score": 0.361469327134749,
                        "answer": "restrained",
                        "hit": false
                    },
                    {
                        "score": 0.36144737098725604,
                        "answer": "precursor",
                        "hit": false
                    },
                    {
                        "score": 0.3598331444742119,
                        "answer": "offspring",
                        "hit": false
                    },
                    {
                        "score": 0.3535606729521677,
                        "answer": "viewing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "create"
                ],
                "rank": 6040,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6659075021743774
            },
            {
                "question verbose": "What is to develop ",
                "b": "develop",
                "expected answer": [
                    "developing"
                ],
                "predictions": [
                    {
                        "score": 0.43535386137680626,
                        "answer": "happanning",
                        "hit": false
                    },
                    {
                        "score": 0.41142586212639254,
                        "answer": "convict",
                        "hit": false
                    },
                    {
                        "score": 0.4112575687469533,
                        "answer": "facesthat",
                        "hit": false
                    },
                    {
                        "score": 0.4105763787811042,
                        "answer": "harvin",
                        "hit": false
                    },
                    {
                        "score": 0.40158770973527547,
                        "answer": "bodes",
                        "hit": false
                    },
                    {
                        "score": 0.3990461283950387,
                        "answer": "grandpa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develop"
                ],
                "rank": 9360,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6976470500230789
            },
            {
                "question verbose": "What is to encourage ",
                "b": "encourage",
                "expected answer": [
                    "encouraging"
                ],
                "predictions": [
                    {
                        "score": 0.46414917483273116,
                        "answer": "unfettered",
                        "hit": false
                    },
                    {
                        "score": 0.4616895289457817,
                        "answer": "uh",
                        "hit": false
                    },
                    {
                        "score": 0.46148718158766666,
                        "answer": "rationalize",
                        "hit": false
                    },
                    {
                        "score": 0.45683533154045297,
                        "answer": "opnion",
                        "hit": false
                    },
                    {
                        "score": 0.45303111389811757,
                        "answer": "charter",
                        "hit": false
                    },
                    {
                        "score": 0.446246518769234,
                        "answer": "unforgivable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "encourage"
                ],
                "rank": 2273,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6895357668399811
            },
            {
                "question verbose": "What is to enjoy ",
                "b": "enjoy",
                "expected answer": [
                    "enjoying"
                ],
                "predictions": [
                    {
                        "score": 0.4052740377344366,
                        "answer": "companion",
                        "hit": false
                    },
                    {
                        "score": 0.3797325837750295,
                        "answer": "visitor",
                        "hit": false
                    },
                    {
                        "score": 0.3779657262283381,
                        "answer": "mansfield",
                        "hit": false
                    },
                    {
                        "score": 0.3767523128357397,
                        "answer": "sounder",
                        "hit": false
                    },
                    {
                        "score": 0.3756651179394301,
                        "answer": "mariel",
                        "hit": false
                    },
                    {
                        "score": 0.369015129255832,
                        "answer": "affleck",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enjoy"
                ],
                "rank": 21,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7472331672906876
            },
            {
                "question verbose": "What is to ensure ",
                "b": "ensure",
                "expected answer": [
                    "ensuring"
                ],
                "predictions": [
                    {
                        "score": 0.5107090212729936,
                        "answer": "excellence",
                        "hit": false
                    },
                    {
                        "score": 0.5101051646742354,
                        "answer": "cocaine",
                        "hit": false
                    },
                    {
                        "score": 0.4885781463228012,
                        "answer": "containing",
                        "hit": false
                    },
                    {
                        "score": 0.48578213729637026,
                        "answer": "sensitive",
                        "hit": false
                    },
                    {
                        "score": 0.476191403388583,
                        "answer": "faisal",
                        "hit": false
                    },
                    {
                        "score": 0.47258403017848954,
                        "answer": "modelling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ensure"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5611089877784252
            },
            {
                "question verbose": "What is to establish ",
                "b": "establish",
                "expected answer": [
                    "establishing"
                ],
                "predictions": [
                    {
                        "score": 0.5262245454512666,
                        "answer": "blacksmith",
                        "hit": false
                    },
                    {
                        "score": 0.5238454488646762,
                        "answer": "cilip",
                        "hit": false
                    },
                    {
                        "score": 0.5141112654691329,
                        "answer": "seawater",
                        "hit": false
                    },
                    {
                        "score": 0.5122528887520794,
                        "answer": "preliminary",
                        "hit": false
                    },
                    {
                        "score": 0.5120007690591772,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.5076404903067859,
                        "answer": "bankhead",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "establish"
                ],
                "rank": 771,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7721138596534729
            },
            {
                "question verbose": "What is to exist ",
                "b": "exist",
                "expected answer": [
                    "existing"
                ],
                "predictions": [
                    {
                        "score": 0.43591867693772035,
                        "answer": "worldwide",
                        "hit": false
                    },
                    {
                        "score": 0.43419621715858053,
                        "answer": "pasture",
                        "hit": false
                    },
                    {
                        "score": 0.43023809504555216,
                        "answer": "dmn",
                        "hit": false
                    },
                    {
                        "score": 0.427734641475165,
                        "answer": "wi",
                        "hit": false
                    },
                    {
                        "score": 0.426509407470137,
                        "answer": "ignorance",
                        "hit": false
                    },
                    {
                        "score": 0.4226931153331657,
                        "answer": "yyou",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "exist"
                ],
                "rank": 13788,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6424540728330612
            },
            {
                "question verbose": "What is to expect ",
                "b": "expect",
                "expected answer": [
                    "expecting"
                ],
                "predictions": [
                    {
                        "score": 0.3815161682191997,
                        "answer": "zinger",
                        "hit": false
                    },
                    {
                        "score": 0.3677381667175125,
                        "answer": "insecurity",
                        "hit": false
                    },
                    {
                        "score": 0.36112342989814616,
                        "answer": "ramble",
                        "hit": false
                    },
                    {
                        "score": 0.3608926207104752,
                        "answer": "genious",
                        "hit": false
                    },
                    {
                        "score": 0.3499540401996586,
                        "answer": "moneywell",
                        "hit": false
                    },
                    {
                        "score": 0.3484909788200808,
                        "answer": "east",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expect"
                ],
                "rank": 153,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6938804537057877
            },
            {
                "question verbose": "What is to follow ",
                "b": "follow",
                "expected answer": [
                    "following"
                ],
                "predictions": [
                    {
                        "score": 0.33895466921002654,
                        "answer": "cousin",
                        "hit": false
                    },
                    {
                        "score": 0.32421588870690365,
                        "answer": "pitting",
                        "hit": false
                    },
                    {
                        "score": 0.32086869570818394,
                        "answer": "chat",
                        "hit": false
                    },
                    {
                        "score": 0.3190256703471908,
                        "answer": "utilizing",
                        "hit": false
                    },
                    {
                        "score": 0.31333308079497135,
                        "answer": "movieline",
                        "hit": false
                    },
                    {
                        "score": 0.311229722126497,
                        "answer": "psychcentral",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "follow"
                ],
                "rank": 9024,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6607147455215454
            },
            {
                "question verbose": "What is to happen ",
                "b": "happen",
                "expected answer": [
                    "happening"
                ],
                "predictions": [
                    {
                        "score": 0.378499599751898,
                        "answer": "wonderful",
                        "hit": false
                    },
                    {
                        "score": 0.37384496265498335,
                        "answer": "drown",
                        "hit": false
                    },
                    {
                        "score": 0.37055760690895106,
                        "answer": "viewed",
                        "hit": false
                    },
                    {
                        "score": 0.36609376489608714,
                        "answer": "feminist",
                        "hit": false
                    },
                    {
                        "score": 0.36482940840222555,
                        "answer": "willful",
                        "hit": false
                    },
                    {
                        "score": 0.3637630381732548,
                        "answer": "bully",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happen"
                ],
                "rank": 14435,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6728410124778748
            },
            {
                "question verbose": "What is to identify ",
                "b": "identify",
                "expected answer": [
                    "identifying"
                ],
                "predictions": [
                    {
                        "score": 0.4469990475726557,
                        "answer": "slung",
                        "hit": false
                    },
                    {
                        "score": 0.44657515720201657,
                        "answer": "fighting",
                        "hit": false
                    },
                    {
                        "score": 0.4396220959797691,
                        "answer": "bonus",
                        "hit": false
                    },
                    {
                        "score": 0.4357228489016356,
                        "answer": "elaborately",
                        "hit": false
                    },
                    {
                        "score": 0.425711920587127,
                        "answer": "orphan",
                        "hit": false
                    },
                    {
                        "score": 0.4229872193737702,
                        "answer": "facesthat",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "identify"
                ],
                "rank": 8103,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6505044996738434
            },
            {
                "question verbose": "What is to improve ",
                "b": "improve",
                "expected answer": [
                    "improving"
                ],
                "predictions": [
                    {
                        "score": 0.4180372914283369,
                        "answer": "vapory",
                        "hit": false
                    },
                    {
                        "score": 0.40301193298003557,
                        "answer": "photograph",
                        "hit": false
                    },
                    {
                        "score": 0.39769422339439264,
                        "answer": "homecoming",
                        "hit": false
                    },
                    {
                        "score": 0.39759839189493995,
                        "answer": "periodic",
                        "hit": false
                    },
                    {
                        "score": 0.3929563393795737,
                        "answer": "excellence",
                        "hit": false
                    },
                    {
                        "score": 0.39091356910838665,
                        "answer": "breeder",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improve"
                ],
                "rank": 12618,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7405537217855453
            },
            {
                "question verbose": "What is to include ",
                "b": "include",
                "expected answer": [
                    "including"
                ],
                "predictions": [
                    {
                        "score": 0.4583702246308531,
                        "answer": "gasbuddy",
                        "hit": false
                    },
                    {
                        "score": 0.4020367758704887,
                        "answer": "nativity",
                        "hit": false
                    },
                    {
                        "score": 0.3979418177873248,
                        "answer": "cousin",
                        "hit": false
                    },
                    {
                        "score": 0.39357685677197285,
                        "answer": "rubble",
                        "hit": false
                    },
                    {
                        "score": 0.3908061977364795,
                        "answer": "rent",
                        "hit": false
                    },
                    {
                        "score": 0.3875985361408069,
                        "answer": "charles",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "include"
                ],
                "rank": 8651,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5936958640813828
            },
            {
                "question verbose": "What is to involve ",
                "b": "involve",
                "expected answer": [
                    "involving"
                ],
                "predictions": [
                    {
                        "score": 0.592088471015973,
                        "answer": "reproducing",
                        "hit": false
                    },
                    {
                        "score": 0.5829794934419485,
                        "answer": "drilling",
                        "hit": false
                    },
                    {
                        "score": 0.5647384514686768,
                        "answer": "nascrc",
                        "hit": false
                    },
                    {
                        "score": 0.5577261409004748,
                        "answer": "artificial",
                        "hit": false
                    },
                    {
                        "score": 0.5519108614312922,
                        "answer": "yearly",
                        "hit": false
                    },
                    {
                        "score": 0.5492670127543079,
                        "answer": "disposed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involve"
                ],
                "rank": 5700,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8042236566543579
            },
            {
                "question verbose": "What is to learn ",
                "b": "learn",
                "expected answer": [
                    "learning"
                ],
                "predictions": [
                    {
                        "score": 0.39147128603437126,
                        "answer": "curriculum",
                        "hit": false
                    },
                    {
                        "score": 0.38522007864345537,
                        "answer": "helicopter",
                        "hit": false
                    },
                    {
                        "score": 0.36391694762538046,
                        "answer": "infanticide",
                        "hit": false
                    },
                    {
                        "score": 0.35922127767761103,
                        "answer": "importance",
                        "hit": false
                    },
                    {
                        "score": 0.35481473937883906,
                        "answer": "innovative",
                        "hit": false
                    },
                    {
                        "score": 0.35053321831207446,
                        "answer": "dalai",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "learn"
                ],
                "rank": 866,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6961189806461334
            },
            {
                "question verbose": "What is to lose ",
                "b": "lose",
                "expected answer": [
                    "losing"
                ],
                "predictions": [
                    {
                        "score": 0.43073444551972107,
                        "answer": "combined",
                        "hit": false
                    },
                    {
                        "score": 0.400155341852779,
                        "answer": "zinger",
                        "hit": false
                    },
                    {
                        "score": 0.3912508994733065,
                        "answer": "dissect",
                        "hit": false
                    },
                    {
                        "score": 0.3857773287251097,
                        "answer": "milions",
                        "hit": false
                    },
                    {
                        "score": 0.3817209453057627,
                        "answer": "recreate",
                        "hit": false
                    },
                    {
                        "score": 0.38008673001862125,
                        "answer": "slurvy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lose"
                ],
                "rank": 7440,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7162255793809891
            },
            {
                "question verbose": "What is to maintain ",
                "b": "maintain",
                "expected answer": [
                    "maintaining"
                ],
                "predictions": [
                    {
                        "score": 0.45520016045399,
                        "answer": "reproducing",
                        "hit": false
                    },
                    {
                        "score": 0.45368842075846405,
                        "answer": "cripple",
                        "hit": false
                    },
                    {
                        "score": 0.44632347570802555,
                        "answer": "geneva",
                        "hit": false
                    },
                    {
                        "score": 0.44421585807989417,
                        "answer": "carted",
                        "hit": false
                    },
                    {
                        "score": 0.44101169261378304,
                        "answer": "elaborately",
                        "hit": false
                    },
                    {
                        "score": 0.43962848978974545,
                        "answer": "novemember",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "maintain"
                ],
                "rank": 12615,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6307853162288666
            },
            {
                "question verbose": "What is to manage ",
                "b": "manage",
                "expected answer": [
                    "managing"
                ],
                "predictions": [
                    {
                        "score": 0.5387982030139533,
                        "answer": "wax",
                        "hit": false
                    },
                    {
                        "score": 0.5347505535376349,
                        "answer": "customisable",
                        "hit": false
                    },
                    {
                        "score": 0.5227571924521578,
                        "answer": "workout",
                        "hit": false
                    },
                    {
                        "score": 0.5134329220744898,
                        "answer": "checking",
                        "hit": false
                    },
                    {
                        "score": 0.5004433683398142,
                        "answer": "kissy",
                        "hit": false
                    },
                    {
                        "score": 0.4957307740582877,
                        "answer": "notebook",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manage"
                ],
                "rank": 12360,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6899134367704391
            },
            {
                "question verbose": "What is to operate ",
                "b": "operate",
                "expected answer": [
                    "operating"
                ],
                "predictions": [
                    {
                        "score": 0.541737107733718,
                        "answer": "drilling",
                        "hit": false
                    },
                    {
                        "score": 0.5068189701933253,
                        "answer": "facesthat",
                        "hit": false
                    },
                    {
                        "score": 0.5062878427853617,
                        "answer": "uproar",
                        "hit": false
                    },
                    {
                        "score": 0.5046915367830074,
                        "answer": "modelling",
                        "hit": false
                    },
                    {
                        "score": 0.49926110101125726,
                        "answer": "schoolyour",
                        "hit": false
                    },
                    {
                        "score": 0.497029019183544,
                        "answer": "storefront",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "operate"
                ],
                "rank": 4964,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.79547980427742
            },
            {
                "question verbose": "What is to perform ",
                "b": "perform",
                "expected answer": [
                    "performing"
                ],
                "predictions": [
                    {
                        "score": 0.49255707317107766,
                        "answer": "relieved",
                        "hit": false
                    },
                    {
                        "score": 0.4764206087583884,
                        "answer": "mariel",
                        "hit": false
                    },
                    {
                        "score": 0.47095501240688903,
                        "answer": "alcoholic",
                        "hit": false
                    },
                    {
                        "score": 0.4679565415364734,
                        "answer": "doctoring",
                        "hit": false
                    },
                    {
                        "score": 0.46531910200725296,
                        "answer": "wonderfully",
                        "hit": false
                    },
                    {
                        "score": 0.46068861529851224,
                        "answer": "curved",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "perform"
                ],
                "rank": 203,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7608058750629425
            },
            {
                "question verbose": "What is to prevent ",
                "b": "prevent",
                "expected answer": [
                    "preventing"
                ],
                "predictions": [
                    {
                        "score": 0.4463256350881514,
                        "answer": "seawater",
                        "hit": false
                    },
                    {
                        "score": 0.44034929348826035,
                        "answer": "condictioning",
                        "hit": false
                    },
                    {
                        "score": 0.43821516123326837,
                        "answer": "stable",
                        "hit": false
                    },
                    {
                        "score": 0.4300771506196278,
                        "answer": "swaziland",
                        "hit": false
                    },
                    {
                        "score": 0.4234337332980395,
                        "answer": "novemember",
                        "hit": false
                    },
                    {
                        "score": 0.42283704470188826,
                        "answer": "marijuana",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "prevent"
                ],
                "rank": 161,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7616011798381805
            },
            {
                "question verbose": "What is to promote ",
                "b": "promote",
                "expected answer": [
                    "promoting"
                ],
                "predictions": [
                    {
                        "score": 0.4276181600403459,
                        "answer": "neatly",
                        "hit": false
                    },
                    {
                        "score": 0.38529050990855523,
                        "answer": "organize",
                        "hit": false
                    },
                    {
                        "score": 0.37611958207032054,
                        "answer": "lingered",
                        "hit": false
                    },
                    {
                        "score": 0.3728714705036831,
                        "answer": "yourtangocom",
                        "hit": false
                    },
                    {
                        "score": 0.3702246329992162,
                        "answer": "kiwi",
                        "hit": false
                    },
                    {
                        "score": 0.36768420099919563,
                        "answer": "bailing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "promote"
                ],
                "rank": 6908,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7673472166061401
            },
            {
                "question verbose": "What is to protect ",
                "b": "protect",
                "expected answer": [
                    "protecting"
                ],
                "predictions": [
                    {
                        "score": 0.43075824219160547,
                        "answer": "methodology",
                        "hit": false
                    },
                    {
                        "score": 0.4176521404977521,
                        "answer": "implicit",
                        "hit": false
                    },
                    {
                        "score": 0.41612440668333456,
                        "answer": "digestive",
                        "hit": false
                    },
                    {
                        "score": 0.41181458482294464,
                        "answer": "classroom",
                        "hit": false
                    },
                    {
                        "score": 0.4116775720238316,
                        "answer": "buffer",
                        "hit": false
                    },
                    {
                        "score": 0.41118101124113576,
                        "answer": "pitting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "protect"
                ],
                "rank": 12568,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6229149997234344
            },
            {
                "question verbose": "What is to provide ",
                "b": "provide",
                "expected answer": [
                    "providing"
                ],
                "predictions": [
                    {
                        "score": 0.3908652622284108,
                        "answer": "practicum",
                        "hit": false
                    },
                    {
                        "score": 0.3797493035028509,
                        "answer": "precursor",
                        "hit": false
                    },
                    {
                        "score": 0.3783423703628936,
                        "answer": "achieving",
                        "hit": false
                    },
                    {
                        "score": 0.3770809961176095,
                        "answer": "searching",
                        "hit": false
                    },
                    {
                        "score": 0.37705911131099984,
                        "answer": "opengl",
                        "hit": false
                    },
                    {
                        "score": 0.3767892193986722,
                        "answer": "pitting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "provide"
                ],
                "rank": 6405,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7134419232606888
            },
            {
                "question verbose": "What is to receive ",
                "b": "receive",
                "expected answer": [
                    "receiving"
                ],
                "predictions": [
                    {
                        "score": 0.40317977801863847,
                        "answer": "sexually",
                        "hit": false
                    },
                    {
                        "score": 0.39290720912054916,
                        "answer": "discomfort",
                        "hit": false
                    },
                    {
                        "score": 0.39225866704618345,
                        "answer": "immoral",
                        "hit": false
                    },
                    {
                        "score": 0.3899653862903412,
                        "answer": "dismiss",
                        "hit": false
                    },
                    {
                        "score": 0.3897758086392857,
                        "answer": "yearly",
                        "hit": false
                    },
                    {
                        "score": 0.38756132665405274,
                        "answer": "mexican",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receive"
                ],
                "rank": 6363,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7349818199872971
            },
            {
                "question verbose": "What is to reduce ",
                "b": "reduce",
                "expected answer": [
                    "reducing"
                ],
                "predictions": [
                    {
                        "score": 0.4364853262190134,
                        "answer": "hepatoxicity",
                        "hit": false
                    },
                    {
                        "score": 0.39550908019168923,
                        "answer": "grantor",
                        "hit": false
                    },
                    {
                        "score": 0.39453726633836855,
                        "answer": "mantra",
                        "hit": false
                    },
                    {
                        "score": 0.392170765836737,
                        "answer": "digestive",
                        "hit": false
                    },
                    {
                        "score": 0.38914035773820177,
                        "answer": "implicit",
                        "hit": false
                    },
                    {
                        "score": 0.38733033325798844,
                        "answer": "balanced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reduce"
                ],
                "rank": 748,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7371290177106857
            },
            {
                "question verbose": "What is to refer ",
                "b": "refer",
                "expected answer": [
                    "referring"
                ],
                "predictions": [
                    {
                        "score": 0.49651692859062724,
                        "answer": "unprofessional",
                        "hit": false
                    },
                    {
                        "score": 0.4812095852729972,
                        "answer": "uncovered",
                        "hit": false
                    },
                    {
                        "score": 0.42667194259722646,
                        "answer": "personnel",
                        "hit": false
                    },
                    {
                        "score": 0.4262707778340919,
                        "answer": "overentitled",
                        "hit": false
                    },
                    {
                        "score": 0.4218172354031664,
                        "answer": "inflates",
                        "hit": false
                    },
                    {
                        "score": 0.4194678153004355,
                        "answer": "artificial",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "refer"
                ],
                "rank": 3789,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7125372290611267
            },
            {
                "question verbose": "What is to remain ",
                "b": "remain",
                "expected answer": [
                    "remaining"
                ],
                "predictions": [
                    {
                        "score": 0.5125077417682555,
                        "answer": "swaziland",
                        "hit": false
                    },
                    {
                        "score": 0.4798396849255035,
                        "answer": "ward",
                        "hit": false
                    },
                    {
                        "score": 0.4697710835417041,
                        "answer": "roe",
                        "hit": false
                    },
                    {
                        "score": 0.46181016191140245,
                        "answer": "publicize",
                        "hit": false
                    },
                    {
                        "score": 0.4608536396318084,
                        "answer": "exported",
                        "hit": false
                    },
                    {
                        "score": 0.45570287804875215,
                        "answer": "adler",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remain"
                ],
                "rank": 14613,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5662587508559227
            },
            {
                "question verbose": "What is to remember ",
                "b": "remember",
                "expected answer": [
                    "remembering"
                ],
                "predictions": [
                    {
                        "score": 0.4493338522445072,
                        "answer": "wonderful",
                        "hit": false
                    },
                    {
                        "score": 0.4191839524161369,
                        "answer": "prevalent",
                        "hit": false
                    },
                    {
                        "score": 0.41425028115196594,
                        "answer": "cancel",
                        "hit": false
                    },
                    {
                        "score": 0.4035284620110793,
                        "answer": "jukebox",
                        "hit": false
                    },
                    {
                        "score": 0.3972415246281047,
                        "answer": "ago",
                        "hit": false
                    },
                    {
                        "score": 0.3880165214391856,
                        "answer": "stationary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remember"
                ],
                "rank": 1368,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7310778498649597
            },
            {
                "question verbose": "What is to represent ",
                "b": "represent",
                "expected answer": [
                    "representing"
                ],
                "predictions": [
                    {
                        "score": 0.5244650586176751,
                        "answer": "burundi",
                        "hit": false
                    },
                    {
                        "score": 0.5175181595897497,
                        "answer": "swaziland",
                        "hit": false
                    },
                    {
                        "score": 0.5019867727140933,
                        "answer": "watershed",
                        "hit": false
                    },
                    {
                        "score": 0.5006259206004932,
                        "answer": "sh",
                        "hit": false
                    },
                    {
                        "score": 0.4971565795075593,
                        "answer": "faisal",
                        "hit": false
                    },
                    {
                        "score": 0.49426844901771366,
                        "answer": "neglected",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "represent"
                ],
                "rank": 7284,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7017470151185989
            },
            {
                "question verbose": "What is to require ",
                "b": "require",
                "expected answer": [
                    "requiring"
                ],
                "predictions": [
                    {
                        "score": 0.3935123136897813,
                        "answer": "implicit",
                        "hit": false
                    },
                    {
                        "score": 0.38340378164922195,
                        "answer": "buyable",
                        "hit": false
                    },
                    {
                        "score": 0.38178084917094324,
                        "answer": "disposed",
                        "hit": false
                    },
                    {
                        "score": 0.38064886374024914,
                        "answer": "poorer",
                        "hit": false
                    },
                    {
                        "score": 0.37811078982487095,
                        "answer": "condictioning",
                        "hit": false
                    },
                    {
                        "score": 0.3738782190237741,
                        "answer": "market",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "require"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.645960196852684
            },
            {
                "question verbose": "What is to seem ",
                "b": "seem",
                "expected answer": [
                    "seeming"
                ],
                "predictions": [
                    {
                        "score": 0.388557827168917,
                        "answer": "fisted",
                        "hit": false
                    },
                    {
                        "score": 0.3688066262685589,
                        "answer": "stationary",
                        "hit": false
                    },
                    {
                        "score": 0.36853294763730693,
                        "answer": "intellect",
                        "hit": false
                    },
                    {
                        "score": 0.36794081813899127,
                        "answer": "bedford",
                        "hit": false
                    },
                    {
                        "score": 0.3639753479073655,
                        "answer": "artificial",
                        "hit": false
                    },
                    {
                        "score": 0.35934831319895927,
                        "answer": "deluded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seem"
                ],
                "rank": 10613,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6875952184200287
            },
            {
                "question verbose": "What is to sit ",
                "b": "sit",
                "expected answer": [
                    "sitting"
                ],
                "predictions": [
                    {
                        "score": 0.640029609444733,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.332302443104639,
                        "answer": "politicial",
                        "hit": false
                    },
                    {
                        "score": 0.3307292885866818,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.31923611081938247,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.314113506107165,
                        "answer": "roleplaying",
                        "hit": false
                    },
                    {
                        "score": 0.30942141516777977,
                        "answer": "psychiatrist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sit"
                ],
                "rank": 8952,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.616092137992382
            },
            {
                "question verbose": "What is to spend ",
                "b": "spend",
                "expected answer": [
                    "spending"
                ],
                "predictions": [
                    {
                        "score": 0.42048267359220964,
                        "answer": "rebutting",
                        "hit": false
                    },
                    {
                        "score": 0.38994676007563844,
                        "answer": "knocking",
                        "hit": false
                    },
                    {
                        "score": 0.38251362044395354,
                        "answer": "arrival",
                        "hit": false
                    },
                    {
                        "score": 0.3801381934088436,
                        "answer": "waited",
                        "hit": false
                    },
                    {
                        "score": 0.37811797587078216,
                        "answer": "wonderful",
                        "hit": false
                    },
                    {
                        "score": 0.3734370805112588,
                        "answer": "satiated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spend"
                ],
                "rank": 12943,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6509886384010315
            },
            {
                "question verbose": "What is to teach ",
                "b": "teach",
                "expected answer": [
                    "teaching"
                ],
                "predictions": [
                    {
                        "score": 0.4566928633816031,
                        "answer": "consist",
                        "hit": false
                    },
                    {
                        "score": 0.44692218332966627,
                        "answer": "pitting",
                        "hit": false
                    },
                    {
                        "score": 0.44413229669827553,
                        "answer": "startup",
                        "hit": false
                    },
                    {
                        "score": 0.44334935859653496,
                        "answer": "entering",
                        "hit": false
                    },
                    {
                        "score": 0.4413725121750883,
                        "answer": "incubator",
                        "hit": false
                    },
                    {
                        "score": 0.4323468366716092,
                        "answer": "practicum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "teach"
                ],
                "rank": 1602,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7118230164051056
            },
            {
                "question verbose": "What is to tell ",
                "b": "tell",
                "expected answer": [
                    "telling"
                ],
                "predictions": [
                    {
                        "score": 0.38864923197147233,
                        "answer": "demeanor",
                        "hit": false
                    },
                    {
                        "score": 0.3698683957467913,
                        "answer": "emailed",
                        "hit": false
                    },
                    {
                        "score": 0.36842817881142753,
                        "answer": "indian",
                        "hit": false
                    },
                    {
                        "score": 0.35761109695990095,
                        "answer": "adores",
                        "hit": false
                    },
                    {
                        "score": 0.34561838318498034,
                        "answer": "replied",
                        "hit": false
                    },
                    {
                        "score": 0.3411559843839886,
                        "answer": "brisk",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tell"
                ],
                "rank": 1485,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7104090750217438
            },
            {
                "question verbose": "What is to understand ",
                "b": "understand",
                "expected answer": [
                    "understanding"
                ],
                "predictions": [
                    {
                        "score": 0.3549476480474471,
                        "answer": "fiercer",
                        "hit": false
                    },
                    {
                        "score": 0.3519907397487718,
                        "answer": "escape",
                        "hit": false
                    },
                    {
                        "score": 0.35156153582682925,
                        "answer": "opnion",
                        "hit": false
                    },
                    {
                        "score": 0.34818664365564883,
                        "answer": "polarizing",
                        "hit": false
                    },
                    {
                        "score": 0.34325872973118293,
                        "answer": "uh",
                        "hit": false
                    },
                    {
                        "score": 0.34287808112802587,
                        "answer": "equally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "understand"
                ],
                "rank": 9244,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6236052066087723
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I06 [verb_inf - Ving].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "bbfdf144-4ef6-4c16-9449-fa784d83bd84",
            "timestamp": "2020-10-22T15:57:39.227046"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to accept ",
                "b": "accept",
                "expected answer": [
                    "accepted"
                ],
                "predictions": [
                    {
                        "score": 0.39307750292980204,
                        "answer": "legal",
                        "hit": false
                    },
                    {
                        "score": 0.3768914984356646,
                        "answer": "journalism",
                        "hit": false
                    },
                    {
                        "score": 0.3622509540956635,
                        "answer": "unattended",
                        "hit": false
                    },
                    {
                        "score": 0.3577967400839029,
                        "answer": "hall",
                        "hit": false
                    },
                    {
                        "score": 0.3529005918471218,
                        "answer": "ncaa",
                        "hit": false
                    },
                    {
                        "score": 0.3465539276317524,
                        "answer": "neutral",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "accept"
                ],
                "rank": 8499,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6444360911846161
            },
            {
                "question verbose": "What is to achieve ",
                "b": "achieve",
                "expected answer": [
                    "achieved"
                ],
                "predictions": [
                    {
                        "score": 0.47173638027399467,
                        "answer": "august",
                        "hit": false
                    },
                    {
                        "score": 0.46930035857820024,
                        "answer": "egypt",
                        "hit": false
                    },
                    {
                        "score": 0.4673096721353645,
                        "answer": "barbados",
                        "hit": false
                    },
                    {
                        "score": 0.4627448729378169,
                        "answer": "scholarship",
                        "hit": false
                    },
                    {
                        "score": 0.4620202630422199,
                        "answer": "blandford",
                        "hit": false
                    },
                    {
                        "score": 0.4609884521008128,
                        "answer": "outcry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "achieve"
                ],
                "rank": 9593,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7199679762125015
            },
            {
                "question verbose": "What is to add ",
                "b": "add",
                "expected answer": [
                    "added"
                ],
                "predictions": [
                    {
                        "score": 0.3806363870918268,
                        "answer": "prep",
                        "hit": false
                    },
                    {
                        "score": 0.3608412949977042,
                        "answer": "cartoon",
                        "hit": false
                    },
                    {
                        "score": 0.3503165612076278,
                        "answer": "formation",
                        "hit": false
                    },
                    {
                        "score": 0.33925376417195013,
                        "answer": "realism",
                        "hit": false
                    },
                    {
                        "score": 0.3381793298883768,
                        "answer": "alda",
                        "hit": false
                    },
                    {
                        "score": 0.32751519638608956,
                        "answer": "span",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "add"
                ],
                "rank": 12540,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6551859229803085
            },
            {
                "question verbose": "What is to agree ",
                "b": "agree",
                "expected answer": [
                    "agreed"
                ],
                "predictions": [
                    {
                        "score": 0.37040238807429454,
                        "answer": "hated",
                        "hit": false
                    },
                    {
                        "score": 0.35443502229501556,
                        "answer": "mindset",
                        "hit": false
                    },
                    {
                        "score": 0.35430222602913014,
                        "answer": "fanatically",
                        "hit": false
                    },
                    {
                        "score": 0.35160962404038854,
                        "answer": "cosmically",
                        "hit": false
                    },
                    {
                        "score": 0.34920073011904734,
                        "answer": "prior",
                        "hit": false
                    },
                    {
                        "score": 0.3490631764759421,
                        "answer": "listened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "agree"
                ],
                "rank": 11553,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5694456249475479
            },
            {
                "question verbose": "What is to allow ",
                "b": "allow",
                "expected answer": [
                    "allowed"
                ],
                "predictions": [
                    {
                        "score": 0.3244315795855247,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.32019252172060075,
                        "answer": "fame",
                        "hit": false
                    },
                    {
                        "score": 0.31260314789594806,
                        "answer": "reinventions",
                        "hit": false
                    },
                    {
                        "score": 0.31125708727644924,
                        "answer": "similar",
                        "hit": false
                    },
                    {
                        "score": 0.30178445789923813,
                        "answer": "waived",
                        "hit": false
                    },
                    {
                        "score": 0.30069342731979354,
                        "answer": "application",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allow"
                ],
                "rank": 577,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6804574579000473
            },
            {
                "question verbose": "What is to announce ",
                "b": "announce",
                "expected answer": [
                    "announced"
                ],
                "predictions": [
                    {
                        "score": 0.4360267683998078,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3174540556341103,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27909294775822424,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.27604213851617354,
                        "answer": "four",
                        "hit": false
                    },
                    {
                        "score": 0.27444295009074776,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.27346062075182626,
                        "answer": "parlor",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "announce"
                ],
                "rank": 5431,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5957832857966423
            },
            {
                "question verbose": "What is to appear ",
                "b": "appear",
                "expected answer": [
                    "appeared"
                ],
                "predictions": [
                    {
                        "score": 0.5096936651284816,
                        "answer": "jerusalem",
                        "hit": false
                    },
                    {
                        "score": 0.48165391804628127,
                        "answer": "registration",
                        "hit": false
                    },
                    {
                        "score": 0.47966865501158107,
                        "answer": "bp",
                        "hit": false
                    },
                    {
                        "score": 0.4767990244097953,
                        "answer": "ondria",
                        "hit": false
                    },
                    {
                        "score": 0.4759492534133311,
                        "answer": "caffeine",
                        "hit": false
                    },
                    {
                        "score": 0.4747077669163847,
                        "answer": "ytd",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appear"
                ],
                "rank": 5423,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.657911017537117
            },
            {
                "question verbose": "What is to apply ",
                "b": "apply",
                "expected answer": [
                    "applied"
                ],
                "predictions": [
                    {
                        "score": 0.36462258671588843,
                        "answer": "differing",
                        "hit": false
                    },
                    {
                        "score": 0.3646066816249592,
                        "answer": "robot",
                        "hit": false
                    },
                    {
                        "score": 0.3496515645091095,
                        "answer": "curator",
                        "hit": false
                    },
                    {
                        "score": 0.349330602992072,
                        "answer": "cafe",
                        "hit": false
                    },
                    {
                        "score": 0.34702820441665466,
                        "answer": "egyptian",
                        "hit": false
                    },
                    {
                        "score": 0.34695405613766184,
                        "answer": "bp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apply"
                ],
                "rank": 11497,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.643709123134613
            },
            {
                "question verbose": "What is to ask ",
                "b": "ask",
                "expected answer": [
                    "asked"
                ],
                "predictions": [
                    {
                        "score": 0.3152194872831459,
                        "answer": "answering",
                        "hit": false
                    },
                    {
                        "score": 0.28500138076152515,
                        "answer": "approval",
                        "hit": false
                    },
                    {
                        "score": 0.2849542838829774,
                        "answer": "suicide",
                        "hit": false
                    },
                    {
                        "score": 0.28323454134842113,
                        "answer": "dropped",
                        "hit": false
                    },
                    {
                        "score": 0.27715038381250173,
                        "answer": "furlong",
                        "hit": false
                    },
                    {
                        "score": 0.2737030935096768,
                        "answer": "liquor",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ask"
                ],
                "rank": 99,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7136455178260803
            },
            {
                "question verbose": "What is to attend ",
                "b": "attend",
                "expected answer": [
                    "attended"
                ],
                "predictions": [
                    {
                        "score": 0.473956727843137,
                        "answer": "scholaryale",
                        "hit": false
                    },
                    {
                        "score": 0.4531894614537284,
                        "answer": "thursday",
                        "hit": false
                    },
                    {
                        "score": 0.4530903552429853,
                        "answer": "unveiled",
                        "hit": false
                    },
                    {
                        "score": 0.44795974072449796,
                        "answer": "commissioner",
                        "hit": false
                    },
                    {
                        "score": 0.44643934689028103,
                        "answer": "premiere",
                        "hit": false
                    },
                    {
                        "score": 0.44371398874669665,
                        "answer": "issued",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "attend"
                ],
                "rank": 146,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7406507581472397
            },
            {
                "question verbose": "What is to become ",
                "b": "become",
                "expected answer": [
                    "became"
                ],
                "predictions": [
                    {
                        "score": 0.26961543119040976,
                        "answer": "suggests",
                        "hit": false
                    },
                    {
                        "score": 0.26686207843509235,
                        "answer": "struggling",
                        "hit": false
                    },
                    {
                        "score": 0.2657192115110647,
                        "answer": "kessler",
                        "hit": false
                    },
                    {
                        "score": 0.24967414944955624,
                        "answer": "connectivity",
                        "hit": false
                    },
                    {
                        "score": 0.24755684501476266,
                        "answer": "special",
                        "hit": false
                    },
                    {
                        "score": 0.24497389694795166,
                        "answer": "determined",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "become"
                ],
                "rank": 3754,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6039954274892807
            },
            {
                "question verbose": "What is to believe ",
                "b": "believe",
                "expected answer": [
                    "believed"
                ],
                "predictions": [
                    {
                        "score": 0.3080786787136868,
                        "answer": "calling",
                        "hit": false
                    },
                    {
                        "score": 0.27960889866342864,
                        "answer": "decided",
                        "hit": false
                    },
                    {
                        "score": 0.27474592387705854,
                        "answer": "gaffe",
                        "hit": false
                    },
                    {
                        "score": 0.27256871090709356,
                        "answer": "sam",
                        "hit": false
                    },
                    {
                        "score": 0.2676441955884984,
                        "answer": "brother",
                        "hit": false
                    },
                    {
                        "score": 0.2582173561090154,
                        "answer": "displayed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believe"
                ],
                "rank": 2311,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6529524028301239
            },
            {
                "question verbose": "What is to consider ",
                "b": "consider",
                "expected answer": [
                    "considered"
                ],
                "predictions": [
                    {
                        "score": 0.34285274483132283,
                        "answer": "gather",
                        "hit": false
                    },
                    {
                        "score": 0.3317886451508724,
                        "answer": "filming",
                        "hit": false
                    },
                    {
                        "score": 0.33064158872946176,
                        "answer": "contested",
                        "hit": false
                    },
                    {
                        "score": 0.325797409934305,
                        "answer": "polling",
                        "hit": false
                    },
                    {
                        "score": 0.3221689372404059,
                        "answer": "spinner",
                        "hit": false
                    },
                    {
                        "score": 0.3174073026227521,
                        "answer": "picking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consider"
                ],
                "rank": 12451,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5652385130524635
            },
            {
                "question verbose": "What is to continue ",
                "b": "continue",
                "expected answer": [
                    "continued"
                ],
                "predictions": [
                    {
                        "score": 0.3524000442007496,
                        "answer": "alongside",
                        "hit": false
                    },
                    {
                        "score": 0.3290588898224113,
                        "answer": "veterinarian",
                        "hit": false
                    },
                    {
                        "score": 0.32826567179807353,
                        "answer": "statistic",
                        "hit": false
                    },
                    {
                        "score": 0.32773175234225804,
                        "answer": "enrollment",
                        "hit": false
                    },
                    {
                        "score": 0.3221621468803289,
                        "answer": "dec",
                        "hit": false
                    },
                    {
                        "score": 0.31384779529876144,
                        "answer": "olympus",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continue"
                ],
                "rank": 10517,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5984244346618652
            },
            {
                "question verbose": "What is to create ",
                "b": "create",
                "expected answer": [
                    "created"
                ],
                "predictions": [
                    {
                        "score": 0.3671623043118018,
                        "answer": "businessman",
                        "hit": false
                    },
                    {
                        "score": 0.35126112906695667,
                        "answer": "engagement",
                        "hit": false
                    },
                    {
                        "score": 0.34231270981010187,
                        "answer": "flotilla",
                        "hit": false
                    },
                    {
                        "score": 0.30710385605152724,
                        "answer": "sending",
                        "hit": false
                    },
                    {
                        "score": 0.30019240406884967,
                        "answer": "dodd",
                        "hit": false
                    },
                    {
                        "score": 0.29857680156184413,
                        "answer": "ranged",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "create"
                ],
                "rank": 766,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6585180163383484
            },
            {
                "question verbose": "What is to decide ",
                "b": "decide",
                "expected answer": [
                    "decided"
                ],
                "predictions": [
                    {
                        "score": 0.44347180418674365,
                        "answer": "launched",
                        "hit": false
                    },
                    {
                        "score": 0.4374430392515645,
                        "answer": "increasingly",
                        "hit": false
                    },
                    {
                        "score": 0.4348466482398761,
                        "answer": "tagged",
                        "hit": false
                    },
                    {
                        "score": 0.433744678677527,
                        "answer": "circuit",
                        "hit": false
                    },
                    {
                        "score": 0.41885049271658775,
                        "answer": "favor",
                        "hit": false
                    },
                    {
                        "score": 0.4153488155598801,
                        "answer": "dropping",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "decide"
                ],
                "rank": 8320,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6157394424080849
            },
            {
                "question verbose": "What is to describe ",
                "b": "describe",
                "expected answer": [
                    "described"
                ],
                "predictions": [
                    {
                        "score": 0.375938505128398,
                        "answer": "control",
                        "hit": false
                    },
                    {
                        "score": 0.3740753315697251,
                        "answer": "presence",
                        "hit": false
                    },
                    {
                        "score": 0.3426315272836053,
                        "answer": "interconnected",
                        "hit": false
                    },
                    {
                        "score": 0.34063997068583524,
                        "answer": "cosmically",
                        "hit": false
                    },
                    {
                        "score": 0.3357324901336152,
                        "answer": "stallone",
                        "hit": false
                    },
                    {
                        "score": 0.3353204702321332,
                        "answer": "lynn",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "describe"
                ],
                "rank": 12975,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5976549386978149
            },
            {
                "question verbose": "What is to develop ",
                "b": "develop",
                "expected answer": [
                    "developed"
                ],
                "predictions": [
                    {
                        "score": 0.33003068532310326,
                        "answer": "slfsi",
                        "hit": false
                    },
                    {
                        "score": 0.32429735706100127,
                        "answer": "weighty",
                        "hit": false
                    },
                    {
                        "score": 0.3168886308022831,
                        "answer": "prolonged",
                        "hit": false
                    },
                    {
                        "score": 0.3168308287141279,
                        "answer": "tutored",
                        "hit": false
                    },
                    {
                        "score": 0.31622178257261835,
                        "answer": "conventional",
                        "hit": false
                    },
                    {
                        "score": 0.31618568409179687,
                        "answer": "mazut",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develop"
                ],
                "rank": 6089,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7008457779884338
            },
            {
                "question verbose": "What is to discover ",
                "b": "discover",
                "expected answer": [
                    "discovered"
                ],
                "predictions": [
                    {
                        "score": 0.5290322869715489,
                        "answer": "eighteen",
                        "hit": false
                    },
                    {
                        "score": 0.5117486010723946,
                        "answer": "midwest",
                        "hit": false
                    },
                    {
                        "score": 0.4971083794305039,
                        "answer": "unwillingness",
                        "hit": false
                    },
                    {
                        "score": 0.49575861410728594,
                        "answer": "arrogance",
                        "hit": false
                    },
                    {
                        "score": 0.4920375045121138,
                        "answer": "kerr",
                        "hit": false
                    },
                    {
                        "score": 0.49143052128092923,
                        "answer": "chased",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "discover"
                ],
                "rank": 6874,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7177061885595322
            },
            {
                "question verbose": "What is to enjoy ",
                "b": "enjoy",
                "expected answer": [
                    "enjoyed"
                ],
                "predictions": [
                    {
                        "score": 0.3689942429038759,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.3681885087596409,
                        "answer": "traveled",
                        "hit": false
                    },
                    {
                        "score": 0.36163310476909477,
                        "answer": "discontent",
                        "hit": false
                    },
                    {
                        "score": 0.3564547976082345,
                        "answer": "journalism",
                        "hit": false
                    },
                    {
                        "score": 0.3511344650785752,
                        "answer": "marrow",
                        "hit": false
                    },
                    {
                        "score": 0.3504792899755416,
                        "answer": "portland",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enjoy"
                ],
                "rank": 10535,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6762211471796036
            },
            {
                "question verbose": "What is to ensure ",
                "b": "ensure",
                "expected answer": [
                    "ensured"
                ],
                "predictions": [
                    {
                        "score": 0.48365507751192055,
                        "answer": "cocaine",
                        "hit": false
                    },
                    {
                        "score": 0.44514286541609893,
                        "answer": "faisal",
                        "hit": false
                    },
                    {
                        "score": 0.4355676480424384,
                        "answer": "dos",
                        "hit": false
                    },
                    {
                        "score": 0.4290483029693121,
                        "answer": "etiology",
                        "hit": false
                    },
                    {
                        "score": 0.41128262528884657,
                        "answer": "rifle",
                        "hit": false
                    },
                    {
                        "score": 0.40846426899258503,
                        "answer": "benzene",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ensure"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5611089877784252
            },
            {
                "question verbose": "What is to establish ",
                "b": "establish",
                "expected answer": [
                    "established"
                ],
                "predictions": [
                    {
                        "score": 0.5819689087982887,
                        "answer": "presbyterian",
                        "hit": false
                    },
                    {
                        "score": 0.5453352631395304,
                        "answer": "fayetteville",
                        "hit": false
                    },
                    {
                        "score": 0.5368133604939342,
                        "answer": "unveiled",
                        "hit": false
                    },
                    {
                        "score": 0.5282264186804287,
                        "answer": "everett",
                        "hit": false
                    },
                    {
                        "score": 0.5265235508598808,
                        "answer": "nascar",
                        "hit": false
                    },
                    {
                        "score": 0.5246097885717736,
                        "answer": "effectuate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "establish"
                ],
                "rank": 13598,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7429205179214478
            },
            {
                "question verbose": "What is to expect ",
                "b": "expect",
                "expected answer": [
                    "expected"
                ],
                "predictions": [
                    {
                        "score": 0.4057525379504587,
                        "answer": "droid",
                        "hit": false
                    },
                    {
                        "score": 0.4005216406021901,
                        "answer": "stature",
                        "hit": false
                    },
                    {
                        "score": 0.3863222871603739,
                        "answer": "east",
                        "hit": false
                    },
                    {
                        "score": 0.3754705376453138,
                        "answer": "joe",
                        "hit": false
                    },
                    {
                        "score": 0.3702633015851183,
                        "answer": "january",
                        "hit": false
                    },
                    {
                        "score": 0.3662119550039482,
                        "answer": "wtic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expect"
                ],
                "rank": 57,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6904089152812958
            },
            {
                "question verbose": "What is to follow ",
                "b": "follow",
                "expected answer": [
                    "followed"
                ],
                "predictions": [
                    {
                        "score": 0.3354456110882297,
                        "answer": "questioner",
                        "hit": false
                    },
                    {
                        "score": 0.3188447182739412,
                        "answer": "replete",
                        "hit": false
                    },
                    {
                        "score": 0.31460638006408886,
                        "answer": "axelrod",
                        "hit": false
                    },
                    {
                        "score": 0.3145102941637276,
                        "answer": "guide",
                        "hit": false
                    },
                    {
                        "score": 0.3072377337101686,
                        "answer": "prevention",
                        "hit": false
                    },
                    {
                        "score": 0.3048015037729301,
                        "answer": "nutrition",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "follow"
                ],
                "rank": 7897,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.595556914806366
            },
            {
                "question verbose": "What is to hear ",
                "b": "hear",
                "expected answer": [
                    "heard"
                ],
                "predictions": [
                    {
                        "score": 0.32516076737581895,
                        "answer": "period",
                        "hit": false
                    },
                    {
                        "score": 0.3147816610202348,
                        "answer": "laughable",
                        "hit": false
                    },
                    {
                        "score": 0.31471566204470725,
                        "answer": "differs",
                        "hit": false
                    },
                    {
                        "score": 0.30615023455282775,
                        "answer": "remembered",
                        "hit": false
                    },
                    {
                        "score": 0.2981034357344063,
                        "answer": "packard",
                        "hit": false
                    },
                    {
                        "score": 0.29043471716521285,
                        "answer": "rig",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hear"
                ],
                "rank": 1318,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6478893011808395
            },
            {
                "question verbose": "What is to identify ",
                "b": "identify",
                "expected answer": [
                    "identified"
                ],
                "predictions": [
                    {
                        "score": 0.47658379867174544,
                        "answer": "posed",
                        "hit": false
                    },
                    {
                        "score": 0.419364385522808,
                        "answer": "abandoned",
                        "hit": false
                    },
                    {
                        "score": 0.41780979805430885,
                        "answer": "surf",
                        "hit": false
                    },
                    {
                        "score": 0.41334915037880793,
                        "answer": "bethesda",
                        "hit": false
                    },
                    {
                        "score": 0.4087322119674069,
                        "answer": "desk",
                        "hit": false
                    },
                    {
                        "score": 0.40513456409443427,
                        "answer": "furlough",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "identify"
                ],
                "rank": 6381,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6352439969778061
            },
            {
                "question verbose": "What is to improve ",
                "b": "improve",
                "expected answer": [
                    "improved"
                ],
                "predictions": [
                    {
                        "score": 0.3701362134721345,
                        "answer": "continuing",
                        "hit": false
                    },
                    {
                        "score": 0.3565724847806687,
                        "answer": "required",
                        "hit": false
                    },
                    {
                        "score": 0.3487575698109572,
                        "answer": "exceeded",
                        "hit": false
                    },
                    {
                        "score": 0.3445141914853502,
                        "answer": "scholarship",
                        "hit": false
                    },
                    {
                        "score": 0.3431332210385273,
                        "answer": "periodic",
                        "hit": false
                    },
                    {
                        "score": 0.34008730181809643,
                        "answer": "rhode",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improve"
                ],
                "rank": 2001,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7480752766132355
            },
            {
                "question verbose": "What is to include ",
                "b": "include",
                "expected answer": [
                    "included"
                ],
                "predictions": [
                    {
                        "score": 0.46739357775302287,
                        "answer": "gasbuddy",
                        "hit": false
                    },
                    {
                        "score": 0.44138337464907373,
                        "answer": "upbeat",
                        "hit": false
                    },
                    {
                        "score": 0.42970099978276516,
                        "answer": "twelve",
                        "hit": false
                    },
                    {
                        "score": 0.4294976368701371,
                        "answer": "rent",
                        "hit": false
                    },
                    {
                        "score": 0.4215377593426432,
                        "answer": "davis",
                        "hit": false
                    },
                    {
                        "score": 0.4089449308717488,
                        "answer": "cristie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "include"
                ],
                "rank": 11025,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6668106019496918
            },
            {
                "question verbose": "What is to introduce ",
                "b": "introduce",
                "expected answer": [
                    "introduced"
                ],
                "predictions": [
                    {
                        "score": 0.5400189152591213,
                        "answer": "phonenix",
                        "hit": false
                    },
                    {
                        "score": 0.5373446334341421,
                        "answer": "shenzhen",
                        "hit": false
                    },
                    {
                        "score": 0.5372418832773911,
                        "answer": "dark",
                        "hit": false
                    },
                    {
                        "score": 0.5280628777848567,
                        "answer": "midwest",
                        "hit": false
                    },
                    {
                        "score": 0.5074938558947548,
                        "answer": "divided",
                        "hit": false
                    },
                    {
                        "score": 0.4982908321111447,
                        "answer": "carroll",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "introduce"
                ],
                "rank": 4491,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6782597750425339
            },
            {
                "question verbose": "What is to involve ",
                "b": "involve",
                "expected answer": [
                    "involved"
                ],
                "predictions": [
                    {
                        "score": 0.5476150979561164,
                        "answer": "guideline",
                        "hit": false
                    },
                    {
                        "score": 0.5234750378549986,
                        "answer": "nasrc",
                        "hit": false
                    },
                    {
                        "score": 0.49945019868451046,
                        "answer": "roe",
                        "hit": false
                    },
                    {
                        "score": 0.4983979807522464,
                        "answer": "thrift",
                        "hit": false
                    },
                    {
                        "score": 0.49763563046981707,
                        "answer": "gustafson",
                        "hit": false
                    },
                    {
                        "score": 0.49669330424192054,
                        "answer": "barbados",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involve"
                ],
                "rank": 8773,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7391476035118103
            },
            {
                "question verbose": "What is to locate ",
                "b": "locate",
                "expected answer": [
                    "located"
                ],
                "predictions": [
                    {
                        "score": 0.4405232687267664,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3136858426583575,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2875213934583955,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2852174988102744,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.2730613337120196,
                        "answer": "four",
                        "hit": false
                    },
                    {
                        "score": 0.27227913447333707,
                        "answer": "jerusalem",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locate"
                ],
                "rank": 10886,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5709576308727264
            },
            {
                "question verbose": "What is to lose ",
                "b": "lose",
                "expected answer": [
                    "lost"
                ],
                "predictions": [
                    {
                        "score": 0.3568449242818462,
                        "answer": "pound",
                        "hit": false
                    },
                    {
                        "score": 0.3561993124776366,
                        "answer": "unharmed",
                        "hit": false
                    },
                    {
                        "score": 0.3543152134207567,
                        "answer": "rally",
                        "hit": false
                    },
                    {
                        "score": 0.35287006124500975,
                        "answer": "lighted",
                        "hit": false
                    },
                    {
                        "score": 0.3499181163680149,
                        "answer": "notch",
                        "hit": false
                    },
                    {
                        "score": 0.34957637982808115,
                        "answer": "bethesda",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lose"
                ],
                "rank": 5742,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7046797126531601
            },
            {
                "question verbose": "What is to manage ",
                "b": "manage",
                "expected answer": [
                    "managed"
                ],
                "predictions": [
                    {
                        "score": 0.4645870485441939,
                        "answer": "refund",
                        "hit": false
                    },
                    {
                        "score": 0.45899059769884715,
                        "answer": "idling",
                        "hit": false
                    },
                    {
                        "score": 0.4498558722600473,
                        "answer": "commute",
                        "hit": false
                    },
                    {
                        "score": 0.4486535618901615,
                        "answer": "sticky",
                        "hit": false
                    },
                    {
                        "score": 0.4458515932498211,
                        "answer": "file",
                        "hit": false
                    },
                    {
                        "score": 0.4347190101214831,
                        "answer": "spy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manage"
                ],
                "rank": 10035,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6262456476688385
            },
            {
                "question verbose": "What is to marry ",
                "b": "marry",
                "expected answer": [
                    "married"
                ],
                "predictions": [
                    {
                        "score": 0.5692101186416317,
                        "answer": "wsj",
                        "hit": false
                    },
                    {
                        "score": 0.5347808663214972,
                        "answer": "idling",
                        "hit": false
                    },
                    {
                        "score": 0.5318768382419806,
                        "answer": "sgt",
                        "hit": false
                    },
                    {
                        "score": 0.5186403692611853,
                        "answer": "fell",
                        "hit": false
                    },
                    {
                        "score": 0.5177700811098073,
                        "answer": "liquor",
                        "hit": false
                    },
                    {
                        "score": 0.5151898399673994,
                        "answer": "scandal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marry"
                ],
                "rank": 96,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7945473492145538
            },
            {
                "question verbose": "What is to perform ",
                "b": "perform",
                "expected answer": [
                    "performed"
                ],
                "predictions": [
                    {
                        "score": 0.4431217692141383,
                        "answer": "eighteen",
                        "hit": false
                    },
                    {
                        "score": 0.4418370874644242,
                        "answer": "married",
                        "hit": false
                    },
                    {
                        "score": 0.42592311315933307,
                        "answer": "levy",
                        "hit": false
                    },
                    {
                        "score": 0.4249857492531428,
                        "answer": "easton",
                        "hit": false
                    },
                    {
                        "score": 0.42364190997069023,
                        "answer": "staging",
                        "hit": false
                    },
                    {
                        "score": 0.42061290116366123,
                        "answer": "loft",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "perform"
                ],
                "rank": 10015,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7470180094242096
            },
            {
                "question verbose": "What is to provide ",
                "b": "provide",
                "expected answer": [
                    "provided"
                ],
                "predictions": [
                    {
                        "score": 0.3901183731525586,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.37381117527506547,
                        "answer": "initially",
                        "hit": false
                    },
                    {
                        "score": 0.35987442832642497,
                        "answer": "basin",
                        "hit": false
                    },
                    {
                        "score": 0.3563936968334177,
                        "answer": "immersed",
                        "hit": false
                    },
                    {
                        "score": 0.35370320380397496,
                        "answer": "tanzania",
                        "hit": false
                    },
                    {
                        "score": 0.35055208987641157,
                        "answer": "practicum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "provide"
                ],
                "rank": 10176,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7074712067842484
            },
            {
                "question verbose": "What is to publish ",
                "b": "publish",
                "expected answer": [
                    "published"
                ],
                "predictions": [
                    {
                        "score": 0.6020472013333225,
                        "answer": "bp",
                        "hit": false
                    },
                    {
                        "score": 0.5919347916343858,
                        "answer": "chilean",
                        "hit": false
                    },
                    {
                        "score": 0.5864196022978885,
                        "answer": "confederation",
                        "hit": false
                    },
                    {
                        "score": 0.5839884314325232,
                        "answer": "geneva",
                        "hit": false
                    },
                    {
                        "score": 0.5807389839057532,
                        "answer": "fascination",
                        "hit": false
                    },
                    {
                        "score": 0.5783925584248831,
                        "answer": "chased",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publish"
                ],
                "rank": 3622,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7212502211332321
            },
            {
                "question verbose": "What is to receive ",
                "b": "receive",
                "expected answer": [
                    "received"
                ],
                "predictions": [
                    {
                        "score": 0.40785435395372244,
                        "answer": "korea",
                        "hit": false
                    },
                    {
                        "score": 0.3864721512967481,
                        "answer": "individualized",
                        "hit": false
                    },
                    {
                        "score": 0.3516828042137692,
                        "answer": "woth",
                        "hit": false
                    },
                    {
                        "score": 0.350076119513614,
                        "answer": "tort",
                        "hit": false
                    },
                    {
                        "score": 0.3490926160804233,
                        "answer": "eritrea",
                        "hit": false
                    },
                    {
                        "score": 0.34442913865876634,
                        "answer": "reception",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receive"
                ],
                "rank": 8499,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6034379750490189
            },
            {
                "question verbose": "What is to reduce ",
                "b": "reduce",
                "expected answer": [
                    "reduced"
                ],
                "predictions": [
                    {
                        "score": 0.42123989523059524,
                        "answer": "dos",
                        "hit": false
                    },
                    {
                        "score": 0.39574236460076845,
                        "answer": "initiative",
                        "hit": false
                    },
                    {
                        "score": 0.38518767847249163,
                        "answer": "balanced",
                        "hit": false
                    },
                    {
                        "score": 0.36766817937680574,
                        "answer": "inflammatory",
                        "hit": false
                    },
                    {
                        "score": 0.36665627057779765,
                        "answer": "rat",
                        "hit": false
                    },
                    {
                        "score": 0.36462486294657104,
                        "answer": "alongside",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reduce"
                ],
                "rank": 2587,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6818975955247879
            },
            {
                "question verbose": "What is to refer ",
                "b": "refer",
                "expected answer": [
                    "referred"
                ],
                "predictions": [
                    {
                        "score": 0.42683149436686935,
                        "answer": "iea",
                        "hit": false
                    },
                    {
                        "score": 0.4229767394765533,
                        "answer": "uncovered",
                        "hit": false
                    },
                    {
                        "score": 0.4144932002824001,
                        "answer": "floored",
                        "hit": false
                    },
                    {
                        "score": 0.41237934280793687,
                        "answer": "abedin",
                        "hit": false
                    },
                    {
                        "score": 0.40308629241940697,
                        "answer": "accumulation",
                        "hit": false
                    },
                    {
                        "score": 0.4004790088096971,
                        "answer": "originated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "refer"
                ],
                "rank": 2336,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7521297633647919
            },
            {
                "question verbose": "What is to relate ",
                "b": "relate",
                "expected answer": [
                    "related"
                ],
                "predictions": [
                    {
                        "score": 0.5461425290629072,
                        "answer": "morale",
                        "hit": false
                    },
                    {
                        "score": 0.5459994120115121,
                        "answer": "martin",
                        "hit": false
                    },
                    {
                        "score": 0.545833599876989,
                        "answer": "retained",
                        "hit": false
                    },
                    {
                        "score": 0.5444614048102518,
                        "answer": "represented",
                        "hit": false
                    },
                    {
                        "score": 0.5412011574152762,
                        "answer": "businessman",
                        "hit": false
                    },
                    {
                        "score": 0.5275477998538309,
                        "answer": "grove",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "relate"
                ],
                "rank": 13546,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6438585817813873
            },
            {
                "question verbose": "What is to remain ",
                "b": "remain",
                "expected answer": [
                    "remained"
                ],
                "predictions": [
                    {
                        "score": 0.5009872939923745,
                        "answer": "roe",
                        "hit": false
                    },
                    {
                        "score": 0.49509541180574307,
                        "answer": "ghent",
                        "hit": false
                    },
                    {
                        "score": 0.49413372489291624,
                        "answer": "statistic",
                        "hit": false
                    },
                    {
                        "score": 0.48353956196053766,
                        "answer": "independence",
                        "hit": false
                    },
                    {
                        "score": 0.4796316465610117,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.46948561059795957,
                        "answer": "medina",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remain"
                ],
                "rank": 1632,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6752761155366898
            },
            {
                "question verbose": "What is to replace ",
                "b": "replace",
                "expected answer": [
                    "replaced"
                ],
                "predictions": [
                    {
                        "score": 0.4287228818238802,
                        "answer": "dlc",
                        "hit": false
                    },
                    {
                        "score": 0.41058414628258666,
                        "answer": "diary",
                        "hit": false
                    },
                    {
                        "score": 0.403697581324505,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.4022651251568771,
                        "answer": "maintained",
                        "hit": false
                    },
                    {
                        "score": 0.3906238586763469,
                        "answer": "suggests",
                        "hit": false
                    },
                    {
                        "score": 0.38586977690409685,
                        "answer": "precision",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "replace"
                ],
                "rank": 11247,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6970378160476685
            },
            {
                "question verbose": "What is to require ",
                "b": "require",
                "expected answer": [
                    "required"
                ],
                "predictions": [
                    {
                        "score": 0.3597264217012021,
                        "answer": "guideline",
                        "hit": false
                    },
                    {
                        "score": 0.35681877826444486,
                        "answer": "profitable",
                        "hit": false
                    },
                    {
                        "score": 0.3488107119218407,
                        "answer": "authorizing",
                        "hit": false
                    },
                    {
                        "score": 0.33640861059985017,
                        "answer": "stable",
                        "hit": false
                    },
                    {
                        "score": 0.33507389148423133,
                        "answer": "scoping",
                        "hit": false
                    },
                    {
                        "score": 0.3338289573075338,
                        "answer": "leftover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "require"
                ],
                "rank": 14943,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5366739667952061
            },
            {
                "question verbose": "What is to seem ",
                "b": "seem",
                "expected answer": [
                    "seemed"
                ],
                "predictions": [
                    {
                        "score": 0.3294170241718792,
                        "answer": "unwillingness",
                        "hit": false
                    },
                    {
                        "score": 0.31279542940437377,
                        "answer": "appears",
                        "hit": false
                    },
                    {
                        "score": 0.2988703125590537,
                        "answer": "earthquake",
                        "hit": false
                    },
                    {
                        "score": 0.2961511997331606,
                        "answer": "harmless",
                        "hit": false
                    },
                    {
                        "score": 0.28853223257310884,
                        "answer": "nb",
                        "hit": false
                    },
                    {
                        "score": 0.28565063280267006,
                        "answer": "jsob",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seem"
                ],
                "rank": 9880,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5860188454389572
            },
            {
                "question verbose": "What is to send ",
                "b": "send",
                "expected answer": [
                    "sent"
                ],
                "predictions": [
                    {
                        "score": 0.3560459502441613,
                        "answer": "loft",
                        "hit": false
                    },
                    {
                        "score": 0.3512107673502319,
                        "answer": "suspended",
                        "hit": false
                    },
                    {
                        "score": 0.34711319568045435,
                        "answer": "april",
                        "hit": false
                    },
                    {
                        "score": 0.34466810684581567,
                        "answer": "mccloud",
                        "hit": false
                    },
                    {
                        "score": 0.34152346561209707,
                        "answer": "intern",
                        "hit": false
                    },
                    {
                        "score": 0.33825340455564823,
                        "answer": "jerusalem",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "send"
                ],
                "rank": 5871,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5937300324440002
            },
            {
                "question verbose": "What is to spend ",
                "b": "spend",
                "expected answer": [
                    "spent"
                ],
                "predictions": [
                    {
                        "score": 0.42196569127513456,
                        "answer": "spent",
                        "hit": true
                    },
                    {
                        "score": 0.40173093394494325,
                        "answer": "month",
                        "hit": false
                    },
                    {
                        "score": 0.39325422966266,
                        "answer": "leftover",
                        "hit": false
                    },
                    {
                        "score": 0.3715727088587716,
                        "answer": "telling",
                        "hit": false
                    },
                    {
                        "score": 0.3704867943254769,
                        "answer": "bleep",
                        "hit": false
                    },
                    {
                        "score": 0.36966474944387934,
                        "answer": "posed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spend"
                ],
                "rank": 0,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.750649482011795
            },
            {
                "question verbose": "What is to tell ",
                "b": "tell",
                "expected answer": [
                    "told"
                ],
                "predictions": [
                    {
                        "score": 0.33656562317792077,
                        "answer": "swf",
                        "hit": false
                    },
                    {
                        "score": 0.33266483015187204,
                        "answer": "brother",
                        "hit": false
                    },
                    {
                        "score": 0.3205466258199947,
                        "answer": "tonight",
                        "hit": false
                    },
                    {
                        "score": 0.3174666274642576,
                        "answer": "duplass",
                        "hit": false
                    },
                    {
                        "score": 0.30391087436278935,
                        "answer": "gave",
                        "hit": false
                    },
                    {
                        "score": 0.3027542344368062,
                        "answer": "farm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tell"
                ],
                "rank": 7120,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6771625578403473
            },
            {
                "question verbose": "What is to understand ",
                "b": "understand",
                "expected answer": [
                    "understood"
                ],
                "predictions": [
                    {
                        "score": 0.3235493280633037,
                        "answer": "saloon",
                        "hit": false
                    },
                    {
                        "score": 0.32271654220158996,
                        "answer": "laughed",
                        "hit": false
                    },
                    {
                        "score": 0.3213102271655247,
                        "answer": "morrie",
                        "hit": false
                    },
                    {
                        "score": 0.3100018505982803,
                        "answer": "hispanic",
                        "hit": false
                    },
                    {
                        "score": 0.3057857169634823,
                        "answer": "aplomb",
                        "hit": false
                    },
                    {
                        "score": 0.29261137692685013,
                        "answer": "pounding",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "understand"
                ],
                "rank": 14514,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6831851452589035
            },
            {
                "question verbose": "What is to unite ",
                "b": "unite",
                "expected answer": [
                    "united"
                ],
                "predictions": [
                    {
                        "score": 0.5076403591861998,
                        "answer": "rifle",
                        "hit": false
                    },
                    {
                        "score": 0.49405022315933705,
                        "answer": "scoping",
                        "hit": false
                    },
                    {
                        "score": 0.4914242761763901,
                        "answer": "digger",
                        "hit": false
                    },
                    {
                        "score": 0.48895834878000877,
                        "answer": "liquor",
                        "hit": false
                    },
                    {
                        "score": 0.4870561587248446,
                        "answer": "upheaval",
                        "hit": false
                    },
                    {
                        "score": 0.4842485092585219,
                        "answer": "cibc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "unite"
                ],
                "rank": 4240,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6882483214139938
            }
        ],
        "result": {
            "cnt_questions_correct": 1,
            "cnt_questions_total": 50,
            "accuracy": 0.02
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I07 [verb_inf - Ved].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "cb5c2c0b-bb7f-46b3-9325-7055b90d8805",
            "timestamp": "2020-10-22T15:57:40.539661"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to adding ",
                "b": "adding",
                "expected answer": [
                    "adds"
                ],
                "predictions": [
                    {
                        "score": 0.5109536028327548,
                        "answer": "cgi",
                        "hit": false
                    },
                    {
                        "score": 0.45420378499382436,
                        "answer": "rated",
                        "hit": false
                    },
                    {
                        "score": 0.42474216593953146,
                        "answer": "pg",
                        "hit": false
                    },
                    {
                        "score": 0.4172013322424918,
                        "answer": "suscribed",
                        "hit": false
                    },
                    {
                        "score": 0.41616342945870566,
                        "answer": "orca",
                        "hit": false
                    },
                    {
                        "score": 0.4107692907389917,
                        "answer": "deflect",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adding"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6520016938447952
            },
            {
                "question verbose": "What is to advertising ",
                "b": "advertising",
                "expected answer": [
                    "advertises"
                ],
                "predictions": [
                    {
                        "score": 0.3928425760018283,
                        "answer": "acute",
                        "hit": false
                    },
                    {
                        "score": 0.3875024319360611,
                        "answer": "emailing",
                        "hit": false
                    },
                    {
                        "score": 0.36928366623174586,
                        "answer": "moral",
                        "hit": false
                    },
                    {
                        "score": 0.3679207179603907,
                        "answer": "perpetuated",
                        "hit": false
                    },
                    {
                        "score": 0.3625077050703254,
                        "answer": "superb",
                        "hit": false
                    },
                    {
                        "score": 0.360114692770606,
                        "answer": "countered",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "advertising"
                ],
                "rank": 7916,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7048955112695694
            },
            {
                "question verbose": "What is to allowing ",
                "b": "allowing",
                "expected answer": [
                    "allows"
                ],
                "predictions": [
                    {
                        "score": 0.41285905494589603,
                        "answer": "reasoning",
                        "hit": false
                    },
                    {
                        "score": 0.4022100645430334,
                        "answer": "latent",
                        "hit": false
                    },
                    {
                        "score": 0.3976292901803687,
                        "answer": "activist",
                        "hit": false
                    },
                    {
                        "score": 0.3930008764792287,
                        "answer": "oftentimes",
                        "hit": false
                    },
                    {
                        "score": 0.39267028788505853,
                        "answer": "equipped",
                        "hit": false
                    },
                    {
                        "score": 0.3829708868501022,
                        "answer": "possessing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allowing"
                ],
                "rank": 2630,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7372707426548004
            },
            {
                "question verbose": "What is to appearing ",
                "b": "appearing",
                "expected answer": [
                    "appears"
                ],
                "predictions": [
                    {
                        "score": 0.5264930499398729,
                        "answer": "interestingly",
                        "hit": false
                    },
                    {
                        "score": 0.522850846856587,
                        "answer": "antithesis",
                        "hit": false
                    },
                    {
                        "score": 0.5188204350661805,
                        "answer": "momrules",
                        "hit": false
                    },
                    {
                        "score": 0.5075423324632973,
                        "answer": "undestand",
                        "hit": false
                    },
                    {
                        "score": 0.49741886646698297,
                        "answer": "unrealistic",
                        "hit": false
                    },
                    {
                        "score": 0.49628719680458283,
                        "answer": "bif",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appearing"
                ],
                "rank": 6996,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7244163602590561
            },
            {
                "question verbose": "What is to applying ",
                "b": "applying",
                "expected answer": [
                    "applies"
                ],
                "predictions": [
                    {
                        "score": 0.5372061301440471,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.505966920735745,
                        "answer": "subcategories",
                        "hit": false
                    },
                    {
                        "score": 0.502278241503104,
                        "answer": "inaugural",
                        "hit": false
                    },
                    {
                        "score": 0.49902171570878207,
                        "answer": "suscribed",
                        "hit": false
                    },
                    {
                        "score": 0.4958761467514651,
                        "answer": "equipped",
                        "hit": false
                    },
                    {
                        "score": 0.4917516123884341,
                        "answer": "lillard",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "applying"
                ],
                "rank": 12848,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7050471901893616
            },
            {
                "question verbose": "What is to asking ",
                "b": "asking",
                "expected answer": [
                    "asks"
                ],
                "predictions": [
                    {
                        "score": 0.37797776175114767,
                        "answer": "answering",
                        "hit": false
                    },
                    {
                        "score": 0.37025351551179236,
                        "answer": "call",
                        "hit": false
                    },
                    {
                        "score": 0.35914201496119036,
                        "answer": "crucially",
                        "hit": false
                    },
                    {
                        "score": 0.3587467251054283,
                        "answer": "volvo",
                        "hit": false
                    },
                    {
                        "score": 0.3524128704930094,
                        "answer": "overturn",
                        "hit": false
                    },
                    {
                        "score": 0.3382436589773001,
                        "answer": "correctness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "asking"
                ],
                "rank": 9374,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6421930491924286
            },
            {
                "question verbose": "What is to becoming ",
                "b": "becoming",
                "expected answer": [
                    "becomes"
                ],
                "predictions": [
                    {
                        "score": 0.35973369772724195,
                        "answer": "asserts",
                        "hit": false
                    },
                    {
                        "score": 0.3485927714615047,
                        "answer": "injured",
                        "hit": false
                    },
                    {
                        "score": 0.3462179732848299,
                        "answer": "busily",
                        "hit": false
                    },
                    {
                        "score": 0.34362773149506115,
                        "answer": "hellscape",
                        "hit": false
                    },
                    {
                        "score": 0.3418224042027921,
                        "answer": "forsythe",
                        "hit": false
                    },
                    {
                        "score": 0.3399269316995511,
                        "answer": "applebees",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "becoming"
                ],
                "rank": 6768,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6542586088180542
            },
            {
                "question verbose": "What is to believing ",
                "b": "believing",
                "expected answer": [
                    "believes"
                ],
                "predictions": [
                    {
                        "score": 0.4823377264315632,
                        "answer": "fdr",
                        "hit": false
                    },
                    {
                        "score": 0.4683432000984352,
                        "answer": "trap",
                        "hit": false
                    },
                    {
                        "score": 0.45657973811384567,
                        "answer": "amendment",
                        "hit": false
                    },
                    {
                        "score": 0.43773643250098565,
                        "answer": "speech",
                        "hit": false
                    },
                    {
                        "score": 0.4368348505396572,
                        "answer": "witch",
                        "hit": false
                    },
                    {
                        "score": 0.4253790015819544,
                        "answer": "oftentimes",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believing"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5744932293891907
            },
            {
                "question verbose": "What is to considering ",
                "b": "considering",
                "expected answer": [
                    "considers"
                ],
                "predictions": [
                    {
                        "score": 0.43905865301496627,
                        "answer": "preaching",
                        "hit": false
                    },
                    {
                        "score": 0.4147739901301539,
                        "answer": "circumvent",
                        "hit": false
                    },
                    {
                        "score": 0.4062860743388941,
                        "answer": "hazard",
                        "hit": false
                    },
                    {
                        "score": 0.4040266428016773,
                        "answer": "overturn",
                        "hit": false
                    },
                    {
                        "score": 0.40229774568660137,
                        "answer": "possessing",
                        "hit": false
                    },
                    {
                        "score": 0.3984831444009697,
                        "answer": "trivia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "considering"
                ],
                "rank": 8101,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.735605463385582
            },
            {
                "question verbose": "What is to consisting ",
                "b": "consisting",
                "expected answer": [
                    "consists"
                ],
                "predictions": [
                    {
                        "score": 0.773400303980038,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.35135268466947434,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.31550424908845204,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.31362044383217164,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.31296322962432865,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.30577766648536253,
                        "answer": "vamp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consisting"
                ],
                "rank": 1260,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6593961864709854
            },
            {
                "question verbose": "What is to containing ",
                "b": "containing",
                "expected answer": [
                    "contains"
                ],
                "predictions": [
                    {
                        "score": 0.5083916566496027,
                        "answer": "ripping",
                        "hit": false
                    },
                    {
                        "score": 0.4988823908241651,
                        "answer": "unrealistic",
                        "hit": false
                    },
                    {
                        "score": 0.498652111355014,
                        "answer": "unsoundness",
                        "hit": false
                    },
                    {
                        "score": 0.49633554770216387,
                        "answer": "strenous",
                        "hit": false
                    },
                    {
                        "score": 0.49590567315335515,
                        "answer": "overturn",
                        "hit": false
                    },
                    {
                        "score": 0.49309878566274173,
                        "answer": "catastrophic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "containing"
                ],
                "rank": 13843,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7556607127189636
            },
            {
                "question verbose": "What is to continuing ",
                "b": "continuing",
                "expected answer": [
                    "continues"
                ],
                "predictions": [
                    {
                        "score": 0.38321806675902725,
                        "answer": "stylistically",
                        "hit": false
                    },
                    {
                        "score": 0.3786586440175099,
                        "answer": "undertake",
                        "hit": false
                    },
                    {
                        "score": 0.3740571562185722,
                        "answer": "wesley",
                        "hit": false
                    },
                    {
                        "score": 0.36453589968529415,
                        "answer": "spank",
                        "hit": false
                    },
                    {
                        "score": 0.3617075006553679,
                        "answer": "antithesis",
                        "hit": false
                    },
                    {
                        "score": 0.3613355242423593,
                        "answer": "bandwagon",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continuing"
                ],
                "rank": 13431,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6450931429862976
            },
            {
                "question verbose": "What is to creating ",
                "b": "creating",
                "expected answer": [
                    "creates"
                ],
                "predictions": [
                    {
                        "score": 0.3858362250188652,
                        "answer": "rooted",
                        "hit": false
                    },
                    {
                        "score": 0.3784883380500675,
                        "answer": "spank",
                        "hit": false
                    },
                    {
                        "score": 0.35894821825674283,
                        "answer": "norm",
                        "hit": false
                    },
                    {
                        "score": 0.35263148294681823,
                        "answer": "behaviour",
                        "hit": false
                    },
                    {
                        "score": 0.352106506352327,
                        "answer": "colloquialism",
                        "hit": false
                    },
                    {
                        "score": 0.3517050236746839,
                        "answer": "privy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "creating"
                ],
                "rank": 691,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7661955952644348
            },
            {
                "question verbose": "What is to depending ",
                "b": "depending",
                "expected answer": [
                    "depends"
                ],
                "predictions": [
                    {
                        "score": 0.4570949662446265,
                        "answer": "advertisement",
                        "hit": false
                    },
                    {
                        "score": 0.45128577965897126,
                        "answer": "cargo",
                        "hit": false
                    },
                    {
                        "score": 0.4478428912674585,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.4409231448687941,
                        "answer": "pondering",
                        "hit": false
                    },
                    {
                        "score": 0.4299814455504201,
                        "answer": "stiff",
                        "hit": false
                    },
                    {
                        "score": 0.4297926774188034,
                        "answer": "undecideds",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "depending"
                ],
                "rank": 8898,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6820841580629349
            },
            {
                "question verbose": "What is to describing ",
                "b": "describing",
                "expected answer": [
                    "describes"
                ],
                "predictions": [
                    {
                        "score": 0.4771495684329952,
                        "answer": "wiki",
                        "hit": false
                    },
                    {
                        "score": 0.4767196399385527,
                        "answer": "duane",
                        "hit": false
                    },
                    {
                        "score": 0.45968024536125246,
                        "answer": "gallop",
                        "hit": false
                    },
                    {
                        "score": 0.4584264531189054,
                        "answer": "shill",
                        "hit": false
                    },
                    {
                        "score": 0.44419700708026005,
                        "answer": "ltcurrent",
                        "hit": false
                    },
                    {
                        "score": 0.4391629251623861,
                        "answer": "analyzing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "describing"
                ],
                "rank": 8311,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6931122094392776
            },
            {
                "question verbose": "What is to developing ",
                "b": "developing",
                "expected answer": [
                    "develops"
                ],
                "predictions": [
                    {
                        "score": 0.40048456936377536,
                        "answer": "acute",
                        "hit": false
                    },
                    {
                        "score": 0.39165842641354587,
                        "answer": "breakdown",
                        "hit": false
                    },
                    {
                        "score": 0.38574597202153976,
                        "answer": "bandwagon",
                        "hit": false
                    },
                    {
                        "score": 0.3799067623961105,
                        "answer": "coughed",
                        "hit": false
                    },
                    {
                        "score": 0.376010692393278,
                        "answer": "spit",
                        "hit": false
                    },
                    {
                        "score": 0.37507393933340677,
                        "answer": "lucrative",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "developing"
                ],
                "rank": 4484,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7234517484903336
            },
            {
                "question verbose": "What is to discovering ",
                "b": "discovering",
                "expected answer": [
                    "discovers"
                ],
                "predictions": [
                    {
                        "score": 0.45773611552429727,
                        "answer": "romcom",
                        "hit": false
                    },
                    {
                        "score": 0.45061172964348567,
                        "answer": "storied",
                        "hit": false
                    },
                    {
                        "score": 0.4480918646511764,
                        "answer": "momrules",
                        "hit": false
                    },
                    {
                        "score": 0.4467224695394791,
                        "answer": "atheism",
                        "hit": false
                    },
                    {
                        "score": 0.4430867683279073,
                        "answer": "debater",
                        "hit": false
                    },
                    {
                        "score": 0.4414536667297753,
                        "answer": "obsolete",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "discovering"
                ],
                "rank": 2869,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7636062204837799
            },
            {
                "question verbose": "What is to enabling ",
                "b": "enabling",
                "expected answer": [
                    "enables"
                ],
                "predictions": [
                    {
                        "score": 0.7710468805447162,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.353002620565539,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3285971184283656,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.3142416768375554,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.31348724853548793,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3029837569360602,
                        "answer": "impactwith",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "enabling"
                ],
                "rank": 1137,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6953732967376709
            },
            {
                "question verbose": "What is to existing ",
                "b": "existing",
                "expected answer": [
                    "exists"
                ],
                "predictions": [
                    {
                        "score": 0.3929793283939722,
                        "answer": "troubled",
                        "hit": false
                    },
                    {
                        "score": 0.3638271657498263,
                        "answer": "grandfathering",
                        "hit": false
                    },
                    {
                        "score": 0.36282196896931423,
                        "answer": "donated",
                        "hit": false
                    },
                    {
                        "score": 0.35831218628369615,
                        "answer": "delivering",
                        "hit": false
                    },
                    {
                        "score": 0.3570354118068825,
                        "answer": "oftentimes",
                        "hit": false
                    },
                    {
                        "score": 0.3566668213003032,
                        "answer": "unattended",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "existing"
                ],
                "rank": 5369,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7411689162254333
            },
            {
                "question verbose": "What is to explaining ",
                "b": "explaining",
                "expected answer": [
                    "explains"
                ],
                "predictions": [
                    {
                        "score": 0.3929877060692189,
                        "answer": "transmediated",
                        "hit": false
                    },
                    {
                        "score": 0.3618705716360627,
                        "answer": "lock",
                        "hit": false
                    },
                    {
                        "score": 0.3585447684846244,
                        "answer": "idaho",
                        "hit": false
                    },
                    {
                        "score": 0.35709035124952926,
                        "answer": "exquisite",
                        "hit": false
                    },
                    {
                        "score": 0.3565182306125082,
                        "answer": "herod",
                        "hit": false
                    },
                    {
                        "score": 0.3535809012474093,
                        "answer": "conference",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "explaining"
                ],
                "rank": 7790,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6345226913690567
            },
            {
                "question verbose": "What is to following ",
                "b": "following",
                "expected answer": [
                    "follows"
                ],
                "predictions": [
                    {
                        "score": 0.3129649207043055,
                        "answer": "histogram",
                        "hit": false
                    },
                    {
                        "score": 0.3129438185433054,
                        "answer": "resulted",
                        "hit": false
                    },
                    {
                        "score": 0.30915541611576924,
                        "answer": "restriction",
                        "hit": false
                    },
                    {
                        "score": 0.3027732665196674,
                        "answer": "footlick",
                        "hit": false
                    },
                    {
                        "score": 0.3000363384467664,
                        "answer": "endowment",
                        "hit": false
                    },
                    {
                        "score": 0.2989269903109132,
                        "answer": "anxiety",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "following"
                ],
                "rank": 8949,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6315862834453583
            },
            {
                "question verbose": "What is to happening ",
                "b": "happening",
                "expected answer": [
                    "happens"
                ],
                "predictions": [
                    {
                        "score": 0.4086781755716314,
                        "answer": "fifantasy",
                        "hit": false
                    },
                    {
                        "score": 0.4042899607018662,
                        "answer": "letting",
                        "hit": false
                    },
                    {
                        "score": 0.3875583145051066,
                        "answer": "optimised",
                        "hit": false
                    },
                    {
                        "score": 0.3857333792927416,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.38166519995471604,
                        "answer": "racial",
                        "hit": false
                    },
                    {
                        "score": 0.3760501167491138,
                        "answer": "faff",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happening"
                ],
                "rank": 7922,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7443990111351013
            },
            {
                "question verbose": "What is to hearing ",
                "b": "hearing",
                "expected answer": [
                    "hears"
                ],
                "predictions": [
                    {
                        "score": 0.36288851963696367,
                        "answer": "resembled",
                        "hit": false
                    },
                    {
                        "score": 0.3514742866930375,
                        "answer": "mar",
                        "hit": false
                    },
                    {
                        "score": 0.3487077773264871,
                        "answer": "concludes",
                        "hit": false
                    },
                    {
                        "score": 0.33901571689286786,
                        "answer": "forgave",
                        "hit": false
                    },
                    {
                        "score": 0.33778139910595595,
                        "answer": "mulblecore",
                        "hit": false
                    },
                    {
                        "score": 0.3377570372705922,
                        "answer": "princess",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hearing"
                ],
                "rank": 11347,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6408580392599106
            },
            {
                "question verbose": "What is to improving ",
                "b": "improving",
                "expected answer": [
                    "improves"
                ],
                "predictions": [
                    {
                        "score": 0.4231782225240669,
                        "answer": "lock",
                        "hit": false
                    },
                    {
                        "score": 0.4016419260344464,
                        "answer": "coughed",
                        "hit": false
                    },
                    {
                        "score": 0.388046287892695,
                        "answer": "shitty",
                        "hit": false
                    },
                    {
                        "score": 0.3873591791401401,
                        "answer": "confession",
                        "hit": false
                    },
                    {
                        "score": 0.37353310041528026,
                        "answer": "quicksand",
                        "hit": false
                    },
                    {
                        "score": 0.3721036411055085,
                        "answer": "steer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improving"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6325519382953644
            },
            {
                "question verbose": "What is to including ",
                "b": "including",
                "expected answer": [
                    "includes"
                ],
                "predictions": [
                    {
                        "score": 0.33108208547611373,
                        "answer": "scandinavia",
                        "hit": false
                    },
                    {
                        "score": 0.3213444143403364,
                        "answer": "laundry",
                        "hit": false
                    },
                    {
                        "score": 0.3211011887965903,
                        "answer": "column",
                        "hit": false
                    },
                    {
                        "score": 0.31786414013415,
                        "answer": "histogram",
                        "hit": false
                    },
                    {
                        "score": 0.3142141821095484,
                        "answer": "duane",
                        "hit": false
                    },
                    {
                        "score": 0.3124563315334371,
                        "answer": "budweiser",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "including"
                ],
                "rank": 13133,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5948636904358864
            },
            {
                "question verbose": "What is to involving ",
                "b": "involving",
                "expected answer": [
                    "involves"
                ],
                "predictions": [
                    {
                        "score": 0.3649163411195629,
                        "answer": "elegant",
                        "hit": false
                    },
                    {
                        "score": 0.36279747648851013,
                        "answer": "thomas",
                        "hit": false
                    },
                    {
                        "score": 0.36118901247352575,
                        "answer": "ore",
                        "hit": false
                    },
                    {
                        "score": 0.35667144352859703,
                        "answer": "cecile",
                        "hit": false
                    },
                    {
                        "score": 0.35295446082107623,
                        "answer": "prickly",
                        "hit": false
                    },
                    {
                        "score": 0.3508039781571922,
                        "answer": "stuffed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involving"
                ],
                "rank": 13811,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5985526144504547
            },
            {
                "question verbose": "What is to learning ",
                "b": "learning",
                "expected answer": [
                    "learns"
                ],
                "predictions": [
                    {
                        "score": 0.3273445897256269,
                        "answer": "forbid",
                        "hit": false
                    },
                    {
                        "score": 0.32267607444893076,
                        "answer": "zami",
                        "hit": false
                    },
                    {
                        "score": 0.3128603521895383,
                        "answer": "deferred",
                        "hit": false
                    },
                    {
                        "score": 0.3124142385915574,
                        "answer": "fossiliferous",
                        "hit": false
                    },
                    {
                        "score": 0.3122097085477274,
                        "answer": "refrigerator",
                        "hit": false
                    },
                    {
                        "score": 0.31216983875877996,
                        "answer": "researched",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "learning"
                ],
                "rank": 1128,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7035969495773315
            },
            {
                "question verbose": "What is to losing ",
                "b": "losing",
                "expected answer": [
                    "loses"
                ],
                "predictions": [
                    {
                        "score": 0.3668649960845271,
                        "answer": "unsoundness",
                        "hit": false
                    },
                    {
                        "score": 0.35911451886150464,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.3541084650927687,
                        "answer": "bursting",
                        "hit": false
                    },
                    {
                        "score": 0.3488252858955658,
                        "answer": "honorable",
                        "hit": false
                    },
                    {
                        "score": 0.34275733793618895,
                        "answer": "endearing",
                        "hit": false
                    },
                    {
                        "score": 0.3378950022779014,
                        "answer": "testimony",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "losing"
                ],
                "rank": 12184,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6765381097793579
            },
            {
                "question verbose": "What is to managing ",
                "b": "managing",
                "expected answer": [
                    "manages"
                ],
                "predictions": [
                    {
                        "score": 0.41914874035773836,
                        "answer": "paralysis",
                        "hit": false
                    },
                    {
                        "score": 0.40894581494094956,
                        "answer": "ucla",
                        "hit": false
                    },
                    {
                        "score": 0.40186433980017244,
                        "answer": "scholar",
                        "hit": false
                    },
                    {
                        "score": 0.39972035662462646,
                        "answer": "electronics",
                        "hit": false
                    },
                    {
                        "score": 0.3907705212208981,
                        "answer": "ramses",
                        "hit": false
                    },
                    {
                        "score": 0.39037879576180273,
                        "answer": "neo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "managing"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5783624798059464
            },
            {
                "question verbose": "What is to occurring ",
                "b": "occurring",
                "expected answer": [
                    "occurs"
                ],
                "predictions": [
                    {
                        "score": 0.5329411251450341,
                        "answer": "advertisement",
                        "hit": false
                    },
                    {
                        "score": 0.4965290795538354,
                        "answer": "warmer",
                        "hit": false
                    },
                    {
                        "score": 0.4925677275515665,
                        "answer": "attached",
                        "hit": false
                    },
                    {
                        "score": 0.49130496772296456,
                        "answer": "romcom",
                        "hit": false
                    },
                    {
                        "score": 0.4786791868939465,
                        "answer": "martian",
                        "hit": false
                    },
                    {
                        "score": 0.47587040732015845,
                        "answer": "hazard",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "occurring"
                ],
                "rank": 5232,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7829684019088745
            },
            {
                "question verbose": "What is to operating ",
                "b": "operating",
                "expected answer": [
                    "operates"
                ],
                "predictions": [
                    {
                        "score": 0.4052036411602563,
                        "answer": "possessing",
                        "hit": false
                    },
                    {
                        "score": 0.3908291454009037,
                        "answer": "administrative",
                        "hit": false
                    },
                    {
                        "score": 0.3894677430012057,
                        "answer": "salary",
                        "hit": false
                    },
                    {
                        "score": 0.387544047072617,
                        "answer": "uncontrolled",
                        "hit": false
                    },
                    {
                        "score": 0.38326049860315037,
                        "answer": "semantic",
                        "hit": false
                    },
                    {
                        "score": 0.3818354336557281,
                        "answer": "bezel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "operating"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5601250939071178
            },
            {
                "question verbose": "What is to performing ",
                "b": "performing",
                "expected answer": [
                    "performs"
                ],
                "predictions": [
                    {
                        "score": 0.4462808844461719,
                        "answer": "possessing",
                        "hit": false
                    },
                    {
                        "score": 0.4448469069782616,
                        "answer": "kaminska",
                        "hit": false
                    },
                    {
                        "score": 0.44452534844437075,
                        "answer": "overturn",
                        "hit": false
                    },
                    {
                        "score": 0.44439128054152566,
                        "answer": "keypresses",
                        "hit": false
                    },
                    {
                        "score": 0.4384125714155546,
                        "answer": "martian",
                        "hit": false
                    },
                    {
                        "score": 0.4331539286473891,
                        "answer": "advertisement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "performing"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6752878725528717
            },
            {
                "question verbose": "What is to promoting ",
                "b": "promoting",
                "expected answer": [
                    "promotes"
                ],
                "predictions": [
                    {
                        "score": 0.5537547595664856,
                        "answer": "cecile",
                        "hit": false
                    },
                    {
                        "score": 0.5302267034214663,
                        "answer": "rocked",
                        "hit": false
                    },
                    {
                        "score": 0.5091939775926536,
                        "answer": "ol",
                        "hit": false
                    },
                    {
                        "score": 0.5069680237772103,
                        "answer": "precedes",
                        "hit": false
                    },
                    {
                        "score": 0.5003493195233962,
                        "answer": "endowing",
                        "hit": false
                    },
                    {
                        "score": 0.497828940562365,
                        "answer": "mummy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "promoting"
                ],
                "rank": 4292,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8474940955638885
            },
            {
                "question verbose": "What is to providing ",
                "b": "providing",
                "expected answer": [
                    "provides"
                ],
                "predictions": [
                    {
                        "score": 0.3496615044254457,
                        "answer": "neo",
                        "hit": false
                    },
                    {
                        "score": 0.3372735051662029,
                        "answer": "tremendous",
                        "hit": false
                    },
                    {
                        "score": 0.33632427808929255,
                        "answer": "mickley",
                        "hit": false
                    },
                    {
                        "score": 0.33124037835398384,
                        "answer": "oftentimes",
                        "hit": false
                    },
                    {
                        "score": 0.33049482624726545,
                        "answer": "homeopathy",
                        "hit": false
                    },
                    {
                        "score": 0.3283934679952598,
                        "answer": "absolutist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "providing"
                ],
                "rank": 9010,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6846518963575363
            },
            {
                "question verbose": "What is to publishing ",
                "b": "publishing",
                "expected answer": [
                    "publishes"
                ],
                "predictions": [
                    {
                        "score": 0.42635539684875845,
                        "answer": "ore",
                        "hit": false
                    },
                    {
                        "score": 0.4138839593095712,
                        "answer": "taylor",
                        "hit": false
                    },
                    {
                        "score": 0.4106508898669526,
                        "answer": "martian",
                        "hit": false
                    },
                    {
                        "score": 0.4100686599399356,
                        "answer": "lucrative",
                        "hit": false
                    },
                    {
                        "score": 0.4088983587692887,
                        "answer": "endowment",
                        "hit": false
                    },
                    {
                        "score": 0.4074950795318836,
                        "answer": "pushover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publishing"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5746742635965347
            },
            {
                "question verbose": "What is to receiving ",
                "b": "receiving",
                "expected answer": [
                    "receives"
                ],
                "predictions": [
                    {
                        "score": 0.5350004321029938,
                        "answer": "momrules",
                        "hit": false
                    },
                    {
                        "score": 0.5292746325287112,
                        "answer": "overturn",
                        "hit": false
                    },
                    {
                        "score": 0.5281152039479989,
                        "answer": "obscurity",
                        "hit": false
                    },
                    {
                        "score": 0.5088984239106821,
                        "answer": "rocked",
                        "hit": false
                    },
                    {
                        "score": 0.5058998308467867,
                        "answer": "ripping",
                        "hit": false
                    },
                    {
                        "score": 0.5047580057766231,
                        "answer": "fossiliferous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receiving"
                ],
                "rank": 2305,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.802861362695694
            },
            {
                "question verbose": "What is to reducing ",
                "b": "reducing",
                "expected answer": [
                    "reduces"
                ],
                "predictions": [
                    {
                        "score": 0.45594148916584987,
                        "answer": "rushed",
                        "hit": false
                    },
                    {
                        "score": 0.44743366536615187,
                        "answer": "entertained",
                        "hit": false
                    },
                    {
                        "score": 0.43849883330459005,
                        "answer": "advertisement",
                        "hit": false
                    },
                    {
                        "score": 0.4356692961934552,
                        "answer": "capillary",
                        "hit": false
                    },
                    {
                        "score": 0.43559650775451847,
                        "answer": "inverted",
                        "hit": false
                    },
                    {
                        "score": 0.43170550810567193,
                        "answer": "homeopathy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reducing"
                ],
                "rank": 11459,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8096466660499573
            },
            {
                "question verbose": "What is to referring ",
                "b": "referring",
                "expected answer": [
                    "refers"
                ],
                "predictions": [
                    {
                        "score": 0.544029516247499,
                        "answer": "advertisement",
                        "hit": false
                    },
                    {
                        "score": 0.5347730095338719,
                        "answer": "perpetuated",
                        "hit": false
                    },
                    {
                        "score": 0.5347420888883122,
                        "answer": "bursting",
                        "hit": false
                    },
                    {
                        "score": 0.5232703881344559,
                        "answer": "antithesis",
                        "hit": false
                    },
                    {
                        "score": 0.5216494211013839,
                        "answer": "advocating",
                        "hit": false
                    },
                    {
                        "score": 0.5193167371763667,
                        "answer": "oftentimes",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "referring"
                ],
                "rank": 13178,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8244030177593231
            },
            {
                "question verbose": "What is to relating ",
                "b": "relating",
                "expected answer": [
                    "relates"
                ],
                "predictions": [
                    {
                        "score": 0.4928334602619048,
                        "answer": "bursting",
                        "hit": false
                    },
                    {
                        "score": 0.4804340510205845,
                        "answer": "gang",
                        "hit": false
                    },
                    {
                        "score": 0.4745403885927809,
                        "answer": "archive",
                        "hit": false
                    },
                    {
                        "score": 0.4715512630754109,
                        "answer": "optimised",
                        "hit": false
                    },
                    {
                        "score": 0.47016702372717223,
                        "answer": "stylistically",
                        "hit": false
                    },
                    {
                        "score": 0.46700295524269636,
                        "answer": "counseling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "relating"
                ],
                "rank": 9174,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7398038655519485
            },
            {
                "question verbose": "What is to remaining ",
                "b": "remaining",
                "expected answer": [
                    "remains"
                ],
                "predictions": [
                    {
                        "score": 0.46826192747371476,
                        "answer": "unlikely",
                        "hit": false
                    },
                    {
                        "score": 0.454695005031622,
                        "answer": "sheriff",
                        "hit": false
                    },
                    {
                        "score": 0.4506575880715221,
                        "answer": "repugs",
                        "hit": false
                    },
                    {
                        "score": 0.44160020641916065,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.4363724831255254,
                        "answer": "moral",
                        "hit": false
                    },
                    {
                        "score": 0.4318094210441601,
                        "answer": "equipped",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remaining"
                ],
                "rank": 7943,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6716465950012207
            },
            {
                "question verbose": "What is to representing ",
                "b": "representing",
                "expected answer": [
                    "represents"
                ],
                "predictions": [
                    {
                        "score": 0.4215655174464691,
                        "answer": "logo",
                        "hit": false
                    },
                    {
                        "score": 0.40683981422580373,
                        "answer": "inverted",
                        "hit": false
                    },
                    {
                        "score": 0.40369848786296425,
                        "answer": "relieving",
                        "hit": false
                    },
                    {
                        "score": 0.4027378792212132,
                        "answer": "compose",
                        "hit": false
                    },
                    {
                        "score": 0.4026774696346482,
                        "answer": "stretching",
                        "hit": false
                    },
                    {
                        "score": 0.40253845516459036,
                        "answer": "wordpress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "representing"
                ],
                "rank": 13266,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6902474910020828
            },
            {
                "question verbose": "What is to requiring ",
                "b": "requiring",
                "expected answer": [
                    "requires"
                ],
                "predictions": [
                    {
                        "score": 0.7772898995700156,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34031265423255597,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.33864170724983983,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3304103279406713,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.3237715112310179,
                        "answer": "impactwith",
                        "hit": false
                    },
                    {
                        "score": 0.3209518910652717,
                        "answer": "vamp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "requiring"
                ],
                "rank": 13335,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5568048767745495
            },
            {
                "question verbose": "What is to seeming ",
                "b": "seeming",
                "expected answer": [
                    "seems"
                ],
                "predictions": [
                    {
                        "score": 0.4988246312489006,
                        "answer": "greasy",
                        "hit": false
                    },
                    {
                        "score": 0.4976426621061799,
                        "answer": "circumvent",
                        "hit": false
                    },
                    {
                        "score": 0.48753232533030155,
                        "answer": "equipped",
                        "hit": false
                    },
                    {
                        "score": 0.475719330646278,
                        "answer": "crucially",
                        "hit": false
                    },
                    {
                        "score": 0.47154936693355565,
                        "answer": "keypresses",
                        "hit": false
                    },
                    {
                        "score": 0.4606432231333093,
                        "answer": "repugs",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seeming"
                ],
                "rank": 3723,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.68523208796978
            },
            {
                "question verbose": "What is to sitting ",
                "b": "sitting",
                "expected answer": [
                    "sits"
                ],
                "predictions": [
                    {
                        "score": 0.4335542967003278,
                        "answer": "commander",
                        "hit": false
                    },
                    {
                        "score": 0.43131789822678757,
                        "answer": "racial",
                        "hit": false
                    },
                    {
                        "score": 0.4109614303101178,
                        "answer": "porch",
                        "hit": false
                    },
                    {
                        "score": 0.40533072001024867,
                        "answer": "armchair",
                        "hit": false
                    },
                    {
                        "score": 0.4043978576579764,
                        "answer": "promo",
                        "hit": false
                    },
                    {
                        "score": 0.39934004658355543,
                        "answer": "armageddon",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sitting"
                ],
                "rank": 1196,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7670010924339294
            },
            {
                "question verbose": "What is to spending ",
                "b": "spending",
                "expected answer": [
                    "spends"
                ],
                "predictions": [
                    {
                        "score": 0.3891697730036503,
                        "answer": "scaling",
                        "hit": false
                    },
                    {
                        "score": 0.364311906883306,
                        "answer": "greasy",
                        "hit": false
                    },
                    {
                        "score": 0.3528351747736225,
                        "answer": "zimbabwean",
                        "hit": false
                    },
                    {
                        "score": 0.3471631920594359,
                        "answer": "kulfi",
                        "hit": false
                    },
                    {
                        "score": 0.332650715871113,
                        "answer": "fossiliferous",
                        "hit": false
                    },
                    {
                        "score": 0.32611092497383376,
                        "answer": "antithesis",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spending"
                ],
                "rank": 8197,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6620299816131592
            },
            {
                "question verbose": "What is to suggesting ",
                "b": "suggesting",
                "expected answer": [
                    "suggests"
                ],
                "predictions": [
                    {
                        "score": 0.44783604948952477,
                        "answer": "resolved",
                        "hit": false
                    },
                    {
                        "score": 0.4419258599678725,
                        "answer": "countered",
                        "hit": false
                    },
                    {
                        "score": 0.4399870176159728,
                        "answer": "subsequent",
                        "hit": false
                    },
                    {
                        "score": 0.4362813878009176,
                        "answer": "demeaning",
                        "hit": false
                    },
                    {
                        "score": 0.4320944827589811,
                        "answer": "resolvable",
                        "hit": false
                    },
                    {
                        "score": 0.43131222281670784,
                        "answer": "cursory",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "suggesting"
                ],
                "rank": 9977,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7321194261312485
            },
            {
                "question verbose": "What is to teaching ",
                "b": "teaching",
                "expected answer": [
                    "teaches"
                ],
                "predictions": [
                    {
                        "score": 0.369496229420877,
                        "answer": "passion",
                        "hit": false
                    },
                    {
                        "score": 0.34951437293273174,
                        "answer": "unexpected",
                        "hit": false
                    },
                    {
                        "score": 0.34529287718810336,
                        "answer": "complication",
                        "hit": false
                    },
                    {
                        "score": 0.344229591182702,
                        "answer": "teacher",
                        "hit": false
                    },
                    {
                        "score": 0.34133803687766395,
                        "answer": "rocked",
                        "hit": false
                    },
                    {
                        "score": 0.3404698941177189,
                        "answer": "boston",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "teaching"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5671478435397148
            },
            {
                "question verbose": "What is to telling ",
                "b": "telling",
                "expected answer": [
                    "tells"
                ],
                "predictions": [
                    {
                        "score": 0.39635120326945306,
                        "answer": "worship",
                        "hit": false
                    },
                    {
                        "score": 0.3783922437117459,
                        "answer": "moral",
                        "hit": false
                    },
                    {
                        "score": 0.35266810029420315,
                        "answer": "tweeted",
                        "hit": false
                    },
                    {
                        "score": 0.34702492780282024,
                        "answer": "fanboys",
                        "hit": false
                    },
                    {
                        "score": 0.34691600469607087,
                        "answer": "min",
                        "hit": false
                    },
                    {
                        "score": 0.3420630296864504,
                        "answer": "malaysian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "telling"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5888199359178543
            },
            {
                "question verbose": "What is to thanking ",
                "b": "thanking",
                "expected answer": [
                    "thanks"
                ],
                "predictions": [
                    {
                        "score": 0.5348870658031681,
                        "answer": "advertisement",
                        "hit": false
                    },
                    {
                        "score": 0.4611516496215745,
                        "answer": "martian",
                        "hit": false
                    },
                    {
                        "score": 0.45975215303989375,
                        "answer": "rocked",
                        "hit": false
                    },
                    {
                        "score": 0.45767072676373255,
                        "answer": "tarnished",
                        "hit": false
                    },
                    {
                        "score": 0.438807180169545,
                        "answer": "rebut",
                        "hit": false
                    },
                    {
                        "score": 0.43831570537590225,
                        "answer": "bait",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "thanking"
                ],
                "rank": 11851,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6896533519029617
            },
            {
                "question verbose": "What is to understanding ",
                "b": "understanding",
                "expected answer": [
                    "understands"
                ],
                "predictions": [
                    {
                        "score": 0.3702616405018289,
                        "answer": "denying",
                        "hit": false
                    },
                    {
                        "score": 0.3553988731352366,
                        "answer": "animating",
                        "hit": false
                    },
                    {
                        "score": 0.3440104543502773,
                        "answer": "humane",
                        "hit": false
                    },
                    {
                        "score": 0.34093894322634977,
                        "answer": "theist",
                        "hit": false
                    },
                    {
                        "score": 0.3406771476910252,
                        "answer": "oneness",
                        "hit": false
                    },
                    {
                        "score": 0.3404710035165728,
                        "answer": "narrowly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "understanding"
                ],
                "rank": 5199,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7012058645486832
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I08 [verb_Ving - 3pSg].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "63511f90-4102-4647-bc17-9f70e72cb650",
            "timestamp": "2020-10-22T15:57:41.857289"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to adding ",
                "b": "adding",
                "expected answer": [
                    "added"
                ],
                "predictions": [
                    {
                        "score": 0.3753619105065927,
                        "answer": "trail",
                        "hit": false
                    },
                    {
                        "score": 0.3675710484906349,
                        "answer": "profitable",
                        "hit": false
                    },
                    {
                        "score": 0.3669966475546064,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.36233783670263886,
                        "answer": "fredericksburg",
                        "hit": false
                    },
                    {
                        "score": 0.3554468418167219,
                        "answer": "award",
                        "hit": false
                    },
                    {
                        "score": 0.35484008507090814,
                        "answer": "obrian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adding"
                ],
                "rank": 10461,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6195649430155754
            },
            {
                "question verbose": "What is to agreeing ",
                "b": "agreeing",
                "expected answer": [
                    "agreed"
                ],
                "predictions": [
                    {
                        "score": 0.4853295552687144,
                        "answer": "boehlert",
                        "hit": false
                    },
                    {
                        "score": 0.47642658309581254,
                        "answer": "fairy",
                        "hit": false
                    },
                    {
                        "score": 0.4755186786045722,
                        "answer": "conflate",
                        "hit": false
                    },
                    {
                        "score": 0.4583930800555507,
                        "answer": "kicked",
                        "hit": false
                    },
                    {
                        "score": 0.4543035269056773,
                        "answer": "sherwood",
                        "hit": false
                    },
                    {
                        "score": 0.45250388761422866,
                        "answer": "wolfowitz",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "agreeing"
                ],
                "rank": 10288,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6559378355741501
            },
            {
                "question verbose": "What is to allowing ",
                "b": "allowing",
                "expected answer": [
                    "allowed"
                ],
                "predictions": [
                    {
                        "score": 0.35632641846055946,
                        "answer": "tagged",
                        "hit": false
                    },
                    {
                        "score": 0.34011556133688675,
                        "answer": "hp",
                        "hit": false
                    },
                    {
                        "score": 0.3384851039513511,
                        "answer": "unable",
                        "hit": false
                    },
                    {
                        "score": 0.33669340604949843,
                        "answer": "excite",
                        "hit": false
                    },
                    {
                        "score": 0.33660481880321735,
                        "answer": "sro",
                        "hit": false
                    },
                    {
                        "score": 0.3351650765650056,
                        "answer": "ahles",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allowing"
                ],
                "rank": 12233,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5959311276674271
            },
            {
                "question verbose": "What is to announcing ",
                "b": "announcing",
                "expected answer": [
                    "announced"
                ],
                "predictions": [
                    {
                        "score": 0.5361425429077123,
                        "answer": "mining",
                        "hit": false
                    },
                    {
                        "score": 0.5327861680044443,
                        "answer": "ghent",
                        "hit": false
                    },
                    {
                        "score": 0.518625219348042,
                        "answer": "nobel",
                        "hit": false
                    },
                    {
                        "score": 0.5136988303153004,
                        "answer": "twelve",
                        "hit": false
                    },
                    {
                        "score": 0.5112689141384397,
                        "answer": "quebec",
                        "hit": false
                    },
                    {
                        "score": 0.5088652488913694,
                        "answer": "independence",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "announcing"
                ],
                "rank": 260,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7495482563972473
            },
            {
                "question verbose": "What is to appearing ",
                "b": "appearing",
                "expected answer": [
                    "appeared"
                ],
                "predictions": [
                    {
                        "score": 0.4866721059203169,
                        "answer": "evading",
                        "hit": false
                    },
                    {
                        "score": 0.4744649712607154,
                        "answer": "upheld",
                        "hit": false
                    },
                    {
                        "score": 0.4676394236845423,
                        "answer": "wowww",
                        "hit": false
                    },
                    {
                        "score": 0.45991257956087944,
                        "answer": "marcello",
                        "hit": false
                    },
                    {
                        "score": 0.45266258696022443,
                        "answer": "conflate",
                        "hit": false
                    },
                    {
                        "score": 0.45231742098584093,
                        "answer": "pleathis",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appearing"
                ],
                "rank": 6368,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6872948557138443
            },
            {
                "question verbose": "What is to applying ",
                "b": "applying",
                "expected answer": [
                    "applied"
                ],
                "predictions": [
                    {
                        "score": 0.4600591668322745,
                        "answer": "unopposed",
                        "hit": false
                    },
                    {
                        "score": 0.4501220621798097,
                        "answer": "peripheal",
                        "hit": false
                    },
                    {
                        "score": 0.4480425974060561,
                        "answer": "michlewitz",
                        "hit": false
                    },
                    {
                        "score": 0.4464365977756162,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.44343288875338244,
                        "answer": "vociferous",
                        "hit": false
                    },
                    {
                        "score": 0.44159502916061405,
                        "answer": "acquired",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "applying"
                ],
                "rank": 14121,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7051860392093658
            },
            {
                "question verbose": "What is to appointing ",
                "b": "appointing",
                "expected answer": [
                    "appointed"
                ],
                "predictions": [
                    {
                        "score": 0.32593546444362737,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.2986223736004081,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.2871890959733463,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2855139966729859,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.279184194066049,
                        "answer": "depending",
                        "hit": false
                    },
                    {
                        "score": 0.27631416143244825,
                        "answer": "fewer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appointing"
                ],
                "rank": 8078,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5835117697715759
            },
            {
                "question verbose": "What is to asking ",
                "b": "asking",
                "expected answer": [
                    "asked"
                ],
                "predictions": [
                    {
                        "score": 0.37087553267930073,
                        "answer": "completed",
                        "hit": false
                    },
                    {
                        "score": 0.33825065030381823,
                        "answer": "relate",
                        "hit": false
                    },
                    {
                        "score": 0.3371944627581748,
                        "answer": "viet",
                        "hit": false
                    },
                    {
                        "score": 0.333030848671885,
                        "answer": "michael",
                        "hit": false
                    },
                    {
                        "score": 0.3215197151559941,
                        "answer": "praise",
                        "hit": false
                    },
                    {
                        "score": 0.32116262052464467,
                        "answer": "ecri",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "asking"
                ],
                "rank": 45,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7152826637029648
            },
            {
                "question verbose": "What is to attending ",
                "b": "attending",
                "expected answer": [
                    "attended"
                ],
                "predictions": [
                    {
                        "score": 0.46888264765047827,
                        "answer": "accident",
                        "hit": false
                    },
                    {
                        "score": 0.4620096220973905,
                        "answer": "monica",
                        "hit": false
                    },
                    {
                        "score": 0.4321910949828878,
                        "answer": "commissioner",
                        "hit": false
                    },
                    {
                        "score": 0.4242903264976792,
                        "answer": "nebraska",
                        "hit": false
                    },
                    {
                        "score": 0.41963609572990174,
                        "answer": "birmingham",
                        "hit": false
                    },
                    {
                        "score": 0.4064139585020899,
                        "answer": "rock",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "attending"
                ],
                "rank": 2511,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7223018109798431
            },
            {
                "question verbose": "What is to becoming ",
                "b": "becoming",
                "expected answer": [
                    "became"
                ],
                "predictions": [
                    {
                        "score": 0.4005722480958299,
                        "answer": "noise",
                        "hit": false
                    },
                    {
                        "score": 0.3927975850432449,
                        "answer": "statistic",
                        "hit": false
                    },
                    {
                        "score": 0.3664487372787497,
                        "answer": "obrian",
                        "hit": false
                    },
                    {
                        "score": 0.3630849576623724,
                        "answer": "remained",
                        "hit": false
                    },
                    {
                        "score": 0.36118406128241853,
                        "answer": "discovering",
                        "hit": false
                    },
                    {
                        "score": 0.35580452753326053,
                        "answer": "prime",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "becoming"
                ],
                "rank": 11115,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5821319371461868
            },
            {
                "question verbose": "What is to considering ",
                "b": "considering",
                "expected answer": [
                    "considered"
                ],
                "predictions": [
                    {
                        "score": 0.3994350766331335,
                        "answer": "tnt",
                        "hit": false
                    },
                    {
                        "score": 0.39453142517275813,
                        "answer": "maintained",
                        "hit": false
                    },
                    {
                        "score": 0.3795097011209801,
                        "answer": "swinging",
                        "hit": false
                    },
                    {
                        "score": 0.3760146377694126,
                        "answer": "expounding",
                        "hit": false
                    },
                    {
                        "score": 0.3720537174675839,
                        "answer": "polling",
                        "hit": false
                    },
                    {
                        "score": 0.36754426741076585,
                        "answer": "six",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "considering"
                ],
                "rank": 4260,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6607636362314224
            },
            {
                "question verbose": "What is to containing ",
                "b": "containing",
                "expected answer": [
                    "contained"
                ],
                "predictions": [
                    {
                        "score": 0.5338231010345028,
                        "answer": "arab",
                        "hit": false
                    },
                    {
                        "score": 0.5251897507977662,
                        "answer": "polling",
                        "hit": false
                    },
                    {
                        "score": 0.5046077532336734,
                        "answer": "mellon",
                        "hit": false
                    },
                    {
                        "score": 0.4945690141583009,
                        "answer": "upheld",
                        "hit": false
                    },
                    {
                        "score": 0.49356556773455745,
                        "answer": "vociferous",
                        "hit": false
                    },
                    {
                        "score": 0.4880958628055938,
                        "answer": "legg",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "containing"
                ],
                "rank": 5321,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7896404266357422
            },
            {
                "question verbose": "What is to continuing ",
                "b": "continuing",
                "expected answer": [
                    "continued"
                ],
                "predictions": [
                    {
                        "score": 0.428571805777077,
                        "answer": "harmed",
                        "hit": false
                    },
                    {
                        "score": 0.42053085132274237,
                        "answer": "transition",
                        "hit": false
                    },
                    {
                        "score": 0.415290607874168,
                        "answer": "fascination",
                        "hit": false
                    },
                    {
                        "score": 0.4124452414670342,
                        "answer": "revelation",
                        "hit": false
                    },
                    {
                        "score": 0.4091642775048101,
                        "answer": "undisputed",
                        "hit": false
                    },
                    {
                        "score": 0.40672727307545326,
                        "answer": "revival",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continuing"
                ],
                "rank": 12521,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6368687152862549
            },
            {
                "question verbose": "What is to creating ",
                "b": "creating",
                "expected answer": [
                    "created"
                ],
                "predictions": [
                    {
                        "score": 0.38120141291119136,
                        "answer": "relate",
                        "hit": false
                    },
                    {
                        "score": 0.37583325720934413,
                        "answer": "privy",
                        "hit": false
                    },
                    {
                        "score": 0.36961452341915174,
                        "answer": "peril",
                        "hit": false
                    },
                    {
                        "score": 0.3676383451038761,
                        "answer": "discourse",
                        "hit": false
                    },
                    {
                        "score": 0.36672787877836066,
                        "answer": "liable",
                        "hit": false
                    },
                    {
                        "score": 0.3532698178775614,
                        "answer": "butazolidan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "creating"
                ],
                "rank": 4392,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.639451265335083
            },
            {
                "question verbose": "What is to deciding ",
                "b": "deciding",
                "expected answer": [
                    "decided"
                ],
                "predictions": [
                    {
                        "score": 0.42112584509259054,
                        "answer": "legal",
                        "hit": false
                    },
                    {
                        "score": 0.4078006017094646,
                        "answer": "ramification",
                        "hit": false
                    },
                    {
                        "score": 0.4000025064508696,
                        "answer": "programme",
                        "hit": false
                    },
                    {
                        "score": 0.3927841253270009,
                        "answer": "vociferous",
                        "hit": false
                    },
                    {
                        "score": 0.39174058574687487,
                        "answer": "disseminating",
                        "hit": false
                    },
                    {
                        "score": 0.39009739037274216,
                        "answer": "handed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deciding"
                ],
                "rank": 5680,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6449117362499237
            },
            {
                "question verbose": "What is to describing ",
                "b": "describing",
                "expected answer": [
                    "described"
                ],
                "predictions": [
                    {
                        "score": 0.4949522515476001,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.47904362863612365,
                        "answer": "damaging",
                        "hit": false
                    },
                    {
                        "score": 0.47401936139886813,
                        "answer": "advocate",
                        "hit": false
                    },
                    {
                        "score": 0.46776441959441034,
                        "answer": "potent",
                        "hit": false
                    },
                    {
                        "score": 0.4538267610183252,
                        "answer": "transferred",
                        "hit": false
                    },
                    {
                        "score": 0.4528274925101912,
                        "answer": "finance",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "describing"
                ],
                "rank": 3898,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7391836792230606
            },
            {
                "question verbose": "What is to developing ",
                "b": "developing",
                "expected answer": [
                    "developed"
                ],
                "predictions": [
                    {
                        "score": 0.41727164403722694,
                        "answer": "associated",
                        "hit": false
                    },
                    {
                        "score": 0.411211664868987,
                        "answer": "supervising",
                        "hit": false
                    },
                    {
                        "score": 0.4027341682412014,
                        "answer": "melt",
                        "hit": false
                    },
                    {
                        "score": 0.39995861535868354,
                        "answer": "barbados",
                        "hit": false
                    },
                    {
                        "score": 0.38838711515802965,
                        "answer": "economic",
                        "hit": false
                    },
                    {
                        "score": 0.3856420449559225,
                        "answer": "annual",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "developing"
                ],
                "rank": 8882,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6961470544338226
            },
            {
                "question verbose": "What is to establishing ",
                "b": "establishing",
                "expected answer": [
                    "established"
                ],
                "predictions": [
                    {
                        "score": 0.5935150832863916,
                        "answer": "wrapped",
                        "hit": false
                    },
                    {
                        "score": 0.5823086985459988,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.5787324922193738,
                        "answer": "bp",
                        "hit": false
                    },
                    {
                        "score": 0.5741101343022443,
                        "answer": "sticky",
                        "hit": false
                    },
                    {
                        "score": 0.5615216152887126,
                        "answer": "greenwich",
                        "hit": false
                    },
                    {
                        "score": 0.5609210548050187,
                        "answer": "enveloping",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "establishing"
                ],
                "rank": 15011,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7051859050989151
            },
            {
                "question verbose": "What is to existing ",
                "b": "existing",
                "expected answer": [
                    "existed"
                ],
                "predictions": [
                    {
                        "score": 0.41611679078388153,
                        "answer": "owned",
                        "hit": false
                    },
                    {
                        "score": 0.4101884037894547,
                        "answer": "lesser",
                        "hit": false
                    },
                    {
                        "score": 0.4079182903850501,
                        "answer": "donated",
                        "hit": false
                    },
                    {
                        "score": 0.4049944059983224,
                        "answer": "controlled",
                        "hit": false
                    },
                    {
                        "score": 0.4037511510416683,
                        "answer": "peril",
                        "hit": false
                    },
                    {
                        "score": 0.4020602089151157,
                        "answer": "accepted",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "existing"
                ],
                "rank": 4324,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6646809577941895
            },
            {
                "question verbose": "What is to expecting ",
                "b": "expecting",
                "expected answer": [
                    "expected"
                ],
                "predictions": [
                    {
                        "score": 0.6032882306216529,
                        "answer": "legg",
                        "hit": false
                    },
                    {
                        "score": 0.5813269038874076,
                        "answer": "floored",
                        "hit": false
                    },
                    {
                        "score": 0.5711495630080705,
                        "answer": "upheld",
                        "hit": false
                    },
                    {
                        "score": 0.5680622591087866,
                        "answer": "treaty",
                        "hit": false
                    },
                    {
                        "score": 0.5615625518824207,
                        "answer": "chemist",
                        "hit": false
                    },
                    {
                        "score": 0.5609405327242885,
                        "answer": "outcry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expecting"
                ],
                "rank": 2530,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7906807065010071
            },
            {
                "question verbose": "What is to failing ",
                "b": "failing",
                "expected answer": [
                    "failed"
                ],
                "predictions": [
                    {
                        "score": 0.43551497674343426,
                        "answer": "lend",
                        "hit": false
                    },
                    {
                        "score": 0.4331507335461582,
                        "answer": "homer",
                        "hit": false
                    },
                    {
                        "score": 0.42548603183895095,
                        "answer": "hideous",
                        "hit": false
                    },
                    {
                        "score": 0.4218368702249107,
                        "answer": "wowww",
                        "hit": false
                    },
                    {
                        "score": 0.41881477652573706,
                        "answer": "tougher",
                        "hit": false
                    },
                    {
                        "score": 0.4120779388475267,
                        "answer": "jumbled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "failing"
                ],
                "rank": 15113,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6036241427063942
            },
            {
                "question verbose": "What is to following ",
                "b": "following",
                "expected answer": [
                    "followed"
                ],
                "predictions": [
                    {
                        "score": 0.3563152569872282,
                        "answer": "ghent",
                        "hit": false
                    },
                    {
                        "score": 0.3443434549425585,
                        "answer": "bottled",
                        "hit": false
                    },
                    {
                        "score": 0.3419819635225634,
                        "answer": "maintained",
                        "hit": false
                    },
                    {
                        "score": 0.3354153574300864,
                        "answer": "bp",
                        "hit": false
                    },
                    {
                        "score": 0.3311642544046798,
                        "answer": "kosresearch",
                        "hit": false
                    },
                    {
                        "score": 0.32920131768085276,
                        "answer": "workshop",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "following"
                ],
                "rank": 13730,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5496388524770737
            },
            {
                "question verbose": "What is to hearing ",
                "b": "hearing",
                "expected answer": [
                    "heard"
                ],
                "predictions": [
                    {
                        "score": 0.43454721354590276,
                        "answer": "gustafson",
                        "hit": false
                    },
                    {
                        "score": 0.41383590535840276,
                        "answer": "privatizing",
                        "hit": false
                    },
                    {
                        "score": 0.40355078502147396,
                        "answer": "revival",
                        "hit": false
                    },
                    {
                        "score": 0.3971833879485591,
                        "answer": "peril",
                        "hit": false
                    },
                    {
                        "score": 0.39151841836512746,
                        "answer": "gallantly",
                        "hit": false
                    },
                    {
                        "score": 0.3906935174754243,
                        "answer": "demanded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hearing"
                ],
                "rank": 1889,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6474763005971909
            },
            {
                "question verbose": "What is to improving ",
                "b": "improving",
                "expected answer": [
                    "improved"
                ],
                "predictions": [
                    {
                        "score": 0.42392358134016705,
                        "answer": "eve",
                        "hit": false
                    },
                    {
                        "score": 0.41831762154623087,
                        "answer": "pd",
                        "hit": false
                    },
                    {
                        "score": 0.41163755908921856,
                        "answer": "bolt",
                        "hit": false
                    },
                    {
                        "score": 0.3967440676492599,
                        "answer": "identical",
                        "hit": false
                    },
                    {
                        "score": 0.39434392266014373,
                        "answer": "twin",
                        "hit": false
                    },
                    {
                        "score": 0.3935501339983753,
                        "answer": "suicide",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "improving"
                ],
                "rank": 9788,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6772523373365402
            },
            {
                "question verbose": "What is to including ",
                "b": "including",
                "expected answer": [
                    "included"
                ],
                "predictions": [
                    {
                        "score": 0.4187990733582939,
                        "answer": "june",
                        "hit": false
                    },
                    {
                        "score": 0.4049173820215032,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.39231945738661006,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.38952092429312696,
                        "answer": "pair",
                        "hit": false
                    },
                    {
                        "score": 0.387379309171756,
                        "answer": "barbados",
                        "hit": false
                    },
                    {
                        "score": 0.38282628620467574,
                        "answer": "veterinarian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "including"
                ],
                "rank": 13302,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5808409452438354
            },
            {
                "question verbose": "What is to introducing ",
                "b": "introducing",
                "expected answer": [
                    "introduced"
                ],
                "predictions": [
                    {
                        "score": 0.20305721489352296,
                        "answer": "miranda",
                        "hit": false
                    },
                    {
                        "score": 0.1863788479380443,
                        "answer": "gravelyvoicejim",
                        "hit": false
                    },
                    {
                        "score": 0.1788565232898,
                        "answer": "pmd",
                        "hit": false
                    },
                    {
                        "score": 0.15636695096766054,
                        "answer": "vnf",
                        "hit": false
                    },
                    {
                        "score": 0.1491088557737539,
                        "answer": "arthurjake",
                        "hit": false
                    },
                    {
                        "score": 0.14875132677845931,
                        "answer": "banned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "introducing"
                ],
                "rank": 9852,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4868979509919882
            },
            {
                "question verbose": "What is to involving ",
                "b": "involving",
                "expected answer": [
                    "involved"
                ],
                "predictions": [
                    {
                        "score": 0.37993081646785526,
                        "answer": "property",
                        "hit": false
                    },
                    {
                        "score": 0.3588949250071808,
                        "answer": "transferred",
                        "hit": false
                    },
                    {
                        "score": 0.358601628986455,
                        "answer": "sic",
                        "hit": false
                    },
                    {
                        "score": 0.35138403003785057,
                        "answer": "acquired",
                        "hit": false
                    },
                    {
                        "score": 0.3482926942355709,
                        "answer": "stonington",
                        "hit": false
                    },
                    {
                        "score": 0.3481169735643737,
                        "answer": "praising",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involving"
                ],
                "rank": 5425,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7472778707742691
            },
            {
                "question verbose": "What is to locating ",
                "b": "locating",
                "expected answer": [
                    "located"
                ],
                "predictions": [
                    {
                        "score": 0.3212004400082479,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.29466221698467754,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.2777237019845756,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2739632818222215,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.27270014910584867,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.2668421290476151,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locating"
                ],
                "rank": 10070,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5709576308727264
            },
            {
                "question verbose": "What is to losing ",
                "b": "losing",
                "expected answer": [
                    "lost"
                ],
                "predictions": [
                    {
                        "score": 0.4002300276046313,
                        "answer": "accident",
                        "hit": false
                    },
                    {
                        "score": 0.3616360198363499,
                        "answer": "confidant",
                        "hit": false
                    },
                    {
                        "score": 0.35915635943709734,
                        "answer": "tumble",
                        "hit": false
                    },
                    {
                        "score": 0.35421483752115,
                        "answer": "imminent",
                        "hit": false
                    },
                    {
                        "score": 0.35098099772159624,
                        "answer": "transfer",
                        "hit": false
                    },
                    {
                        "score": 0.34168687434659994,
                        "answer": "stair",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "losing"
                ],
                "rank": 10064,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6372756063938141
            },
            {
                "question verbose": "What is to managing ",
                "b": "managing",
                "expected answer": [
                    "managed"
                ],
                "predictions": [
                    {
                        "score": 0.4279880977219429,
                        "answer": "corrupt",
                        "hit": false
                    },
                    {
                        "score": 0.4198944079723627,
                        "answer": "independence",
                        "hit": false
                    },
                    {
                        "score": 0.4126574364722457,
                        "answer": "legg",
                        "hit": false
                    },
                    {
                        "score": 0.4125294963629615,
                        "answer": "swap",
                        "hit": false
                    },
                    {
                        "score": 0.4099947393492133,
                        "answer": "chamber",
                        "hit": false
                    },
                    {
                        "score": 0.40994558604343817,
                        "answer": "iraqi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "managing"
                ],
                "rank": 13489,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5777646601200104
            },
            {
                "question verbose": "What is to marrying ",
                "b": "marrying",
                "expected answer": [
                    "married"
                ],
                "predictions": [
                    {
                        "score": 0.3285431962042056,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.30142808014050154,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.2993869624872458,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.28900137775528534,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.2821869681972631,
                        "answer": "sentinel",
                        "hit": false
                    },
                    {
                        "score": 0.2815067977192463,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marrying"
                ],
                "rank": 11057,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5751892998814583
            },
            {
                "question verbose": "What is to operating ",
                "b": "operating",
                "expected answer": [
                    "operated"
                ],
                "predictions": [
                    {
                        "score": 0.4153965883038978,
                        "answer": "margin",
                        "hit": false
                    },
                    {
                        "score": 0.39846314934451016,
                        "answer": "suscription",
                        "hit": false
                    },
                    {
                        "score": 0.3961671885761735,
                        "answer": "statistic",
                        "hit": false
                    },
                    {
                        "score": 0.39484577081048183,
                        "answer": "annual",
                        "hit": false
                    },
                    {
                        "score": 0.39442552735278136,
                        "answer": "contribution",
                        "hit": false
                    },
                    {
                        "score": 0.39201631057713693,
                        "answer": "ratio",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "operating"
                ],
                "rank": 433,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7551829218864441
            },
            {
                "question verbose": "What is to performing ",
                "b": "performing",
                "expected answer": [
                    "performed"
                ],
                "predictions": [
                    {
                        "score": 0.40613371677498905,
                        "answer": "revival",
                        "hit": false
                    },
                    {
                        "score": 0.3983161419009908,
                        "answer": "bp",
                        "hit": false
                    },
                    {
                        "score": 0.39555732381098124,
                        "answer": "sticky",
                        "hit": false
                    },
                    {
                        "score": 0.3914255207217141,
                        "answer": "intervening",
                        "hit": false
                    },
                    {
                        "score": 0.39010901999203745,
                        "answer": "obrian",
                        "hit": false
                    },
                    {
                        "score": 0.3859511702783283,
                        "answer": "swap",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "performing"
                ],
                "rank": 12127,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.757535845041275
            },
            {
                "question verbose": "What is to proposing ",
                "b": "proposing",
                "expected answer": [
                    "proposed"
                ],
                "predictions": [
                    {
                        "score": 0.4242495573889236,
                        "answer": "sticky",
                        "hit": false
                    },
                    {
                        "score": 0.41091792249305303,
                        "answer": "thawing",
                        "hit": false
                    },
                    {
                        "score": 0.40645368923555686,
                        "answer": "swinging",
                        "hit": false
                    },
                    {
                        "score": 0.40274274670078525,
                        "answer": "athleticism",
                        "hit": false
                    },
                    {
                        "score": 0.4015113530106745,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.4008419651152831,
                        "answer": "treaty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "proposing"
                ],
                "rank": 3647,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7357346266508102
            },
            {
                "question verbose": "What is to providing ",
                "b": "providing",
                "expected answer": [
                    "provided"
                ],
                "predictions": [
                    {
                        "score": 0.4282915620976707,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.4044662942382809,
                        "answer": "collective",
                        "hit": false
                    },
                    {
                        "score": 0.382479580046066,
                        "answer": "oversight",
                        "hit": false
                    },
                    {
                        "score": 0.3677909189758886,
                        "answer": "revival",
                        "hit": false
                    },
                    {
                        "score": 0.35774783164837926,
                        "answer": "located",
                        "hit": false
                    },
                    {
                        "score": 0.3569256801276062,
                        "answer": "divide",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "providing"
                ],
                "rank": 12768,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6758823245763779
            },
            {
                "question verbose": "What is to publishing ",
                "b": "publishing",
                "expected answer": [
                    "published"
                ],
                "predictions": [
                    {
                        "score": 0.4467961397314845,
                        "answer": "indusrty",
                        "hit": false
                    },
                    {
                        "score": 0.43860936419164354,
                        "answer": "advocate",
                        "hit": false
                    },
                    {
                        "score": 0.43657478087789275,
                        "answer": "trump",
                        "hit": false
                    },
                    {
                        "score": 0.4318230314825494,
                        "answer": "egyptian",
                        "hit": false
                    },
                    {
                        "score": 0.430951789015197,
                        "answer": "upheld",
                        "hit": false
                    },
                    {
                        "score": 0.4237513275192513,
                        "answer": "ncaa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publishing"
                ],
                "rank": 1679,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.68892702460289
            },
            {
                "question verbose": "What is to receiving ",
                "b": "receiving",
                "expected answer": [
                    "received"
                ],
                "predictions": [
                    {
                        "score": 0.534008471802567,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.5221716633674117,
                        "answer": "sticky",
                        "hit": false
                    },
                    {
                        "score": 0.5144119477850113,
                        "answer": "mama",
                        "hit": false
                    },
                    {
                        "score": 0.5136147892544585,
                        "answer": "profitable",
                        "hit": false
                    },
                    {
                        "score": 0.5120354031036158,
                        "answer": "poem",
                        "hit": false
                    },
                    {
                        "score": 0.4864286003524197,
                        "answer": "bungalow",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receiving"
                ],
                "rank": 9815,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6596288532018661
            },
            {
                "question verbose": "What is to reducing ",
                "b": "reducing",
                "expected answer": [
                    "reduced"
                ],
                "predictions": [
                    {
                        "score": 0.536734186449376,
                        "answer": "kosresearch",
                        "hit": false
                    },
                    {
                        "score": 0.5105998341841653,
                        "answer": "legg",
                        "hit": false
                    },
                    {
                        "score": 0.489891082312177,
                        "answer": "flipcharts",
                        "hit": false
                    },
                    {
                        "score": 0.4884935555582803,
                        "answer": "barbados",
                        "hit": false
                    },
                    {
                        "score": 0.48795660501664456,
                        "answer": "wrapped",
                        "hit": false
                    },
                    {
                        "score": 0.48161995253390477,
                        "answer": "temperature",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reducing"
                ],
                "rank": 1062,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8200662434101105
            },
            {
                "question verbose": "What is to relating ",
                "b": "relating",
                "expected answer": [
                    "related"
                ],
                "predictions": [
                    {
                        "score": 0.4805166140439122,
                        "answer": "settle",
                        "hit": false
                    },
                    {
                        "score": 0.46670236718093044,
                        "answer": "peril",
                        "hit": false
                    },
                    {
                        "score": 0.45705525443152284,
                        "answer": "upheld",
                        "hit": false
                    },
                    {
                        "score": 0.4512159080992652,
                        "answer": "fixed",
                        "hit": false
                    },
                    {
                        "score": 0.45021969367156484,
                        "answer": "undisputed",
                        "hit": false
                    },
                    {
                        "score": 0.44580145794192,
                        "answer": "vacancy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "relating"
                ],
                "rank": 14895,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6603416353464127
            },
            {
                "question verbose": "What is to remaining ",
                "b": "remaining",
                "expected answer": [
                    "remained"
                ],
                "predictions": [
                    {
                        "score": 0.4341922968414613,
                        "answer": "opting",
                        "hit": false
                    },
                    {
                        "score": 0.43366204528237234,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.4319616803924229,
                        "answer": "tomorrow",
                        "hit": false
                    },
                    {
                        "score": 0.4146347610193729,
                        "answer": "consecutive",
                        "hit": false
                    },
                    {
                        "score": 0.41038327595747237,
                        "answer": "prioritizes",
                        "hit": false
                    },
                    {
                        "score": 0.40350415127081546,
                        "answer": "unopposed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remaining"
                ],
                "rank": 11519,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6085167154669762
            },
            {
                "question verbose": "What is to replacing ",
                "b": "replacing",
                "expected answer": [
                    "replaced"
                ],
                "predictions": [
                    {
                        "score": 0.6107879927752107,
                        "answer": "bungalow",
                        "hit": false
                    },
                    {
                        "score": 0.593171994837628,
                        "answer": "grimy",
                        "hit": false
                    },
                    {
                        "score": 0.5758409858836638,
                        "answer": "poem",
                        "hit": false
                    },
                    {
                        "score": 0.5756088856085085,
                        "answer": "tumble",
                        "hit": false
                    },
                    {
                        "score": 0.5679651932226214,
                        "answer": "peripheal",
                        "hit": false
                    },
                    {
                        "score": 0.5628783781512513,
                        "answer": "traveled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "replacing"
                ],
                "rank": 14552,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.730993926525116
            },
            {
                "question verbose": "What is to representing ",
                "b": "representing",
                "expected answer": [
                    "represented"
                ],
                "predictions": [
                    {
                        "score": 0.42126425520154215,
                        "answer": "demanded",
                        "hit": false
                    },
                    {
                        "score": 0.40978654940829645,
                        "answer": "bnat",
                        "hit": false
                    },
                    {
                        "score": 0.400291699987111,
                        "answer": "accumulate",
                        "hit": false
                    },
                    {
                        "score": 0.3959804606729987,
                        "answer": "pleathis",
                        "hit": false
                    },
                    {
                        "score": 0.39518250186111403,
                        "answer": "tanzania",
                        "hit": false
                    },
                    {
                        "score": 0.39471860777370527,
                        "answer": "monthly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "representing"
                ],
                "rank": 6180,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6882021129131317
            },
            {
                "question verbose": "What is to requiring ",
                "b": "requiring",
                "expected answer": [
                    "required"
                ],
                "predictions": [
                    {
                        "score": 0.3169553805076939,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.3011836331486099,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.29401012146972466,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.29058304232857957,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.28726431362542637,
                        "answer": "depending",
                        "hit": false
                    },
                    {
                        "score": 0.2866812693036894,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "requiring"
                ],
                "rank": 4991,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6065603941679001
            },
            {
                "question verbose": "What is to sending ",
                "b": "sending",
                "expected answer": [
                    "sent"
                ],
                "predictions": [
                    {
                        "score": 0.4671356859220508,
                        "answer": "relate",
                        "hit": false
                    },
                    {
                        "score": 0.42975255460264244,
                        "answer": "peppertree",
                        "hit": false
                    },
                    {
                        "score": 0.420268783429705,
                        "answer": "twisted",
                        "hit": false
                    },
                    {
                        "score": 0.3996048061898497,
                        "answer": "became",
                        "hit": false
                    },
                    {
                        "score": 0.38847931896994653,
                        "answer": "canada",
                        "hit": false
                    },
                    {
                        "score": 0.38719218839803704,
                        "answer": "nebraska",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sending"
                ],
                "rank": 336,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6999224424362183
            },
            {
                "question verbose": "What is to spending ",
                "b": "spending",
                "expected answer": [
                    "spent"
                ],
                "predictions": [
                    {
                        "score": 0.45652923147255353,
                        "answer": "fiscal",
                        "hit": false
                    },
                    {
                        "score": 0.42674540650822423,
                        "answer": "pushed",
                        "hit": false
                    },
                    {
                        "score": 0.399214902131899,
                        "answer": "bush",
                        "hit": false
                    },
                    {
                        "score": 0.3990665075237751,
                        "answer": "february",
                        "hit": false
                    },
                    {
                        "score": 0.38638088750615557,
                        "answer": "balanced",
                        "hit": false
                    },
                    {
                        "score": 0.38616383667441445,
                        "answer": "congressional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spending"
                ],
                "rank": 64,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7166740894317627
            },
            {
                "question verbose": "What is to suffering ",
                "b": "suffering",
                "expected answer": [
                    "suffered"
                ],
                "predictions": [
                    {
                        "score": 0.46881553258972686,
                        "answer": "peril",
                        "hit": false
                    },
                    {
                        "score": 0.448284856624097,
                        "answer": "eternal",
                        "hit": false
                    },
                    {
                        "score": 0.44618848232836067,
                        "answer": "upheld",
                        "hit": false
                    },
                    {
                        "score": 0.4436834735048986,
                        "answer": "imminent",
                        "hit": false
                    },
                    {
                        "score": 0.44256601912856497,
                        "answer": "conflate",
                        "hit": false
                    },
                    {
                        "score": 0.44166104214952046,
                        "answer": "mph",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "suffering"
                ],
                "rank": 723,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7606357932090759
            },
            {
                "question verbose": "What is to teaching ",
                "b": "teaching",
                "expected answer": [
                    "taught"
                ],
                "predictions": [
                    {
                        "score": 0.418182662250096,
                        "answer": "rhode",
                        "hit": false
                    },
                    {
                        "score": 0.417709459898135,
                        "answer": "expertise",
                        "hit": false
                    },
                    {
                        "score": 0.40149985175378655,
                        "answer": "absence",
                        "hit": false
                    },
                    {
                        "score": 0.4010961947101901,
                        "answer": "fairy",
                        "hit": false
                    },
                    {
                        "score": 0.38548085349626365,
                        "answer": "traveled",
                        "hit": false
                    },
                    {
                        "score": 0.37536751928728807,
                        "answer": "invited",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "teaching"
                ],
                "rank": 4625,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6985800564289093
            },
            {
                "question verbose": "What is to telling ",
                "b": "telling",
                "expected answer": [
                    "told"
                ],
                "predictions": [
                    {
                        "score": 0.4090604605736902,
                        "answer": "brother",
                        "hit": false
                    },
                    {
                        "score": 0.40866335417612304,
                        "answer": "sister",
                        "hit": false
                    },
                    {
                        "score": 0.3975901356449934,
                        "answer": "period",
                        "hit": false
                    },
                    {
                        "score": 0.39384379107605116,
                        "answer": "tumble",
                        "hit": false
                    },
                    {
                        "score": 0.3888026861113244,
                        "answer": "leftover",
                        "hit": false
                    },
                    {
                        "score": 0.3875082239496747,
                        "answer": "peripheal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "telling"
                ],
                "rank": 6884,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6469299644231796
            },
            {
                "question verbose": "What is to understanding ",
                "b": "understanding",
                "expected answer": [
                    "understood"
                ],
                "predictions": [
                    {
                        "score": 0.33463253169388085,
                        "answer": "algal",
                        "hit": false
                    },
                    {
                        "score": 0.3312124074267394,
                        "answer": "collective",
                        "hit": false
                    },
                    {
                        "score": 0.33099699883834977,
                        "answer": "champagne",
                        "hit": false
                    },
                    {
                        "score": 0.3131041850407642,
                        "answer": "persuded",
                        "hit": false
                    },
                    {
                        "score": 0.3080555261997218,
                        "answer": "opic",
                        "hit": false
                    },
                    {
                        "score": 0.30659819888109047,
                        "answer": "culture",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "understanding"
                ],
                "rank": 12725,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6968267858028412
            },
            {
                "question verbose": "What is to uniting ",
                "b": "uniting",
                "expected answer": [
                    "united"
                ],
                "predictions": [
                    {
                        "score": 0.3153563134433524,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.28806922717121936,
                        "answer": "trailer",
                        "hit": false
                    },
                    {
                        "score": 0.28391653353151286,
                        "answer": "speculating",
                        "hit": false
                    },
                    {
                        "score": 0.28389400179971175,
                        "answer": "disappear",
                        "hit": false
                    },
                    {
                        "score": 0.27647053858099174,
                        "answer": "depending",
                        "hit": false
                    },
                    {
                        "score": 0.27161060757722205,
                        "answer": "fewer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "uniting"
                ],
                "rank": 1528,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5979825481772423
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I09 [verb_Ving - Ved].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "e2aba43f-a5ed-4129-bee6-029cc735c751",
            "timestamp": "2020-10-22T15:57:43.444537"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to adds ",
                "b": "adds",
                "expected answer": [
                    "added"
                ],
                "predictions": [
                    {
                        "score": 0.31634531656181836,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2757858095212265,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.26429203166229825,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.2592760888345965,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.2555117245876286,
                        "answer": "quarter",
                        "hit": false
                    },
                    {
                        "score": 0.25476756146414936,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "adds"
                ],
                "rank": 9878,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5775802358984947
            },
            {
                "question verbose": "What is to agrees ",
                "b": "agrees",
                "expected answer": [
                    "agreed"
                ],
                "predictions": [
                    {
                        "score": 0.5868542619764516,
                        "answer": "oversees",
                        "hit": false
                    },
                    {
                        "score": 0.5841070653560099,
                        "answer": "arizona",
                        "hit": false
                    },
                    {
                        "score": 0.5762899819902998,
                        "answer": "postmitochondrial",
                        "hit": false
                    },
                    {
                        "score": 0.5748018639036521,
                        "answer": "philadelphia",
                        "hit": false
                    },
                    {
                        "score": 0.5692164638533657,
                        "answer": "nasrc",
                        "hit": false
                    },
                    {
                        "score": 0.5689600874426968,
                        "answer": "morale",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "agrees"
                ],
                "rank": 200,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7883330583572388
            },
            {
                "question verbose": "What is to allows ",
                "b": "allows",
                "expected answer": [
                    "allowed"
                ],
                "predictions": [
                    {
                        "score": 0.38284495205113117,
                        "answer": "curiosity",
                        "hit": false
                    },
                    {
                        "score": 0.36807769572894883,
                        "answer": "melty",
                        "hit": false
                    },
                    {
                        "score": 0.35820113607953163,
                        "answer": "worn",
                        "hit": false
                    },
                    {
                        "score": 0.34823623317542557,
                        "answer": "bean",
                        "hit": false
                    },
                    {
                        "score": 0.34774787539553104,
                        "answer": "image",
                        "hit": false
                    },
                    {
                        "score": 0.34556527021314,
                        "answer": "fainters",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allows"
                ],
                "rank": 11705,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5735064148902893
            },
            {
                "question verbose": "What is to announces ",
                "b": "announces",
                "expected answer": [
                    "announced"
                ],
                "predictions": [
                    {
                        "score": 0.30791448892184947,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27017183106299214,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.25929548337508757,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.2506228377670369,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.24798587457840082,
                        "answer": "leftover",
                        "hit": false
                    },
                    {
                        "score": 0.24506132253315002,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "announces"
                ],
                "rank": 1017,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5957832857966423
            },
            {
                "question verbose": "What is to appears ",
                "b": "appears",
                "expected answer": [
                    "appeared"
                ],
                "predictions": [
                    {
                        "score": 0.4203541032667843,
                        "answer": "lesser",
                        "hit": false
                    },
                    {
                        "score": 0.41137306814463404,
                        "answer": "awarded",
                        "hit": false
                    },
                    {
                        "score": 0.40934277084510284,
                        "answer": "funding",
                        "hit": false
                    },
                    {
                        "score": 0.4069406315922048,
                        "answer": "boost",
                        "hit": false
                    },
                    {
                        "score": 0.3976156646943989,
                        "answer": "issued",
                        "hit": false
                    },
                    {
                        "score": 0.3899442917351827,
                        "answer": "formally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appears"
                ],
                "rank": 10156,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5912636816501617
            },
            {
                "question verbose": "What is to applies ",
                "b": "applies",
                "expected answer": [
                    "applied"
                ],
                "predictions": [
                    {
                        "score": 0.39096718828454624,
                        "answer": "polling",
                        "hit": false
                    },
                    {
                        "score": 0.3698794232774804,
                        "answer": "pounding",
                        "hit": false
                    },
                    {
                        "score": 0.3660170775342108,
                        "answer": "relatively",
                        "hit": false
                    },
                    {
                        "score": 0.35718850211843867,
                        "answer": "mesa",
                        "hit": false
                    },
                    {
                        "score": 0.3568960867624853,
                        "answer": "geneva",
                        "hit": false
                    },
                    {
                        "score": 0.3527136782449171,
                        "answer": "required",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "applies"
                ],
                "rank": 2371,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.730294331908226
            },
            {
                "question verbose": "What is to appoints ",
                "b": "appoints",
                "expected answer": [
                    "appointed"
                ],
                "predictions": [
                    {
                        "score": 0.3098293534783694,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27521758497637905,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.26258835521273455,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.24932259981448615,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.2492014693003425,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.24908354761021648,
                        "answer": "five",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "appoints"
                ],
                "rank": 7367,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5835117697715759
            },
            {
                "question verbose": "What is to asks ",
                "b": "asks",
                "expected answer": [
                    "asked"
                ],
                "predictions": [
                    {
                        "score": 0.44315858409382514,
                        "answer": "detail",
                        "hit": false
                    },
                    {
                        "score": 0.42621107503052535,
                        "answer": "rove",
                        "hit": false
                    },
                    {
                        "score": 0.4138737451763678,
                        "answer": "rocksteady",
                        "hit": false
                    },
                    {
                        "score": 0.4113450152713977,
                        "answer": "biden",
                        "hit": false
                    },
                    {
                        "score": 0.4084550851359025,
                        "answer": "questioner",
                        "hit": false
                    },
                    {
                        "score": 0.4062571908728024,
                        "answer": "kubrick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "asks"
                ],
                "rank": 367,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.71214659512043
            },
            {
                "question verbose": "What is to becomes ",
                "b": "becomes",
                "expected answer": [
                    "became"
                ],
                "predictions": [
                    {
                        "score": 0.3915001280128916,
                        "answer": "volatile",
                        "hit": false
                    },
                    {
                        "score": 0.3785400777699608,
                        "answer": "pace",
                        "hit": false
                    },
                    {
                        "score": 0.37507972331282613,
                        "answer": "manageable",
                        "hit": false
                    },
                    {
                        "score": 0.3727253876043233,
                        "answer": "ramification",
                        "hit": false
                    },
                    {
                        "score": 0.36568155793452306,
                        "answer": "rate",
                        "hit": false
                    },
                    {
                        "score": 0.3614153092403368,
                        "answer": "ytd",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "becomes"
                ],
                "rank": 14449,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5510138012468815
            },
            {
                "question verbose": "What is to believes ",
                "b": "believes",
                "expected answer": [
                    "believed"
                ],
                "predictions": [
                    {
                        "score": 0.31056937308111937,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.260018416066471,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.25555867787080117,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.24743642064071494,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.24666855145155073,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.2458820132543329,
                        "answer": "quarter",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "believes"
                ],
                "rank": 14984,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5216725803911686
            },
            {
                "question verbose": "What is to considers ",
                "b": "considers",
                "expected answer": [
                    "considered"
                ],
                "predictions": [
                    {
                        "score": 0.570989060290039,
                        "answer": "january",
                        "hit": false
                    },
                    {
                        "score": 0.5547959846770266,
                        "answer": "revelation",
                        "hit": false
                    },
                    {
                        "score": 0.5541509082893223,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.5527377062127329,
                        "answer": "discriminatory",
                        "hit": false
                    },
                    {
                        "score": 0.5524233326628742,
                        "answer": "veterinarian",
                        "hit": false
                    },
                    {
                        "score": 0.5486340594375152,
                        "answer": "philadelphia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "considers"
                ],
                "rank": 6456,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7363200634717941
            },
            {
                "question verbose": "What is to consists ",
                "b": "consists",
                "expected answer": [
                    "consisted"
                ],
                "predictions": [
                    {
                        "score": 0.4662057182123942,
                        "answer": "spreading",
                        "hit": false
                    },
                    {
                        "score": 0.4605271915380992,
                        "answer": "individualized",
                        "hit": false
                    },
                    {
                        "score": 0.4512512400016248,
                        "answer": "controlled",
                        "hit": false
                    },
                    {
                        "score": 0.449144337439499,
                        "answer": "differs",
                        "hit": false
                    },
                    {
                        "score": 0.44856234259848066,
                        "answer": "doubled",
                        "hit": false
                    },
                    {
                        "score": 0.44194848654408836,
                        "answer": "formally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "consists"
                ],
                "rank": 1776,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8445214629173279
            },
            {
                "question verbose": "What is to contains ",
                "b": "contains",
                "expected answer": [
                    "contained"
                ],
                "predictions": [
                    {
                        "score": 0.4445100622443543,
                        "answer": "expertise",
                        "hit": false
                    },
                    {
                        "score": 0.44339036042058144,
                        "answer": "utilized",
                        "hit": false
                    },
                    {
                        "score": 0.4291222692744302,
                        "answer": "egypt",
                        "hit": false
                    },
                    {
                        "score": 0.42807556463806323,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.4259323360276266,
                        "answer": "gardening",
                        "hit": false
                    },
                    {
                        "score": 0.42166675032303985,
                        "answer": "linked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "contains"
                ],
                "rank": 14406,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6515049338340759
            },
            {
                "question verbose": "What is to continues ",
                "b": "continues",
                "expected answer": [
                    "continued"
                ],
                "predictions": [
                    {
                        "score": 0.4301457420787949,
                        "answer": "ap",
                        "hit": false
                    },
                    {
                        "score": 0.40839227758715324,
                        "answer": "formally",
                        "hit": false
                    },
                    {
                        "score": 0.39969291211326624,
                        "answer": "contraceptive",
                        "hit": false
                    },
                    {
                        "score": 0.399589114813361,
                        "answer": "region",
                        "hit": false
                    },
                    {
                        "score": 0.392144639139534,
                        "answer": "mayor",
                        "hit": false
                    },
                    {
                        "score": 0.3918793955611815,
                        "answer": "gov",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "continues"
                ],
                "rank": 4595,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6774882972240448
            },
            {
                "question verbose": "What is to creates ",
                "b": "creates",
                "expected answer": [
                    "created"
                ],
                "predictions": [
                    {
                        "score": 0.4671747904006768,
                        "answer": "earnings",
                        "hit": false
                    },
                    {
                        "score": 0.46110210203633945,
                        "answer": "doubled",
                        "hit": false
                    },
                    {
                        "score": 0.45041817867912975,
                        "answer": "lower",
                        "hit": false
                    },
                    {
                        "score": 0.4496520525587248,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.4469693658898623,
                        "answer": "salicylic",
                        "hit": false
                    },
                    {
                        "score": 0.44411934464131464,
                        "answer": "olympus",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "creates"
                ],
                "rank": 7167,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.660224199295044
            },
            {
                "question verbose": "What is to decides ",
                "b": "decides",
                "expected answer": [
                    "decided"
                ],
                "predictions": [
                    {
                        "score": 0.5692843004279392,
                        "answer": "incumbent",
                        "hit": false
                    },
                    {
                        "score": 0.522142154514773,
                        "answer": "congressional",
                        "hit": false
                    },
                    {
                        "score": 0.5066211615451196,
                        "answer": "fascination",
                        "hit": false
                    },
                    {
                        "score": 0.4972033789845991,
                        "answer": "boehlert",
                        "hit": false
                    },
                    {
                        "score": 0.4943213892348605,
                        "answer": "ghent",
                        "hit": false
                    },
                    {
                        "score": 0.48904697920966056,
                        "answer": "trigger",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "decides"
                ],
                "rank": 1981,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6615352034568787
            },
            {
                "question verbose": "What is to describes ",
                "b": "describes",
                "expected answer": [
                    "described"
                ],
                "predictions": [
                    {
                        "score": 0.4755751982220998,
                        "answer": "hated",
                        "hit": false
                    },
                    {
                        "score": 0.46915051208828484,
                        "answer": "eucalyptus",
                        "hit": false
                    },
                    {
                        "score": 0.4459074736381558,
                        "answer": "bristol",
                        "hit": false
                    },
                    {
                        "score": 0.43387770653177576,
                        "answer": "longest",
                        "hit": false
                    },
                    {
                        "score": 0.42751441424431363,
                        "answer": "discontent",
                        "hit": false
                    },
                    {
                        "score": 0.42648545488999234,
                        "answer": "imu",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "describes"
                ],
                "rank": 4029,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6776494681835175
            },
            {
                "question verbose": "What is to develops ",
                "b": "develops",
                "expected answer": [
                    "developed"
                ],
                "predictions": [
                    {
                        "score": 0.5586218156998501,
                        "answer": "revelation",
                        "hit": false
                    },
                    {
                        "score": 0.5468986570524014,
                        "answer": "morale",
                        "hit": false
                    },
                    {
                        "score": 0.5445218365758474,
                        "answer": "indusrty",
                        "hit": false
                    },
                    {
                        "score": 0.5304563563856679,
                        "answer": "injection",
                        "hit": false
                    },
                    {
                        "score": 0.5296828617835853,
                        "answer": "loo",
                        "hit": false
                    },
                    {
                        "score": 0.5252204902865923,
                        "answer": "flippantly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "develops"
                ],
                "rank": 10280,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7467696815729141
            },
            {
                "question verbose": "What is to establishes ",
                "b": "establishes",
                "expected answer": [
                    "established"
                ],
                "predictions": [
                    {
                        "score": 0.5842423210034485,
                        "answer": "eeoc",
                        "hit": false
                    },
                    {
                        "score": 0.5637494057933017,
                        "answer": "filed",
                        "hit": false
                    },
                    {
                        "score": 0.5607742849722629,
                        "answer": "percentage",
                        "hit": false
                    },
                    {
                        "score": 0.5592441484609518,
                        "answer": "earnings",
                        "hit": false
                    },
                    {
                        "score": 0.5545322596028383,
                        "answer": "formally",
                        "hit": false
                    },
                    {
                        "score": 0.5537659040668695,
                        "answer": "revelation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "establishes"
                ],
                "rank": 14270,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7715645432472229
            },
            {
                "question verbose": "What is to expects ",
                "b": "expects",
                "expected answer": [
                    "expected"
                ],
                "predictions": [
                    {
                        "score": 0.5246397119497503,
                        "answer": "rate",
                        "hit": false
                    },
                    {
                        "score": 0.5079017658954846,
                        "answer": "outcry",
                        "hit": false
                    },
                    {
                        "score": 0.49725323497273044,
                        "answer": "advisor",
                        "hit": false
                    },
                    {
                        "score": 0.4910487562049927,
                        "answer": "international",
                        "hit": false
                    },
                    {
                        "score": 0.4796439980279118,
                        "answer": "equivalent",
                        "hit": false
                    },
                    {
                        "score": 0.4723915428959011,
                        "answer": "dioxide",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "expects"
                ],
                "rank": 7051,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6404354572296143
            },
            {
                "question verbose": "What is to fails ",
                "b": "fails",
                "expected answer": [
                    "failed"
                ],
                "predictions": [
                    {
                        "score": 0.4084533650837318,
                        "answer": "powerpoint",
                        "hit": false
                    },
                    {
                        "score": 0.4031611327417644,
                        "answer": "yield",
                        "hit": false
                    },
                    {
                        "score": 0.4020446375277654,
                        "answer": "master",
                        "hit": false
                    },
                    {
                        "score": 0.4016484862696148,
                        "answer": "redistricting",
                        "hit": false
                    },
                    {
                        "score": 0.4010828095334411,
                        "answer": "survivor",
                        "hit": false
                    },
                    {
                        "score": 0.3982038530485968,
                        "answer": "gained",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fails"
                ],
                "rank": 6005,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7152768969535828
            },
            {
                "question verbose": "What is to follows ",
                "b": "follows",
                "expected answer": [
                    "followed"
                ],
                "predictions": [
                    {
                        "score": 0.5171624568082724,
                        "answer": "eighteen",
                        "hit": false
                    },
                    {
                        "score": 0.496121848179364,
                        "answer": "barbados",
                        "hit": false
                    },
                    {
                        "score": 0.4908935832152126,
                        "answer": "olympus",
                        "hit": false
                    },
                    {
                        "score": 0.4848013205976634,
                        "answer": "newton",
                        "hit": false
                    },
                    {
                        "score": 0.48369170910555337,
                        "answer": "rose",
                        "hit": false
                    },
                    {
                        "score": 0.4831065851979324,
                        "answer": "assassination",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "follows"
                ],
                "rank": 3535,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7196513265371323
            },
            {
                "question verbose": "What is to happens ",
                "b": "happens",
                "expected answer": [
                    "happened"
                ],
                "predictions": [
                    {
                        "score": 0.3814017938934078,
                        "answer": "changed",
                        "hit": false
                    },
                    {
                        "score": 0.3700121261349233,
                        "answer": "flipped",
                        "hit": false
                    },
                    {
                        "score": 0.3696380961533115,
                        "answer": "mall",
                        "hit": false
                    },
                    {
                        "score": 0.3646492314806109,
                        "answer": "wtic",
                        "hit": false
                    },
                    {
                        "score": 0.35446269451104384,
                        "answer": "favorable",
                        "hit": false
                    },
                    {
                        "score": 0.34505682789221487,
                        "answer": "rohling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happens"
                ],
                "rank": 644,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6498570889234543
            },
            {
                "question verbose": "What is to hears ",
                "b": "hears",
                "expected answer": [
                    "heard"
                ],
                "predictions": [
                    {
                        "score": 0.44459928018773226,
                        "answer": "rally",
                        "hit": false
                    },
                    {
                        "score": 0.4302845232207889,
                        "answer": "january",
                        "hit": false
                    },
                    {
                        "score": 0.42599591821719035,
                        "answer": "dropping",
                        "hit": false
                    },
                    {
                        "score": 0.4195922481664441,
                        "answer": "maintained",
                        "hit": false
                    },
                    {
                        "score": 0.41865772072968704,
                        "answer": "butt",
                        "hit": false
                    },
                    {
                        "score": 0.4179658604224831,
                        "answer": "suffolk",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hears"
                ],
                "rank": 3845,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7047375738620758
            },
            {
                "question verbose": "What is to includes ",
                "b": "includes",
                "expected answer": [
                    "included"
                ],
                "predictions": [
                    {
                        "score": 0.4245642295046735,
                        "answer": "starring",
                        "hit": false
                    },
                    {
                        "score": 0.4064914024234433,
                        "answer": "screening",
                        "hit": false
                    },
                    {
                        "score": 0.40498858083549,
                        "answer": "animated",
                        "hit": false
                    },
                    {
                        "score": 0.3968501941600281,
                        "answer": "announced",
                        "hit": false
                    },
                    {
                        "score": 0.3942909642346368,
                        "answer": "knife",
                        "hit": false
                    },
                    {
                        "score": 0.3856146538317259,
                        "answer": "bedroom",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "includes"
                ],
                "rank": 8739,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7061155438423157
            },
            {
                "question verbose": "What is to intends ",
                "b": "intends",
                "expected answer": [
                    "intended"
                ],
                "predictions": [
                    {
                        "score": 0.4976383809712515,
                        "answer": "dropping",
                        "hit": false
                    },
                    {
                        "score": 0.4948407402382572,
                        "answer": "concerning",
                        "hit": false
                    },
                    {
                        "score": 0.48571891167301656,
                        "answer": "doubled",
                        "hit": false
                    },
                    {
                        "score": 0.4838763635780475,
                        "answer": "inserted",
                        "hit": false
                    },
                    {
                        "score": 0.4799099614616683,
                        "answer": "swaziland",
                        "hit": false
                    },
                    {
                        "score": 0.47069821605057516,
                        "answer": "manageable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "intends"
                ],
                "rank": 9941,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6701707988977432
            },
            {
                "question verbose": "What is to introduces ",
                "b": "introduces",
                "expected answer": [
                    "introduced"
                ],
                "predictions": [
                    {
                        "score": 0.31986458007679336,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27856903937004185,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.27660789834773297,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.26210404975214235,
                        "answer": "quarter",
                        "hit": false
                    },
                    {
                        "score": 0.2598657603503985,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.2526619850883964,
                        "answer": "leftover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "introduces"
                ],
                "rank": 15236,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.49245684314519167
            },
            {
                "question verbose": "What is to involves ",
                "b": "involves",
                "expected answer": [
                    "involved"
                ],
                "predictions": [
                    {
                        "score": 0.41372576199482697,
                        "answer": "duane",
                        "hit": false
                    },
                    {
                        "score": 0.39415719667949867,
                        "answer": "gish",
                        "hit": false
                    },
                    {
                        "score": 0.38971211701658,
                        "answer": "shill",
                        "hit": false
                    },
                    {
                        "score": 0.3802421646581425,
                        "answer": "tactic",
                        "hit": false
                    },
                    {
                        "score": 0.37925487167039446,
                        "answer": "creationist",
                        "hit": false
                    },
                    {
                        "score": 0.3708418536336639,
                        "answer": "gallop",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "involves"
                ],
                "rank": 3164,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6665482670068741
            },
            {
                "question verbose": "What is to locates ",
                "b": "locates",
                "expected answer": [
                    "located"
                ],
                "predictions": [
                    {
                        "score": 0.30297975458843335,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.24774753913984107,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.24550363548940238,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.24459236211838378,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.24429867657122464,
                        "answer": "quarter",
                        "hit": false
                    },
                    {
                        "score": 0.24264561801756243,
                        "answer": "fewer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "locates"
                ],
                "rank": 8263,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5709576308727264
            },
            {
                "question verbose": "What is to loses ",
                "b": "loses",
                "expected answer": [
                    "lost"
                ],
                "predictions": [
                    {
                        "score": 0.5298065376481901,
                        "answer": "dropping",
                        "hit": false
                    },
                    {
                        "score": 0.522445616662613,
                        "answer": "ballot",
                        "hit": false
                    },
                    {
                        "score": 0.5151535137342017,
                        "answer": "january",
                        "hit": false
                    },
                    {
                        "score": 0.5119640501566217,
                        "answer": "region",
                        "hit": false
                    },
                    {
                        "score": 0.5109889305042288,
                        "answer": "allgeier",
                        "hit": false
                    },
                    {
                        "score": 0.5067581950252279,
                        "answer": "giannoulias",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "loses"
                ],
                "rank": 14720,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6830050051212311
            },
            {
                "question verbose": "What is to manages ",
                "b": "manages",
                "expected answer": [
                    "managed"
                ],
                "predictions": [
                    {
                        "score": 0.29994892437742166,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2663955364538254,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.2569031307899884,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.24690747958033551,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.2444487365009157,
                        "answer": "utah",
                        "hit": false
                    },
                    {
                        "score": 0.24391449191870893,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "manages"
                ],
                "rank": 8033,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6295684576034546
            },
            {
                "question verbose": "What is to marries ",
                "b": "marries",
                "expected answer": [
                    "married"
                ],
                "predictions": [
                    {
                        "score": 0.31413562344168555,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2647812653582611,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.25389484542602225,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.2509525596230317,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.2503349520822712,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.24301737490614972,
                        "answer": "quarter",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "marries"
                ],
                "rank": 8218,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5751892998814583
            },
            {
                "question verbose": "What is to occurs ",
                "b": "occurs",
                "expected answer": [
                    "occurred"
                ],
                "predictions": [
                    {
                        "score": 0.4068475819771025,
                        "answer": "valley",
                        "hit": false
                    },
                    {
                        "score": 0.3936070761224246,
                        "answer": "casing",
                        "hit": false
                    },
                    {
                        "score": 0.3817591947537336,
                        "answer": "scholarship",
                        "hit": false
                    },
                    {
                        "score": 0.3784302093255766,
                        "answer": "etiology",
                        "hit": false
                    },
                    {
                        "score": 0.37761718026412877,
                        "answer": "dos",
                        "hit": false
                    },
                    {
                        "score": 0.3766471399015696,
                        "answer": "ethnicity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "occurs"
                ],
                "rank": 12493,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7146576195955276
            },
            {
                "question verbose": "What is to operates ",
                "b": "operates",
                "expected answer": [
                    "operated"
                ],
                "predictions": [
                    {
                        "score": 0.31613524755886563,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27173856669329394,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.25322689200086124,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.24752403762170694,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.24363478526609095,
                        "answer": "quarter",
                        "hit": false
                    },
                    {
                        "score": 0.23955243221255884,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "operates"
                ],
                "rank": 7731,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5935384258627892
            },
            {
                "question verbose": "What is to performs ",
                "b": "performs",
                "expected answer": [
                    "performed"
                ],
                "predictions": [
                    {
                        "score": 0.3095412344403488,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.2762265364157734,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.27412323269908756,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.258279147992129,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.25142764068834145,
                        "answer": "quarter",
                        "hit": false
                    },
                    {
                        "score": 0.24960790785170148,
                        "answer": "five",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "performs"
                ],
                "rank": 10903,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.616327665746212
            },
            {
                "question verbose": "What is to proposes ",
                "b": "proposes",
                "expected answer": [
                    "proposed"
                ],
                "predictions": [
                    {
                        "score": 0.29400849552182234,
                        "answer": "proposed",
                        "hit": true
                    },
                    {
                        "score": 0.26812739620393505,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.2577475108692585,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.25616139131125654,
                        "answer": "headed",
                        "hit": false
                    },
                    {
                        "score": 0.2557792602568407,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.2555008468611033,
                        "answer": "fewer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "proposes"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.706113263964653
            },
            {
                "question verbose": "What is to provides ",
                "b": "provides",
                "expected answer": [
                    "provided"
                ],
                "predictions": [
                    {
                        "score": 0.48186227210526,
                        "answer": "mammogram",
                        "hit": false
                    },
                    {
                        "score": 0.46168493450428677,
                        "answer": "emergency",
                        "hit": false
                    },
                    {
                        "score": 0.4385962196813396,
                        "answer": "basin",
                        "hit": false
                    },
                    {
                        "score": 0.43645312630851935,
                        "answer": "parenthood",
                        "hit": false
                    },
                    {
                        "score": 0.43618074247503663,
                        "answer": "kerr",
                        "hit": false
                    },
                    {
                        "score": 0.4327476451880858,
                        "answer": "presbyterian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "provides"
                ],
                "rank": 5622,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6797110289335251
            },
            {
                "question verbose": "What is to publishes ",
                "b": "publishes",
                "expected answer": [
                    "published"
                ],
                "predictions": [
                    {
                        "score": 0.32103020949195116,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27422093058121183,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.26690243626163146,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.2662356158139469,
                        "answer": "gene",
                        "hit": false
                    },
                    {
                        "score": 0.26267408095747535,
                        "answer": "leftover",
                        "hit": false
                    },
                    {
                        "score": 0.2578981470962473,
                        "answer": "redesign",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "publishes"
                ],
                "rank": 15212,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5015606938395649
            },
            {
                "question verbose": "What is to receives ",
                "b": "receives",
                "expected answer": [
                    "received"
                ],
                "predictions": [
                    {
                        "score": 0.4940850581665679,
                        "answer": "retailer",
                        "hit": false
                    },
                    {
                        "score": 0.4778032075609648,
                        "answer": "marrow",
                        "hit": false
                    },
                    {
                        "score": 0.4718232473931687,
                        "answer": "esophageal",
                        "hit": false
                    },
                    {
                        "score": 0.47129025973249966,
                        "answer": "birmingham",
                        "hit": false
                    },
                    {
                        "score": 0.4696969360716193,
                        "answer": "eighteen",
                        "hit": false
                    },
                    {
                        "score": 0.46861337517288737,
                        "answer": "presbyterian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "receives"
                ],
                "rank": 5537,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6615118831396103
            },
            {
                "question verbose": "What is to refers ",
                "b": "refers",
                "expected answer": [
                    "referred"
                ],
                "predictions": [
                    {
                        "score": 0.5185535769993538,
                        "answer": "trig",
                        "hit": false
                    },
                    {
                        "score": 0.5070546184626573,
                        "answer": "default",
                        "hit": false
                    },
                    {
                        "score": 0.5067145805343876,
                        "answer": "rent",
                        "hit": false
                    },
                    {
                        "score": 0.4970506354591399,
                        "answer": "truck",
                        "hit": false
                    },
                    {
                        "score": 0.4967120179372971,
                        "answer": "birmingham",
                        "hit": false
                    },
                    {
                        "score": 0.4921523118138705,
                        "answer": "josh",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "refers"
                ],
                "rank": 6323,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7568299174308777
            },
            {
                "question verbose": "What is to relates ",
                "b": "relates",
                "expected answer": [
                    "related"
                ],
                "predictions": [
                    {
                        "score": 0.5227992867359814,
                        "answer": "bonanza",
                        "hit": false
                    },
                    {
                        "score": 0.5219362604609376,
                        "answer": "osteoporosis",
                        "hit": false
                    },
                    {
                        "score": 0.5158647847678715,
                        "answer": "dairy",
                        "hit": false
                    },
                    {
                        "score": 0.5154618802734974,
                        "answer": "etiology",
                        "hit": false
                    },
                    {
                        "score": 0.5093076021284746,
                        "answer": "intake",
                        "hit": false
                    },
                    {
                        "score": 0.5026071670248242,
                        "answer": "decapitated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "relates"
                ],
                "rank": 8152,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7042031288146973
            },
            {
                "question verbose": "What is to remains ",
                "b": "remains",
                "expected answer": [
                    "remained"
                ],
                "predictions": [
                    {
                        "score": 0.4377540672194614,
                        "answer": "doubled",
                        "hit": false
                    },
                    {
                        "score": 0.4296579128015329,
                        "answer": "polling",
                        "hit": false
                    },
                    {
                        "score": 0.4233574558087448,
                        "answer": "nasdaq",
                        "hit": false
                    },
                    {
                        "score": 0.4223757536799914,
                        "answer": "marco",
                        "hit": false
                    },
                    {
                        "score": 0.4192207315365916,
                        "answer": "caput",
                        "hit": false
                    },
                    {
                        "score": 0.40816829516321734,
                        "answer": "survivor",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "remains"
                ],
                "rank": 2405,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6521691381931305
            },
            {
                "question verbose": "What is to replaces ",
                "b": "replaces",
                "expected answer": [
                    "replaced"
                ],
                "predictions": [
                    {
                        "score": 0.5344429392859243,
                        "answer": "revelation",
                        "hit": false
                    },
                    {
                        "score": 0.49768393525183174,
                        "answer": "efficacy",
                        "hit": false
                    },
                    {
                        "score": 0.49635832068456637,
                        "answer": "january",
                        "hit": false
                    },
                    {
                        "score": 0.4908437703413589,
                        "answer": "pounding",
                        "hit": false
                    },
                    {
                        "score": 0.48862562401726084,
                        "answer": "inserted",
                        "hit": false
                    },
                    {
                        "score": 0.4855123086518021,
                        "answer": "chalked",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "replaces"
                ],
                "rank": 12917,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.764437735080719
            },
            {
                "question verbose": "What is to represents ",
                "b": "represents",
                "expected answer": [
                    "represented"
                ],
                "predictions": [
                    {
                        "score": 0.48142457460956534,
                        "answer": "accumulation",
                        "hit": false
                    },
                    {
                        "score": 0.47958193825590273,
                        "answer": "caput",
                        "hit": false
                    },
                    {
                        "score": 0.47934858093297233,
                        "answer": "worldwide",
                        "hit": false
                    },
                    {
                        "score": 0.4786232596128622,
                        "answer": "warming",
                        "hit": false
                    },
                    {
                        "score": 0.47526639538818083,
                        "answer": "doubled",
                        "hit": false
                    },
                    {
                        "score": 0.4727143640404817,
                        "answer": "esophageal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "represents"
                ],
                "rank": 4512,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7173597812652588
            },
            {
                "question verbose": "What is to requires ",
                "b": "requires",
                "expected answer": [
                    "required"
                ],
                "predictions": [
                    {
                        "score": 0.4086546417433209,
                        "answer": "range",
                        "hit": false
                    },
                    {
                        "score": 0.386644328872595,
                        "answer": "glossy",
                        "hit": false
                    },
                    {
                        "score": 0.3821751429051878,
                        "answer": "disfavor",
                        "hit": false
                    },
                    {
                        "score": 0.362021291449897,
                        "answer": "relatively",
                        "hit": false
                    },
                    {
                        "score": 0.35879950265298843,
                        "answer": "expert",
                        "hit": false
                    },
                    {
                        "score": 0.3584887746188241,
                        "answer": "etiology",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "requires"
                ],
                "rank": 1637,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6394556313753128
            },
            {
                "question verbose": "What is to seems ",
                "b": "seems",
                "expected answer": [
                    "seemed"
                ],
                "predictions": [
                    {
                        "score": 0.2802131030102053,
                        "answer": "clinton",
                        "hit": false
                    },
                    {
                        "score": 0.2682079435607528,
                        "answer": "favor",
                        "hit": false
                    },
                    {
                        "score": 0.25778677203894085,
                        "answer": "disfavor",
                        "hit": false
                    },
                    {
                        "score": 0.24897621776599926,
                        "answer": "precisely",
                        "hit": false
                    },
                    {
                        "score": 0.24866205780667242,
                        "answer": "reached",
                        "hit": false
                    },
                    {
                        "score": 0.24628879694141162,
                        "answer": "diligence",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seems"
                ],
                "rank": 12553,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5970643311738968
            },
            {
                "question verbose": "What is to sends ",
                "b": "sends",
                "expected answer": [
                    "sent"
                ],
                "predictions": [
                    {
                        "score": 0.5024040916687428,
                        "answer": "doubled",
                        "hit": false
                    },
                    {
                        "score": 0.48365418496048257,
                        "answer": "whither",
                        "hit": false
                    },
                    {
                        "score": 0.47812157464919847,
                        "answer": "january",
                        "hit": false
                    },
                    {
                        "score": 0.4775343345445844,
                        "answer": "fuller",
                        "hit": false
                    },
                    {
                        "score": 0.4761526169748357,
                        "answer": "dropping",
                        "hit": false
                    },
                    {
                        "score": 0.4740197956870874,
                        "answer": "appealed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sends"
                ],
                "rank": 2866,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7153002768754959
            },
            {
                "question verbose": "What is to spends ",
                "b": "spends",
                "expected answer": [
                    "spent"
                ],
                "predictions": [
                    {
                        "score": 0.4973540834680824,
                        "answer": "stepped",
                        "hit": false
                    },
                    {
                        "score": 0.4897319135517319,
                        "answer": "chatting",
                        "hit": false
                    },
                    {
                        "score": 0.488251409046112,
                        "answer": "trig",
                        "hit": false
                    },
                    {
                        "score": 0.46681225337846516,
                        "answer": "restaurant",
                        "hit": false
                    },
                    {
                        "score": 0.46157541256750423,
                        "answer": "morgan",
                        "hit": false
                    },
                    {
                        "score": 0.4582633467627898,
                        "answer": "twenty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spends"
                ],
                "rank": 9,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7738893926143646
            },
            {
                "question verbose": "What is to suggests ",
                "b": "suggests",
                "expected answer": [
                    "suggested"
                ],
                "predictions": [
                    {
                        "score": 0.5066357953843191,
                        "answer": "pinocchios",
                        "hit": false
                    },
                    {
                        "score": 0.4641641044943384,
                        "answer": "notch",
                        "hit": false
                    },
                    {
                        "score": 0.45991414221889165,
                        "answer": "kessler",
                        "hit": false
                    },
                    {
                        "score": 0.45959450980344424,
                        "answer": "nasrc",
                        "hit": false
                    },
                    {
                        "score": 0.4510075119092418,
                        "answer": "chemist",
                        "hit": false
                    },
                    {
                        "score": 0.4468171075361383,
                        "answer": "considerable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "suggests"
                ],
                "rank": 2691,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6983995288610458
            },
            {
                "question verbose": "What is to tells ",
                "b": "tells",
                "expected answer": [
                    "told"
                ],
                "predictions": [
                    {
                        "score": 0.31704370221394423,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27300725267266934,
                        "answer": "granted",
                        "hit": false
                    },
                    {
                        "score": 0.26685533277770634,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.2565475578380249,
                        "answer": "redesign",
                        "hit": false
                    },
                    {
                        "score": 0.25587185233373777,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.2540793575461474,
                        "answer": "headed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tells"
                ],
                "rank": 9560,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5583440586924553
            }
        ],
        "result": {
            "cnt_questions_correct": 1,
            "cnt_questions_total": 50,
            "accuracy": 0.02
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "1_Inflectional_morphology",
            "subcategory": "I10 [verb_3pSg - Ved].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "d6e101db-9f57-413d-ab02-53d37aea4eee",
            "timestamp": "2020-10-22T15:57:44.876726"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to allosaurus ",
                "b": "allosaurus",
                "expected answer": [
                    "dinosaur",
                    "reptile",
                    "bird",
                    "archosaur",
                    "archosaurian",
                    "archosaurian_reptile",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6308128604511157,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2215315196986075,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21927902986119466,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21587010183994182,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.21545678993174056,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21154768702301754,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "allosaurus"
                ],
                "rank": 7,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6553508341312408
            },
            {
                "question verbose": "What is to anaconda ",
                "b": "anaconda",
                "expected answer": [
                    "snake",
                    "reptile",
                    "boa",
                    "serpent",
                    "ophidian"
                ],
                "predictions": [
                    {
                        "score": 0.6303038067667185,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23703920303940937,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21435629169417025,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.21401455668140418,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21327524975296797,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.21135935646146256,
                        "answer": "overnight",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "anaconda"
                ],
                "rank": 6,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to ant ",
                "b": "ant",
                "expected answer": [
                    "insect",
                    "invertebrate",
                    "creature",
                    "beast",
                    "hymenopteran",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "arthropod",
                    "hymenopterous_insect",
                    "animate_being",
                    "physical_hymenopteron",
                    "physical_entity",
                    "hymenopter",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.4888749658839016,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.47845131571809274,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.44498697358083,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.4040545104199432,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.38943342303404527,
                        "answer": "propagated",
                        "hit": false
                    },
                    {
                        "score": 0.37577099682015885,
                        "answer": "pluck",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ant"
                ],
                "rank": 3047,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6763644963502884
            },
            {
                "question verbose": "What is to beaver ",
                "b": "beaver",
                "expected answer": [
                    "rodent",
                    "vertebrate",
                    "creature",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "gnawer",
                    "animate_being",
                    "living_thing",
                    "placental_mammal",
                    "craniate",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.37564544771178165,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.36341369133684315,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3245155125840607,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.29873515465283307,
                        "answer": "pusher",
                        "hit": false
                    },
                    {
                        "score": 0.2945506858448942,
                        "answer": "pluck",
                        "hit": false
                    },
                    {
                        "score": 0.29147644381300924,
                        "answer": "crack",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beaver"
                ],
                "rank": 2141,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5890942737460136
            },
            {
                "question verbose": "What is to bee ",
                "b": "bee",
                "expected answer": [
                    "insect",
                    "invertebrate",
                    "creature",
                    "beast",
                    "hymenopteran",
                    "being",
                    "animal",
                    "insect",
                    "organism",
                    "fauna",
                    "arthropod",
                    "hymenopterous_insect",
                    "animate_being",
                    "hymenopteron",
                    "hymenopter",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.4548173571254635,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.41768608707258403,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3656841092606093,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.35372379662150266,
                        "answer": "understood",
                        "hit": false
                    },
                    {
                        "score": 0.34645386895813934,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.33630416913001177,
                        "answer": "rip",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bee"
                ],
                "rank": 2753,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5914601683616638
            },
            {
                "question verbose": "What is to beetle ",
                "b": "beetle",
                "expected answer": [
                    "insect",
                    "invertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "insect",
                    "organism",
                    "fauna",
                    "arthropod",
                    "animate_being",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.4058693559917262,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.397591886545858,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.32980053629779305,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3102186117005909,
                        "answer": "bailouts",
                        "hit": false
                    },
                    {
                        "score": 0.3038765749427722,
                        "answer": "powder",
                        "hit": false
                    },
                    {
                        "score": 0.2997927122679574,
                        "answer": "rip",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beetle"
                ],
                "rank": 3604,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5828202366828918
            },
            {
                "question verbose": "What is to buffalo ",
                "b": "buffalo",
                "expected answer": [
                    "bovid",
                    "mammal",
                    "bison",
                    "cow",
                    "vertebrate",
                    "creature",
                    "ungulate",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "artiodactyl",
                    "ruminant",
                    "chordate",
                    "eutherian",
                    "mammalian",
                    "bovine",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_hoofed_mammal",
                    "physical_entity",
                    "even-toed_ungulate",
                    "artiodactyl_mammal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.377045375905859,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3556835506671495,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.3229992027013706,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3115248672897486,
                        "answer": "crack",
                        "hit": false
                    },
                    {
                        "score": 0.2936615428918623,
                        "answer": "suffering",
                        "hit": false
                    },
                    {
                        "score": 0.2928809375369994,
                        "answer": "propagated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "buffalo"
                ],
                "rank": 2583,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.585297703742981
            },
            {
                "question verbose": "What is to butterfly ",
                "b": "butterfly",
                "expected answer": [
                    "insect",
                    "creature",
                    "beast",
                    "lepidopteran",
                    "animal",
                    "organism",
                    "fauna",
                    "arthropod",
                    "lepidopterous_insect",
                    "lepidopteron",
                    "animate_being",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6245140542593155,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22793211417552742,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22479907780378566,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22256730450196283,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.22250470539809547,
                        "answer": "overnight",
                        "hit": false
                    },
                    {
                        "score": 0.22130859651585547,
                        "answer": "lose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "butterfly"
                ],
                "rank": 3459,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cat ",
                "b": "cat",
                "expected answer": [
                    "feline",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "felid",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "animate_being",
                    "placental_mammal",
                    "craniate",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.35924687967130153,
                        "answer": "feline",
                        "hit": true
                    },
                    {
                        "score": 0.34407205473767527,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.307951705810444,
                        "answer": "desperate",
                        "hit": false
                    },
                    {
                        "score": 0.278092734677757,
                        "answer": "fbi",
                        "hit": false
                    },
                    {
                        "score": 0.27097995896916294,
                        "answer": "unbridled",
                        "hit": false
                    },
                    {
                        "score": 0.27004413691047663,
                        "answer": "diaper",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cat"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.774515688419342
            },
            {
                "question verbose": "What is to chicken ",
                "b": "chicken",
                "expected answer": [
                    "fowl",
                    "bird",
                    "vertebrate",
                    "poultry",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "poultry",
                    "chordate",
                    "domestic_fowl",
                    "animate_being",
                    "gallinaceous_bird",
                    "craniate",
                    "gallinacean",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.29020789746253267,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.2571797638577562,
                        "answer": "pluck",
                        "hit": false
                    },
                    {
                        "score": 0.247576382649063,
                        "answer": "abundance",
                        "hit": false
                    },
                    {
                        "score": 0.2458844811883033,
                        "answer": "complication",
                        "hit": false
                    },
                    {
                        "score": 0.2320348451633754,
                        "answer": "sanctified",
                        "hit": false
                    },
                    {
                        "score": 0.2314856946233654,
                        "answer": "delving",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chicken"
                ],
                "rank": 1503,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6053173542022705
            },
            {
                "question verbose": "What is to chimpanzee ",
                "b": "chimpanzee",
                "expected answer": [
                    "primate",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "hominid",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "ape",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_physical_entity",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6252649133554786,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23607169503140044,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22928274372613278,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.22754711145491632,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.22378523900247163,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21856157775728746,
                        "answer": "poutrage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chimpanzee"
                ],
                "rank": 1872,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to chinchilla ",
                "b": "chinchilla",
                "expected answer": [
                    "rodent",
                    "vertebrate",
                    "creature",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "gnawer",
                    "animate_being",
                    "living_thing",
                    "placental_mammal",
                    "craniate",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.2882273291654975,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.28370255321999205,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2712973230146748,
                        "answer": "propagated",
                        "hit": false
                    },
                    {
                        "score": 0.27078304595856945,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.2570917561748304,
                        "answer": "delving",
                        "hit": false
                    },
                    {
                        "score": 0.253126704285393,
                        "answer": "discouraged",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chinchilla"
                ],
                "rank": 1183,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5718269720673561
            },
            {
                "question verbose": "What is to cobra ",
                "b": "cobra",
                "expected answer": [
                    "snake",
                    "reptile",
                    "elapid",
                    "elapid_snake",
                    "serpent",
                    "ophidian"
                ],
                "predictions": [
                    {
                        "score": 0.461688333795736,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.428088825933718,
                        "answer": "snake",
                        "hit": true
                    },
                    {
                        "score": 0.41986417059224906,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.35811949820910866,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.35443816505041825,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.3455911866019065,
                        "answer": "enlighten",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cobra"
                ],
                "rank": 1,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8383024334907532
            },
            {
                "question verbose": "What is to cockroach ",
                "b": "cockroach",
                "expected answer": [
                    "insect",
                    "invertebrate",
                    "creature",
                    "beast",
                    "dictyopterous_insect",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "arthropod",
                    "animate_being",
                    "physical_entity",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.5172632124918448,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.48617157938623506,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.42721930163566674,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.41669817914658297,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.41076342569290897,
                        "answer": "opera",
                        "hit": false
                    },
                    {
                        "score": 0.4094721244156181,
                        "answer": "freezing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cockroach"
                ],
                "rank": 1441,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7059800922870636
            },
            {
                "question verbose": "What is to cow ",
                "b": "cow",
                "expected answer": [
                    "bovid",
                    "mammal",
                    "cattle",
                    "vertebrate",
                    "creature",
                    "ungulate",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "artiodactyl",
                    "ruminant",
                    "chordate",
                    "eutherian",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_hoofed_mammal",
                    "physical_entity",
                    "even-toed_ungulate",
                    "artiodactyl_mammal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.624227645652479,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22978184235143576,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22420254541363688,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2200067528397145,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21879267202992572,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.21652399394073535,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cow"
                ],
                "rank": 2741,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to coyote ",
                "b": "coyote",
                "expected answer": [
                    "canine",
                    "vertebrate",
                    "creature",
                    "beast",
                    "canid",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "locomote",
                    "placental_mammal",
                    "craniate",
                    "domesticated_animal",
                    "physical_entity",
                    "domestic_animal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6242932545630915,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22679175484563993,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2231722981547883,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22232050467901304,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.22027795644687104,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.2156744926987501,
                        "answer": "dinosaur",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "coyote"
                ],
                "rank": 2740,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to deer ",
                "b": "deer",
                "expected answer": [
                    "bovid",
                    "mammal",
                    "vertebrate",
                    "creature",
                    "ungulate",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "artiodactyl",
                    "ruminant",
                    "chordate",
                    "eutherian",
                    "mammalian",
                    "bovine",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_hoofed_mammal",
                    "physical_entity",
                    "even-toed_ungulate",
                    "artiodactyl_mammal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6249544194678568,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2236788117096698,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22192842885320063,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21425995456142027,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.21391794127359137,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21349568820287712,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deer"
                ],
                "rank": 2469,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dog ",
                "b": "dog",
                "expected answer": [
                    "canine",
                    "vertebrate",
                    "creature",
                    "beast",
                    "canid",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "locomote",
                    "placental_mammal",
                    "craniate",
                    "domesticated_animal",
                    "physical_entity",
                    "domestic_animal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.4179860451638722,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.40486978559013725,
                        "answer": "deserve",
                        "hit": false
                    },
                    {
                        "score": 0.4024278240441491,
                        "answer": "complicated",
                        "hit": false
                    },
                    {
                        "score": 0.39434727541883813,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.3881648260447485,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.38443336297067116,
                        "answer": "unbridled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dog"
                ],
                "rank": 1861,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6415438055992126
            },
            {
                "question verbose": "What is to duck ",
                "b": "duck",
                "expected answer": [
                    "fowl",
                    "bird",
                    "vertebrate",
                    "poultry",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "poultry",
                    "chordate",
                    "domestic_fowl",
                    "waterfowl",
                    "water_fowl",
                    "animate_being",
                    "anseriform_bird",
                    "craniate",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.30216087731342606,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3015189856515408,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.29397543564216005,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.25442488449594053,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2448586272341439,
                        "answer": "pusher",
                        "hit": false
                    },
                    {
                        "score": 0.24457621844212765,
                        "answer": "twist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "duck"
                ],
                "rank": 2074,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6068727225065231
            },
            {
                "question verbose": "What is to eagle ",
                "b": "eagle",
                "expected answer": [
                    "raptor",
                    "bird",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "chordate",
                    "animate_being",
                    "craniate",
                    "iving_thing",
                    "raptorial_bird",
                    "bird_of_prey",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.29265587357381356,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.2662223847013617,
                        "answer": "meant",
                        "hit": false
                    },
                    {
                        "score": 0.24955123361815854,
                        "answer": "bailouts",
                        "hit": false
                    },
                    {
                        "score": 0.23944928474991717,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22525381103368372,
                        "answer": "rammed",
                        "hit": false
                    },
                    {
                        "score": 0.21970906542052726,
                        "answer": "bubble",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "eagle"
                ],
                "rank": 1141,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6013351678848267
            },
            {
                "question verbose": "What is to falcon ",
                "b": "falcon",
                "expected answer": [
                    "raptor",
                    "bird",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "chordate",
                    "animate_being",
                    "craniate",
                    "living_thing",
                    "raptorial_bird",
                    "bird_of_prey",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6246106594558035,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2355484763363117,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2247734685545826,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22432126742182862,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.22241938277755807,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.22177875222938886,
                        "answer": "poutrage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "falcon"
                ],
                "rank": 2644,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to fox ",
                "b": "fox",
                "expected answer": [
                    "canine",
                    "vertebrate",
                    "creature",
                    "beast",
                    "canid",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "locomote",
                    "placental_mammal",
                    "craniate",
                    "domesticated_animal",
                    "physical_entity",
                    "domestic_animal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.3031118086359325,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.27962520266746177,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.25010803160123607,
                        "answer": "ledger",
                        "hit": false
                    },
                    {
                        "score": 0.24267110644301942,
                        "answer": "vga",
                        "hit": false
                    },
                    {
                        "score": 0.24227526878183886,
                        "answer": "pleaser",
                        "hit": false
                    },
                    {
                        "score": 0.2409962034114693,
                        "answer": "snake",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fox"
                ],
                "rank": 3779,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6103095263242722
            },
            {
                "question verbose": "What is to gibbon ",
                "b": "gibbon",
                "expected answer": [
                    "primate",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "ape",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_physical_entity",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6245492231631219,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23408741579439882,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2214154834283486,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2190435073587106,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.2163275500842797,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.216165039516084,
                        "answer": "lose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gibbon"
                ],
                "rank": 2261,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to goat ",
                "b": "goat",
                "expected answer": [
                    "bovid",
                    "mammal",
                    "vertebrate",
                    "ungulate",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "artiodactyl",
                    "chordate",
                    "eutherian",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "hoofed_mammal",
                    "even-toed_ungulate",
                    "artiodactyl_mammal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6249183124615019,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23357053102965072,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22413839139557457,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.223735727863123,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.2231358284924987,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21908154208130332,
                        "answer": "poutrage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goat"
                ],
                "rank": 2673,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to goose ",
                "b": "goose",
                "expected answer": [
                    "fowl",
                    "bird",
                    "vertebrate",
                    "poultry",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "poultry",
                    "chordate",
                    "domestic_fowl",
                    "waterfowl",
                    "water_fowl",
                    "animate_being",
                    "anseriform_bird",
                    "craniate",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.4445103323481258,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.36598102421626816,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3435328509890519,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3199837653190534,
                        "answer": "abundance",
                        "hit": false
                    },
                    {
                        "score": 0.3173256340876894,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2967725642742265,
                        "answer": "sustain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goose"
                ],
                "rank": 3377,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6308809667825699
            },
            {
                "question verbose": "What is to gorilla ",
                "b": "gorilla",
                "expected answer": [
                    "primate",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "hominid",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "ape",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_physical_entity",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.5055948742927667,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.4254624187099679,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3922030144078423,
                        "answer": "understood",
                        "hit": false
                    },
                    {
                        "score": 0.38979615258573,
                        "answer": "cuba",
                        "hit": false
                    },
                    {
                        "score": 0.3762296795817435,
                        "answer": "overnight",
                        "hit": false
                    },
                    {
                        "score": 0.3737982134576357,
                        "answer": "feline",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gorilla"
                ],
                "rank": 2237,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6704548895359039
            },
            {
                "question verbose": "What is to hawk ",
                "b": "hawk",
                "expected answer": [
                    "raptor",
                    "bird",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "chordate",
                    "animate_being",
                    "craniate",
                    "living_thing",
                    "raptorial_bird",
                    "bird_of_prey",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.3877020010184984,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.33466748409931474,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.287274816504084,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2864659561272094,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.28516313848475666,
                        "answer": "hometown",
                        "hit": false
                    },
                    {
                        "score": 0.2820743016164015,
                        "answer": "sustain",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hawk"
                ],
                "rank": 3553,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6146642491221428
            },
            {
                "question verbose": "What is to human ",
                "b": "human",
                "expected answer": [
                    "primate",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "hominid",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "ape",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_physical_entity",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.28875580285438646,
                        "answer": "border",
                        "hit": false
                    },
                    {
                        "score": 0.2716206779885314,
                        "answer": "completely",
                        "hit": false
                    },
                    {
                        "score": 0.26067452532618085,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2497386747294721,
                        "answer": "law",
                        "hit": false
                    },
                    {
                        "score": 0.2448358352236555,
                        "answer": "satisfaction",
                        "hit": false
                    },
                    {
                        "score": 0.24459549953877624,
                        "answer": "aspect",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "human"
                ],
                "rank": 122,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5520252361893654
            },
            {
                "question verbose": "What is to jackal ",
                "b": "jackal",
                "expected answer": [
                    "canine",
                    "vertebrate",
                    "creature",
                    "beast",
                    "canid",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "locomote",
                    "placental_mammal",
                    "craniate",
                    "domesticated_animal",
                    "physical_entity",
                    "domestic_animal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6249122355388463,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23714391471857113,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.23134815257664823,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.22463078635022307,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2241480564635534,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21847889475800675,
                        "answer": "ultimately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jackal"
                ],
                "rank": 2652,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to jaguar ",
                "b": "jaguar",
                "expected answer": [
                    "feline",
                    "cat",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "felid",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "animate_being",
                    "placental_mammal",
                    "craniate",
                    "big_cat"
                ],
                "predictions": [
                    {
                        "score": 0.6313666925150122,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23213025519461192,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22391699757207917,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.22364509234398006,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2228864693844788,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.2158717223553504,
                        "answer": "dinosaur",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jaguar"
                ],
                "rank": 714,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5880415812134743
            },
            {
                "question verbose": "What is to leopard ",
                "b": "leopard",
                "expected answer": [
                    "feline",
                    "cat",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "causal_agent",
                    "chordate",
                    "felid",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "animate_being",
                    "placental_mammal",
                    "craniate",
                    "big_cat"
                ],
                "predictions": [
                    {
                        "score": 0.6310158131267657,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22217660731882644,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2186816817768578,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21839452415350674,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.21534682543306213,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.2150589321707143,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "leopard"
                ],
                "rank": 791,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5880415812134743
            },
            {
                "question verbose": "What is to lion ",
                "b": "lion",
                "expected answer": [
                    "feline",
                    "cat",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "felid",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "animate_being",
                    "placental_mammal",
                    "craniate",
                    "big_cat"
                ],
                "predictions": [
                    {
                        "score": 0.3456643132902561,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3077551997490427,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.2907710787305981,
                        "answer": "feline",
                        "hit": true
                    },
                    {
                        "score": 0.2598317909928817,
                        "answer": "pluck",
                        "hit": false
                    },
                    {
                        "score": 0.2587482316228815,
                        "answer": "league",
                        "hit": false
                    },
                    {
                        "score": 0.24386219881801915,
                        "answer": "theater",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lion"
                ],
                "rank": 2,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.722999781370163
            },
            {
                "question verbose": "What is to mamba ",
                "b": "mamba",
                "expected answer": [
                    "snake",
                    "reptile",
                    "elapid",
                    "elapid_snake",
                    "serpent",
                    "ophidian"
                ],
                "predictions": [
                    {
                        "score": 0.6308414884104223,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22211992036564893,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21622548287509624,
                        "answer": "overnight",
                        "hit": false
                    },
                    {
                        "score": 0.21447209792151067,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21312148874495287,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.2103828116491479,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mamba"
                ],
                "rank": 6,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to mouse ",
                "b": "mouse",
                "expected answer": [
                    "rodent",
                    "vertebrate",
                    "creature",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "gnawer",
                    "animate_being",
                    "living_thing",
                    "placental_mammal",
                    "craniate",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.3551364725751998,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3372222444006029,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3070933490443594,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.2861937544811783,
                        "answer": "gutting",
                        "hit": false
                    },
                    {
                        "score": 0.28042857647704245,
                        "answer": "insincere",
                        "hit": false
                    },
                    {
                        "score": 0.2739943818736506,
                        "answer": "strongest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mouse"
                ],
                "rank": 2975,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6057921424508095
            },
            {
                "question verbose": "What is to orangutan ",
                "b": "orangutan",
                "expected answer": [
                    "primate",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "hominid",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "ape",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "placental_mammal",
                    "craniate",
                    "physical_physical_entity",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6249732212128855,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23575652769220404,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22737781477979732,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.22447928904912492,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22423729132791007,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.2238068091090145,
                        "answer": "overnight",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "orangutan"
                ],
                "rank": 1514,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to owl ",
                "b": "owl",
                "expected answer": [
                    "raptor",
                    "bird",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "chordate",
                    "animate_being",
                    "craniate",
                    "living_thing",
                    "raptorial_bird",
                    "bird_of_prey",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.45138609843811195,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.4305459234000378,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3835844173559513,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.36631787729712323,
                        "answer": "pluck",
                        "hit": false
                    },
                    {
                        "score": 0.34199941616490087,
                        "answer": "border",
                        "hit": false
                    },
                    {
                        "score": 0.33682726925818945,
                        "answer": "rediscover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "owl"
                ],
                "rank": 2945,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5906610041856766
            },
            {
                "question verbose": "What is to pony ",
                "b": "pony",
                "expected answer": [
                    "bovid",
                    "mammal",
                    "horse",
                    "vertebrate",
                    "creature",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "eutherian",
                    "mammal",
                    "equine",
                    "mammalian",
                    "equid",
                    "living_thing",
                    "odd-toed_ungulate",
                    "animate_being",
                    "eutherian_mammal",
                    "perissodactyl",
                    "placental_mammal",
                    "craniate",
                    "hoofed_mammal",
                    "physical_entity",
                    "perissodactyl_mammal",
                    "equus_caballus"
                ],
                "predictions": [
                    {
                        "score": 0.492632846747022,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.49157601995009065,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.427993069852575,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.3935196669277686,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.37857024339781353,
                        "answer": "overnight",
                        "hit": false
                    },
                    {
                        "score": 0.3757602066057255,
                        "answer": "complicated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pony"
                ],
                "rank": 1755,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6871518939733505
            },
            {
                "question verbose": "What is to porcupine ",
                "b": "porcupine",
                "expected answer": [
                    "rodent",
                    "vertebrate",
                    "creature",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "gnawer",
                    "animate_being",
                    "living_thing",
                    "placental_mammal",
                    "craniate",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6242771095192267,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22892250539853287,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22443914946299112,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22372213644670164,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21968005268253182,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.21713626657953472,
                        "answer": "overnight",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "porcupine"
                ],
                "rank": 2349,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to quail ",
                "b": "quail",
                "expected answer": [
                    "fowl",
                    "bird",
                    "vertebrate",
                    "poultry",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "poultry",
                    "chordate",
                    "wild_fowl",
                    "wildfowl",
                    "animate_being",
                    "gallinaceous_bird",
                    "craniate",
                    "gallinacean",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.6242413943147463,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23684924624731066,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22652545149813708,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.22233666560719337,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.21844583219315244,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.21765219663679855,
                        "answer": "lose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "quail"
                ],
                "rank": 2675,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to rattlesnake ",
                "b": "rattlesnake",
                "expected answer": [
                    "snake",
                    "reptile",
                    "pit_viper",
                    "serpent",
                    "ophidian"
                ],
                "predictions": [
                    {
                        "score": 0.6307533949130397,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23673636235724374,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21505946214606128,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.21373639902237654,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21360064360608846,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.2119475171847955,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rattlesnake"
                ],
                "rank": 6,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to squirrel ",
                "b": "squirrel",
                "expected answer": [
                    "rodent",
                    "vertebrate",
                    "creature",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "gnawer",
                    "animate_being",
                    "living_thing",
                    "placental_mammal",
                    "craniate",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6234585199633754,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23283921437193997,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22401835177639864,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2196285810114148,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.2157867364732951,
                        "answer": "overnight",
                        "hit": false
                    },
                    {
                        "score": 0.21464885691594138,
                        "answer": "lose",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "squirrel"
                ],
                "rank": 2461,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to stegosaurus ",
                "b": "stegosaurus",
                "expected answer": [
                    "dinosaur",
                    "reptile",
                    "armored_dinosaur",
                    "archosaur",
                    "archosaurian",
                    "archosaurian_reptile",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6308871979770331,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22778622646968885,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.2230801377566761,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21717542811667762,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.21490196880962603,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21328715174220655,
                        "answer": "poutrage",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stegosaurus"
                ],
                "rank": 8,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6553508341312408
            },
            {
                "question verbose": "What is to tiger ",
                "b": "tiger",
                "expected answer": [
                    "feline",
                    "cat",
                    "beast",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "felid",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "eutherian_mammal",
                    "animate_being",
                    "placental_mammal",
                    "craniate",
                    "big_cat"
                ],
                "predictions": [
                    {
                        "score": 0.6310384862355574,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22637469418219885,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.22585881522510287,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.22410052356088986,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.2240328011471133,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2172396152439002,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tiger"
                ],
                "rank": 851,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5880415812134743
            },
            {
                "question verbose": "What is to triceratops ",
                "b": "triceratops",
                "expected answer": [
                    "dinosaur",
                    "reptile",
                    "ceratopsian",
                    "horned_dinosaur",
                    "archosaur",
                    "archosaurian",
                    "archosaurian_reptile",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.630639351836782,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22251731246587592,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21859820345652964,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21797617048532605,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21487619097353386,
                        "answer": "poutrage",
                        "hit": false
                    },
                    {
                        "score": 0.21155591448809966,
                        "answer": "happening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "triceratops"
                ],
                "rank": 6,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6553508341312408
            },
            {
                "question verbose": "What is to turkey ",
                "b": "turkey",
                "expected answer": [
                    "fowl",
                    "bird",
                    "vertebrate",
                    "poultry",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "poultry",
                    "chordate",
                    "domestic_fowl",
                    "animate_being",
                    "gallinaceous_bird",
                    "craniate",
                    "gallinacean",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.44227681535386004,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.4320709920243401,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.3968151019236353,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.3626036426761387,
                        "answer": "danger",
                        "hit": false
                    },
                    {
                        "score": 0.35410014148929125,
                        "answer": "discouraged",
                        "hit": false
                    },
                    {
                        "score": 0.3482698527098815,
                        "answer": "bailouts",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "turkey"
                ],
                "rank": 2562,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6434292942285538
            },
            {
                "question verbose": "What is to tyrannosaurus ",
                "b": "tyrannosaurus",
                "expected answer": [
                    "dinosaur",
                    "reptile",
                    "theropod",
                    "theropod_dinosaur",
                    "bird-footed_dinosaur",
                    "archosaur",
                    "archosaurian",
                    "archosaurian_reptile",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.63082709981782,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.22443278202156186,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.2217270606797858,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.21765743106286803,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.21218379523055608,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.20999044299942285,
                        "answer": "overnight",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tyrannosaurus"
                ],
                "rank": 8,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6553508341312408
            },
            {
                "question verbose": "What is to velociraptor ",
                "b": "velociraptor",
                "expected answer": [
                    "dinosaur",
                    "reptile",
                    "maniraptor",
                    "archosaur",
                    "archosaurian",
                    "archosaurian_reptile",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6314029688426965,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.23734053812455622,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.23048100028299226,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.22798406844312316,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.22666072539325238,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.21586851014114125,
                        "answer": "overnight",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "velociraptor"
                ],
                "rank": 11,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6553508341312408
            },
            {
                "question verbose": "What is to viper ",
                "b": "viper",
                "expected answer": [
                    "snake",
                    "reptile",
                    "snake",
                    "serpent",
                    "ophidian"
                ],
                "predictions": [
                    {
                        "score": 0.6315262453419558,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.24629300113871613,
                        "answer": "diaper",
                        "hit": false
                    },
                    {
                        "score": 0.23046590519122503,
                        "answer": "lose",
                        "hit": false
                    },
                    {
                        "score": 0.22457963251847524,
                        "answer": "happening",
                        "hit": false
                    },
                    {
                        "score": 0.2179438349274172,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.21787183568527727,
                        "answer": "overnight",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "viper"
                ],
                "rank": 8,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to vulture ",
                "b": "vulture",
                "expected answer": [
                    "raptor",
                    "bird",
                    "vertebrate",
                    "creature",
                    "beast",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "chordate",
                    "animate_being",
                    "craniate",
                    "living_thing",
                    "raptorial_bird",
                    "bird_of_prey",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4850116670681071,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.46452232545994676,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.4405612312233199,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.3865064330766354,
                        "answer": "powder",
                        "hit": false
                    },
                    {
                        "score": 0.38351998320172415,
                        "answer": "rediscover",
                        "hit": false
                    },
                    {
                        "score": 0.37573735205671704,
                        "answer": "border",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vulture"
                ],
                "rank": 914,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.611456423997879
            },
            {
                "question verbose": "What is to wolf ",
                "b": "wolf",
                "expected answer": [
                    "canine",
                    "vertebrate",
                    "creature",
                    "beast",
                    "canid",
                    "being",
                    "animal",
                    "organism",
                    "fauna",
                    "placental",
                    "carnivore",
                    "chordate",
                    "eutherian",
                    "mammal",
                    "mammalian",
                    "animate_being",
                    "eutherian_mammal",
                    "locomote",
                    "placental_mammal",
                    "craniate",
                    "domesticated_animal",
                    "physical_entity",
                    "domestic_animal",
                    "living_thing"
                ],
                "predictions": [
                    {
                        "score": 0.42284752585600477,
                        "answer": "snake",
                        "hit": false
                    },
                    {
                        "score": 0.41839511254280115,
                        "answer": "dinosaur",
                        "hit": false
                    },
                    {
                        "score": 0.41392559293688513,
                        "answer": "feline",
                        "hit": false
                    },
                    {
                        "score": 0.37470827373598525,
                        "answer": "unbridled",
                        "hit": false
                    },
                    {
                        "score": 0.36580574605338145,
                        "answer": "opera",
                        "hit": false
                    },
                    {
                        "score": 0.3435307168855401,
                        "answer": "rediscover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wolf"
                ],
                "rank": 2709,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6455591470003128
            }
        ],
        "result": {
            "cnt_questions_correct": 1,
            "cnt_questions_total": 50,
            "accuracy": 0.02
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L01 [hypernyms - animals].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "5cdf8501-45c4-4dd9-a60d-d3511c04a0a6",
            "timestamp": "2020-10-22T15:57:46.316798"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to armchair ",
                "b": "armchair",
                "expected answer": [
                    "chair",
                    "seat",
                    "piece_of_furniture",
                    "article_of_furniture",
                    "furnishing",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4663009477398018,
                        "answer": "rogue",
                        "hit": false
                    },
                    {
                        "score": 0.4538643231539762,
                        "answer": "sunset",
                        "hit": false
                    },
                    {
                        "score": 0.4507196713277322,
                        "answer": "dot",
                        "hit": false
                    },
                    {
                        "score": 0.43995366662748236,
                        "answer": "glamorous",
                        "hit": false
                    },
                    {
                        "score": 0.43895980189784195,
                        "answer": "newarc",
                        "hit": false
                    },
                    {
                        "score": 0.4387417203310845,
                        "answer": "trod",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "armchair"
                ],
                "rank": 6738,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6716059297323227
            },
            {
                "question verbose": "What is to blender ",
                "b": "blender",
                "expected answer": [
                    "appliance",
                    "mixer",
                    "kitchen_utensil",
                    "utensil",
                    "implement",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4735203579124619,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34227295554589526,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3082749300407449,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.28668388728208727,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27360152140472893,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.2683321075321044,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "blender"
                ],
                "rank": 5069,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to bracelet ",
                "b": "bracelet",
                "expected answer": [
                    "jewelry",
                    "band",
                    "strip",
                    "slip",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "jewellery",
                    "adornment",
                    "decoration",
                    "ornament",
                    "ornamentation",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.47211946635202223,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3188314777125028,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30839205941010367,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2745711436429402,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.2684032284491635,
                        "answer": "dot",
                        "hit": false
                    },
                    {
                        "score": 0.26308669204713436,
                        "answer": "probably",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bracelet"
                ],
                "rank": 3690,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to brooch ",
                "b": "brooch",
                "expected answer": [
                    "jewelry",
                    "jewellery",
                    "adornment",
                    "decoration",
                    "ornament",
                    "ornamentation",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4728693922000458,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.324616511530954,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3171237984399355,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2823577950627401,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27668266449553075,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.27118121234289433,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brooch"
                ],
                "rank": 8304,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cake ",
                "b": "cake",
                "expected answer": [
                    "dessert",
                    "baked_goods",
                    "food",
                    "solid_food",
                    "course",
                    "nutriment",
                    "nourishment",
                    "nutrition",
                    "sustenance",
                    "aliment",
                    "alimentation",
                    "victuals",
                    "food",
                    "nutrient",
                    "substance",
                    "matter",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.40442668224383704,
                        "answer": "newarc",
                        "hit": false
                    },
                    {
                        "score": 0.4010791097696653,
                        "answer": "clothes",
                        "hit": false
                    },
                    {
                        "score": 0.38984449909564733,
                        "answer": "retailer",
                        "hit": false
                    },
                    {
                        "score": 0.38966244587699783,
                        "answer": "detailed",
                        "hit": false
                    },
                    {
                        "score": 0.3871666586817569,
                        "answer": "chick",
                        "hit": false
                    },
                    {
                        "score": 0.3857577925898285,
                        "answer": "capulet",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cake"
                ],
                "rank": 4103,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7823480069637299
            },
            {
                "question verbose": "What is to computer ",
                "b": "computer",
                "expected answer": [
                    "device",
                    "machine",
                    "gadget",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "expert",
                    "person",
                    "individual",
                    "someone",
                    "somebody",
                    "mortal",
                    "soul",
                    "causal_agent",
                    "cause",
                    "causal_agency",
                    "organism",
                    "being",
                    "physical_entity",
                    "living_thing",
                    "animate_thing",
                    "entity",
                    "unit",
                    "object",
                    "physical_object",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.43477629697746206,
                        "answer": "ipad",
                        "hit": false
                    },
                    {
                        "score": 0.433151277129653,
                        "answer": "android",
                        "hit": false
                    },
                    {
                        "score": 0.41507405777475315,
                        "answer": "mobile",
                        "hit": false
                    },
                    {
                        "score": 0.4083396522324763,
                        "answer": "optional",
                        "hit": false
                    },
                    {
                        "score": 0.4023245339142186,
                        "answer": "convertible",
                        "hit": false
                    },
                    {
                        "score": 0.397835910934664,
                        "answer": "device",
                        "hit": true
                    }
                ],
                "set_exclude": [
                    "computer"
                ],
                "rank": 5,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7594298422336578
            },
            {
                "question verbose": "What is to croissant ",
                "b": "croissant",
                "expected answer": [
                    "pastry",
                    "bun",
                    "roll",
                    "bread",
                    "breadstuff",
                    "staff_of_life",
                    "baked_goods",
                    "starches",
                    "food",
                    "solid_food",
                    "foodstuff",
                    "food_product",
                    "solid",
                    "food",
                    "nutrient",
                    "matter",
                    "substance",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.47413488439548973,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32264277561428956,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.29668281716859596,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2683233000810358,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2658602322547703,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2566471720653996,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "croissant"
                ],
                "rank": 2330,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cup ",
                "b": "cup",
                "expected answer": [
                    "tableware",
                    "crockery",
                    "dishware",
                    "ware",
                    "article",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "container"
                ],
                "predictions": [
                    {
                        "score": 0.33151139656362966,
                        "answer": "turtle",
                        "hit": false
                    },
                    {
                        "score": 0.3263565865432605,
                        "answer": "stacked",
                        "hit": false
                    },
                    {
                        "score": 0.31688659395953533,
                        "answer": "olga",
                        "hit": false
                    },
                    {
                        "score": 0.315687176946776,
                        "answer": "feed",
                        "hit": false
                    },
                    {
                        "score": 0.31565412337354404,
                        "answer": "station",
                        "hit": false
                    },
                    {
                        "score": 0.31085261939052583,
                        "answer": "servicenanny",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cup"
                ],
                "rank": 36,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.570232667028904
            },
            {
                "question verbose": "What is to denim ",
                "b": "denim",
                "expected answer": [
                    "fabric",
                    "cloth",
                    "material",
                    "textile",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "garment",
                    "clothing",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.486282568506804,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3539966926369209,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3124092843924104,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.28991362665445997,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2824444295172963,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.25275293404861043,
                        "answer": "godly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "denim"
                ],
                "rank": 2055,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5881936848163605
            },
            {
                "question verbose": "What is to deodorant ",
                "b": "deodorant",
                "expected answer": [
                    "toiletry",
                    "toilet_articles",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.47323035279851183,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3236679966529097,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.305744696290855,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.28200244361455595,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.27449921559634266,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.25595879762817125,
                        "answer": "dot",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deodorant"
                ],
                "rank": 7427,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to desk ",
                "b": "desk",
                "expected answer": [
                    "furniture",
                    "table",
                    "piece_of_furniture",
                    "article_of_furniture",
                    "furnishing",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.46940651540866196,
                        "answer": "stagnant",
                        "hit": false
                    },
                    {
                        "score": 0.4312598960722116,
                        "answer": "capulet",
                        "hit": false
                    },
                    {
                        "score": 0.4248392658521811,
                        "answer": "kickstarter",
                        "hit": false
                    },
                    {
                        "score": 0.42463882770807226,
                        "answer": "ui",
                        "hit": false
                    },
                    {
                        "score": 0.4244433048432774,
                        "answer": "installed",
                        "hit": false
                    },
                    {
                        "score": 0.42056712277319636,
                        "answer": "mish",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "desk"
                ],
                "rank": 69,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.769163966178894
            },
            {
                "question verbose": "What is to diary ",
                "b": "diary",
                "expected answer": [
                    "journal",
                    "writing",
                    "written_material",
                    "piece_of_writing",
                    "written_communication",
                    "written_language",
                    "black_and_white",
                    "communication",
                    "abstraction",
                    "abstract_entity",
                    "entity",
                    "journal",
                    "book",
                    "volume",
                    "product",
                    "production",
                    "creation",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.39355309724638393,
                        "answer": "rogue",
                        "hit": false
                    },
                    {
                        "score": 0.37779017286580513,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.3657833987056243,
                        "answer": "redmond",
                        "hit": false
                    },
                    {
                        "score": 0.35658808417156473,
                        "answer": "inspiring",
                        "hit": false
                    },
                    {
                        "score": 0.3555905304563447,
                        "answer": "span",
                        "hit": false
                    },
                    {
                        "score": 0.3549445448074571,
                        "answer": "listening",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "diary"
                ],
                "rank": 5342,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5419655367732048
            },
            {
                "question verbose": "What is to dishwasher ",
                "b": "dishwasher",
                "expected answer": [
                    "appliance",
                    "machine",
                    "entity",
                    "unit",
                    "object",
                    "physical_object",
                    "white_goods",
                    "home_appliance",
                    "household_appliance",
                    "durables",
                    "durable_goods",
                    "consumer_durables",
                    "consumer_goods",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.389438328350547,
                        "answer": "touched",
                        "hit": false
                    },
                    {
                        "score": 0.3877897067567954,
                        "answer": "channel",
                        "hit": false
                    },
                    {
                        "score": 0.38000249431809946,
                        "answer": "door",
                        "hit": false
                    },
                    {
                        "score": 0.3796539061433282,
                        "answer": "impress",
                        "hit": false
                    },
                    {
                        "score": 0.37042501505420683,
                        "answer": "rogue",
                        "hit": false
                    },
                    {
                        "score": 0.37010940079267113,
                        "answer": "hacking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dishwasher"
                ],
                "rank": 4415,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5258790403604507
            },
            {
                "question verbose": "What is to dress ",
                "b": "dress",
                "expected answer": [
                    "clothes",
                    "garment",
                    "clothing",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.3206851810705241,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.31182984709729006,
                        "answer": "anthropology",
                        "hit": false
                    },
                    {
                        "score": 0.308862187098355,
                        "answer": "furniture",
                        "hit": false
                    },
                    {
                        "score": 0.3063102857852802,
                        "answer": "cuban",
                        "hit": false
                    },
                    {
                        "score": 0.3044045089787804,
                        "answer": "unite",
                        "hit": false
                    },
                    {
                        "score": 0.30309134897311385,
                        "answer": "potus",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dress"
                ],
                "rank": 494,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6568032801151276
            },
            {
                "question verbose": "What is to fridge ",
                "b": "fridge",
                "expected answer": [
                    "appliance",
                    "icebox",
                    "white_goods",
                    "home_appliance",
                    "household_appliance",
                    "appliance",
                    "durables",
                    "durable_goods",
                    "consumer_durables",
                    "consumer_goods",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4726060638007624,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33480457185443124,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.31645572665084143,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.272381016665985,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27022938713428196,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2689025076501284,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fridge"
                ],
                "rank": 7955,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to gasoline ",
                "b": "gasoline",
                "expected answer": [
                    "fuel",
                    "hydrocarbon",
                    "substance",
                    "organic_compound",
                    "matter",
                    "compound",
                    "chemical_compound",
                    "physical_entity",
                    "chemical",
                    "chemical_substance",
                    "entity",
                    "material",
                    "stuff",
                    "substance",
                    "part",
                    "portion",
                    "component_part",
                    "component",
                    "constituent",
                    "relation",
                    "abstraction",
                    "abstract_entity"
                ],
                "predictions": [
                    {
                        "score": 0.3824074102977832,
                        "answer": "rubble",
                        "hit": false
                    },
                    {
                        "score": 0.38162545836713063,
                        "answer": "parlor",
                        "hit": false
                    },
                    {
                        "score": 0.3759775085778825,
                        "answer": "east",
                        "hit": false
                    },
                    {
                        "score": 0.3639825400365428,
                        "answer": "zmievski",
                        "hit": false
                    },
                    {
                        "score": 0.3589325976828374,
                        "answer": "humbly",
                        "hit": false
                    },
                    {
                        "score": 0.35739963987550794,
                        "answer": "updike",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gasoline"
                ],
                "rank": 2823,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6581296771764755
            },
            {
                "question verbose": "What is to grapefruit ",
                "b": "grapefruit",
                "expected answer": [
                    "citrus",
                    "citrus_fruit",
                    "citrous_fruit",
                    "fruit",
                    "edible_fruit",
                    "fruit",
                    "produce",
                    "green_goods",
                    "green_groceries",
                    "garden_truck",
                    "food",
                    "matter",
                    "natural_object",
                    "physical_entity",
                    "unit",
                    "entity",
                    "object",
                    "physical_object"
                ],
                "predictions": [
                    {
                        "score": 0.47403375723189467,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3208828068722568,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30390621788047517,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27465297194621185,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.26694722488711004,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.26286028886612944,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "grapefruit"
                ],
                "rank": 12,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to hairnet ",
                "b": "hairnet",
                "expected answer": [
                    "net",
                    "network",
                    "mesh",
                    "meshing",
                    "meshwork",
                    "fabric",
                    "cloth",
                    "material",
                    "textile",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4865594762703278,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.323615878123995,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.3184830345309986,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.29426524550994937,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.283178981824526,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.2690301860601049,
                        "answer": "dot",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hairnet"
                ],
                "rank": 9588,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5575529374182224
            },
            {
                "question verbose": "What is to hairpin ",
                "b": "hairpin",
                "expected answer": [
                    "pin",
                    "fastener",
                    "fastening",
                    "holdfast",
                    "fixing",
                    "restraint",
                    "constraint",
                    "device",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4857025748863727,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33686163005687275,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3019989184965999,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.28259489481312305,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2700419333661377,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.25649672013219665,
                        "answer": "dot",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hairpin"
                ],
                "rank": 492,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6968243718147278
            },
            {
                "question verbose": "What is to hamburger ",
                "b": "hamburger",
                "expected answer": [
                    "sandwich",
                    "snack_food",
                    "dish",
                    "nutriment",
                    "nourishment",
                    "nutrition",
                    "sustenance",
                    "aliment",
                    "alimentation",
                    "victuals",
                    "food",
                    "nutrient",
                    "substance",
                    "matter",
                    "physical_entity",
                    "entity",
                    "beef",
                    "boeuf",
                    "meat",
                    "food",
                    "solid_food",
                    "solid",
                    "matter",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.5323005105562638,
                        "answer": "contributed",
                        "hit": false
                    },
                    {
                        "score": 0.49947542806601086,
                        "answer": "fortress",
                        "hit": false
                    },
                    {
                        "score": 0.4810111282974061,
                        "answer": "superstorm",
                        "hit": false
                    },
                    {
                        "score": 0.4808970548173417,
                        "answer": "specializes",
                        "hit": false
                    },
                    {
                        "score": 0.4770366561701569,
                        "answer": "entrepreneur",
                        "hit": false
                    },
                    {
                        "score": 0.4728510509691243,
                        "answer": "haigh",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hamburger"
                ],
                "rank": 3428,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5606126226484776
            },
            {
                "question verbose": "What is to jacket ",
                "b": "jacket",
                "expected answer": [
                    "clothes",
                    "coat",
                    "overgarment",
                    "outer_garment",
                    "garment",
                    "clothing",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4375547595462508,
                        "answer": "coat",
                        "hit": true
                    },
                    {
                        "score": 0.4033203564931715,
                        "answer": "socialite",
                        "hit": false
                    },
                    {
                        "score": 0.3768921564137967,
                        "answer": "bedspread",
                        "hit": false
                    },
                    {
                        "score": 0.375786798324947,
                        "answer": "fur",
                        "hit": false
                    },
                    {
                        "score": 0.37403845073284275,
                        "answer": "mink",
                        "hit": false
                    },
                    {
                        "score": 0.3698128282717916,
                        "answer": "carpet",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jacket"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7073073387145996
            },
            {
                "question verbose": "What is to jeans ",
                "b": "jeans",
                "expected answer": [
                    "trousers",
                    "pants",
                    "workwear",
                    "garment",
                    "clothing",
                    "clothes",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4743647573404941,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3319552168059189,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3103739868259663,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27840938423023515,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2625569291285743,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.26169825136478253,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jeans"
                ],
                "rank": 324,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to juicer ",
                "b": "juicer",
                "expected answer": [
                    "utensil",
                    "physical_entity",
                    "entity",
                    "unit",
                    "object",
                    "physical_object",
                    "squeezer",
                    "kitchen_utensil",
                    "implement",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4742967636258412,
                        "answer": "jukebox",
                        "hit": false
                    },
                    {
                        "score": 0.4646831006355399,
                        "answer": "humbly",
                        "hit": false
                    },
                    {
                        "score": 0.46110152953062056,
                        "answer": "frugally",
                        "hit": false
                    },
                    {
                        "score": 0.4599200947201006,
                        "answer": "asshats",
                        "hit": false
                    },
                    {
                        "score": 0.45386091453831495,
                        "answer": "monavie",
                        "hit": false
                    },
                    {
                        "score": 0.4500597426712271,
                        "answer": "unprepared",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "juicer"
                ],
                "rank": 8509,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6543298363685608
            },
            {
                "question verbose": "What is to lemon ",
                "b": "lemon",
                "expected answer": [
                    "citrus",
                    "citrus_fruit",
                    "citrous_fruit",
                    "fruit",
                    "edible_fruit",
                    "fruit",
                    "produce",
                    "green_goods",
                    "green_groceries",
                    "garden_truck",
                    "food",
                    "matter",
                    "natural_object",
                    "physical_entity",
                    "unit",
                    "entity",
                    "object",
                    "physical_object"
                ],
                "predictions": [
                    {
                        "score": 0.42782112720382687,
                        "answer": "haley",
                        "hit": false
                    },
                    {
                        "score": 0.41704118376210986,
                        "answer": "reselling",
                        "hit": false
                    },
                    {
                        "score": 0.4161667094764785,
                        "answer": "clothes",
                        "hit": false
                    },
                    {
                        "score": 0.4155925549888259,
                        "answer": "capulet",
                        "hit": false
                    },
                    {
                        "score": 0.4141747217003419,
                        "answer": "rebel",
                        "hit": false
                    },
                    {
                        "score": 0.4080513129831376,
                        "answer": "richer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lemon"
                ],
                "rank": 106,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6479519158601761
            },
            {
                "question verbose": "What is to lotion ",
                "b": "lotion",
                "expected answer": [
                    "toiletry",
                    "toilet_articles",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.47111577393172804,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31598802977664525,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30742931440981114,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27541204174878775,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.2720921203698993,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.2674268482880609,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lotion"
                ],
                "rank": 6180,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to mascara ",
                "b": "mascara",
                "expected answer": [
                    "makeup",
                    "make-up",
                    "war_paint",
                    "cosmetic",
                    "toiletry",
                    "toilet_articles",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.485664358534449,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3193645192243578,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3027519463290605,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2723313481342872,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.2695610607581848,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.263286191913312,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mascara"
                ],
                "rank": 6998,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5915014296770096
            },
            {
                "question verbose": "What is to necklace ",
                "b": "necklace",
                "expected answer": [
                    "jewelry",
                    "jewellery",
                    "adornment",
                    "decoration",
                    "ornament",
                    "ornamentation",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4736271128461824,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33946859901424004,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30477396168029935,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2929946745494246,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.271690583365191,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.2658728924744858,
                        "answer": "except",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "necklace"
                ],
                "rank": 7723,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to notebook ",
                "b": "notebook",
                "expected answer": [
                    "book",
                    "volume",
                    "product",
                    "production",
                    "creation",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "portable_computer",
                    "personal_computer",
                    "pc",
                    "microcomputer",
                    "digital_computer",
                    "computer",
                    "computing_machine",
                    "computing_device",
                    "data_processor",
                    "electronic_computer",
                    "information_processing_system",
                    "machine",
                    "device",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.48768943001104315,
                        "answer": "clothes",
                        "hit": false
                    },
                    {
                        "score": 0.47497510788613395,
                        "answer": "nostalgic",
                        "hit": false
                    },
                    {
                        "score": 0.4680773332001503,
                        "answer": "profile",
                        "hit": false
                    },
                    {
                        "score": 0.46746715999451866,
                        "answer": "perishable",
                        "hit": false
                    },
                    {
                        "score": 0.45542456868226616,
                        "answer": "profoundly",
                        "hit": false
                    },
                    {
                        "score": 0.4507084510007583,
                        "answer": "wookiee",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "notebook"
                ],
                "rank": 393,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6356008797883987
            },
            {
                "question verbose": "What is to notepad ",
                "b": "notepad",
                "expected answer": [
                    "pad",
                    "pad_of_paper",
                    "tablet",
                    "paper",
                    "material",
                    "stuff",
                    "substance",
                    "matter",
                    "part",
                    "portion",
                    "component_part",
                    "component",
                    "constituent",
                    "physical_entity",
                    "relation",
                    "entity",
                    "abstraction",
                    "abstract_entity"
                ],
                "predictions": [
                    {
                        "score": 0.4827569089264458,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32492535813676715,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.288572857991909,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.272146274053699,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2552328550586431,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.25290954294157,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "notepad"
                ],
                "rank": 909,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6449885219335556
            },
            {
                "question verbose": "What is to pastry ",
                "b": "pastry",
                "expected answer": [
                    "food",
                    "dough",
                    "concoction",
                    "mixture",
                    "intermixture",
                    "foodstuff",
                    "food_product",
                    "food",
                    "nutrient",
                    "substance",
                    "matter",
                    "physical_entity",
                    "entity",
                    "baked_goods",
                    "solid_food",
                    "solid",
                    "matter",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4867230574276778,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3396359847806775,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30284513767331384,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2883158463250145,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2760727769896124,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.27190482893200024,
                        "answer": "dot",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pastry"
                ],
                "rank": 946,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5424718111753464
            },
            {
                "question verbose": "What is to peach ",
                "b": "peach",
                "expected answer": [
                    "fruit",
                    "drupe",
                    "stone_fruit",
                    "edible_fruit",
                    "fruit",
                    "produce",
                    "green_goods",
                    "green_groceries",
                    "garden_truck",
                    "food",
                    "matter",
                    "natural_object",
                    "physical_entity",
                    "unit",
                    "entity",
                    "object",
                    "physical_object"
                ],
                "predictions": [
                    {
                        "score": 0.5279624202414039,
                        "answer": "clothes",
                        "hit": false
                    },
                    {
                        "score": 0.49549180645742724,
                        "answer": "custom",
                        "hit": false
                    },
                    {
                        "score": 0.4870568387492434,
                        "answer": "frugally",
                        "hit": false
                    },
                    {
                        "score": 0.4818691038801423,
                        "answer": "printing",
                        "hit": false
                    },
                    {
                        "score": 0.47028529146967,
                        "answer": "stagnant",
                        "hit": false
                    },
                    {
                        "score": 0.4698062792396189,
                        "answer": "capulet",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "peach"
                ],
                "rank": 829,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7738497257232666
            },
            {
                "question verbose": "What is to perfume ",
                "b": "perfume",
                "expected answer": [
                    "toiletry",
                    "toilet_articles",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.471681648515362,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32362340428083025,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3140238285809757,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27877363596889787,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2663146046326357,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.26607268966523556,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "perfume"
                ],
                "rank": 8084,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to photo ",
                "b": "photo",
                "expected answer": [
                    "picture",
                    "image",
                    "representation",
                    "creation",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.3475235376494095,
                        "answer": "visiting",
                        "hit": false
                    },
                    {
                        "score": 0.29857775837087003,
                        "answer": "clip",
                        "hit": false
                    },
                    {
                        "score": 0.29486793972664166,
                        "answer": "application",
                        "hit": false
                    },
                    {
                        "score": 0.2874864289134987,
                        "answer": "salvation",
                        "hit": false
                    },
                    {
                        "score": 0.283218908966772,
                        "answer": "device",
                        "hit": false
                    },
                    {
                        "score": 0.2828113159859822,
                        "answer": "article",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "photo"
                ],
                "rank": 5905,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6245084255933762
            },
            {
                "question verbose": "What is to pie ",
                "b": "pie",
                "expected answer": [
                    "pastry",
                    "baked_goods",
                    "food",
                    "solid_food",
                    "solid",
                    "matter",
                    "physical_entity",
                    "entity",
                    "indo-european",
                    "indo-european_language",
                    "indo-hittite",
                    "natural_language",
                    "tongue",
                    "language",
                    "linguistic_communication",
                    "communication",
                    "abstraction",
                    "abstract_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.47443623029155096,
                        "answer": "cent",
                        "hit": false
                    },
                    {
                        "score": 0.44520259738662743,
                        "answer": "operating",
                        "hit": false
                    },
                    {
                        "score": 0.4350203844642549,
                        "answer": "morcrosoftapple",
                        "hit": false
                    },
                    {
                        "score": 0.40024396501548215,
                        "answer": "ratio",
                        "hit": false
                    },
                    {
                        "score": 0.39190423051776957,
                        "answer": "brunswick",
                        "hit": false
                    },
                    {
                        "score": 0.3851554118061056,
                        "answer": "cash",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pie"
                ],
                "rank": 4674,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6404280811548233
            },
            {
                "question verbose": "What is to plum ",
                "b": "plum",
                "expected answer": [
                    "fruit",
                    "drupe",
                    "stone_fruit",
                    "edible_fruit",
                    "fruit",
                    "produce",
                    "green_goods",
                    "green_groceries",
                    "garden_truck",
                    "food",
                    "matter",
                    "natural_object",
                    "physical_entity",
                    "unit",
                    "entity",
                    "object",
                    "physical_object"
                ],
                "predictions": [
                    {
                        "score": 0.48424003549946926,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33495619602780513,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.2909368805735345,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2734768747357289,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2668251918742375,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2542144433047447,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "plum"
                ],
                "rank": 25,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6836603879928589
            },
            {
                "question verbose": "What is to postcard ",
                "b": "postcard",
                "expected answer": [
                    "card",
                    "correspondence",
                    "first_class",
                    "1st_class",
                    "first-class_mail",
                    "1st-class_mail",
                    "written_communication",
                    "written_language",
                    "black_and_white",
                    "mail",
                    "communication",
                    "message",
                    "abstraction",
                    "abstract_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.5131448230354064,
                        "answer": "freezing",
                        "hit": false
                    },
                    {
                        "score": 0.5122647973470313,
                        "answer": "clothes",
                        "hit": false
                    },
                    {
                        "score": 0.4910152666906129,
                        "answer": "wwwcarolineclemmonscom",
                        "hit": false
                    },
                    {
                        "score": 0.4891374555996297,
                        "answer": "passionate",
                        "hit": false
                    },
                    {
                        "score": 0.4855956438793457,
                        "answer": "rogue",
                        "hit": false
                    },
                    {
                        "score": 0.47856925702194114,
                        "answer": "monavie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "postcard"
                ],
                "rank": 1558,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6248230040073395
            },
            {
                "question verbose": "What is to shelf ",
                "b": "shelf",
                "expected answer": [
                    "furniture",
                    "support",
                    "piece_of_furniture",
                    "article_of_furniture",
                    "furnishing",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.5113806209804439,
                        "answer": "contributed",
                        "hit": false
                    },
                    {
                        "score": 0.4876780303037813,
                        "answer": "publication",
                        "hit": false
                    },
                    {
                        "score": 0.48684928650913006,
                        "answer": "chronicling",
                        "hit": false
                    },
                    {
                        "score": 0.43563310294392094,
                        "answer": "enlighten",
                        "hit": false
                    },
                    {
                        "score": 0.4161235695429297,
                        "answer": "worried",
                        "hit": false
                    },
                    {
                        "score": 0.4135249930234347,
                        "answer": "jan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shelf"
                ],
                "rank": 213,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7195537686347961
            },
            {
                "question verbose": "What is to shirt ",
                "b": "shirt",
                "expected answer": [
                    "clothes",
                    "garment",
                    "clothing",
                    "garment",
                    "clothing",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.48251822989872506,
                        "answer": "trek",
                        "hit": false
                    },
                    {
                        "score": 0.39092507268225674,
                        "answer": "succeed",
                        "hit": false
                    },
                    {
                        "score": 0.3820073811343958,
                        "answer": "staple",
                        "hit": false
                    },
                    {
                        "score": 0.38187410168328356,
                        "answer": "classic",
                        "hit": false
                    },
                    {
                        "score": 0.3720949969450232,
                        "answer": "comedy",
                        "hit": false
                    },
                    {
                        "score": 0.37074230512996303,
                        "answer": "masterpiece",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shirt"
                ],
                "rank": 14,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7298619449138641
            },
            {
                "question verbose": "What is to sidewalk ",
                "b": "sidewalk",
                "expected answer": [
                    "walk",
                    "walkway",
                    "paseo",
                    "path",
                    "way",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4872161643060381,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3330065547105206,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.306764476554493,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2801520577002826,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.27549596115803765,
                        "answer": "sc",
                        "hit": false
                    },
                    {
                        "score": 0.2753480070122935,
                        "answer": "wyatt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sidewalk"
                ],
                "rank": 5131,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4824319425970316
            },
            {
                "question verbose": "What is to skirt ",
                "b": "skirt",
                "expected answer": [
                    "clothes",
                    "garment",
                    "clothing",
                    "garment",
                    "clothing",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4567139703660146,
                        "answer": "frugally",
                        "hit": false
                    },
                    {
                        "score": 0.45098024647361107,
                        "answer": "rogue",
                        "hit": false
                    },
                    {
                        "score": 0.428218849630734,
                        "answer": "cancel",
                        "hit": false
                    },
                    {
                        "score": 0.41269841961811027,
                        "answer": "touched",
                        "hit": false
                    },
                    {
                        "score": 0.4055135504728055,
                        "answer": "rub",
                        "hit": false
                    },
                    {
                        "score": 0.4052119490301158,
                        "answer": "module",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "skirt"
                ],
                "rank": 16,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7704052627086639
            },
            {
                "question verbose": "What is to sofa ",
                "b": "sofa",
                "expected answer": [
                    "furniture",
                    "seat",
                    "piece_of_furniture",
                    "article_of_furniture",
                    "furnishing",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.48519916804325813,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32398667274834814,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.313872390470449,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2698000820573792,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2688057613841611,
                        "answer": "dot",
                        "hit": false
                    },
                    {
                        "score": 0.2658973042673184,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sofa"
                ],
                "rank": 1179,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6058728024363518
            },
            {
                "question verbose": "What is to stapler ",
                "b": "stapler",
                "expected answer": [
                    "device",
                    "machine",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.48707147110941684,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3212318071882369,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3070897608215756,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.270167469716869,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2683266183013197,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.26651921573702503,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "stapler"
                ],
                "rank": 8333,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5007363458862528
            },
            {
                "question verbose": "What is to sunscreen ",
                "b": "sunscreen",
                "expected answer": [
                    "cream",
                    "lotion",
                    "ointment",
                    "emollient",
                    "toiletry",
                    "toilet_articles",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4852855354260523,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3285093077338541,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.30620339646213895,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27889398243632935,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.268492990052061,
                        "answer": "sc",
                        "hit": false
                    },
                    {
                        "score": 0.2673398549705169,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sunscreen"
                ],
                "rank": 1500,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6816111952066422
            },
            {
                "question verbose": "What is to sweater ",
                "b": "sweater",
                "expected answer": [
                    "clothes",
                    "garment",
                    "clothing",
                    "article_of_clothing",
                    "vesture",
                    "wear",
                    "wearable",
                    "habiliment",
                    "consumer_goods",
                    "covering",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4628344429431027,
                        "answer": "clothes",
                        "hit": true
                    },
                    {
                        "score": 0.4541805253011565,
                        "answer": "comicon",
                        "hit": false
                    },
                    {
                        "score": 0.45100862126396407,
                        "answer": "frugally",
                        "hit": false
                    },
                    {
                        "score": 0.4509611197504458,
                        "answer": "luis",
                        "hit": false
                    },
                    {
                        "score": 0.4487616018833863,
                        "answer": "refurbishing",
                        "hit": false
                    },
                    {
                        "score": 0.43684114474339886,
                        "answer": "resentment",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sweater"
                ],
                "rank": 0,
                "landing_b": false,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8141528069972992
            },
            {
                "question verbose": "What is to toaster ",
                "b": "toaster",
                "expected answer": [
                    "appliance",
                    "entity",
                    "unit",
                    "object",
                    "physical_object",
                    "kitchen_appliance",
                    "home_appliance",
                    "household_appliance",
                    "durables",
                    "durable_goods",
                    "consumer_durables",
                    "consumer_goods",
                    "commodity",
                    "trade_good",
                    "good",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4721612891701567,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3294515302787367,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.31596123491200084,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2828510222953672,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2716048403037848,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.262520522670479,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "toaster"
                ],
                "rank": 8188,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to toothbrush ",
                "b": "toothbrush",
                "expected answer": [
                    "brush",
                    "toiletry",
                    "toilet_articles",
                    "implement",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "mustache",
                    "moustache",
                    "facial_hair",
                    "hair",
                    "body_covering",
                    "covering",
                    "natural_covering",
                    "cover",
                    "natural_object",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.48677725484898426,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3347875638796291,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3154932833686297,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2827138974953447,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.26959856348854283,
                        "answer": "wyatt",
                        "hit": false
                    },
                    {
                        "score": 0.2609726806516732,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "toothbrush"
                ],
                "rank": 4986,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5990359038114548
            },
            {
                "question verbose": "What is to tub ",
                "b": "tub",
                "expected answer": [
                    "container",
                    "vessel",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.43654667547806536,
                        "answer": "stagnant",
                        "hit": false
                    },
                    {
                        "score": 0.43403161934021034,
                        "answer": "crawling",
                        "hit": false
                    },
                    {
                        "score": 0.42724613466834727,
                        "answer": "slfsi",
                        "hit": false
                    },
                    {
                        "score": 0.4262067855824802,
                        "answer": "detailed",
                        "hit": false
                    },
                    {
                        "score": 0.4260975988726794,
                        "answer": "newarc",
                        "hit": false
                    },
                    {
                        "score": 0.42560641017149126,
                        "answer": "epically",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tub"
                ],
                "rank": 328,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8137020766735077
            },
            {
                "question verbose": "What is to tv ",
                "b": "tv",
                "expected answer": [
                    "device",
                    "machine",
                    "receiver",
                    "receiving_system",
                    "set",
                    "electronic_equipment",
                    "equipment",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "broadcasting",
                    "telecommunication",
                    "telecom",
                    "medium",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.3663181377632537,
                        "answer": "worried",
                        "hit": false
                    },
                    {
                        "score": 0.36568116310966786,
                        "answer": "radio",
                        "hit": false
                    },
                    {
                        "score": 0.3532220221542499,
                        "answer": "billboard",
                        "hit": false
                    },
                    {
                        "score": 0.3320504279727919,
                        "answer": "meta",
                        "hit": false
                    },
                    {
                        "score": 0.3306815345668193,
                        "answer": "linux",
                        "hit": false
                    },
                    {
                        "score": 0.3302738968762188,
                        "answer": "anonymous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tv"
                ],
                "rank": 18,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5418184623122215
            },
            {
                "question verbose": "What is to vase ",
                "b": "vase",
                "expected answer": [
                    "jar",
                    "vessel",
                    "container",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4746471310679658,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3257883295182567,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3038666108104786,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.2706959791263635,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2691076112926032,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2548808395025198,
                        "answer": "except",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vase"
                ],
                "rank": 1410,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to wristband ",
                "b": "wristband",
                "expected answer": [
                    "band",
                    "strip",
                    "slip",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity",
                    "band",
                    "strip",
                    "slip",
                    "artifact",
                    "artefact",
                    "unit",
                    "object",
                    "physical_object",
                    "physical_entity",
                    "entity"
                ],
                "predictions": [
                    {
                        "score": 0.4858376273280055,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.34017388774308643,
                        "answer": "maybe",
                        "hit": false
                    },
                    {
                        "score": 0.3056987946005446,
                        "answer": "convinced",
                        "hit": false
                    },
                    {
                        "score": 0.27344517792442075,
                        "answer": "probably",
                        "hit": false
                    },
                    {
                        "score": 0.2644172130849948,
                        "answer": "godly",
                        "hit": false
                    },
                    {
                        "score": 0.2633792731786252,
                        "answer": "except",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wristband"
                ],
                "rank": 5520,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5919611155986786
            }
        ],
        "result": {
            "cnt_questions_correct": 2,
            "cnt_questions_total": 50,
            "accuracy": 0.04
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L02 [hypernyms - misc].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "32ccc968-d1ae-40cc-b133-157a31a5d65a",
            "timestamp": "2020-10-22T15:57:47.241320"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to backpack ",
                "b": "backpack",
                "expected answer": [
                    "daypack",
                    "kitbag",
                    "kit_bag"
                ],
                "predictions": [
                    {
                        "score": 0.769786972755934,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27411484558811905,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2719087868158636,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.2645873124978636,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2585998000099734,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.255232227985518,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "backpack"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to bag ",
                "b": "bag",
                "expected answer": [
                    "pouch",
                    "backpack",
                    "knapsack",
                    "packsack",
                    "rucksack",
                    "haversack",
                    "beanbag",
                    "bladder",
                    "pouch",
                    "carryall",
                    "holdall",
                    "tote",
                    "tote",
                    "drawstring",
                    "dust",
                    "vacuum",
                    "envelope",
                    "gasbag",
                    "gamebag",
                    "golf",
                    "gunnysack",
                    "gunny",
                    "burlap",
                    "ice",
                    "mailbag",
                    "postbag",
                    "nosebag",
                    "feedbag",
                    "pannier",
                    "plastic",
                    "purse",
                    "ragbag",
                    "rosin",
                    "sachet",
                    "sack",
                    "poke",
                    "paper",
                    "carrier",
                    "saddlebag",
                    "sandbag",
                    "schoolbag",
                    "shopping",
                    "sick_bag",
                    "sickbag",
                    "skin",
                    "sleeping",
                    "sweat",
                    "tea",
                    "toilet",
                    "sponge",
                    "tool",
                    "tucker-bag"
                ],
                "predictions": [
                    {
                        "score": 0.3184243106232262,
                        "answer": "packard",
                        "hit": false
                    },
                    {
                        "score": 0.31351373258039267,
                        "answer": "thier",
                        "hit": false
                    },
                    {
                        "score": 0.305611841324412,
                        "answer": "abused",
                        "hit": false
                    },
                    {
                        "score": 0.30260351247710543,
                        "answer": "vhs",
                        "hit": false
                    },
                    {
                        "score": 0.29595265146098115,
                        "answer": "fabulously",
                        "hit": false
                    },
                    {
                        "score": 0.293404907127609,
                        "answer": "irishman",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bag"
                ],
                "rank": 34,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5498596057295799
            },
            {
                "question verbose": "What is to bed ",
                "b": "bed",
                "expected answer": [
                    "bunk",
                    "berth",
                    "built",
                    "built-in",
                    "bunk",
                    "cot",
                    "camp",
                    "couch",
                    "deathbed",
                    "double",
                    "four-poster",
                    "hammock",
                    "sack",
                    "marriage_bed",
                    "murphy",
                    "plank-bed",
                    "platform",
                    "sickbed",
                    "single_bed",
                    "sleigh",
                    "trundle",
                    "truckle",
                    "truckle",
                    "twin",
                    "water",
                    "semi-double",
                    "semidouble"
                ],
                "predictions": [
                    {
                        "score": 0.3673779652253633,
                        "answer": "monday",
                        "hit": false
                    },
                    {
                        "score": 0.3602673848072871,
                        "answer": "crisp",
                        "hit": false
                    },
                    {
                        "score": 0.3551757375919591,
                        "answer": "commercially",
                        "hit": false
                    },
                    {
                        "score": 0.3523664122554456,
                        "answer": "unharmed",
                        "hit": false
                    },
                    {
                        "score": 0.34778175194068445,
                        "answer": "borrow",
                        "hit": false
                    },
                    {
                        "score": 0.344971588803154,
                        "answer": "staten",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bed"
                ],
                "rank": 700,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5801986306905746
            },
            {
                "question verbose": "What is to boat ",
                "b": "boat",
                "expected answer": [
                    "ferry",
                    "ark",
                    "barge",
                    "flatboat",
                    "hoy",
                    "lighter",
                    "bumboat",
                    "canal_boat",
                    "narrow_boat",
                    "narrowboat",
                    "ferryboat",
                    "fireboat",
                    "gondola",
                    "guard_boat",
                    "gunboat",
                    "junk",
                    "longboat",
                    "lugger",
                    "mackinaw",
                    "mackinaw",
                    "mailboat",
                    "mail",
                    "packet",
                    "packet",
                    "motorboat",
                    "powerboat",
                    "pilot",
                    "police",
                    "punt",
                    "river_boat",
                    "scow",
                    "sea_boat",
                    "small_boat",
                    "steamboat",
                    "surfboat",
                    "tender_boat",
                    "pinnace",
                    "cutter_boat",
                    "tugboat",
                    "tug",
                    "towboat",
                    "tower_boat",
                    "motorboat",
                    "yacht",
                    "sail",
                    "row",
                    "canoe",
                    "kayak",
                    "paddle",
                    "paddle_boat"
                ],
                "predictions": [
                    {
                        "score": 0.4177102732731143,
                        "answer": "southern",
                        "hit": false
                    },
                    {
                        "score": 0.41029005587805406,
                        "answer": "indiana",
                        "hit": false
                    },
                    {
                        "score": 0.405073542435059,
                        "answer": "latin",
                        "hit": false
                    },
                    {
                        "score": 0.40180059061240814,
                        "answer": "crisp",
                        "hit": false
                    },
                    {
                        "score": 0.39665927528025974,
                        "answer": "staten",
                        "hit": false
                    },
                    {
                        "score": 0.39299878397413185,
                        "answer": "thompson",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "boat"
                ],
                "rank": 3858,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.769915908575058
            },
            {
                "question verbose": "What is to book ",
                "b": "book",
                "expected answer": [
                    "paperback",
                    "album",
                    "folio",
                    "hardback",
                    "hardcover",
                    "journal",
                    "novel",
                    "order",
                    "paper-back",
                    "softback",
                    "soft-cover",
                    "picture",
                    "sketchbook",
                    "sketch",
                    "notebook"
                ],
                "predictions": [
                    {
                        "score": 0.302298364056152,
                        "answer": "captain",
                        "hit": false
                    },
                    {
                        "score": 0.30173863239520865,
                        "answer": "tarot",
                        "hit": false
                    },
                    {
                        "score": 0.28428838796622996,
                        "answer": "osterwalder",
                        "hit": false
                    },
                    {
                        "score": 0.2784458603035587,
                        "answer": "hopkins",
                        "hit": false
                    },
                    {
                        "score": 0.2719109936110161,
                        "answer": "ecclesiastes",
                        "hit": false
                    },
                    {
                        "score": 0.2712016530419376,
                        "answer": "texted",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "book"
                ],
                "rank": 162,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5614943131804466
            },
            {
                "question verbose": "What is to brush ",
                "b": "brush",
                "expected answer": [
                    "toothbrush",
                    "bottlebrush",
                    "bristle",
                    "clothesbrush",
                    "hairbrush",
                    "nailbrush",
                    "paintbrush",
                    "sable",
                    "scrub",
                    "scrubbing",
                    "scrubber",
                    "shaving_brush"
                ],
                "predictions": [
                    {
                        "score": 0.380948094979267,
                        "answer": "bubbleas",
                        "hit": false
                    },
                    {
                        "score": 0.3654332762596783,
                        "answer": "mlbcom",
                        "hit": false
                    },
                    {
                        "score": 0.36395155696753156,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.36349188319869,
                        "answer": "ecb",
                        "hit": false
                    },
                    {
                        "score": 0.3573814565143171,
                        "answer": "elixir",
                        "hit": false
                    },
                    {
                        "score": 0.35603347520744716,
                        "answer": "ibaf",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brush"
                ],
                "rank": 1971,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5990359038114548
            },
            {
                "question verbose": "What is to burger ",
                "b": "burger",
                "expected answer": [
                    "hamburger",
                    "cheeseburger",
                    "50",
                    "50_burger",
                    "angus_burger",
                    "banquet_burger",
                    "barbecue_burger",
                    "bfsandwich",
                    "utter_burge",
                    "buffalo_burger",
                    "california_burger",
                    "chili_burger",
                    "curry_burger",
                    "hamdog",
                    "salmon_burger",
                    "rice_burger",
                    "veggie_burger"
                ],
                "predictions": [
                    {
                        "score": 0.4151788389515795,
                        "answer": "stonewalling",
                        "hit": false
                    },
                    {
                        "score": 0.4050335340360888,
                        "answer": "packard",
                        "hit": false
                    },
                    {
                        "score": 0.40434286101475003,
                        "answer": "latin",
                        "hit": false
                    },
                    {
                        "score": 0.39609651830352477,
                        "answer": "charging",
                        "hit": false
                    },
                    {
                        "score": 0.3955523852564143,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.3932227506760102,
                        "answer": "dubin",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "burger"
                ],
                "rank": 5368,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8274573087692261
            },
            {
                "question verbose": "What is to camera ",
                "b": "camera",
                "expected answer": [
                    "camcorder",
                    "box_camera",
                    "box_kodak",
                    "candid_camera",
                    "digital_camera",
                    "flash_camera",
                    "motion-picture_camera",
                    "movie_camera",
                    "cine-camera",
                    "point-and-shoot_camera",
                    "point-and-shoot",
                    "polaroid_camera",
                    "polaroid_land_camera",
                    "polaroid",
                    "portrait_camera",
                    "reflex_camera",
                    "webcam",
                    "webcamera",
                    "sound_camera"
                ],
                "predictions": [
                    {
                        "score": 0.2222341023649183,
                        "answer": "mahli",
                        "hit": false
                    },
                    {
                        "score": 0.2194717622225132,
                        "answer": "speed",
                        "hit": false
                    },
                    {
                        "score": 0.21674837860637364,
                        "answer": "sponge",
                        "hit": false
                    },
                    {
                        "score": 0.21258152863783078,
                        "answer": "swedish",
                        "hit": false
                    },
                    {
                        "score": 0.20613863564186724,
                        "answer": "brook",
                        "hit": false
                    },
                    {
                        "score": 0.2050614452782719,
                        "answer": "shake",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "camera"
                ],
                "rank": 9961,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.524047264829278
            },
            {
                "question verbose": "What is to candy ",
                "b": "candy",
                "expected answer": [
                    "lollipop",
                    "candy_bar",
                    "carob_bar",
                    "hard_candy",
                    "brandyball",
                    "patty",
                    "bonbon",
                    "brittle",
                    "toffee",
                    "toffy",
                    "butterscotch",
                    "candy_cane",
                    "candy_corn",
                    "caramel",
                    "cotton_candy",
                    "spun_sugar",
                    "candyfloss",
                    "dragee",
                    "fondant",
                    "fudge",
                    "gumdrop",
                    "honey_crisp",
                    "mint",
                    "mint_candy",
                    "horehound",
                    "jelly_bean",
                    "jelly_egg",
                    "kiss",
                    "candy_kiss",
                    "licorice",
                    "liquorice",
                    "life_saver",
                    "sucker",
                    "all-day_sucker",
                    "lozenge",
                    "marshmallow",
                    "marzipan",
                    "marchpane",
                    "nougat",
                    "nougat_bar",
                    "nut_bar",
                    "peanut_bar",
                    "popcorn_ball",
                    "praline",
                    "rock_candy",
                    "rock",
                    "sugar_candy",
                    "sugarplum",
                    "taffy",
                    "truffle",
                    "chocolate_truffle",
                    "turkish_delight",
                    "easter_egg"
                ],
                "predictions": [
                    {
                        "score": 0.3954352289997527,
                        "answer": "fabulously",
                        "hit": false
                    },
                    {
                        "score": 0.3930710704358099,
                        "answer": "hopeless",
                        "hit": false
                    },
                    {
                        "score": 0.38666734466171826,
                        "answer": "outer",
                        "hit": false
                    },
                    {
                        "score": 0.38195282258525926,
                        "answer": "toured",
                        "hit": false
                    },
                    {
                        "score": 0.37689515428719367,
                        "answer": "olive",
                        "hit": false
                    },
                    {
                        "score": 0.3691288620966714,
                        "answer": "mlbcom",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "candy"
                ],
                "rank": 3662,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5682393610477448
            },
            {
                "question verbose": "What is to car ",
                "b": "car",
                "expected answer": [
                    "limousine",
                    "convertible",
                    "ambulance",
                    "beach",
                    "wagon",
                    "station",
                    "estate",
                    "waggon",
                    "bus",
                    "jalopy",
                    "heap",
                    "cab",
                    "hack",
                    "taxi",
                    "taxicab",
                    "compact",
                    "coupe",
                    "cruiser",
                    "police",
                    "patrol",
                    "prowl",
                    "squad",
                    "electric",
                    "hardtop",
                    "hatchback",
                    "hot-rod",
                    "jeep",
                    "landrover",
                    "limo",
                    "loaner",
                    "minicar",
                    "minivan",
                    "model",
                    "pace",
                    "racer",
                    "race",
                    "racing",
                    "roadster",
                    "runabout",
                    "two-seater",
                    "sedan",
                    "saloon",
                    "sports",
                    "sport",
                    "sport",
                    "suv",
                    "stanley",
                    "steamer",
                    "stock",
                    "subcompact",
                    "touring",
                    "tourer",
                    "used",
                    "secondhand"
                ],
                "predictions": [
                    {
                        "score": 0.262376524866244,
                        "answer": "cave",
                        "hit": false
                    },
                    {
                        "score": 0.25958798630428903,
                        "answer": "bus",
                        "hit": true
                    },
                    {
                        "score": 0.25808928095009803,
                        "answer": "oyelowo",
                        "hit": false
                    },
                    {
                        "score": 0.24841806698175892,
                        "answer": "humidity",
                        "hit": false
                    },
                    {
                        "score": 0.24813692723115469,
                        "answer": "swamp",
                        "hit": false
                    },
                    {
                        "score": 0.24441941195142647,
                        "answer": "kneel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "car"
                ],
                "rank": 1,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5839418396353722
            },
            {
                "question verbose": "What is to church ",
                "b": "church",
                "expected answer": [
                    "chapel",
                    "abbey",
                    "basilica",
                    "cathedral",
                    "duomo",
                    "kirk"
                ],
                "predictions": [
                    {
                        "score": 0.2697504352831686,
                        "answer": "hating",
                        "hit": false
                    },
                    {
                        "score": 0.25453742065871365,
                        "answer": "lgbt",
                        "hit": false
                    },
                    {
                        "score": 0.24586303607368132,
                        "answer": "woodward",
                        "hit": false
                    },
                    {
                        "score": 0.24506292763075435,
                        "answer": "white",
                        "hit": false
                    },
                    {
                        "score": 0.2266766702056761,
                        "answer": "oft",
                        "hit": false
                    },
                    {
                        "score": 0.21881939973780604,
                        "answer": "jubilee",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "church"
                ],
                "rank": 9837,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5906175225973129
            },
            {
                "question verbose": "What is to citrus ",
                "b": "citrus",
                "expected answer": [
                    "lemon",
                    "orange",
                    "lime",
                    "mandarin",
                    "tangerine",
                    "yuzu"
                ],
                "predictions": [
                    {
                        "score": 0.7756268473647191,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28147427709454853,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2658053900712122,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2559311133444236,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.2449118252978701,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.24308825782436566,
                        "answer": "puncture",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "citrus"
                ],
                "rank": 214,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6479519158601761
            },
            {
                "question verbose": "What is to cloud ",
                "b": "cloud",
                "expected answer": [
                    "thundercloud",
                    "cirrocumulus",
                    "cirrocumulus",
                    "cirrostratus",
                    "cirrus",
                    "contrail",
                    "condensation",
                    "trail",
                    "cumulonimbus",
                    "cumulonimbus",
                    "cumulus",
                    "nacreous",
                    "nebule",
                    "nimbus",
                    "rain",
                    "storm",
                    "stratus",
                    "mushroom",
                    "smoke"
                ],
                "predictions": [
                    {
                        "score": 0.3633338395603753,
                        "answer": "chess",
                        "hit": false
                    },
                    {
                        "score": 0.36228930055002356,
                        "answer": "rayanne",
                        "hit": false
                    },
                    {
                        "score": 0.35993668002830853,
                        "answer": "eric",
                        "hit": false
                    },
                    {
                        "score": 0.35857499720882474,
                        "answer": "nincompoop",
                        "hit": false
                    },
                    {
                        "score": 0.35595437509484296,
                        "answer": "vhs",
                        "hit": false
                    },
                    {
                        "score": 0.35569264045438237,
                        "answer": "cairo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cloud"
                ],
                "rank": 1657,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6282123029232025
            },
            {
                "question verbose": "What is to collar ",
                "b": "collar",
                "expected answer": [
                    "choker",
                    "ruff",
                    "ruffle",
                    "clerical",
                    "roman_collar",
                    "dog_collar",
                    "eton_collar",
                    "rabato_collar",
                    "rebato_collar",
                    "turtleneck",
                    "polo-neck",
                    "neckpiece"
                ],
                "predictions": [
                    {
                        "score": 0.413288226008264,
                        "answer": "craftsmanship",
                        "hit": false
                    },
                    {
                        "score": 0.41203152518003433,
                        "answer": "bestselling",
                        "hit": false
                    },
                    {
                        "score": 0.40917517083962746,
                        "answer": "latin",
                        "hit": false
                    },
                    {
                        "score": 0.4057341203846917,
                        "answer": "mahogany",
                        "hit": false
                    },
                    {
                        "score": 0.4047371626831355,
                        "answer": "behave",
                        "hit": false
                    },
                    {
                        "score": 0.4018652793591008,
                        "answer": "gap",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "collar"
                ],
                "rank": 7346,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6223726570606232
            },
            {
                "question verbose": "What is to color ",
                "b": "color",
                "expected answer": [
                    "white",
                    "black",
                    "blue",
                    "green",
                    "red",
                    "yello",
                    "orange",
                    "vermilion",
                    "amber",
                    "chartreuse",
                    "teal",
                    "violet",
                    "magenta",
                    "rose",
                    "azure",
                    "cyan",
                    "indigo",
                    "purple",
                    "brown"
                ],
                "predictions": [
                    {
                        "score": 0.2733927383188689,
                        "answer": "cake",
                        "hit": false
                    },
                    {
                        "score": 0.2557590196438423,
                        "answer": "sized",
                        "hit": false
                    },
                    {
                        "score": 0.2517426498746505,
                        "answer": "craftsmanship",
                        "hit": false
                    },
                    {
                        "score": 0.2474476164205713,
                        "answer": "mahogany",
                        "hit": false
                    },
                    {
                        "score": 0.2472895053200015,
                        "answer": "bold",
                        "hit": false
                    },
                    {
                        "score": 0.24471789442142933,
                        "answer": "fawning",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "color"
                ],
                "rank": 107,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.669365718960762
            },
            {
                "question verbose": "What is to computer ",
                "b": "computer",
                "expected answer": [
                    "laptop",
                    "desktop",
                    "tablet",
                    "smartphone",
                    "supercomputer",
                    "iphone",
                    "ipad"
                ],
                "predictions": [
                    {
                        "score": 0.2508181991240753,
                        "answer": "commute",
                        "hit": false
                    },
                    {
                        "score": 0.24933127124662058,
                        "answer": "bandwidth",
                        "hit": false
                    },
                    {
                        "score": 0.2473585367696451,
                        "answer": "cake",
                        "hit": false
                    },
                    {
                        "score": 0.2461816112573955,
                        "answer": "ecb",
                        "hit": false
                    },
                    {
                        "score": 0.24470074334417066,
                        "answer": "xx",
                        "hit": false
                    },
                    {
                        "score": 0.2422919490294901,
                        "answer": "rayanne",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "computer"
                ],
                "rank": 3145,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7668955326080322
            },
            {
                "question verbose": "What is to container ",
                "b": "container",
                "expected answer": [
                    "bag",
                    "handbag",
                    "pocketbook",
                    "purse",
                    "basket",
                    "handbasket",
                    "bin",
                    "bowl",
                    "pipe",
                    "bowl",
                    "box",
                    "bread-bin",
                    "breadbox",
                    "bunker",
                    "can",
                    "tin",
                    "canister",
                    "cannister",
                    "capsule",
                    "cargo",
                    "case",
                    "display",
                    "showcase",
                    "vitrine",
                    "cassette",
                    "cup",
                    "cylinder",
                    "dice",
                    "cup",
                    "box",
                    "dish",
                    "dispenser",
                    "drawer",
                    "dumpster",
                    "empty",
                    "envelope",
                    "glass",
                    "bag",
                    "bottle",
                    "mailer",
                    "manger",
                    "trough",
                    "measure",
                    "mold",
                    "mould",
                    "cast",
                    "package",
                    "parcel",
                    "pan",
                    "pod",
                    "pot",
                    "flowerpot",
                    "flask",
                    "receptacle",
                    "reliquary",
                    "saltcellar",
                    "savings",
                    "bank",
                    "coin",
                    "money",
                    "bank",
                    "scuttle",
                    "coal",
                    "scuttle",
                    "shaker",
                    "spoon",
                    "thimble",
                    "capsule",
                    "vessel",
                    "basket",
                    "bin",
                    "waste-paper",
                    "wastebasket",
                    "waste",
                    "file",
                    "can",
                    "pot",
                    "workbasket",
                    "workbox",
                    "workbag"
                ],
                "predictions": [
                    {
                        "score": 0.38535408695877565,
                        "answer": "crisp",
                        "hit": false
                    },
                    {
                        "score": 0.37187074692608085,
                        "answer": "clarke",
                        "hit": false
                    },
                    {
                        "score": 0.3633723392006757,
                        "answer": "modus",
                        "hit": false
                    },
                    {
                        "score": 0.3632800588069247,
                        "answer": "plenary",
                        "hit": false
                    },
                    {
                        "score": 0.3617454071067087,
                        "answer": "trimmed",
                        "hit": false
                    },
                    {
                        "score": 0.35535694139680934,
                        "answer": "mccoy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "container"
                ],
                "rank": 399,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6900485903024673
            },
            {
                "question verbose": "What is to cookware ",
                "b": "cookware",
                "expected answer": [
                    "pot",
                    "baster",
                    "chafing",
                    "dish",
                    "cooker",
                    "sheet",
                    "tray",
                    "enamelware",
                    "grid",
                    "gridiron",
                    "griddle",
                    "pan",
                    "poacher",
                    "skimmer",
                    "steamer",
                    "turner"
                ],
                "predictions": [
                    {
                        "score": 0.42886842124006486,
                        "answer": "olive",
                        "hit": false
                    },
                    {
                        "score": 0.4086820581678981,
                        "answer": "modus",
                        "hit": false
                    },
                    {
                        "score": 0.4073830571236107,
                        "answer": "swamp",
                        "hit": false
                    },
                    {
                        "score": 0.40019036369303673,
                        "answer": "tracksuit",
                        "hit": false
                    },
                    {
                        "score": 0.39570255685288264,
                        "answer": "psyched",
                        "hit": false
                    },
                    {
                        "score": 0.38445795630521146,
                        "answer": "plenary",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cookware"
                ],
                "rank": 1451,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6508787423372269
            },
            {
                "question verbose": "What is to cup ",
                "b": "cup",
                "expected answer": [
                    "teacup",
                    "beaker",
                    "chalice",
                    "goblet",
                    "coffee_cup",
                    "dixie",
                    "paper_cup",
                    "grace",
                    "kylix",
                    "cylix",
                    "mustache_cup",
                    "moustache_cup",
                    "scyphus",
                    "grail"
                ],
                "predictions": [
                    {
                        "score": 0.4081233625481752,
                        "answer": "nascar",
                        "hit": false
                    },
                    {
                        "score": 0.4001463125222737,
                        "answer": "unveiled",
                        "hit": false
                    },
                    {
                        "score": 0.3888737508490948,
                        "answer": "noon",
                        "hit": false
                    },
                    {
                        "score": 0.37891692100132507,
                        "answer": "highway",
                        "hit": false
                    },
                    {
                        "score": 0.37788748966181995,
                        "answer": "morrissey",
                        "hit": false
                    },
                    {
                        "score": 0.37442702582076876,
                        "answer": "discriminated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cup"
                ],
                "rank": 14916,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.570232667028904
            },
            {
                "question verbose": "What is to cushion ",
                "b": "cushion",
                "expected answer": [
                    "pincushion",
                    "pillow",
                    "air",
                    "inflatable",
                    "gaddi",
                    "hassock",
                    "headrest",
                    "pillow",
                    "seat"
                ],
                "predictions": [
                    {
                        "score": 0.4456700124575052,
                        "answer": "elixir",
                        "hit": false
                    },
                    {
                        "score": 0.4284822950044732,
                        "answer": "jursdictions",
                        "hit": false
                    },
                    {
                        "score": 0.4204400845646326,
                        "answer": "charging",
                        "hit": false
                    },
                    {
                        "score": 0.4132564749074332,
                        "answer": "retains",
                        "hit": false
                    },
                    {
                        "score": 0.4096132343621562,
                        "answer": "finn",
                        "hit": false
                    },
                    {
                        "score": 0.4054381819272982,
                        "answer": "nincompoop",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cushion"
                ],
                "rank": 5526,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.649088516831398
            },
            {
                "question verbose": "What is to cutlery ",
                "b": "cutlery",
                "expected answer": [
                    "knife",
                    "fork",
                    "spoon",
                    "tablefork",
                    "teaspoon",
                    "dessert_spoon",
                    "salad_fork",
                    "carving_fork"
                ],
                "predictions": [
                    {
                        "score": 0.7778826682258898,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2698708822153491,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.26791365150562224,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2648853765322316,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2588225693742802,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2576806927573972,
                        "answer": "skype",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cutlery"
                ],
                "rank": 9569,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5773291066288948
            },
            {
                "question verbose": "What is to dessert ",
                "b": "dessert",
                "expected answer": [
                    "cake",
                    "ambrosia",
                    "baked_alaska",
                    "blancmange",
                    "charlotte",
                    "compote",
                    "fruit_compote",
                    "dumpling",
                    "flan",
                    "frozen_dessert",
                    "junket",
                    "mold",
                    "mould",
                    "mousse",
                    "pavlova",
                    "peach_melba",
                    "pudding",
                    "pud",
                    "pudding",
                    "syllabub",
                    "sillabub",
                    "tiramisu",
                    "whip",
                    "zabaglione",
                    "sabayon",
                    "charlotte_russe",
                    "apple_dumpling",
                    "frozen_custard",
                    "soft_ice_cream",
                    "frozen_pudding",
                    "frozen_yogurt",
                    "ice-cream_cake",
                    "icebox_cake",
                    "ice-cream_cone",
                    "ice-cream_sundae",
                    "sundae",
                    "ice",
                    "frappe",
                    "ice_cream",
                    "icecream",
                    "ice_lolly",
                    "lolly",
                    "lollipop",
                    "popsicle",
                    "ice_milk",
                    "parfait",
                    "sherbert",
                    "sherbet",
                    "snowball",
                    "snowball",
                    "split",
                    "chocolate_mousse",
                    "trifle",
                    "brown_betty",
                    "chocolate_pudding",
                    "duff",
                    "plum_duff",
                    "flummery",
                    "nesselrode",
                    "nesselrode_pudding",
                    "pease_pudding",
                    "plum_pudding",
                    "christmas_pudding",
                    "roly-poly",
                    "roly-poly_pudding",
                    "steamed_pudding",
                    "suet_pudding",
                    "tapioca_pudding",
                    "vanilla_pudding",
                    "prune_whip",
                    "water_ice",
                    "sorbet",
                    "chocolate_ice_cream",
                    "neapolitan_ice_cream",
                    "peach_ice_cream",
                    "strawberry_ice_cream",
                    "tutti-frutti",
                    "vanilla_ice_cream",
                    "banana_split",
                    "tipsy_cake",
                    "spotted_dick",
                    "choc-ice"
                ],
                "predictions": [
                    {
                        "score": 0.45634279872799427,
                        "answer": "olive",
                        "hit": false
                    },
                    {
                        "score": 0.44417265454021587,
                        "answer": "scandinavia",
                        "hit": false
                    },
                    {
                        "score": 0.43864801350740523,
                        "answer": "oyelowo",
                        "hit": false
                    },
                    {
                        "score": 0.43291609668858183,
                        "answer": "texted",
                        "hit": false
                    },
                    {
                        "score": 0.43276109024174064,
                        "answer": "beaver",
                        "hit": false
                    },
                    {
                        "score": 0.42843487957421045,
                        "answer": "bathtub",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dessert"
                ],
                "rank": 2216,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7823480069637299
            },
            {
                "question verbose": "What is to dress ",
                "b": "dress",
                "expected answer": [
                    "gown",
                    "caftan",
                    "kaftan",
                    "chemise",
                    "sack",
                    "shift",
                    "coatdress",
                    "cocktail_dress",
                    "sheath",
                    "dirndl",
                    "gown",
                    "jumper",
                    "pinafore",
                    "pinny",
                    "kirtle",
                    "morning",
                    "evening",
                    "muumuu",
                    "polonaise",
                    "sari",
                    "saree",
                    "shirtdress",
                    "strapless",
                    "sundress"
                ],
                "predictions": [
                    {
                        "score": 0.3828082420118337,
                        "answer": "transform",
                        "hit": false
                    },
                    {
                        "score": 0.3502791211918983,
                        "answer": "bloc",
                        "hit": false
                    },
                    {
                        "score": 0.3328173287330935,
                        "answer": "elixir",
                        "hit": false
                    },
                    {
                        "score": 0.3267086955310702,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.31767343689799843,
                        "answer": "latin",
                        "hit": false
                    },
                    {
                        "score": 0.3159644423558554,
                        "answer": "cio",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dress"
                ],
                "rank": 1108,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6203543171286583
            },
            {
                "question verbose": "What is to drum ",
                "b": "drum",
                "expected answer": [
                    "tambourine",
                    "bass_drum",
                    "gran_casa",
                    "bongo",
                    "bongo_drum",
                    "snare_drum",
                    "snare",
                    "side_drum",
                    "tabor",
                    "tabour",
                    "tambour",
                    "tenor_drum",
                    "tom-tom",
                    "timbrel",
                    "djembe",
                    "doumbek",
                    "darbuka"
                ],
                "predictions": [
                    {
                        "score": 0.38835792712843004,
                        "answer": "puncture",
                        "hit": false
                    },
                    {
                        "score": 0.38269233070149283,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.3796880825310881,
                        "answer": "olive",
                        "hit": false
                    },
                    {
                        "score": 0.3743449830305118,
                        "answer": "retains",
                        "hit": false
                    },
                    {
                        "score": 0.36860729398978076,
                        "answer": "mccoy",
                        "hit": false
                    },
                    {
                        "score": 0.3629244097781945,
                        "answer": "dev",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "drum"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6512516140937805
            },
            {
                "question verbose": "What is to emotion ",
                "b": "emotion",
                "expected answer": [
                    "anger",
                    "choler",
                    "ire",
                    "anxiety",
                    "conditioned_emotional_response",
                    "cer",
                    "conditioned_emotion",
                    "emotional_state",
                    "spirit",
                    "fear",
                    "fearfulness",
                    "fright",
                    "fear",
                    "reverence",
                    "awe",
                    "veneration",
                    "hate",
                    "hatred",
                    "joy",
                    "joyousness",
                    "joyfulness",
                    "love",
                    "annoyance",
                    "chafe",
                    "vexation",
                    "bad_temper",
                    "ill_temper",
                    "dander",
                    "hackles",
                    "fury",
                    "rage",
                    "madness",
                    "huffiness",
                    "indignation",
                    "outrage",
                    "infuriation",
                    "enragement",
                    "umbrage",
                    "offense",
                    "offence",
                    "angst",
                    "anxiousness",
                    "disquiet",
                    "concern",
                    "care",
                    "fear",
                    "discomfiture",
                    "discomposure",
                    "disconcertion",
                    "disconcertment",
                    "edginess",
                    "uneasiness",
                    "inquietude",
                    "disquietude",
                    "insecurity",
                    "jitteriness",
                    "jumpiness",
                    "nervousness",
                    "restiveness",
                    "scruple",
                    "qualm",
                    "misgiving",
                    "sinking",
                    "sinking_feeling",
                    "worry",
                    "trouble",
                    "ecstasy",
                    "rapture",
                    "transport",
                    "exaltation",
                    "raptus",
                    "embarrassment",
                    "gratification",
                    "satisfaction",
                    "happiness",
                    "felicity",
                    "state",
                    "unhappiness",
                    "alarm",
                    "dismay",
                    "consternation",
                    "apprehension",
                    "apprehensiveness",
                    "dread",
                    "creeps",
                    "frisson",
                    "shiver",
                    "chill",
                    "quiver",
                    "shudder",
                    "thrill",
                    "tingle",
                    "horror",
                    "hysteria",
                    "intimidation",
                    "panic",
                    "terror",
                    "affright",
                    "scare",
                    "panic_attack",
                    "stage_fright",
                    "timidity",
                    "timidness",
                    "timorousness",
                    "abhorrence",
                    "abomination",
                    "detestation",
                    "execration",
                    "loathing",
                    "odium",
                    "despisal",
                    "despising",
                    "hostility",
                    "enmity",
                    "ill_will",
                    "malevolence",
                    "malignity",
                    "misanthropy",
                    "misogamy",
                    "misogyny",
                    "misogynism",
                    "misology",
                    "misoneism",
                    "misopedia",
                    "murderousness",
                    "elation",
                    "high_spirits",
                    "lightness",
                    "exhilaration",
                    "excitement",
                    "exuberance",
                    "exultation",
                    "jubilance",
                    "jubilancy",
                    "jubilation",
                    "agape",
                    "agape",
                    "agape_love",
                    "amorousness",
                    "enamoredness",
                    "ardor",
                    "ardour",
                    "benevolence",
                    "devotion",
                    "devotedness",
                    "filial_love",
                    "heartstrings",
                    "lovingness",
                    "caring",
                    "loyalty",
                    "puppy_love",
                    "calf_love",
                    "crush",
                    "infatuation",
                    "worship",
                    "adoration",
                    "aggravation",
                    "exasperation",
                    "displeasure",
                    "frustration",
                    "harassment",
                    "torment",
                    "pique",
                    "temper",
                    "irritation",
                    "fit",
                    "tantrum",
                    "scene",
                    "conniption",
                    "irascibility",
                    "short_temper",
                    "spleen",
                    "quick_temper",
                    "lividity",
                    "wrath",
                    "dudgeon",
                    "high_dudgeon",
                    "willies",
                    "comfort",
                    "quality_of_life",
                    "blessedness",
                    "beatitude",
                    "beatification",
                    "radiance",
                    "embitterment",
                    "sadness",
                    "sorrow",
                    "sorrowfulness",
                    "chill",
                    "pall",
                    "foreboding",
                    "premonition",
                    "presentiment",
                    "boding",
                    "gloom",
                    "gloominess",
                    "somberness",
                    "sombreness",
                    "suspense",
                    "trepidation",
                    "swivet",
                    "cold_feet",
                    "diffidence",
                    "self-doubt",
                    "self-distrust",
                    "shyness",
                    "aggression",
                    "aggressiveness",
                    "animosity",
                    "animus",
                    "bad_blood",
                    "antagonism",
                    "belligerence",
                    "belligerency",
                    "class_feeling",
                    "resentment",
                    "bitterness",
                    "gall",
                    "rancor",
                    "rancour",
                    "maleficence",
                    "malice",
                    "maliciousness",
                    "spite",
                    "spitefulness",
                    "venom",
                    "vindictiveness",
                    "vengefulness",
                    "misocainea",
                    "euphoria",
                    "euphory",
                    "bang",
                    "boot",
                    "charge",
                    "rush",
                    "flush",
                    "thrill",
                    "kick",
                    "intoxication",
                    "titillation",
                    "triumph",
                    "beneficence",
                    "warmheartedness",
                    "warmth",
                    "nirvana",
                    "enlightenment",
                    "mourning",
                    "bereavement",
                    "poignance",
                    "poignancy",
                    "presage",
                    "shadow",
                    "hesitance",
                    "hesitancy",
                    "unassertiveness",
                    "warpath",
                    "envy",
                    "enviousness",
                    "grudge",
                    "score",
                    "grievance",
                    "heartburning",
                    "sulkiness",
                    "huffishness",
                    "covetousness",
                    "jealousy",
                    "green-eyed_monster",
                    "penis_envy"
                ],
                "predictions": [
                    {
                        "score": 0.25347089612649193,
                        "answer": "ant",
                        "hit": false
                    },
                    {
                        "score": 0.25235300068446065,
                        "answer": "stalkrazzi",
                        "hit": false
                    },
                    {
                        "score": 0.2509873591374532,
                        "answer": "backed",
                        "hit": false
                    },
                    {
                        "score": 0.25003225332877366,
                        "answer": "outgrow",
                        "hit": false
                    },
                    {
                        "score": 0.24889374985551915,
                        "answer": "collection",
                        "hit": false
                    },
                    {
                        "score": 0.24883941848832414,
                        "answer": "pitting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "emotion"
                ],
                "rank": 69,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6614764332771301
            },
            {
                "question verbose": "What is to flask ",
                "b": "flask",
                "expected answer": [
                    "thermos",
                    "ampulla",
                    "canteen",
                    "erlenmeyer_flask",
                    "hipflask",
                    "pocket_flask",
                    "round-bottom_flask",
                    "vacuum_flask",
                    "vacuum_bottle",
                    "dewar_flask",
                    "dewar",
                    "thermos_bottle",
                    "thermos_flask"
                ],
                "predictions": [
                    {
                        "score": 0.7766398055926793,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26909428953708214,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2657393679661249,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.26456035938254685,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.2579929601804912,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.254893219844925,
                        "answer": "suppression",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "flask"
                ],
                "rank": 2061,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6801343858242035
            },
            {
                "question verbose": "What is to guitar ",
                "b": "guitar",
                "expected answer": [
                    "ukulele",
                    "bass",
                    "bass_guitar",
                    "acoustic_guitar",
                    "cittern",
                    "cithern",
                    "cither",
                    "citole",
                    "gittern",
                    "electric_guitar",
                    "hawaiian",
                    "steel",
                    "uke"
                ],
                "predictions": [
                    {
                        "score": 0.33917817310310305,
                        "answer": "ecb",
                        "hit": false
                    },
                    {
                        "score": 0.3225738885243178,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.3181162512103608,
                        "answer": "righty",
                        "hit": false
                    },
                    {
                        "score": 0.31351141166135327,
                        "answer": "sessoms",
                        "hit": false
                    },
                    {
                        "score": 0.3028351862260873,
                        "answer": "airport",
                        "hit": false
                    },
                    {
                        "score": 0.30122373470523783,
                        "answer": "tamed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guitar"
                ],
                "rank": 1678,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6524188667535782
            },
            {
                "question verbose": "What is to gun ",
                "b": "gun",
                "expected answer": [
                    "rifle",
                    "air",
                    "airgun",
                    "antiaircraft",
                    "flak",
                    "flack",
                    "pom-pom",
                    "ack-ack",
                    "breechloader",
                    "cannon",
                    "firearm",
                    "piece",
                    "small-arm",
                    "gas",
                    "minute",
                    "quaker",
                    "set",
                    "spring",
                    "whaling"
                ],
                "predictions": [
                    {
                        "score": 0.3206378022281548,
                        "answer": "papa",
                        "hit": false
                    },
                    {
                        "score": 0.3115334755258584,
                        "answer": "jones",
                        "hit": false
                    },
                    {
                        "score": 0.30776210157995226,
                        "answer": "destabilize",
                        "hit": false
                    },
                    {
                        "score": 0.30750639589834017,
                        "answer": "unharmed",
                        "hit": false
                    },
                    {
                        "score": 0.2988192958861557,
                        "answer": "thier",
                        "hit": false
                    },
                    {
                        "score": 0.2950517405779157,
                        "answer": "crisp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gun"
                ],
                "rank": 1091,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7817746698856354
            },
            {
                "question verbose": "What is to jewel ",
                "b": "jewel",
                "expected answer": [
                    "diamond",
                    "ruby",
                    "pearl",
                    "emerald",
                    "sapphire"
                ],
                "predictions": [
                    {
                        "score": 0.775570281605348,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.25980292297812285,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.25697536229064055,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.25586309284812514,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.25531532793509437,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.24406411578228535,
                        "answer": "compelled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jewel"
                ],
                "rank": 8339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5983358845114708
            },
            {
                "question verbose": "What is to jewelry ",
                "b": "jewelry",
                "expected answer": [
                    "bracelet",
                    "bead",
                    "bijou",
                    "bling",
                    "bling_bling",
                    "bangle",
                    "clip",
                    "cufflink",
                    "earring",
                    "jewel",
                    "gem",
                    "precious_stone",
                    "necklace",
                    "pin",
                    "ring",
                    "band",
                    "tie_clip",
                    "bugle",
                    "anklet",
                    "ankle_bracelet",
                    "armilla",
                    "pendant_earring",
                    "drop_earring",
                    "eardrop",
                    "crown_jewel",
                    "diamond",
                    "emerald",
                    "pearl",
                    "ruby",
                    "sapphire",
                    "solitaire",
                    "chain",
                    "chain",
                    "string",
                    "strand",
                    "choker",
                    "collar",
                    "dog_collar",
                    "neckband",
                    "brooch",
                    "broach",
                    "breastpin",
                    "scatter_pin",
                    "stickpin",
                    "tie_tack",
                    "tiepin",
                    "scarfpin",
                    "annulet",
                    "engagement_ring",
                    "mourning_ring",
                    "ringlet",
                    "signet_ring",
                    "seal_ring",
                    "wedding_ring",
                    "wedding_band",
                    "ice",
                    "sparkler",
                    "seed_pearl",
                    "sunburst"
                ],
                "predictions": [
                    {
                        "score": 0.77161887245726,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2841206414688645,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2705043025807467,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.2657153576769526,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2618208580009589,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.2603768182412238,
                        "answer": "skype",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jewelry"
                ],
                "rank": 2394,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to mixer ",
                "b": "mixer",
                "expected answer": [
                    "blender",
                    "liquidizer",
                    "liquidiser",
                    "eggbeater",
                    "eggwhisk",
                    "electric",
                    "whisk"
                ],
                "predictions": [
                    {
                        "score": 0.7698520163265001,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2717827658379993,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2681238097346716,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2613230876877888,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.2608782844318563,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.25284062605816165,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mixer"
                ],
                "rank": 9750,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to month ",
                "b": "month",
                "expected answer": [
                    "january",
                    "february",
                    "march",
                    "april",
                    "may",
                    "june",
                    "july",
                    "august",
                    "september",
                    "october",
                    "november",
                    "december",
                    "jan",
                    "feb",
                    "mar",
                    "apr",
                    "jun",
                    "jul",
                    "aug",
                    "sep",
                    "oct",
                    "nov",
                    "dec"
                ],
                "predictions": [
                    {
                        "score": 0.302471221209181,
                        "answer": "investigation",
                        "hit": false
                    },
                    {
                        "score": 0.2924488783783375,
                        "answer": "consecutive",
                        "hit": false
                    },
                    {
                        "score": 0.29150324561177776,
                        "answer": "clwr",
                        "hit": false
                    },
                    {
                        "score": 0.2848858854864,
                        "answer": "spent",
                        "hit": false
                    },
                    {
                        "score": 0.2822739290845697,
                        "answer": "payment",
                        "hit": false
                    },
                    {
                        "score": 0.28088392601631396,
                        "answer": "cristie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "month"
                ],
                "rank": 57,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7262440919876099
            },
            {
                "question verbose": "What is to oven ",
                "b": "oven",
                "expected answer": [
                    "broiler",
                    "dutch_oven",
                    "gas_oven",
                    "rotisserie",
                    "tandoor"
                ],
                "predictions": [
                    {
                        "score": 0.41566848976055076,
                        "answer": "repub",
                        "hit": false
                    },
                    {
                        "score": 0.41112867867938035,
                        "answer": "bubbleas",
                        "hit": false
                    },
                    {
                        "score": 0.41080013181862424,
                        "answer": "latin",
                        "hit": false
                    },
                    {
                        "score": 0.4008807080841342,
                        "answer": "stonewalling",
                        "hit": false
                    },
                    {
                        "score": 0.4004915306111094,
                        "answer": "eric",
                        "hit": false
                    },
                    {
                        "score": 0.4003449454136427,
                        "answer": "pocket",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "oven"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6442776918411255
            },
            {
                "question verbose": "What is to painting ",
                "b": "painting",
                "expected answer": [
                    "watercolor",
                    "abstraction",
                    "cityscape",
                    "daub",
                    "distemper",
                    "finger-painting",
                    "icon",
                    "ikon",
                    "landscape",
                    "miniature",
                    "illumination",
                    "monochrome",
                    "mural",
                    "wall",
                    "nude",
                    "nude_painting",
                    "oil",
                    "pentimento",
                    "sand",
                    "seascape",
                    "waterscape",
                    "semi-abstraction",
                    "still_life",
                    "tanka",
                    "water-color",
                    "watercolour",
                    "water-colour",
                    "fresco",
                    "graffitti"
                ],
                "predictions": [
                    {
                        "score": 0.38914520267097175,
                        "answer": "mccoy",
                        "hit": false
                    },
                    {
                        "score": 0.3524144563250058,
                        "answer": "murphy",
                        "hit": false
                    },
                    {
                        "score": 0.3424386861616932,
                        "answer": "modus",
                        "hit": false
                    },
                    {
                        "score": 0.33772944942094696,
                        "answer": "thatcher",
                        "hit": false
                    },
                    {
                        "score": 0.3376087067382486,
                        "answer": "foresee",
                        "hit": false
                    },
                    {
                        "score": 0.3293451411059664,
                        "answer": "sponge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "painting"
                ],
                "rank": 6384,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5986441820859909
            },
            {
                "question verbose": "What is to poem ",
                "b": "poem",
                "expected answer": [
                    "haiku",
                    "abecedarius",
                    "alcaic",
                    "alcaic_verse",
                    "ballad",
                    "lay",
                    "ballade",
                    "blank_verse",
                    "elegy",
                    "lament",
                    "epic_poem",
                    "heroic_poem",
                    "epic",
                    "epos",
                    "free_verse",
                    "vers_libre",
                    "lyric",
                    "lyric_poem",
                    "rondeau",
                    "rondel",
                    "sonnet",
                    "tanka",
                    "terza_rima",
                    "verse",
                    "rhyme",
                    "versicle"
                ],
                "predictions": [
                    {
                        "score": 0.4194660799880546,
                        "answer": "suspicious",
                        "hit": false
                    },
                    {
                        "score": 0.3946602384087744,
                        "answer": "swamp",
                        "hit": false
                    },
                    {
                        "score": 0.3876569243941176,
                        "answer": "borrow",
                        "hit": false
                    },
                    {
                        "score": 0.37751020381871314,
                        "answer": "torrent",
                        "hit": false
                    },
                    {
                        "score": 0.3750645087535404,
                        "answer": "crisp",
                        "hit": false
                    },
                    {
                        "score": 0.3721921329575914,
                        "answer": "thier",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "poem"
                ],
                "rank": 2627,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6062076836824417
            },
            {
                "question verbose": "What is to railway ",
                "b": "railway",
                "expected answer": [
                    "monorail",
                    "cable",
                    "funicular",
                    "cog",
                    "rack",
                    "elevated",
                    "el",
                    "overhead",
                    "metro",
                    "tube",
                    "underground",
                    "subway",
                    "rail"
                ],
                "predictions": [
                    {
                        "score": 0.4352195876790389,
                        "answer": "charging",
                        "hit": false
                    },
                    {
                        "score": 0.41443755454177833,
                        "answer": "jursdictions",
                        "hit": false
                    },
                    {
                        "score": 0.40113770512863733,
                        "answer": "policymakers",
                        "hit": false
                    },
                    {
                        "score": 0.3889553947133351,
                        "answer": "actuarial",
                        "hit": false
                    },
                    {
                        "score": 0.38537157135596334,
                        "answer": "muslim",
                        "hit": false
                    },
                    {
                        "score": 0.3852853836915231,
                        "answer": "slog",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "railway"
                ],
                "rank": 401,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6179596781730652
            },
            {
                "question verbose": "What is to season ",
                "b": "season",
                "expected answer": [
                    "spring",
                    "harvest",
                    "haying",
                    "fall",
                    "autumn",
                    "springtime",
                    "summer",
                    "summertime",
                    "winter",
                    "wintertime",
                    "rainy",
                    "dry",
                    "growing",
                    "seedtime",
                    "sheepshearing",
                    "holiday",
                    "high",
                    "peak",
                    "off-season",
                    "preseason",
                    "baseball",
                    "basketball",
                    "exhibition",
                    "concert",
                    "fishing",
                    "football",
                    "hockey",
                    "hunting",
                    "social",
                    "theatrical",
                    "whitsun",
                    "whitsuntide",
                    "whitweek"
                ],
                "predictions": [
                    {
                        "score": 0.30149520107968675,
                        "answer": "preparation",
                        "hit": false
                    },
                    {
                        "score": 0.295892455376332,
                        "answer": "concluded",
                        "hit": false
                    },
                    {
                        "score": 0.2929451673293756,
                        "answer": "declining",
                        "hit": false
                    },
                    {
                        "score": 0.28030746180930344,
                        "answer": "heading",
                        "hit": false
                    },
                    {
                        "score": 0.2764120199479082,
                        "answer": "osf",
                        "hit": false
                    },
                    {
                        "score": 0.27037809590295325,
                        "answer": "netherlands",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "season"
                ],
                "rank": 89,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7693566977977753
            },
            {
                "question verbose": "What is to seat ",
                "b": "seat",
                "expected answer": [
                    "chair",
                    "bench",
                    "box",
                    "ottoman",
                    "pouf",
                    "pouffe",
                    "puff",
                    "hassock",
                    "sofa",
                    "couch",
                    "lounge",
                    "stool",
                    "toilet"
                ],
                "predictions": [
                    {
                        "score": 0.36435546373864347,
                        "answer": "arbor",
                        "hit": false
                    },
                    {
                        "score": 0.3361997999554818,
                        "answer": "witte",
                        "hit": false
                    },
                    {
                        "score": 0.3213449016146096,
                        "answer": "freddie",
                        "hit": false
                    },
                    {
                        "score": 0.31735879229496256,
                        "answer": "allgeier",
                        "hit": false
                    },
                    {
                        "score": 0.31480782581349254,
                        "answer": "locked",
                        "hit": false
                    },
                    {
                        "score": 0.31433128874942745,
                        "answer": "representative",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seat"
                ],
                "rank": 2588,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6475209593772888
            },
            {
                "question verbose": "What is to shelf ",
                "b": "shelf",
                "expected answer": [
                    "bookshelf",
                    "hob",
                    "mantel",
                    "mantelpiece",
                    "mantle",
                    "mantlepiece",
                    "chimneypiece",
                    "overmantel",
                    "berm"
                ],
                "predictions": [
                    {
                        "score": 0.3622443465246159,
                        "answer": "publication",
                        "hit": false
                    },
                    {
                        "score": 0.3550187475192553,
                        "answer": "tarot",
                        "hit": false
                    },
                    {
                        "score": 0.32940547534960557,
                        "answer": "texted",
                        "hit": false
                    },
                    {
                        "score": 0.3246286499792296,
                        "answer": "celebration",
                        "hit": false
                    },
                    {
                        "score": 0.3146971316221327,
                        "answer": "bombast",
                        "hit": false
                    },
                    {
                        "score": 0.31200762115771796,
                        "answer": "collaborative",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shelf"
                ],
                "rank": 3809,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7473149001598358
            },
            {
                "question verbose": "What is to shirt ",
                "b": "shirt",
                "expected answer": [
                    "polo",
                    "camise",
                    "dashiki",
                    "daishiki",
                    "dress",
                    "evening",
                    "hair",
                    "jersey",
                    "t-shirt",
                    "tee",
                    "kurta",
                    "sport",
                    "tank",
                    "work-shirt"
                ],
                "predictions": [
                    {
                        "score": 0.3005151883581484,
                        "answer": "tee",
                        "hit": true
                    },
                    {
                        "score": 0.2900272820264222,
                        "answer": "thorned",
                        "hit": false
                    },
                    {
                        "score": 0.2898102463553933,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.2891743393141196,
                        "answer": "mccoy",
                        "hit": false
                    },
                    {
                        "score": 0.2870227321225245,
                        "answer": "hosting",
                        "hit": false
                    },
                    {
                        "score": 0.2807094518032992,
                        "answer": "shameful",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shirt"
                ],
                "rank": 0,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6964245736598969
            },
            {
                "question verbose": "What is to shoes ",
                "b": "shoes",
                "expected answer": [
                    "sneakers",
                    "anklet",
                    "baby_shoes",
                    "balmoral",
                    "blucher",
                    "bowling_shoes",
                    "brogan",
                    "brogue",
                    "clodhopper",
                    "work_shoes",
                    "calceus",
                    "chopine",
                    "platform_shoes",
                    "chukka",
                    "boots",
                    "cleats",
                    "congress_shoes",
                    "gaiters",
                    "flipper_shoes",
                    "fin_shoes",
                    "ghillie",
                    "gillie",
                    "gym_shoes",
                    "tennis_shoes",
                    "loafers",
                    "moccasins",
                    "mocassins",
                    "oxford_shoes",
                    "pump_shoes",
                    "running_shoes",
                    "sabot",
                    "wooden_shoes",
                    "sandals",
                    "slingbacks",
                    "sling_shoes",
                    "walker_shoes",
                    "walking_shoes",
                    "wedgie",
                    "wing_shoes",
                    "tip_shoes",
                    "stiletto",
                    "heeled"
                ],
                "predictions": [
                    {
                        "score": 0.7687757206414384,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.26526125720771143,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.26348452014053303,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.2598789596980671,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.2579555684249349,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.257573207608601,
                        "answer": "skype",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shoes"
                ],
                "rank": 1184,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to sofa ",
                "b": "sofa",
                "expected answer": [
                    "divan",
                    "diwan",
                    "convertible_sofa",
                    "daybed",
                    "loveseat",
                    "settee",
                    "squab"
                ],
                "predictions": [
                    {
                        "score": 0.7703503734341233,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.27308234557325634,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.2728734292978599,
                        "answer": "melodrama",
                        "hit": false
                    },
                    {
                        "score": 0.26868150456584816,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.26616119235731783,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.26227992838927994,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sofa"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to song ",
                "b": "song",
                "expected answer": [
                    "lullaby",
                    "religious_song",
                    "anthem",
                    "aria",
                    "ballad",
                    "lay",
                    "barcarole",
                    "barcarolle",
                    "ditty",
                    "dirge",
                    "coronach",
                    "lament",
                    "requiem",
                    "threnody",
                    "drinking_song",
                    "folk_song",
                    "folksong",
                    "folk_ballad",
                    "lied",
                    "love_song",
                    "love-song",
                    "cradlesong",
                    "berceuse",
                    "oldie",
                    "golden_oldie",
                    "partsong",
                    "prothalamion",
                    "prothalamium",
                    "roundelay",
                    "scolion",
                    "banquet_song",
                    "serenade",
                    "torch_song",
                    "work_song"
                ],
                "predictions": [
                    {
                        "score": 0.3876514786411153,
                        "answer": "fabulously",
                        "hit": false
                    },
                    {
                        "score": 0.3263620702904307,
                        "answer": "blackmon",
                        "hit": false
                    },
                    {
                        "score": 0.32592872965601705,
                        "answer": "briefcase",
                        "hit": false
                    },
                    {
                        "score": 0.314869733109899,
                        "answer": "mlbcom",
                        "hit": false
                    },
                    {
                        "score": 0.31063746408872583,
                        "answer": "dance",
                        "hit": false
                    },
                    {
                        "score": 0.3029443951288501,
                        "answer": "cairo",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "song"
                ],
                "rank": 2147,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5929350554943085
            },
            {
                "question verbose": "What is to spice ",
                "b": "spice",
                "expected answer": [
                    "pepper",
                    "allspice",
                    "ajwain",
                    "cumin",
                    "caraway_seed",
                    "cardamom",
                    "cassia",
                    "cayenne",
                    "celery_seeds",
                    "chile_pepper",
                    "chile",
                    "cinnamon",
                    "clove",
                    "coriander",
                    "dill",
                    "fennel",
                    "fenugreek",
                    "frankincense",
                    "galangal",
                    "garlic",
                    "ginger",
                    "horseradish",
                    "jalapeo",
                    "juniper",
                    "licorice",
                    "mace",
                    "mustard",
                    "nutmeg",
                    "onion",
                    "paprika",
                    "peppercorns",
                    "saffron",
                    "savory",
                    "sesame",
                    "anise",
                    "sumac",
                    "tabasco",
                    "tamarind",
                    "turmeric",
                    "five_spice_powder",
                    "ginger",
                    "powdered_ginger",
                    "chinese_anise",
                    "star_anise",
                    "star_aniseed"
                ],
                "predictions": [
                    {
                        "score": 0.2013212129943842,
                        "answer": "trickle",
                        "hit": false
                    },
                    {
                        "score": 0.18877879347025667,
                        "answer": "join",
                        "hit": false
                    },
                    {
                        "score": 0.1884632452120724,
                        "answer": "puncture",
                        "hit": false
                    },
                    {
                        "score": 0.18506715683744596,
                        "answer": "isle",
                        "hit": false
                    },
                    {
                        "score": 0.1790836951657251,
                        "answer": "deaf",
                        "hit": false
                    },
                    {
                        "score": 0.1720109997212784,
                        "answer": "stranded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spice"
                ],
                "rank": 3427,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5980594307184219
            },
            {
                "question verbose": "What is to sweater ",
                "b": "sweater",
                "expected answer": [
                    "turtleneck",
                    "cardigan",
                    "pullover",
                    "slipover",
                    "turtle",
                    "polo-neck"
                ],
                "predictions": [
                    {
                        "score": 0.39643273925858896,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.3948543397370572,
                        "answer": "trimmed",
                        "hit": false
                    },
                    {
                        "score": 0.39150244197086503,
                        "answer": "jursdictions",
                        "hit": false
                    },
                    {
                        "score": 0.3867272485271422,
                        "answer": "vacuum",
                        "hit": false
                    },
                    {
                        "score": 0.3771184244308789,
                        "answer": "temped",
                        "hit": false
                    },
                    {
                        "score": 0.3758834844827399,
                        "answer": "unharmed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sweater"
                ],
                "rank": 9060,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6381354480981827
            },
            {
                "question verbose": "What is to tool ",
                "b": "tool",
                "expected answer": [
                    "rake",
                    "abrader",
                    "abradant",
                    "bender",
                    "clincher",
                    "comb",
                    "cutting_implement",
                    "drill",
                    "eolith",
                    "fork",
                    "gang",
                    "garden_tool",
                    "lawn_tool",
                    "grapnel",
                    "grapple",
                    "grappler",
                    "grappling_hook",
                    "grappling_iron",
                    "hack",
                    "hand_tool",
                    "hoe",
                    "jack",
                    "jaws_of_life",
                    "neolith",
                    "paleolith",
                    "pestle",
                    "muller",
                    "pounder",
                    "plow",
                    "plough",
                    "power_tool",
                    "punch",
                    "puncher",
                    "ram",
                    "rounder",
                    "saw_set",
                    "shaping_tool",
                    "strickle",
                    "stylus",
                    "style",
                    "tamp",
                    "tamper",
                    "tamping_bar",
                    "tap",
                    "upset",
                    "swage"
                ],
                "predictions": [
                    {
                        "score": 0.30982558739056787,
                        "answer": "blackmon",
                        "hit": false
                    },
                    {
                        "score": 0.2861513895085145,
                        "answer": "academic",
                        "hit": false
                    },
                    {
                        "score": 0.27736820032309534,
                        "answer": "erez",
                        "hit": false
                    },
                    {
                        "score": 0.2572981172650394,
                        "answer": "thermos",
                        "hit": false
                    },
                    {
                        "score": 0.2571155843432496,
                        "answer": "hardship",
                        "hit": false
                    },
                    {
                        "score": 0.2521906342309158,
                        "answer": "progress",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tool"
                ],
                "rank": 1413,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6219989359378815
            },
            {
                "question verbose": "What is to toy ",
                "b": "toy",
                "expected answer": [
                    "doll",
                    "ball",
                    "balloon",
                    "cockhorse",
                    "dolly",
                    "dollhouse",
                    "house",
                    "frisbee",
                    "hobby",
                    "hobbyhorse",
                    "rocking",
                    "horse",
                    "hula-hoop",
                    "jack-in-the-box",
                    "jungle",
                    "gym",
                    "jumping",
                    "jack",
                    "kaleidoscope",
                    "kite",
                    "lego",
                    "meccano",
                    "shooter",
                    "pinata",
                    "pinwheel",
                    "playhouse",
                    "wendy",
                    "pogo",
                    "stick",
                    "popgun",
                    "rattle",
                    "sandbox",
                    "sandpile",
                    "sandpit",
                    "seesaw",
                    "teeter",
                    "teeter-totter",
                    "teetertotter",
                    "teeterboard",
                    "tilting",
                    "board",
                    "dandle",
                    "slide",
                    "playground",
                    "sliding",
                    "slingshot",
                    "sling",
                    "catapult",
                    "stick",
                    "horse",
                    "swing",
                    "teddy",
                    "bear",
                    "top",
                    "whirligig",
                    "teetotum",
                    "spinning",
                    "train",
                    "water_pistol",
                    "gun",
                    "squirt",
                    "squirter",
                    "yo-yo"
                ],
                "predictions": [
                    {
                        "score": 0.32708590365953444,
                        "answer": "commute",
                        "hit": false
                    },
                    {
                        "score": 0.32440469003832967,
                        "answer": "hadid",
                        "hit": false
                    },
                    {
                        "score": 0.32006991705673554,
                        "answer": "rome",
                        "hit": false
                    },
                    {
                        "score": 0.3140699572952293,
                        "answer": "oyelowo",
                        "hit": false
                    },
                    {
                        "score": 0.3131649943448867,
                        "answer": "ezquerra",
                        "hit": false
                    },
                    {
                        "score": 0.31214797260318466,
                        "answer": "columbia",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "toy"
                ],
                "rank": 411,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7363449037075043
            },
            {
                "question verbose": "What is to trousers ",
                "b": "trousers",
                "expected answer": [
                    "jeans",
                    "bellbottom_trousers",
                    "bell-bottoms",
                    "bellbottom_pants",
                    "breeches",
                    "knee_breeches",
                    "knee_pants",
                    "knickerbockers",
                    "knickers",
                    "chino",
                    "churidars",
                    "cords",
                    "corduroys",
                    "flannel",
                    "gabardine",
                    "tweed",
                    "white",
                    "jean",
                    "blue_jean",
                    "denim",
                    "jodhpurs",
                    "jodhpur_breeches",
                    "riding_breeches",
                    "long_trousers",
                    "long_pants",
                    "pajama",
                    "pyjama",
                    "pantaloon",
                    "pedal_pusher",
                    "toreador_pants",
                    "salwar",
                    "shalwar",
                    "short_pants",
                    "shorts",
                    "trunks",
                    "slacks",
                    "stretch_pants",
                    "sweat_pants",
                    "sweatpants",
                    "trews",
                    "britches",
                    "buckskins",
                    "plus_fours",
                    "trunk_hose",
                    "levi's",
                    "levis",
                    "bermuda_shorts",
                    "jamaica_shorts",
                    "hot_pants",
                    "lederhosen"
                ],
                "predictions": [
                    {
                        "score": 0.7694041253078527,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2840764618293553,
                        "answer": "compelled",
                        "hit": false
                    },
                    {
                        "score": 0.276677990815661,
                        "answer": "dev",
                        "hit": false
                    },
                    {
                        "score": 0.26349819277260395,
                        "answer": "chinese",
                        "hit": false
                    },
                    {
                        "score": 0.26232634120985054,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.26072925489881515,
                        "answer": "harm",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "trousers"
                ],
                "rank": 3552,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to weapon ",
                "b": "weapon",
                "expected answer": [
                    "gun",
                    "bow",
                    "bow",
                    "arrow",
                    "dart",
                    "knucks",
                    "knuckles",
                    "flamethrower",
                    "knife",
                    "missile",
                    "pike",
                    "projectile",
                    "slasher",
                    "sling",
                    "spear",
                    "lance",
                    "shaft",
                    "stun",
                    "baton",
                    "sword",
                    "blade",
                    "brand",
                    "steel",
                    "tomahawk",
                    "hatchet",
                    "fire"
                ],
                "predictions": [
                    {
                        "score": 0.3053022453545042,
                        "answer": "modernize",
                        "hit": false
                    },
                    {
                        "score": 0.3039728575207053,
                        "answer": "stroller",
                        "hit": false
                    },
                    {
                        "score": 0.3019060685318541,
                        "answer": "policymakers",
                        "hit": false
                    },
                    {
                        "score": 0.2968516535656581,
                        "answer": "jacket",
                        "hit": false
                    },
                    {
                        "score": 0.2914006957011251,
                        "answer": "nutrition",
                        "hit": false
                    },
                    {
                        "score": 0.29035522637314654,
                        "answer": "sable",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weapon"
                ],
                "rank": 1541,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6496982425451279
            },
            {
                "question verbose": "What is to weekday ",
                "b": "weekday",
                "expected answer": [
                    "monday",
                    "workday",
                    "working_day",
                    "work_day",
                    "feria",
                    "mon",
                    "tuesday",
                    "tues",
                    "wednesday",
                    "midweek",
                    "wed",
                    "thursday",
                    "th",
                    "friday",
                    "fri",
                    "saturday",
                    "sabbatum",
                    "sat"
                ],
                "predictions": [
                    {
                        "score": 0.46725285172812553,
                        "answer": "pocket",
                        "hit": false
                    },
                    {
                        "score": 0.44848526282546136,
                        "answer": "viliations",
                        "hit": false
                    },
                    {
                        "score": 0.44569683347238104,
                        "answer": "sometime",
                        "hit": false
                    },
                    {
                        "score": 0.4312579262799214,
                        "answer": "briefcase",
                        "hit": false
                    },
                    {
                        "score": 0.4293120402100414,
                        "answer": "crisp",
                        "hit": false
                    },
                    {
                        "score": 0.42656032118255244,
                        "answer": "battleship",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "weekday"
                ],
                "rank": 1312,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7343623042106628
            }
        ],
        "result": {
            "cnt_questions_correct": 1,
            "cnt_questions_total": 50,
            "accuracy": 0.02
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L03 [hyponyms - misc].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "a37c8982-7580-4732-97d8-d6adf789963c",
            "timestamp": "2020-10-22T15:57:48.387786"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to atmosphere ",
                "b": "atmosphere",
                "expected answer": [
                    "gas",
                    "oxygen",
                    "hydrogen",
                    "nitrogen",
                    "ozone"
                ],
                "predictions": [
                    {
                        "score": 0.5270659339594308,
                        "answer": "manuscript",
                        "hit": false
                    },
                    {
                        "score": 0.5106377610719163,
                        "answer": "purity",
                        "hit": false
                    },
                    {
                        "score": 0.4891544347079826,
                        "answer": "remade",
                        "hit": false
                    },
                    {
                        "score": 0.48711498627295546,
                        "answer": "purse",
                        "hit": false
                    },
                    {
                        "score": 0.4821923451914774,
                        "answer": "doooooooooo",
                        "hit": false
                    },
                    {
                        "score": 0.4758706872825295,
                        "answer": "preferential",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "atmosphere"
                ],
                "rank": 2337,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6565676778554916
            },
            {
                "question verbose": "What is to bag ",
                "b": "bag",
                "expected answer": [
                    "leather",
                    "fabric",
                    "plastic"
                ],
                "predictions": [
                    {
                        "score": 0.3384732051987313,
                        "answer": "youll",
                        "hit": false
                    },
                    {
                        "score": 0.310787574026591,
                        "answer": "plaza",
                        "hit": false
                    },
                    {
                        "score": 0.30800795963565236,
                        "answer": "substitute",
                        "hit": false
                    },
                    {
                        "score": 0.30731273914466645,
                        "answer": "madonna",
                        "hit": false
                    },
                    {
                        "score": 0.3043051896911954,
                        "answer": "afraid",
                        "hit": false
                    },
                    {
                        "score": 0.3042833988780634,
                        "answer": "peed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bag"
                ],
                "rank": 145,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7656895816326141
            },
            {
                "question verbose": "What is to beach ",
                "b": "beach",
                "expected answer": [
                    "sand",
                    "silicon",
                    "rocks",
                    "pebbles",
                    "atomic_number_14",
                    "si"
                ],
                "predictions": [
                    {
                        "score": 0.3431656121775715,
                        "answer": "brook",
                        "hit": false
                    },
                    {
                        "score": 0.322009456980608,
                        "answer": "water",
                        "hit": false
                    },
                    {
                        "score": 0.3157497228634187,
                        "answer": "motorway",
                        "hit": false
                    },
                    {
                        "score": 0.3140841785097385,
                        "answer": "plummeting",
                        "hit": false
                    },
                    {
                        "score": 0.3108509079721914,
                        "answer": "manuscript",
                        "hit": false
                    },
                    {
                        "score": 0.3099957769223629,
                        "answer": "sheltered",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beach"
                ],
                "rank": 11509,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6272274851799011
            },
            {
                "question verbose": "What is to beard ",
                "b": "beard",
                "expected answer": [
                    "hair"
                ],
                "predictions": [
                    {
                        "score": 0.48203223530745454,
                        "answer": "becaused",
                        "hit": false
                    },
                    {
                        "score": 0.47702140942258486,
                        "answer": "indefinitely",
                        "hit": false
                    },
                    {
                        "score": 0.4687118030628878,
                        "answer": "overlap",
                        "hit": false
                    },
                    {
                        "score": 0.45803474952746015,
                        "answer": "ewmhbus",
                        "hit": false
                    },
                    {
                        "score": 0.45648397908500704,
                        "answer": "recoverable",
                        "hit": false
                    },
                    {
                        "score": 0.4536152521603898,
                        "answer": "polyester",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beard"
                ],
                "rank": 13315,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6875112503767014
            },
            {
                "question verbose": "What is to body ",
                "b": "body",
                "expected answer": [
                    "flesh",
                    "bones"
                ],
                "predictions": [
                    {
                        "score": 0.35111177071395666,
                        "answer": "kg",
                        "hit": false
                    },
                    {
                        "score": 0.337113916104108,
                        "answer": "evaluation",
                        "hit": false
                    },
                    {
                        "score": 0.3366904266100583,
                        "answer": "intrgastrically",
                        "hit": false
                    },
                    {
                        "score": 0.32681386957829434,
                        "answer": "detoxify",
                        "hit": false
                    },
                    {
                        "score": 0.32540233595935786,
                        "answer": "mgkg",
                        "hit": false
                    },
                    {
                        "score": 0.3237143960464525,
                        "answer": "substitute",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "body"
                ],
                "rank": 5993,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6552079916000366
            },
            {
                "question verbose": "What is to boots ",
                "b": "boots",
                "expected answer": [
                    "leather",
                    "canvas"
                ],
                "predictions": [
                    {
                        "score": 0.5855391903502863,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2912649568308294,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.289632938466348,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.28777473538549525,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.28720812807979496,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2825497678939086,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "boots"
                ],
                "rank": 798,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6280124485492706
            },
            {
                "question verbose": "What is to bottle ",
                "b": "bottle",
                "expected answer": [
                    "glass",
                    "plastic"
                ],
                "predictions": [
                    {
                        "score": 0.38668738666454355,
                        "answer": "bust",
                        "hit": false
                    },
                    {
                        "score": 0.3848797614893572,
                        "answer": "frigid",
                        "hit": false
                    },
                    {
                        "score": 0.3843263892406335,
                        "answer": "doooooooooo",
                        "hit": false
                    },
                    {
                        "score": 0.37605355366654863,
                        "answer": "ewmhbus",
                        "hit": false
                    },
                    {
                        "score": 0.373346429976775,
                        "answer": "cardiac",
                        "hit": false
                    },
                    {
                        "score": 0.37087720247923733,
                        "answer": "floppers",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bottle"
                ],
                "rank": 5851,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6749034225940704
            },
            {
                "question verbose": "What is to bowl ",
                "b": "bowl",
                "expected answer": [
                    "glass",
                    "china",
                    "aluminium",
                    "wood",
                    "steel",
                    "plastic",
                    "clay"
                ],
                "predictions": [
                    {
                        "score": 0.46975973916991587,
                        "answer": "cardinal",
                        "hit": false
                    },
                    {
                        "score": 0.46686611015494944,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.4660333714861974,
                        "answer": "reveler",
                        "hit": false
                    },
                    {
                        "score": 0.45716925490204796,
                        "answer": "dorm",
                        "hit": false
                    },
                    {
                        "score": 0.45447313078624146,
                        "answer": "stale",
                        "hit": false
                    },
                    {
                        "score": 0.4537460798353854,
                        "answer": "qualification",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bowl"
                ],
                "rank": 1237,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7039645910263062
            },
            {
                "question verbose": "What is to box ",
                "b": "box",
                "expected answer": [
                    "cardboard",
                    "tin",
                    "boxwood",
                    "turkish_boxwood"
                ],
                "predictions": [
                    {
                        "score": 0.37571841917472654,
                        "answer": "steam",
                        "hit": false
                    },
                    {
                        "score": 0.3572859758239992,
                        "answer": "deluded",
                        "hit": false
                    },
                    {
                        "score": 0.3495450614775531,
                        "answer": "concept",
                        "hit": false
                    },
                    {
                        "score": 0.34129765723384803,
                        "answer": "gutting",
                        "hit": false
                    },
                    {
                        "score": 0.3345384019061419,
                        "answer": "selecting",
                        "hit": false
                    },
                    {
                        "score": 0.3308225923988435,
                        "answer": "profoundly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "box"
                ],
                "rank": 12824,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6509441584348679
            },
            {
                "question verbose": "What is to bread ",
                "b": "bread",
                "expected answer": [
                    "flour",
                    "yeast",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.4236797359226876,
                        "answer": "propane",
                        "hit": false
                    },
                    {
                        "score": 0.42194893000968586,
                        "answer": "footage",
                        "hit": false
                    },
                    {
                        "score": 0.4171109110283869,
                        "answer": "asshats",
                        "hit": false
                    },
                    {
                        "score": 0.416981650176953,
                        "answer": "recoverable",
                        "hit": false
                    },
                    {
                        "score": 0.41477593446832045,
                        "answer": "dorm",
                        "hit": false
                    },
                    {
                        "score": 0.411597369092682,
                        "answer": "payment",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bread"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6078383773565292
            },
            {
                "question verbose": "What is to bronze ",
                "b": "bronze",
                "expected answer": [
                    "copper",
                    "tin",
                    "cu",
                    "atomic_number_29"
                ],
                "predictions": [
                    {
                        "score": 0.5580569086547014,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29705329221190385,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.29219883183085643,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.2893183391549,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2817220441583213,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.2748454858994185,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bronze"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to candy ",
                "b": "candy",
                "expected answer": [
                    "sugar",
                    "nougat",
                    "chocolate",
                    "gelatin",
                    "molasses",
                    "syrup",
                    "honey",
                    "fruit",
                    "nuts"
                ],
                "predictions": [
                    {
                        "score": 0.4527884473831179,
                        "answer": "lovehoneycouk",
                        "hit": false
                    },
                    {
                        "score": 0.4370527481947124,
                        "answer": "octane",
                        "hit": false
                    },
                    {
                        "score": 0.4308462362720352,
                        "answer": "leone",
                        "hit": false
                    },
                    {
                        "score": 0.4281900520675831,
                        "answer": "chemically",
                        "hit": false
                    },
                    {
                        "score": 0.42556483718348814,
                        "answer": "tourist",
                        "hit": false
                    },
                    {
                        "score": 0.42159009085233123,
                        "answer": "obtained",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "candy"
                ],
                "rank": 586,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7994593977928162
            },
            {
                "question verbose": "What is to chocolate ",
                "b": "chocolate",
                "expected answer": [
                    "cocoa",
                    "bean",
                    "cacao",
                    "cacao_bean",
                    "cocoa_bean"
                ],
                "predictions": [
                    {
                        "score": 0.5139916334900555,
                        "answer": "cardiac",
                        "hit": false
                    },
                    {
                        "score": 0.5023551527026375,
                        "answer": "remade",
                        "hit": false
                    },
                    {
                        "score": 0.49353146683582505,
                        "answer": "ultrabooks",
                        "hit": false
                    },
                    {
                        "score": 0.49281949070871073,
                        "answer": "souplining",
                        "hit": false
                    },
                    {
                        "score": 0.49199783081651915,
                        "answer": "gamble",
                        "hit": false
                    },
                    {
                        "score": 0.4881652003688606,
                        "answer": "tackling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chocolate"
                ],
                "rank": 8469,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7589026391506195
            },
            {
                "question verbose": "What is to clothing ",
                "b": "clothing",
                "expected answer": [
                    "fabric",
                    "leather",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.403937872148882,
                        "answer": "campground",
                        "hit": false
                    },
                    {
                        "score": 0.37236686439645134,
                        "answer": "mix",
                        "hit": false
                    },
                    {
                        "score": 0.37083755174299216,
                        "answer": "okiciyap",
                        "hit": false
                    },
                    {
                        "score": 0.3676355420060223,
                        "answer": "solved",
                        "hit": false
                    },
                    {
                        "score": 0.365490556919698,
                        "answer": "anticipate",
                        "hit": false
                    },
                    {
                        "score": 0.3644592433027713,
                        "answer": "activism",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clothing"
                ],
                "rank": 15,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7808521687984467
            },
            {
                "question verbose": "What is to cloud ",
                "b": "cloud",
                "expected answer": [
                    "vapor",
                    "water",
                    "vapour",
                    "water_vapour",
                    "water_vapor"
                ],
                "predictions": [
                    {
                        "score": 0.4503154309108315,
                        "answer": "formulate",
                        "hit": false
                    },
                    {
                        "score": 0.4471710291436122,
                        "answer": "armenia",
                        "hit": false
                    },
                    {
                        "score": 0.4453169264656976,
                        "answer": "comply",
                        "hit": false
                    },
                    {
                        "score": 0.443963543048522,
                        "answer": "deemed",
                        "hit": false
                    },
                    {
                        "score": 0.44090467966673014,
                        "answer": "reviewed",
                        "hit": false
                    },
                    {
                        "score": 0.4389775207783341,
                        "answer": "logitech",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cloud"
                ],
                "rank": 10874,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6282123029232025
            },
            {
                "question verbose": "What is to cocktail ",
                "b": "cocktail",
                "expected answer": [
                    "alcohol",
                    "juice",
                    "water"
                ],
                "predictions": [
                    {
                        "score": 0.5847025071771056,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2855184980429719,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.281707381174787,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.2815890065642822,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.27493611271776797,
                        "answer": "expense",
                        "hit": false
                    },
                    {
                        "score": 0.26680522283238006,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cocktail"
                ],
                "rank": 6274,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.605343334376812
            },
            {
                "question verbose": "What is to concrete ",
                "b": "concrete",
                "expected answer": [
                    "silicon",
                    "cement",
                    "water",
                    "sand",
                    "atomic_number_14",
                    "si"
                ],
                "predictions": [
                    {
                        "score": 0.43937768826790446,
                        "answer": "digitized",
                        "hit": false
                    },
                    {
                        "score": 0.4296519014375848,
                        "answer": "fabric",
                        "hit": false
                    },
                    {
                        "score": 0.4275377055363924,
                        "answer": "mcfd",
                        "hit": false
                    },
                    {
                        "score": 0.42626832203595183,
                        "answer": "harmonizing",
                        "hit": false
                    },
                    {
                        "score": 0.42503443293353116,
                        "answer": "dorm",
                        "hit": false
                    },
                    {
                        "score": 0.42209570933750523,
                        "answer": "swap",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "concrete"
                ],
                "rank": 808,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5550549849867821
            },
            {
                "question verbose": "What is to desk ",
                "b": "desk",
                "expected answer": [
                    "wood",
                    "metal",
                    "steel"
                ],
                "predictions": [
                    {
                        "score": 0.47260618655525133,
                        "answer": "incoherent",
                        "hit": false
                    },
                    {
                        "score": 0.45900193352424146,
                        "answer": "reacted",
                        "hit": false
                    },
                    {
                        "score": 0.4531304464690177,
                        "answer": "obtained",
                        "hit": false
                    },
                    {
                        "score": 0.44739896991098305,
                        "answer": "unprofitable",
                        "hit": false
                    },
                    {
                        "score": 0.4461538998211182,
                        "answer": "madonna",
                        "hit": false
                    },
                    {
                        "score": 0.43796570711709254,
                        "answer": "inroad",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "desk"
                ],
                "rank": 1785,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7021675407886505
            },
            {
                "question verbose": "What is to diamond ",
                "b": "diamond",
                "expected answer": [
                    "carbon"
                ],
                "predictions": [
                    {
                        "score": 0.557555890243683,
                        "answer": "activism",
                        "hit": false
                    },
                    {
                        "score": 0.5317050501104645,
                        "answer": "barebones",
                        "hit": false
                    },
                    {
                        "score": 0.5284092687201392,
                        "answer": "hyperinflation",
                        "hit": false
                    },
                    {
                        "score": 0.5195310944365319,
                        "answer": "reliance",
                        "hit": false
                    },
                    {
                        "score": 0.5160982815818129,
                        "answer": "cozy",
                        "hit": false
                    },
                    {
                        "score": 0.5139482237632502,
                        "answer": "nylon",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "diamond"
                ],
                "rank": 3892,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7040465325117111
            },
            {
                "question verbose": "What is to doorknob ",
                "b": "doorknob",
                "expected answer": [
                    "metal",
                    "steel",
                    "bronze",
                    "wood"
                ],
                "predictions": [
                    {
                        "score": 0.5853628960110411,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.295195132676667,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.2950091071426992,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.2828099821142016,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.27870426156309935,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2782693971763309,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "doorknob"
                ],
                "rank": 1689,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5857278406620026
            },
            {
                "question verbose": "What is to flag ",
                "b": "flag",
                "expected answer": [
                    "fabric",
                    "paper"
                ],
                "predictions": [
                    {
                        "score": 0.4252935629881773,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.41898640721702696,
                        "answer": "wed",
                        "hit": false
                    },
                    {
                        "score": 0.40752265963821355,
                        "answer": "souplining",
                        "hit": false
                    },
                    {
                        "score": 0.40338997211988975,
                        "answer": "tackling",
                        "hit": false
                    },
                    {
                        "score": 0.400823265791084,
                        "answer": "evolved",
                        "hit": false
                    },
                    {
                        "score": 0.39909794344439714,
                        "answer": "hardwiring",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "flag"
                ],
                "rank": 671,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7454158365726471
            },
            {
                "question verbose": "What is to glacier ",
                "b": "glacier",
                "expected answer": [
                    "ice",
                    "water",
                    "oxygen",
                    "hydrogen",
                    "h",
                    "h2o",
                    "atomic_number_8",
                    "atomic_number_1",
                    "o",
                    "water_ice"
                ],
                "predictions": [
                    {
                        "score": 0.5878059749034822,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2887646808119787,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.2882576776861703,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.28709569353308273,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.2852452136251557,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2848740918863073,
                        "answer": "subversion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "glacier"
                ],
                "rank": 447,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5710105746984482
            },
            {
                "question verbose": "What is to glass ",
                "b": "glass",
                "expected answer": [
                    "silicone"
                ],
                "predictions": [
                    {
                        "score": 0.4467761599077904,
                        "answer": "solved",
                        "hit": false
                    },
                    {
                        "score": 0.4438589606414269,
                        "answer": "seem",
                        "hit": false
                    },
                    {
                        "score": 0.42757517618858476,
                        "answer": "overprivileged",
                        "hit": false
                    },
                    {
                        "score": 0.41672969580033736,
                        "answer": "footage",
                        "hit": false
                    },
                    {
                        "score": 0.4160491954775661,
                        "answer": "purity",
                        "hit": false
                    },
                    {
                        "score": 0.4112123927212217,
                        "answer": "anticipate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "glass"
                ],
                "rank": 8726,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.681758239865303
            },
            {
                "question verbose": "What is to house ",
                "b": "house",
                "expected answer": [
                    "bricks",
                    "cement",
                    "wood",
                    "clay"
                ],
                "predictions": [
                    {
                        "score": 0.277111385511379,
                        "answer": "thick",
                        "hit": false
                    },
                    {
                        "score": 0.2645974119144551,
                        "answer": "struck",
                        "hit": false
                    },
                    {
                        "score": 0.26271203732604087,
                        "answer": "idea",
                        "hit": false
                    },
                    {
                        "score": 0.2520186381179008,
                        "answer": "bounty",
                        "hit": false
                    },
                    {
                        "score": 0.24227590026064297,
                        "answer": "workaround",
                        "hit": false
                    },
                    {
                        "score": 0.2396510263703908,
                        "answer": "superiority",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "house"
                ],
                "rank": 783,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5523499622941017
            },
            {
                "question verbose": "What is to ice ",
                "b": "ice",
                "expected answer": [
                    "water",
                    "oxygen",
                    "hydrogen",
                    "water",
                    "h2o",
                    "atomic_number_1",
                    "atomic_number_8",
                    "h",
                    "o"
                ],
                "predictions": [
                    {
                        "score": 0.45651020113822993,
                        "answer": "arctic",
                        "hit": false
                    },
                    {
                        "score": 0.40748096265841743,
                        "answer": "thick",
                        "hit": false
                    },
                    {
                        "score": 0.39910203941877725,
                        "answer": "melting",
                        "hit": false
                    },
                    {
                        "score": 0.3861599233102516,
                        "answer": "essentially",
                        "hit": false
                    },
                    {
                        "score": 0.3767459109244146,
                        "answer": "melt",
                        "hit": false
                    },
                    {
                        "score": 0.35435961527661775,
                        "answer": "struck",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ice"
                ],
                "rank": 565,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6381679624319077
            },
            {
                "question verbose": "What is to icicle ",
                "b": "icicle",
                "expected answer": [
                    "ice",
                    "water"
                ],
                "predictions": [
                    {
                        "score": 0.5355226045019186,
                        "answer": "purity",
                        "hit": false
                    },
                    {
                        "score": 0.5124256277123369,
                        "answer": "deemed",
                        "hit": false
                    },
                    {
                        "score": 0.5107223857556558,
                        "answer": "kindergarten",
                        "hit": false
                    },
                    {
                        "score": 0.5029782812962278,
                        "answer": "qualification",
                        "hit": false
                    },
                    {
                        "score": 0.49349905290858415,
                        "answer": "doooooooooo",
                        "hit": false
                    },
                    {
                        "score": 0.4898200776158635,
                        "answer": "consensual",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "icicle"
                ],
                "rank": 10775,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6826859712600708
            },
            {
                "question verbose": "What is to jam ",
                "b": "jam",
                "expected answer": [
                    "fruit",
                    "sugar",
                    "berries"
                ],
                "predictions": [
                    {
                        "score": 0.5806116300139117,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30538357811856137,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.3023072909692615,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.3019203670389446,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2945435092286334,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.28722522064446293,
                        "answer": "cost",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jam"
                ],
                "rank": 557,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6836603879928589
            },
            {
                "question verbose": "What is to jeans ",
                "b": "jeans",
                "expected answer": [
                    "fabric",
                    "denim"
                ],
                "predictions": [
                    {
                        "score": 0.585100272851516,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30283486780443647,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.29370708021004566,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.2932941603093012,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.2931593709239262,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.2890431779293964,
                        "answer": "partisan",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jeans"
                ],
                "rank": 7225,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5881936848163605
            },
            {
                "question verbose": "What is to lawn ",
                "b": "lawn",
                "expected answer": [
                    "grass"
                ],
                "predictions": [
                    {
                        "score": 0.5241352764940477,
                        "answer": "drainage",
                        "hit": false
                    },
                    {
                        "score": 0.4815755324467848,
                        "answer": "incorrect",
                        "hit": false
                    },
                    {
                        "score": 0.477229234878991,
                        "answer": "alluded",
                        "hit": false
                    },
                    {
                        "score": 0.47439645449250223,
                        "answer": "simultaneous",
                        "hit": false
                    },
                    {
                        "score": 0.46823398393066157,
                        "answer": "barge",
                        "hit": false
                    },
                    {
                        "score": 0.46589635855504097,
                        "answer": "learnt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lawn"
                ],
                "rank": 4591,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8203475177288055
            },
            {
                "question verbose": "What is to lens ",
                "b": "lens",
                "expected answer": [
                    "glass",
                    "plastic"
                ],
                "predictions": [
                    {
                        "score": 0.5069088952692621,
                        "answer": "inroad",
                        "hit": false
                    },
                    {
                        "score": 0.5053514000278371,
                        "answer": "comprising",
                        "hit": false
                    },
                    {
                        "score": 0.49214043957903153,
                        "answer": "fabric",
                        "hit": false
                    },
                    {
                        "score": 0.48906298725572694,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.48382181626316245,
                        "answer": "activism",
                        "hit": false
                    },
                    {
                        "score": 0.48042244646543814,
                        "answer": "issuance",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lens"
                ],
                "rank": 8642,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6836730986833572
            },
            {
                "question verbose": "What is to mirror ",
                "b": "mirror",
                "expected answer": [
                    "glass",
                    "bronze"
                ],
                "predictions": [
                    {
                        "score": 0.500389422887185,
                        "answer": "qualification",
                        "hit": false
                    },
                    {
                        "score": 0.49658403666974893,
                        "answer": "alluded",
                        "hit": false
                    },
                    {
                        "score": 0.4910356795773282,
                        "answer": "purity",
                        "hit": false
                    },
                    {
                        "score": 0.48521129646834954,
                        "answer": "dorm",
                        "hit": false
                    },
                    {
                        "score": 0.4850126474896283,
                        "answer": "anticipate",
                        "hit": false
                    },
                    {
                        "score": 0.4848830212872269,
                        "answer": "simultaneous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mirror"
                ],
                "rank": 3245,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7606417834758759
            },
            {
                "question verbose": "What is to money ",
                "b": "money",
                "expected answer": [
                    "paper",
                    "metal",
                    "silver",
                    "gold",
                    "iron",
                    "copper",
                    "tin"
                ],
                "predictions": [
                    {
                        "score": 0.43292895368914,
                        "answer": "bailouts",
                        "hit": false
                    },
                    {
                        "score": 0.40328868778224763,
                        "answer": "backwards",
                        "hit": false
                    },
                    {
                        "score": 0.4018976716231801,
                        "answer": "borrowing",
                        "hit": false
                    },
                    {
                        "score": 0.39199384275831933,
                        "answer": "unpatriotic",
                        "hit": false
                    },
                    {
                        "score": 0.3860686037127177,
                        "answer": "reinstated",
                        "hit": false
                    },
                    {
                        "score": 0.37972277742611427,
                        "answer": "subversion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "money"
                ],
                "rank": 1813,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5989377945661545
            },
            {
                "question verbose": "What is to ocean ",
                "b": "ocean",
                "expected answer": [
                    "water"
                ],
                "predictions": [
                    {
                        "score": 0.37986364270302503,
                        "answer": "fabric",
                        "hit": false
                    },
                    {
                        "score": 0.3619226872486823,
                        "answer": "manuscript",
                        "hit": false
                    },
                    {
                        "score": 0.35177823059301755,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.3491409653939893,
                        "answer": "monumental",
                        "hit": false
                    },
                    {
                        "score": 0.3489933489965568,
                        "answer": "recycled",
                        "hit": false
                    },
                    {
                        "score": 0.3482931818911517,
                        "answer": "modernity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ocean"
                ],
                "rank": 2913,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6637970954179764
            },
            {
                "question verbose": "What is to omelette ",
                "b": "omelette",
                "expected answer": [
                    "eggs",
                    "egg",
                    "milk",
                    "cheese"
                ],
                "predictions": [
                    {
                        "score": 0.5656360487350622,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2971822845356312,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.2922304257550397,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.2859583807554395,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.2821901872551357,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.2784836369302372,
                        "answer": "allow",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "omelette"
                ],
                "rank": 4030,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to pastry ",
                "b": "pastry",
                "expected answer": [
                    "flour",
                    "egg",
                    "butter",
                    "filling"
                ],
                "predictions": [
                    {
                        "score": 0.5644074181325712,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2944741952139187,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.2918499822198933,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.2872836717496413,
                        "answer": "overnight",
                        "hit": false
                    },
                    {
                        "score": 0.2871516607235777,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.286331684780236,
                        "answer": "cost",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pastry"
                ],
                "rank": 3207,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to penny ",
                "b": "penny",
                "expected answer": [
                    "metal",
                    "alloy",
                    "bronze",
                    "nickel",
                    "zinc",
                    "copper",
                    "tin"
                ],
                "predictions": [
                    {
                        "score": 0.5783820848829508,
                        "answer": "manuscript",
                        "hit": false
                    },
                    {
                        "score": 0.5481566437657778,
                        "answer": "dramatically",
                        "hit": false
                    },
                    {
                        "score": 0.5292619030279034,
                        "answer": "reduced",
                        "hit": false
                    },
                    {
                        "score": 0.5142385006508814,
                        "answer": "exported",
                        "hit": false
                    },
                    {
                        "score": 0.5123954338514906,
                        "answer": "issuance",
                        "hit": false
                    },
                    {
                        "score": 0.511786718746405,
                        "answer": "minimum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "penny"
                ],
                "rank": 2165,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7498875558376312
            },
            {
                "question verbose": "What is to pill ",
                "b": "pill",
                "expected answer": [
                    "medicine",
                    "drug"
                ],
                "predictions": [
                    {
                        "score": 0.4888154507278218,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.4609704424996073,
                        "answer": "sticking",
                        "hit": false
                    },
                    {
                        "score": 0.4532168194553684,
                        "answer": "manuscript",
                        "hit": false
                    },
                    {
                        "score": 0.44789608209816506,
                        "answer": "souplining",
                        "hit": false
                    },
                    {
                        "score": 0.4475534078158247,
                        "answer": "propose",
                        "hit": false
                    },
                    {
                        "score": 0.44347929936701747,
                        "answer": "tackling",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pill"
                ],
                "rank": 12387,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6730351746082306
            },
            {
                "question verbose": "What is to plastic ",
                "b": "plastic",
                "expected answer": [
                    "polymer",
                    "oil",
                    "gas",
                    "coal"
                ],
                "predictions": [
                    {
                        "score": 0.3682580499237743,
                        "answer": "heaped",
                        "hit": false
                    },
                    {
                        "score": 0.3596777504945037,
                        "answer": "plaza",
                        "hit": false
                    },
                    {
                        "score": 0.35599784082935704,
                        "answer": "architect",
                        "hit": false
                    },
                    {
                        "score": 0.3559663423908364,
                        "answer": "gunshield",
                        "hit": false
                    },
                    {
                        "score": 0.35567541969323263,
                        "answer": "grating",
                        "hit": false
                    },
                    {
                        "score": 0.3520149249025331,
                        "answer": "upriver",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "plastic"
                ],
                "rank": 7029,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6102189719676971
            },
            {
                "question verbose": "What is to roof ",
                "b": "roof",
                "expected answer": [
                    "shingles",
                    "tiles",
                    "wood",
                    "metal"
                ],
                "predictions": [
                    {
                        "score": 0.47838804696685083,
                        "answer": "chemically",
                        "hit": false
                    },
                    {
                        "score": 0.4680368059974346,
                        "answer": "blead",
                        "hit": false
                    },
                    {
                        "score": 0.4553125607742575,
                        "answer": "drainage",
                        "hit": false
                    },
                    {
                        "score": 0.4500787724449001,
                        "answer": "qualification",
                        "hit": false
                    },
                    {
                        "score": 0.4482726193941538,
                        "answer": "controversy",
                        "hit": false
                    },
                    {
                        "score": 0.44526482937453377,
                        "answer": "incorrect",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "roof"
                ],
                "rank": 165,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5858341380953789
            },
            {
                "question verbose": "What is to sea ",
                "b": "sea",
                "expected answer": [
                    "water"
                ],
                "predictions": [
                    {
                        "score": 0.585385408397941,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2981172334476245,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.29462448441363837,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.29128862003662975,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.28629758666872623,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.27418860318158317,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sea"
                ],
                "rank": 12489,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5525518953800201
            },
            {
                "question verbose": "What is to snow ",
                "b": "snow",
                "expected answer": [
                    "water",
                    "oxygen",
                    "hydrogen",
                    "flake",
                    "ice",
                    "snowflake",
                    "snowflakes",
                    "h2o",
                    "atomic_number_1",
                    "atomic_number_8",
                    "h",
                    "o"
                ],
                "predictions": [
                    {
                        "score": 0.3881785462391945,
                        "answer": "pusher",
                        "hit": false
                    },
                    {
                        "score": 0.38110137581769554,
                        "answer": "obtained",
                        "hit": false
                    },
                    {
                        "score": 0.3806739867556844,
                        "answer": "purity",
                        "hit": false
                    },
                    {
                        "score": 0.3727270651301991,
                        "answer": "anticipate",
                        "hit": false
                    },
                    {
                        "score": 0.3724202468054866,
                        "answer": "luxor",
                        "hit": false
                    },
                    {
                        "score": 0.37158890699762914,
                        "answer": "cog",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "snow"
                ],
                "rank": 842,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6023551747202873
            },
            {
                "question verbose": "What is to spoon ",
                "b": "spoon",
                "expected answer": [
                    "aluminium",
                    "wood",
                    "steel"
                ],
                "predictions": [
                    {
                        "score": 0.563895437721777,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2907671463587105,
                        "answer": "subversion",
                        "hit": false
                    },
                    {
                        "score": 0.2899726845892363,
                        "answer": "suggest",
                        "hit": false
                    },
                    {
                        "score": 0.28778004479188785,
                        "answer": "partisan",
                        "hit": false
                    },
                    {
                        "score": 0.2868934795947488,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.27819261115531024,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spoon"
                ],
                "rank": 2816,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to steel ",
                "b": "steel",
                "expected answer": [
                    "iron",
                    "pearlite",
                    "carbon",
                    "cementite",
                    "ferrite",
                    "iron_carbide",
                    "fe",
                    "atomic_number_26",
                    "alpha_iron"
                ],
                "predictions": [
                    {
                        "score": 0.503175908523011,
                        "answer": "learnt",
                        "hit": false
                    },
                    {
                        "score": 0.45446980735193415,
                        "answer": "fabric",
                        "hit": false
                    },
                    {
                        "score": 0.44037466175304757,
                        "answer": "eaves",
                        "hit": false
                    },
                    {
                        "score": 0.4326956828936502,
                        "answer": "prospect",
                        "hit": false
                    },
                    {
                        "score": 0.43049682719692883,
                        "answer": "match",
                        "hit": false
                    },
                    {
                        "score": 0.42669412187554384,
                        "answer": "leone",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "steel"
                ],
                "rank": 7012,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.732528418302536
            },
            {
                "question verbose": "What is to table ",
                "b": "table",
                "expected answer": [
                    "wood",
                    "metal",
                    "plastic"
                ],
                "predictions": [
                    {
                        "score": 0.3814246079413923,
                        "answer": "commanding",
                        "hit": false
                    },
                    {
                        "score": 0.3795376767229619,
                        "answer": "timing",
                        "hit": false
                    },
                    {
                        "score": 0.3788799951857298,
                        "answer": "fibre",
                        "hit": false
                    },
                    {
                        "score": 0.36625077047666155,
                        "answer": "capable",
                        "hit": false
                    },
                    {
                        "score": 0.3641390765317213,
                        "answer": "minus",
                        "hit": false
                    },
                    {
                        "score": 0.36129671748900233,
                        "answer": "derived",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "table"
                ],
                "rank": 2688,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6763680875301361
            },
            {
                "question verbose": "What is to wall ",
                "b": "wall",
                "expected answer": [
                    "cement",
                    "bricks",
                    "stones",
                    "rocks",
                    "wood",
                    "cardboard",
                    "board",
                    "plank",
                    "clay",
                    "mud"
                ],
                "predictions": [
                    {
                        "score": 0.37798269172825033,
                        "answer": "bark",
                        "hit": false
                    },
                    {
                        "score": 0.32365196717226064,
                        "answer": "proving",
                        "hit": false
                    },
                    {
                        "score": 0.31615417808213375,
                        "answer": "arrangement",
                        "hit": false
                    },
                    {
                        "score": 0.30891508813082824,
                        "answer": "metal",
                        "hit": false
                    },
                    {
                        "score": 0.3001199117116142,
                        "answer": "image",
                        "hit": false
                    },
                    {
                        "score": 0.2990255931410998,
                        "answer": "interactive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wall"
                ],
                "rank": 2241,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5172606352716684
            },
            {
                "question verbose": "What is to water ",
                "b": "water",
                "expected answer": [
                    "oxygen",
                    "hydrogen",
                    "water",
                    "h2o",
                    "atomic_number_1",
                    "atomic_number_8",
                    "h",
                    "o"
                ],
                "predictions": [
                    {
                        "score": 0.4170039213701675,
                        "answer": "drainage",
                        "hit": false
                    },
                    {
                        "score": 0.37826271386546567,
                        "answer": "solved",
                        "hit": false
                    },
                    {
                        "score": 0.34376550567319886,
                        "answer": "afraid",
                        "hit": false
                    },
                    {
                        "score": 0.333064939901801,
                        "answer": "floppers",
                        "hit": false
                    },
                    {
                        "score": 0.3319123365557657,
                        "answer": "barrier",
                        "hit": false
                    },
                    {
                        "score": 0.32650167703859295,
                        "answer": "cardiac",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "water"
                ],
                "rank": 129,
                "landing_b": true,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6222795248031616
            },
            {
                "question verbose": "What is to wig ",
                "b": "wig",
                "expected answer": [
                    "hair"
                ],
                "predictions": [
                    {
                        "score": 0.46842119599869714,
                        "answer": "reliance",
                        "hit": false
                    },
                    {
                        "score": 0.46500791550784704,
                        "answer": "lovehoneycouk",
                        "hit": false
                    },
                    {
                        "score": 0.45160155482631703,
                        "answer": "profoundly",
                        "hit": false
                    },
                    {
                        "score": 0.45114345511429116,
                        "answer": "autonomy",
                        "hit": false
                    },
                    {
                        "score": 0.4507712461691744,
                        "answer": "manseau",
                        "hit": false
                    },
                    {
                        "score": 0.45028898643719295,
                        "answer": "alluded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wig"
                ],
                "rank": 14365,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6590497046709061
            },
            {
                "question verbose": "What is to wine ",
                "b": "wine",
                "expected answer": [
                    "grapes",
                    "grape"
                ],
                "predictions": [
                    {
                        "score": 0.37234086510045444,
                        "answer": "sao",
                        "hit": false
                    },
                    {
                        "score": 0.3690249156314729,
                        "answer": "immature",
                        "hit": false
                    },
                    {
                        "score": 0.3685601283133049,
                        "answer": "heaped",
                        "hit": false
                    },
                    {
                        "score": 0.36748336648890145,
                        "answer": "distinctly",
                        "hit": false
                    },
                    {
                        "score": 0.36685813249839117,
                        "answer": "scape",
                        "hit": false
                    },
                    {
                        "score": 0.36611421763006163,
                        "answer": "erasing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wine"
                ],
                "rank": 2505,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6051468998193741
            },
            {
                "question verbose": "What is to wire ",
                "b": "wire",
                "expected answer": [
                    "metal"
                ],
                "predictions": [
                    {
                        "score": 0.4182184402954341,
                        "answer": "unpredictably",
                        "hit": false
                    },
                    {
                        "score": 0.4133452178010687,
                        "answer": "regulated",
                        "hit": false
                    },
                    {
                        "score": 0.4093543511402897,
                        "answer": "supplementing",
                        "hit": false
                    },
                    {
                        "score": 0.40864059544241305,
                        "answer": "enables",
                        "hit": false
                    },
                    {
                        "score": 0.40552336239940606,
                        "answer": "paradigm",
                        "hit": false
                    },
                    {
                        "score": 0.40496764129856705,
                        "answer": "plunge",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wire"
                ],
                "rank": 9328,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6559665501117706
            },
            {
                "question verbose": "What is to yogurt ",
                "b": "yogurt",
                "expected answer": [
                    "milk"
                ],
                "predictions": [
                    {
                        "score": 0.47754086658070793,
                        "answer": "mitochondrion",
                        "hit": false
                    },
                    {
                        "score": 0.4737715078909991,
                        "answer": "obtained",
                        "hit": false
                    },
                    {
                        "score": 0.46667133051079335,
                        "answer": "excessively",
                        "hit": false
                    },
                    {
                        "score": 0.45938827351295985,
                        "answer": "manuscript",
                        "hit": false
                    },
                    {
                        "score": 0.4583540787301711,
                        "answer": "dorm",
                        "hit": false
                    },
                    {
                        "score": 0.4552661261418927,
                        "answer": "immerse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "yogurt"
                ],
                "rank": 13099,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8016297221183777
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L04 [meronyms - substance].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "35d4d099-92ef-45c9-8fdd-433cd370f4bc",
            "timestamp": "2020-10-22T15:57:49.586489"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to acrobat ",
                "b": "acrobat",
                "expected answer": [
                    "troupe"
                ],
                "predictions": [
                    {
                        "score": 0.5848676477791467,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2927094603400022,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2753393285736121,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.27503210956859564,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.27277193515266984,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2726781445451238,
                        "answer": "camp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "acrobat"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to antelope ",
                "b": "antelope",
                "expected answer": [
                    "herd"
                ],
                "predictions": [
                    {
                        "score": 0.581191716603463,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29142122477409726,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2890401723555028,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.27016550771408526,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2674563906420123,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2657332738175101,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "antelope"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to bee ",
                "b": "bee",
                "expected answer": [
                    "swarm",
                    "hive",
                    "colony",
                    "drift",
                    "cast",
                    "nest",
                    "rabble",
                    "stand"
                ],
                "predictions": [
                    {
                        "score": 0.4057133565175946,
                        "answer": "ranking",
                        "hit": false
                    },
                    {
                        "score": 0.40331744798903135,
                        "answer": "turgid",
                        "hit": false
                    },
                    {
                        "score": 0.40043769282468955,
                        "answer": "advertised",
                        "hit": false
                    },
                    {
                        "score": 0.4001582779388496,
                        "answer": "bailing",
                        "hit": false
                    },
                    {
                        "score": 0.39782731547440475,
                        "answer": "enrolled",
                        "hit": false
                    },
                    {
                        "score": 0.39724908655197355,
                        "answer": "chord",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bee"
                ],
                "rank": 346,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7414724081754684
            },
            {
                "question verbose": "What is to bird ",
                "b": "bird",
                "expected answer": [
                    "flock"
                ],
                "predictions": [
                    {
                        "score": 0.38782714423842385,
                        "answer": "dint",
                        "hit": false
                    },
                    {
                        "score": 0.3850292410521785,
                        "answer": "fire",
                        "hit": false
                    },
                    {
                        "score": 0.360073185917483,
                        "answer": "lovehoneycouk",
                        "hit": false
                    },
                    {
                        "score": 0.3561970879870742,
                        "answer": "curvature",
                        "hit": false
                    },
                    {
                        "score": 0.3551274459024709,
                        "answer": "depression",
                        "hit": false
                    },
                    {
                        "score": 0.35443118482259983,
                        "answer": "chattering",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bird"
                ],
                "rank": 5509,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7391574531793594
            },
            {
                "question verbose": "What is to book ",
                "b": "book",
                "expected answer": [
                    "library",
                    "shelf",
                    "collection",
                    "accounting",
                    "accounting_system",
                    "method_of_accounting"
                ],
                "predictions": [
                    {
                        "score": 0.41879802572080377,
                        "answer": "bicentennial",
                        "hit": false
                    },
                    {
                        "score": 0.3578135198553475,
                        "answer": "robot",
                        "hit": false
                    },
                    {
                        "score": 0.3560101245834077,
                        "answer": "hopkins",
                        "hit": false
                    },
                    {
                        "score": 0.3525801233392949,
                        "answer": "captain",
                        "hit": false
                    },
                    {
                        "score": 0.34177875028945826,
                        "answer": "chronicling",
                        "hit": false
                    },
                    {
                        "score": 0.3299358203113807,
                        "answer": "ecclesiastes",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "book"
                ],
                "rank": 9,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6016414389014244
            },
            {
                "question verbose": "What is to calf ",
                "b": "calf",
                "expected answer": [
                    "cattle",
                    "herd"
                ],
                "predictions": [
                    {
                        "score": 0.5800778207502398,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2872392145547554,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.27980401166110136,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2714368825968502,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.2674482551276181,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2651883912590215,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "calf"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to car ",
                "b": "car",
                "expected answer": [
                    "train",
                    "procession"
                ],
                "predictions": [
                    {
                        "score": 0.3297618296975412,
                        "answer": "bed",
                        "hit": false
                    },
                    {
                        "score": 0.3140090138001559,
                        "answer": "scandal",
                        "hit": false
                    },
                    {
                        "score": 0.2969870373764061,
                        "answer": "bus",
                        "hit": false
                    },
                    {
                        "score": 0.28823845172391654,
                        "answer": "impractical",
                        "hit": false
                    },
                    {
                        "score": 0.28279277660813573,
                        "answer": "commercial",
                        "hit": false
                    },
                    {
                        "score": 0.2797760233753882,
                        "answer": "occasionally",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "car"
                ],
                "rank": 42,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6910944283008575
            },
            {
                "question verbose": "What is to cat ",
                "b": "cat",
                "expected answer": [
                    "clowder",
                    "glaring",
                    "clutter",
                    "pounce"
                ],
                "predictions": [
                    {
                        "score": 0.5000869243054171,
                        "answer": "spends",
                        "hit": false
                    },
                    {
                        "score": 0.4850493101450526,
                        "answer": "pitch",
                        "hit": false
                    },
                    {
                        "score": 0.462332245797184,
                        "answer": "glorious",
                        "hit": false
                    },
                    {
                        "score": 0.4617115183406195,
                        "answer": "impractical",
                        "hit": false
                    },
                    {
                        "score": 0.44283359109994785,
                        "answer": "ames",
                        "hit": false
                    },
                    {
                        "score": 0.42778201097029545,
                        "answer": "bleaker",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cat"
                ],
                "rank": 5053,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5868597850203514
            },
            {
                "question verbose": "What is to cattle ",
                "b": "cattle",
                "expected answer": [
                    "herd"
                ],
                "predictions": [
                    {
                        "score": 0.5818762005869862,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2834526271651942,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.275394247677736,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2739910296656334,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2707398352382932,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.25785171292705805,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cattle"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to christian ",
                "b": "christian",
                "expected answer": [
                    "congregation",
                    "church",
                    "parish"
                ],
                "predictions": [
                    {
                        "score": 0.3459423592977075,
                        "answer": "denominational",
                        "hit": false
                    },
                    {
                        "score": 0.3103285826751177,
                        "answer": "misconception",
                        "hit": false
                    },
                    {
                        "score": 0.3085425572357539,
                        "answer": "glad",
                        "hit": false
                    },
                    {
                        "score": 0.30566984154171273,
                        "answer": "fdr",
                        "hit": false
                    },
                    {
                        "score": 0.3031256628701857,
                        "answer": "standout",
                        "hit": false
                    },
                    {
                        "score": 0.29858820684090503,
                        "answer": "preaching",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "christian"
                ],
                "rank": 18,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7124049961566925
            },
            {
                "question verbose": "What is to citizen ",
                "b": "citizen",
                "expected answer": [
                    "citizenry",
                    "country",
                    "state"
                ],
                "predictions": [
                    {
                        "score": 0.3150889142058999,
                        "answer": "survival",
                        "hit": false
                    },
                    {
                        "score": 0.313861689789491,
                        "answer": "attucks",
                        "hit": false
                    },
                    {
                        "score": 0.3115823927293076,
                        "answer": "devoted",
                        "hit": false
                    },
                    {
                        "score": 0.30684349983258263,
                        "answer": "missionary",
                        "hit": false
                    },
                    {
                        "score": 0.30630878804643424,
                        "answer": "dedicated",
                        "hit": false
                    },
                    {
                        "score": 0.30617179949707884,
                        "answer": "argentinian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "citizen"
                ],
                "rank": 2441,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6768513470888138
            },
            {
                "question verbose": "What is to college ",
                "b": "college",
                "expected answer": [
                    "university"
                ],
                "predictions": [
                    {
                        "score": 0.42604945644251657,
                        "answer": "graduation",
                        "hit": false
                    },
                    {
                        "score": 0.3912093725991928,
                        "answer": "assembled",
                        "hit": false
                    },
                    {
                        "score": 0.39057411477121995,
                        "answer": "kiwi",
                        "hit": false
                    },
                    {
                        "score": 0.3693615349818553,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.3661711896848534,
                        "answer": "barbara",
                        "hit": false
                    },
                    {
                        "score": 0.35886215053990994,
                        "answer": "cocoa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "college"
                ],
                "rank": 58,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7267105132341385
            },
            {
                "question verbose": "What is to county ",
                "b": "county",
                "expected answer": [
                    "state",
                    "country"
                ],
                "predictions": [
                    {
                        "score": 0.36773364478490317,
                        "answer": "legg",
                        "hit": false
                    },
                    {
                        "score": 0.3615200753128334,
                        "answer": "egypt",
                        "hit": false
                    },
                    {
                        "score": 0.3521069434418395,
                        "answer": "iriarte",
                        "hit": false
                    },
                    {
                        "score": 0.34857136431833924,
                        "answer": "absentee",
                        "hit": false
                    },
                    {
                        "score": 0.34802315991441723,
                        "answer": "bonanza",
                        "hit": false
                    },
                    {
                        "score": 0.34390257804459606,
                        "answer": "f",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "county"
                ],
                "rank": 3463,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.662257581949234
            },
            {
                "question verbose": "What is to cow ",
                "b": "cow",
                "expected answer": [
                    "herd"
                ],
                "predictions": [
                    {
                        "score": 0.581517289597093,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29175103068125186,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.27682609354351284,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.27494213791250516,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2666677150648646,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.265318583158321,
                        "answer": "attucks",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cow"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to crow ",
                "b": "crow",
                "expected answer": [
                    "murder"
                ],
                "predictions": [
                    {
                        "score": 0.4637150864008341,
                        "answer": "commercial",
                        "hit": false
                    },
                    {
                        "score": 0.44263292577028135,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.441266091657338,
                        "answer": "beat",
                        "hit": false
                    },
                    {
                        "score": 0.42361293375450204,
                        "answer": "deduction",
                        "hit": false
                    },
                    {
                        "score": 0.41517851518440957,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.4139677839961552,
                        "answer": "paltry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "crow"
                ],
                "rank": 14378,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.603738322854042
            },
            {
                "question verbose": "What is to division ",
                "b": "division",
                "expected answer": [
                    "company",
                    "business",
                    "platoon",
                    "battle_group"
                ],
                "predictions": [
                    {
                        "score": 0.4368669479968875,
                        "answer": "pacific",
                        "hit": false
                    },
                    {
                        "score": 0.433046827079801,
                        "answer": "boat",
                        "hit": false
                    },
                    {
                        "score": 0.4315830772165923,
                        "answer": "landed",
                        "hit": false
                    },
                    {
                        "score": 0.42073757835495484,
                        "answer": "northern",
                        "hit": false
                    },
                    {
                        "score": 0.41885838841746176,
                        "answer": "kiwi",
                        "hit": false
                    },
                    {
                        "score": 0.41838934693205304,
                        "answer": "filed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "division"
                ],
                "rank": 11643,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.582384318113327
            },
            {
                "question verbose": "What is to elephant ",
                "b": "elephant",
                "expected answer": [
                    "herd"
                ],
                "predictions": [
                    {
                        "score": 0.5177527444290847,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.513353343509946,
                        "answer": "cocoa",
                        "hit": false
                    },
                    {
                        "score": 0.4936119892589538,
                        "answer": "dakota",
                        "hit": false
                    },
                    {
                        "score": 0.4848027772396879,
                        "answer": "crossing",
                        "hit": false
                    },
                    {
                        "score": 0.47903238177021673,
                        "answer": "sown",
                        "hit": false
                    },
                    {
                        "score": 0.4782948000509545,
                        "answer": "barbara",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "elephant"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6032466441392899
            },
            {
                "question verbose": "What is to employee ",
                "b": "employee",
                "expected answer": [
                    "staff",
                    "company"
                ],
                "predictions": [
                    {
                        "score": 0.37221975486063796,
                        "answer": "contribution",
                        "hit": false
                    },
                    {
                        "score": 0.35860861517870274,
                        "answer": "denny",
                        "hit": false
                    },
                    {
                        "score": 0.3481057448982874,
                        "answer": "insurer",
                        "hit": false
                    },
                    {
                        "score": 0.3473326340380057,
                        "answer": "older",
                        "hit": false
                    },
                    {
                        "score": 0.335110414153765,
                        "answer": "younger",
                        "hit": false
                    },
                    {
                        "score": 0.32876066383805314,
                        "answer": "agency",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "employee"
                ],
                "rank": 874,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6629681289196014
            },
            {
                "question verbose": "What is to fish ",
                "b": "fish",
                "expected answer": [
                    "school"
                ],
                "predictions": [
                    {
                        "score": 0.3926303554798052,
                        "answer": "scandal",
                        "hit": false
                    },
                    {
                        "score": 0.3890630689013507,
                        "answer": "river",
                        "hit": false
                    },
                    {
                        "score": 0.3865441187970516,
                        "answer": "pitch",
                        "hit": false
                    },
                    {
                        "score": 0.3760554683654514,
                        "answer": "ladder",
                        "hit": false
                    },
                    {
                        "score": 0.35365643800345153,
                        "answer": "km",
                        "hit": false
                    },
                    {
                        "score": 0.34616399987949553,
                        "answer": "bopd",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fish"
                ],
                "rank": 11322,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6325715482234955
            },
            {
                "question verbose": "What is to flower ",
                "b": "flower",
                "expected answer": [
                    "bouquet",
                    "bunch"
                ],
                "predictions": [
                    {
                        "score": 0.4870221037735554,
                        "answer": "swarm",
                        "hit": false
                    },
                    {
                        "score": 0.4715731174348296,
                        "answer": "participated",
                        "hit": false
                    },
                    {
                        "score": 0.47089452574932716,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.46741626946734155,
                        "answer": "throe",
                        "hit": false
                    },
                    {
                        "score": 0.46020885801628075,
                        "answer": "cocoa",
                        "hit": false
                    },
                    {
                        "score": 0.46018328660422186,
                        "answer": "tome",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "flower"
                ],
                "rank": 10648,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.671718180179596
            },
            {
                "question verbose": "What is to galaxy ",
                "b": "galaxy",
                "expected answer": [
                    "universe"
                ],
                "predictions": [
                    {
                        "score": 0.44112276721006727,
                        "answer": "centered",
                        "hit": false
                    },
                    {
                        "score": 0.42492954082697226,
                        "answer": "nook",
                        "hit": false
                    },
                    {
                        "score": 0.4180483182238369,
                        "answer": "costello",
                        "hit": false
                    },
                    {
                        "score": 0.41627927873090875,
                        "answer": "conniving",
                        "hit": false
                    },
                    {
                        "score": 0.4088602378242576,
                        "answer": "rea",
                        "hit": false
                    },
                    {
                        "score": 0.4034518975732361,
                        "answer": "simultaneous",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "galaxy"
                ],
                "rank": 13412,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7081805020570755
            },
            {
                "question verbose": "What is to goose ",
                "b": "goose",
                "expected answer": [
                    "gaggle"
                ],
                "predictions": [
                    {
                        "score": 0.5126223640507769,
                        "answer": "menendez",
                        "hit": false
                    },
                    {
                        "score": 0.5039463936101842,
                        "answer": "averaged",
                        "hit": false
                    },
                    {
                        "score": 0.5008540313114765,
                        "answer": "ghent",
                        "hit": false
                    },
                    {
                        "score": 0.48801653272961437,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.4879422957146176,
                        "answer": "uptown",
                        "hit": false
                    },
                    {
                        "score": 0.487337848940226,
                        "answer": "nawlins",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "goose"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6308809667825699
            },
            {
                "question verbose": "What is to juror ",
                "b": "juror",
                "expected answer": [
                    "jury"
                ],
                "predictions": [
                    {
                        "score": 0.5991493350105705,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30312021591665195,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.29480614813272127,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2729520944340579,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2649777696125622,
                        "answer": "polished",
                        "hit": false
                    },
                    {
                        "score": 0.26308917075028965,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "juror"
                ],
                "rank": 8203,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6146632879972458
            },
            {
                "question verbose": "What is to kitten ",
                "b": "kitten",
                "expected answer": [
                    "litter"
                ],
                "predictions": [
                    {
                        "score": 0.581542369074202,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2820067797460893,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2718000425441169,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2666145171405421,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2623232434658173,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.26183742840625507,
                        "answer": "camp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "kitten"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to letter ",
                "b": "letter",
                "expected answer": [
                    "alphabet"
                ],
                "predictions": [
                    {
                        "score": 0.4230720787495281,
                        "answer": "rudy",
                        "hit": false
                    },
                    {
                        "score": 0.4073849312834213,
                        "answer": "kozmetsky",
                        "hit": false
                    },
                    {
                        "score": 0.38427975458997676,
                        "answer": "dressed",
                        "hit": false
                    },
                    {
                        "score": 0.3747573780397786,
                        "answer": "lecturer",
                        "hit": false
                    },
                    {
                        "score": 0.3648939538057378,
                        "answer": "document",
                        "hit": false
                    },
                    {
                        "score": 0.36320036960385677,
                        "answer": "gathered",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "letter"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5331677608191967
            },
            {
                "question verbose": "What is to lion ",
                "b": "lion",
                "expected answer": [
                    "pride"
                ],
                "predictions": [
                    {
                        "score": 0.45509566901271803,
                        "answer": "rial",
                        "hit": false
                    },
                    {
                        "score": 0.4360602111272268,
                        "answer": "hawk",
                        "hit": false
                    },
                    {
                        "score": 0.43339924706479327,
                        "answer": "replete",
                        "hit": false
                    },
                    {
                        "score": 0.432343558319744,
                        "answer": "miller",
                        "hit": false
                    },
                    {
                        "score": 0.4314300181665686,
                        "answer": "raf",
                        "hit": false
                    },
                    {
                        "score": 0.4227717936808615,
                        "answer": "martinez",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lion"
                ],
                "rank": 14441,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6232870519161224
            },
            {
                "question verbose": "What is to listener ",
                "b": "listener",
                "expected answer": [
                    "audience"
                ],
                "predictions": [
                    {
                        "score": 0.5962441042421623,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28612735042168197,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2778546082007568,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.26714519608926796,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2560614951443265,
                        "answer": "spendy",
                        "hit": false
                    },
                    {
                        "score": 0.2523514855808296,
                        "answer": "matthew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "listener"
                ],
                "rank": 14152,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5659521594643593
            },
            {
                "question verbose": "What is to member ",
                "b": "member",
                "expected answer": [
                    "club",
                    "team",
                    "group",
                    "band",
                    "community"
                ],
                "predictions": [
                    {
                        "score": 0.44250370019739255,
                        "answer": "staff",
                        "hit": false
                    },
                    {
                        "score": 0.42716869866040275,
                        "answer": "committee",
                        "hit": false
                    },
                    {
                        "score": 0.3951760833357767,
                        "answer": "african",
                        "hit": false
                    },
                    {
                        "score": 0.3813162194075636,
                        "answer": "association",
                        "hit": false
                    },
                    {
                        "score": 0.3809584603931028,
                        "answer": "startup",
                        "hit": false
                    },
                    {
                        "score": 0.37559502956811397,
                        "answer": "political",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "member"
                ],
                "rank": 836,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5228881668299437
            },
            {
                "question verbose": "What is to musician ",
                "b": "musician",
                "expected answer": [
                    "orchestra",
                    "band"
                ],
                "predictions": [
                    {
                        "score": 0.4451755883903432,
                        "answer": "blvd",
                        "hit": false
                    },
                    {
                        "score": 0.4428205995647734,
                        "answer": "instrumental",
                        "hit": false
                    },
                    {
                        "score": 0.41942894487765886,
                        "answer": "longhorn",
                        "hit": false
                    },
                    {
                        "score": 0.4101733102998841,
                        "answer": "startup",
                        "hit": false
                    },
                    {
                        "score": 0.40898510676676897,
                        "answer": "generous",
                        "hit": false
                    },
                    {
                        "score": 0.40272355942948435,
                        "answer": "uptown",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "musician"
                ],
                "rank": 13085,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.595874734222889
            },
            {
                "question verbose": "What is to nomad ",
                "b": "nomad",
                "expected answer": [
                    "horde"
                ],
                "predictions": [
                    {
                        "score": 0.5812569502882874,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28037693662030083,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2791664145261031,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2672320982156064,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.2631049138751176,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2621255775583736,
                        "answer": "attucks",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nomad"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to page ",
                "b": "page",
                "expected answer": [
                    "book",
                    "notebook",
                    "website",
                    "web-site"
                ],
                "predictions": [
                    {
                        "score": 0.3544226448757247,
                        "answer": "discussed",
                        "hit": false
                    },
                    {
                        "score": 0.3313005817939848,
                        "answer": "divorce",
                        "hit": false
                    },
                    {
                        "score": 0.3186511397332866,
                        "answer": "unknown",
                        "hit": false
                    },
                    {
                        "score": 0.3148031003566391,
                        "answer": "misconception",
                        "hit": false
                    },
                    {
                        "score": 0.31173362267522015,
                        "answer": "elitefitnesscom",
                        "hit": false
                    },
                    {
                        "score": 0.3073451303609472,
                        "answer": "floating",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "page"
                ],
                "rank": 62,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.561849620193243
            },
            {
                "question verbose": "What is to parishioner ",
                "b": "parishioner",
                "expected answer": [
                    "parish",
                    "church"
                ],
                "predictions": [
                    {
                        "score": 0.5980815576753729,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2834077732868556,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.2805780473268405,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.27265797033618133,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.26444087355498785,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.25908771338574543,
                        "answer": "approximately",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "parishioner"
                ],
                "rank": 8609,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6195771396160126
            },
            {
                "question verbose": "What is to person ",
                "b": "person",
                "expected answer": [
                    "society",
                    "company",
                    "party",
                    "world"
                ],
                "predictions": [
                    {
                        "score": 0.353848951219342,
                        "answer": "rebutting",
                        "hit": false
                    },
                    {
                        "score": 0.3392492362842477,
                        "answer": "functioning",
                        "hit": false
                    },
                    {
                        "score": 0.3097481992590791,
                        "answer": "conniving",
                        "hit": false
                    },
                    {
                        "score": 0.30929292451006,
                        "answer": "birthday",
                        "hit": false
                    },
                    {
                        "score": 0.3033556906228384,
                        "answer": "jurisdiction",
                        "hit": false
                    },
                    {
                        "score": 0.30317373979784407,
                        "answer": "searching",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "person"
                ],
                "rank": 11799,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6133014932274818
            },
            {
                "question verbose": "What is to photo ",
                "b": "photo",
                "expected answer": [
                    "album",
                    "collection",
                    "library"
                ],
                "predictions": [
                    {
                        "score": 0.33363151480157593,
                        "answer": "adjua",
                        "hit": false
                    },
                    {
                        "score": 0.3194882620329426,
                        "answer": "greave",
                        "hit": false
                    },
                    {
                        "score": 0.30692269587688015,
                        "answer": "preparation",
                        "hit": false
                    },
                    {
                        "score": 0.3051084765794001,
                        "answer": "featured",
                        "hit": false
                    },
                    {
                        "score": 0.2921755450383545,
                        "answer": "impractical",
                        "hit": false
                    },
                    {
                        "score": 0.2833232263509434,
                        "answer": "stranded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "photo"
                ],
                "rank": 2749,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.610934391617775
            },
            {
                "question verbose": "What is to player ",
                "b": "player",
                "expected answer": [
                    "team",
                    "group",
                    "orchestra"
                ],
                "predictions": [
                    {
                        "score": 0.30731707705750033,
                        "answer": "pacific",
                        "hit": false
                    },
                    {
                        "score": 0.292605908925389,
                        "answer": "australian",
                        "hit": false
                    },
                    {
                        "score": 0.2787126063487578,
                        "answer": "enjoyably",
                        "hit": false
                    },
                    {
                        "score": 0.2719216395623348,
                        "answer": "inquiry",
                        "hit": false
                    },
                    {
                        "score": 0.26954651908924226,
                        "answer": "pair",
                        "hit": false
                    },
                    {
                        "score": 0.2681005276703655,
                        "answer": "abl",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "player"
                ],
                "rank": 1339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6768573224544525
            },
            {
                "question verbose": "What is to policeman ",
                "b": "policeman",
                "expected answer": [
                    "police"
                ],
                "predictions": [
                    {
                        "score": 0.5982167407028319,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2888920769958332,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.27604386223574834,
                        "answer": "slick",
                        "hit": false
                    },
                    {
                        "score": 0.27223224951541714,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.26722852199933916,
                        "answer": "polished",
                        "hit": false
                    },
                    {
                        "score": 0.26466500647069047,
                        "answer": "attucks",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "policeman"
                ],
                "rank": 13627,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5457519553601742
            },
            {
                "question verbose": "What is to secretary ",
                "b": "secretary",
                "expected answer": [
                    "staff"
                ],
                "predictions": [
                    {
                        "score": 0.41030547404785134,
                        "answer": "defeat",
                        "hit": false
                    },
                    {
                        "score": 0.4055442264828182,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.40399146398719515,
                        "answer": "crater",
                        "hit": false
                    },
                    {
                        "score": 0.4014175290210209,
                        "answer": "ghent",
                        "hit": false
                    },
                    {
                        "score": 0.3964769365134779,
                        "answer": "gale",
                        "hit": false
                    },
                    {
                        "score": 0.3941296357763458,
                        "answer": "kiwi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "secretary"
                ],
                "rank": 714,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7258955538272858
            },
            {
                "question verbose": "What is to senator ",
                "b": "senator",
                "expected answer": [
                    "senate",
                    "house"
                ],
                "predictions": [
                    {
                        "score": 0.33438622465571005,
                        "answer": "benevolent",
                        "hit": false
                    },
                    {
                        "score": 0.33094802390970374,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.3304628171731727,
                        "answer": "hoffman",
                        "hit": false
                    },
                    {
                        "score": 0.32692266713728957,
                        "answer": "spendy",
                        "hit": false
                    },
                    {
                        "score": 0.3248215793318387,
                        "answer": "semester",
                        "hit": false
                    },
                    {
                        "score": 0.32046642823113786,
                        "answer": "morocco",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "senator"
                ],
                "rank": 628,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7446518540382385
            },
            {
                "question verbose": "What is to sheep ",
                "b": "sheep",
                "expected answer": [
                    "flock"
                ],
                "predictions": [
                    {
                        "score": 0.4624416164695868,
                        "answer": "glorious",
                        "hit": false
                    },
                    {
                        "score": 0.4596437686968472,
                        "answer": "kiwi",
                        "hit": false
                    },
                    {
                        "score": 0.4539072664415191,
                        "answer": "latin",
                        "hit": false
                    },
                    {
                        "score": 0.4534936963723285,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.4462006940276704,
                        "answer": "bailing",
                        "hit": false
                    },
                    {
                        "score": 0.44093926481665313,
                        "answer": "potassium",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sheep"
                ],
                "rank": 7616,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8027791082859039
            },
            {
                "question verbose": "What is to shrub ",
                "b": "shrub",
                "expected answer": [
                    "shrubbery"
                ],
                "predictions": [
                    {
                        "score": 0.5830743301449345,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28779018828496195,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.28595388355786855,
                        "answer": "founded",
                        "hit": false
                    },
                    {
                        "score": 0.28208290890978077,
                        "answer": "uninsured",
                        "hit": false
                    },
                    {
                        "score": 0.27647916599741534,
                        "answer": "approximately",
                        "hit": false
                    },
                    {
                        "score": 0.2721079404275136,
                        "answer": "attucks",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shrub"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to singer ",
                "b": "singer",
                "expected answer": [
                    "choir",
                    "band",
                    "duo",
                    "trio",
                    "quartet"
                ],
                "predictions": [
                    {
                        "score": 0.27484202073681846,
                        "answer": "horroraction",
                        "hit": false
                    },
                    {
                        "score": 0.266321733555984,
                        "answer": "cgi",
                        "hit": false
                    },
                    {
                        "score": 0.2604700319138018,
                        "answer": "pg",
                        "hit": false
                    },
                    {
                        "score": 0.2592124416668368,
                        "answer": "superhero",
                        "hit": false
                    },
                    {
                        "score": 0.2544754030982913,
                        "answer": "renovation",
                        "hit": false
                    },
                    {
                        "score": 0.2486557000098274,
                        "answer": "movie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "singer"
                ],
                "rank": 4524,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5766721144318581
            },
            {
                "question verbose": "What is to soldier ",
                "b": "soldier",
                "expected answer": [
                    "army",
                    "unit",
                    "division",
                    "troop"
                ],
                "predictions": [
                    {
                        "score": 0.3957444224884963,
                        "answer": "minor",
                        "hit": false
                    },
                    {
                        "score": 0.3867723411404188,
                        "answer": "excited",
                        "hit": false
                    },
                    {
                        "score": 0.38375141229070486,
                        "answer": "naughty",
                        "hit": false
                    },
                    {
                        "score": 0.3725347234843528,
                        "answer": "april",
                        "hit": false
                    },
                    {
                        "score": 0.37182560554106575,
                        "answer": "widget",
                        "hit": false
                    },
                    {
                        "score": 0.37076468540435775,
                        "answer": "eligible",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "soldier"
                ],
                "rank": 2744,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7059209495782852
            },
            {
                "question verbose": "What is to song ",
                "b": "song",
                "expected answer": [
                    "album",
                    "collection",
                    "antology",
                    "library"
                ],
                "predictions": [
                    {
                        "score": 0.4035749307008773,
                        "answer": "tricky",
                        "hit": false
                    },
                    {
                        "score": 0.40052734241038135,
                        "answer": "poop",
                        "hit": false
                    },
                    {
                        "score": 0.3946448868708627,
                        "answer": "magneto",
                        "hit": false
                    },
                    {
                        "score": 0.39113690675539425,
                        "answer": "insider",
                        "hit": false
                    },
                    {
                        "score": 0.38798959085842066,
                        "answer": "turgid",
                        "hit": false
                    },
                    {
                        "score": 0.3653686144383109,
                        "answer": "blvd",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "song"
                ],
                "rank": 2383,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6615151464939117
            },
            {
                "question verbose": "What is to spouse ",
                "b": "spouse",
                "expected answer": [
                    "couple",
                    "relationship",
                    "family"
                ],
                "predictions": [
                    {
                        "score": 0.454074702010037,
                        "answer": "ignores",
                        "hit": false
                    },
                    {
                        "score": 0.4338024310673249,
                        "answer": "perceptionopinion",
                        "hit": false
                    },
                    {
                        "score": 0.4327344878668802,
                        "answer": "homelessness",
                        "hit": false
                    },
                    {
                        "score": 0.43256479606558357,
                        "answer": "simultaneous",
                        "hit": false
                    },
                    {
                        "score": 0.43108428071128746,
                        "answer": "jon",
                        "hit": false
                    },
                    {
                        "score": 0.41217673798716137,
                        "answer": "dissemination",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spouse"
                ],
                "rank": 13819,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.605327732861042
            },
            {
                "question verbose": "What is to star ",
                "b": "star",
                "expected answer": [
                    "constellation",
                    "galaxy"
                ],
                "predictions": [
                    {
                        "score": 0.33047029580175424,
                        "answer": "war",
                        "hit": false
                    },
                    {
                        "score": 0.31709982444607715,
                        "answer": "spangled",
                        "hit": false
                    },
                    {
                        "score": 0.3004236884996321,
                        "answer": "heading",
                        "hit": false
                    },
                    {
                        "score": 0.29484568882545886,
                        "answer": "pacific",
                        "hit": false
                    },
                    {
                        "score": 0.28492301961949357,
                        "answer": "ken",
                        "hit": false
                    },
                    {
                        "score": 0.2830483967065915,
                        "answer": "sci",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "star"
                ],
                "rank": 7441,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.659657746553421
            },
            {
                "question verbose": "What is to state ",
                "b": "state",
                "expected answer": [
                    "country",
                    "province"
                ],
                "predictions": [
                    {
                        "score": 0.3256842724147567,
                        "answer": "primary",
                        "hit": false
                    },
                    {
                        "score": 0.3252591544009007,
                        "answer": "accompanied",
                        "hit": false
                    },
                    {
                        "score": 0.32126513889729097,
                        "answer": "traditionally",
                        "hit": false
                    },
                    {
                        "score": 0.3161907922838644,
                        "answer": "competition",
                        "hit": false
                    },
                    {
                        "score": 0.31167921826935713,
                        "answer": "fewer",
                        "hit": false
                    },
                    {
                        "score": 0.30880429030155204,
                        "answer": "agency",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "state"
                ],
                "rank": 94,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6578641831874847
            },
            {
                "question verbose": "What is to student ",
                "b": "student",
                "expected answer": [
                    "class",
                    "school"
                ],
                "predictions": [
                    {
                        "score": 0.4312124026192507,
                        "answer": "startup",
                        "hit": false
                    },
                    {
                        "score": 0.41395509161730415,
                        "answer": "alumnus",
                        "hit": false
                    },
                    {
                        "score": 0.4134669934543863,
                        "answer": "colleague",
                        "hit": false
                    },
                    {
                        "score": 0.4127225668113766,
                        "answer": "miller",
                        "hit": false
                    },
                    {
                        "score": 0.4061103999945168,
                        "answer": "teach",
                        "hit": false
                    },
                    {
                        "score": 0.40493191572733406,
                        "answer": "organized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "student"
                ],
                "rank": 643,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6549723893404007
            },
            {
                "question verbose": "What is to tree ",
                "b": "tree",
                "expected answer": [
                    "forest",
                    "wood",
                    "grove"
                ],
                "predictions": [
                    {
                        "score": 0.34226052582263533,
                        "answer": "pivic",
                        "hit": false
                    },
                    {
                        "score": 0.3398129503929034,
                        "answer": "pacific",
                        "hit": false
                    },
                    {
                        "score": 0.33710017672149606,
                        "answer": "killed",
                        "hit": false
                    },
                    {
                        "score": 0.3118148933662033,
                        "answer": "impractical",
                        "hit": false
                    },
                    {
                        "score": 0.30493251608278144,
                        "answer": "filled",
                        "hit": false
                    },
                    {
                        "score": 0.30470017636519575,
                        "answer": "zenica",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tree"
                ],
                "rank": 1425,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7134205251932144
            },
            {
                "question verbose": "What is to wolf ",
                "b": "wolf",
                "expected answer": [
                    "pack"
                ],
                "predictions": [
                    {
                        "score": 0.4963987759082341,
                        "answer": "turgid",
                        "hit": false
                    },
                    {
                        "score": 0.49135705518571876,
                        "answer": "troubling",
                        "hit": false
                    },
                    {
                        "score": 0.47495330349630116,
                        "answer": "assembled",
                        "hit": false
                    },
                    {
                        "score": 0.4633397674381676,
                        "answer": "freedman",
                        "hit": false
                    },
                    {
                        "score": 0.46318966137225437,
                        "answer": "upheaving",
                        "hit": false
                    },
                    {
                        "score": 0.46235884796328436,
                        "answer": "ames",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wolf"
                ],
                "rank": 6155,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7382641136646271
            },
            {
                "question verbose": "What is to word ",
                "b": "word",
                "expected answer": [
                    "paragraph",
                    "sentence",
                    "text"
                ],
                "predictions": [
                    {
                        "score": 0.28827512641198394,
                        "answer": "spread",
                        "hit": false
                    },
                    {
                        "score": 0.2852891621518516,
                        "answer": "loathing",
                        "hit": false
                    },
                    {
                        "score": 0.28129620288495977,
                        "answer": "darren",
                        "hit": false
                    },
                    {
                        "score": 0.27906461312802316,
                        "answer": "husband",
                        "hit": false
                    },
                    {
                        "score": 0.2737642958008662,
                        "answer": "topic",
                        "hit": false
                    },
                    {
                        "score": 0.2736380627296972,
                        "answer": "polled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "word"
                ],
                "rank": 1577,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6739895939826965
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L05 [meronyms - member].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "8fff4697-82ae-4a16-85f7-f55fac524f32",
            "timestamp": "2020-10-22T15:57:50.808538"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to academia ",
                "b": "academia",
                "expected answer": [
                    "college",
                    "university",
                    "institute"
                ],
                "predictions": [
                    {
                        "score": 0.45691241302177327,
                        "answer": "fellatio",
                        "hit": false
                    },
                    {
                        "score": 0.44925917207868676,
                        "answer": "utopian",
                        "hit": false
                    },
                    {
                        "score": 0.43729869977620295,
                        "answer": "wallet",
                        "hit": false
                    },
                    {
                        "score": 0.43209590001526355,
                        "answer": "calculate",
                        "hit": false
                    },
                    {
                        "score": 0.4279850787064354,
                        "answer": "brighter",
                        "hit": false
                    },
                    {
                        "score": 0.4255872867784016,
                        "answer": "comprehensive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "academia"
                ],
                "rank": 6250,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6524036675691605
            },
            {
                "question verbose": "What is to apartment ",
                "b": "apartment",
                "expected answer": [
                    "bedroom",
                    "room",
                    "bathroom",
                    "kitchen",
                    "kitchenette",
                    "living_room",
                    "pantry",
                    "toilet",
                    "shower_room"
                ],
                "predictions": [
                    {
                        "score": 0.4975227331161774,
                        "answer": "celtic",
                        "hit": false
                    },
                    {
                        "score": 0.4873648886517103,
                        "answer": "medictions",
                        "hit": false
                    },
                    {
                        "score": 0.48724451597178325,
                        "answer": "expanded",
                        "hit": false
                    },
                    {
                        "score": 0.48667723633217985,
                        "answer": "coincide",
                        "hit": false
                    },
                    {
                        "score": 0.48442088566685093,
                        "answer": "incidence",
                        "hit": false
                    },
                    {
                        "score": 0.48304301831185464,
                        "answer": "wrangle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "apartment"
                ],
                "rank": 834,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7288887947797775
            },
            {
                "question verbose": "What is to bird ",
                "b": "bird",
                "expected answer": [
                    "feathers",
                    "plume",
                    "preen",
                    "croup",
                    "calamus",
                    "fowl",
                    "drumstick",
                    "syrinx",
                    "bill",
                    "rump",
                    "vane",
                    "barb",
                    "quill",
                    "shaft",
                    "pecker",
                    "pennon",
                    "feet",
                    "giblets",
                    "alula",
                    "talon",
                    "foot",
                    "pinion",
                    "wing",
                    "wishbone",
                    "nib",
                    "sac",
                    "giblet",
                    "furcula",
                    "bird",
                    "neb",
                    "feather",
                    "gland",
                    "oyster",
                    "thigh",
                    "plumage",
                    "web",
                    "hindquarters",
                    "beak",
                    "parson's_nose",
                    "wishing_bone",
                    "pope's_nose",
                    "spurious_wing",
                    "bastard_wing",
                    "croupe",
                    "air_sac",
                    "quill_feather",
                    "uropygium",
                    "bird's_foot",
                    "uropygial",
                    "second_joint",
                    "uropygial_gland",
                    "preen_gland",
                    "flight_feather",
                    "dark_meat"
                ],
                "predictions": [
                    {
                        "score": 0.382687347058758,
                        "answer": "rain",
                        "hit": false
                    },
                    {
                        "score": 0.36766436991998613,
                        "answer": "cruise",
                        "hit": false
                    },
                    {
                        "score": 0.36683087473681775,
                        "answer": "vibrant",
                        "hit": false
                    },
                    {
                        "score": 0.36628017536229346,
                        "answer": "curvature",
                        "hit": false
                    },
                    {
                        "score": 0.3658969543409878,
                        "answer": "diy",
                        "hit": false
                    },
                    {
                        "score": 0.36446386397118885,
                        "answer": "clearing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bird"
                ],
                "rank": 1555,
                "landing_b": true,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6143772751092911
            },
            {
                "question verbose": "What is to brush ",
                "b": "brush",
                "expected answer": [
                    "bristle",
                    "hold",
                    "shank",
                    "grip",
                    "handgrip",
                    "handle",
                    "head",
                    "stem"
                ],
                "predictions": [
                    {
                        "score": 0.4972311310143284,
                        "answer": "mish",
                        "hit": false
                    },
                    {
                        "score": 0.48453793712478804,
                        "answer": "museumlike",
                        "hit": false
                    },
                    {
                        "score": 0.4837679458532087,
                        "answer": "graft",
                        "hit": false
                    },
                    {
                        "score": 0.4811863446114974,
                        "answer": "kilogram",
                        "hit": false
                    },
                    {
                        "score": 0.47072914756881545,
                        "answer": "carted",
                        "hit": false
                    },
                    {
                        "score": 0.47058386241656613,
                        "answer": "calculate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "brush"
                ],
                "rank": 3604,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5990359038114548
            },
            {
                "question verbose": "What is to bus ",
                "b": "bus",
                "expected answer": [
                    "seats",
                    "conductor",
                    "window",
                    "driver",
                    "roof"
                ],
                "predictions": [
                    {
                        "score": 0.38952670295624536,
                        "answer": "green",
                        "hit": false
                    },
                    {
                        "score": 0.36357701479793536,
                        "answer": "friday",
                        "hit": false
                    },
                    {
                        "score": 0.3626887770811314,
                        "answer": "crater",
                        "hit": false
                    },
                    {
                        "score": 0.3610576647137347,
                        "answer": "hinge",
                        "hit": false
                    },
                    {
                        "score": 0.35394933102490717,
                        "answer": "dome",
                        "hit": false
                    },
                    {
                        "score": 0.3527026290055299,
                        "answer": "commute",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bus"
                ],
                "rank": 72,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.516586946323514
            },
            {
                "question verbose": "What is to byte ",
                "b": "byte",
                "expected answer": [
                    "bit"
                ],
                "predictions": [
                    {
                        "score": 0.5832284604041142,
                        "answer": "museumlike",
                        "hit": false
                    },
                    {
                        "score": 0.570466154390966,
                        "answer": "hinge",
                        "hit": false
                    },
                    {
                        "score": 0.5687074705836502,
                        "answer": "kickstand",
                        "hit": false
                    },
                    {
                        "score": 0.56543497778829,
                        "answer": "optimize",
                        "hit": false
                    },
                    {
                        "score": 0.5620618164809166,
                        "answer": "upriver",
                        "hit": false
                    },
                    {
                        "score": 0.5561054216878107,
                        "answer": "coded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "byte"
                ],
                "rank": 14537,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7163299471139908
            },
            {
                "question verbose": "What is to car ",
                "b": "car",
                "expected answer": [
                    "engine",
                    "armrest",
                    "headrest",
                    "grille",
                    "hinge",
                    "tailpipe",
                    "suspension",
                    "hood",
                    "horn",
                    "hooter",
                    "petrol",
                    "pintle",
                    "trunk",
                    "floorboard",
                    "bar",
                    "gas",
                    "buffer",
                    "door",
                    "stabilizer",
                    "seat",
                    "window",
                    "exhaust",
                    "tailfin",
                    "back",
                    "luggage",
                    "pedal",
                    "cowling",
                    "seatbelt",
                    "high",
                    "wing",
                    "gasoline",
                    "fin",
                    "cowl",
                    "silencer",
                    "gun",
                    "compartment",
                    "rear",
                    "radiator",
                    "accessory",
                    "sunroof",
                    "roof",
                    "lights",
                    "bumper",
                    "glove",
                    "bag",
                    "cushion",
                    "backrest",
                    "accelerator",
                    "fender",
                    "third",
                    "throttle",
                    "bonnet",
                    "air",
                    "gear",
                    "muffler",
                    "mirror",
                    "shock",
                    "reverse",
                    "anti-sway_bar",
                    "tail_fin",
                    "accelerator_pedal",
                    "car_seat",
                    "automobile_horn",
                    "anti-sway",
                    "automobile_engine",
                    "gasoline_engine",
                    "car_door",
                    "shock_absorber",
                    "luggage_compartment",
                    "glove_compartment",
                    "bumper_guard",
                    "exhaust_system",
                    "first_gear",
                    "high_gear",
                    "rear_window",
                    "sunshine-roof",
                    "third_gear",
                    "air_bag",
                    "doorlock",
                    "petrol_engine",
                    "automobile_trunk",
                    "auto_accessory",
                    "inlet_manifold",
                    "horn_button",
                    "running_board",
                    "stabilizer_bar",
                    "motor_horn",
                    "exhaust_manifold",
                    "exhaust_pipe",
                    "radiator_grille",
                    "low_gear",
                    "flexible_joint",
                    "head_restraint",
                    "hood_ornament",
                    "exhaust_valve",
                    "car_mirror",
                    "gas_pedal",
                    "seat_belt",
                    "reverse_gear",
                    "car_horn",
                    "car_window",
                    "suspension_system"
                ],
                "predictions": [
                    {
                        "score": 0.32916257325738807,
                        "answer": "humidity",
                        "hit": false
                    },
                    {
                        "score": 0.32276955043646005,
                        "answer": "wax",
                        "hit": false
                    },
                    {
                        "score": 0.31005773014163274,
                        "answer": "tire",
                        "hit": false
                    },
                    {
                        "score": 0.3014356829420838,
                        "answer": "cooking",
                        "hit": false
                    },
                    {
                        "score": 0.29232178962552235,
                        "answer": "removed",
                        "hit": false
                    },
                    {
                        "score": 0.29071280692201856,
                        "answer": "updated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "car"
                ],
                "rank": 336,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6742325723171234
            },
            {
                "question verbose": "What is to castle ",
                "b": "castle",
                "expected answer": [
                    "donjon",
                    "tower",
                    "turret",
                    "gate",
                    "dungeon",
                    "moat",
                    "keep",
                    "great_hall"
                ],
                "predictions": [
                    {
                        "score": 0.5009594822099916,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31222263582996773,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2824319032607595,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.2740390599547237,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.2717182074337655,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.26115852922262595,
                        "answer": "semi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "castle"
                ],
                "rank": 5613,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to cat ",
                "b": "cat",
                "expected answer": [
                    "whiskers",
                    "coat",
                    "paw",
                    "paws",
                    "fur",
                    "eyes",
                    "back",
                    "claws",
                    "tail",
                    "teeth"
                ],
                "predictions": [
                    {
                        "score": 0.4143778173675604,
                        "answer": "calculate",
                        "hit": false
                    },
                    {
                        "score": 0.39037563857259244,
                        "answer": "facilitates",
                        "hit": false
                    },
                    {
                        "score": 0.36288615431165144,
                        "answer": "five",
                        "hit": false
                    },
                    {
                        "score": 0.36269106518845434,
                        "answer": "prepare",
                        "hit": false
                    },
                    {
                        "score": 0.36078571778610335,
                        "answer": "snippet",
                        "hit": false
                    },
                    {
                        "score": 0.3594271717686666,
                        "answer": "sequence",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cat"
                ],
                "rank": 13,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5868597850203514
            },
            {
                "question verbose": "What is to chair ",
                "b": "chair",
                "expected answer": [
                    "seat",
                    "armrest",
                    "headrest",
                    "armrests",
                    "rest",
                    "pad",
                    "back",
                    "leg",
                    "backrest",
                    "legs"
                ],
                "predictions": [
                    {
                        "score": 0.3929671949102827,
                        "answer": "twentysomething",
                        "hit": false
                    },
                    {
                        "score": 0.38775563865094037,
                        "answer": "swishy",
                        "hit": false
                    },
                    {
                        "score": 0.3785991927122847,
                        "answer": "monitoring",
                        "hit": false
                    },
                    {
                        "score": 0.3776644333335981,
                        "answer": "chanel",
                        "hit": false
                    },
                    {
                        "score": 0.3750261387619478,
                        "answer": "multiple",
                        "hit": false
                    },
                    {
                        "score": 0.37462647071211164,
                        "answer": "msnbc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chair"
                ],
                "rank": 6086,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6475209593772888
            },
            {
                "question verbose": "What is to church ",
                "b": "church",
                "expected answer": [
                    "altar",
                    "choir",
                    "vestry",
                    "transept",
                    "apse",
                    "sanctuary",
                    "rood",
                    "screen",
                    "bema",
                    "nave",
                    "tower",
                    "sacristy",
                    "corner",
                    "chancel",
                    "amen",
                    "chapel",
                    "apsis",
                    "side",
                    "narthex",
                    "presbytery",
                    "side_chapel",
                    "lady_chapel",
                    "church_tower",
                    "rood_screen",
                    "amen_corner"
                ],
                "predictions": [
                    {
                        "score": 0.23493700187342004,
                        "answer": "hating",
                        "hit": false
                    },
                    {
                        "score": 0.2326909284529597,
                        "answer": "antithesis",
                        "hit": false
                    },
                    {
                        "score": 0.2316098565159372,
                        "answer": "romneycare",
                        "hit": false
                    },
                    {
                        "score": 0.22679129770555434,
                        "answer": "carnal",
                        "hit": false
                    },
                    {
                        "score": 0.22442658600788581,
                        "answer": "mmos",
                        "hit": false
                    },
                    {
                        "score": 0.22116784689757488,
                        "answer": "clibash",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "church"
                ],
                "rank": 1456,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6799231618642807
            },
            {
                "question verbose": "What is to comb ",
                "b": "comb",
                "expected answer": [
                    "teeth",
                    "shaft",
                    "grip",
                    "tooth",
                    "handle"
                ],
                "predictions": [
                    {
                        "score": 0.5167966797767488,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30984931615140676,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28294008243817137,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2725719559524317,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.26221659076145226,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.25712332022479345,
                        "answer": "bigger",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "comb"
                ],
                "rank": 4920,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6136773154139519
            },
            {
                "question verbose": "What is to day ",
                "b": "day",
                "expected answer": [
                    "hour",
                    "forenoon",
                    "eventide",
                    "nanosecond",
                    "noonday",
                    "femtosecond",
                    "hr",
                    "minutes",
                    "microsecond",
                    "msec",
                    "time",
                    "midnight",
                    "sec",
                    "nighttime",
                    "min",
                    "morning",
                    "noon",
                    "evening",
                    "dark",
                    "picosecond",
                    "daytime",
                    "minute",
                    "daylight",
                    "s",
                    "high",
                    "even",
                    "quarter",
                    "morn",
                    "noontide",
                    "second",
                    "afternoon",
                    "twelve",
                    "millisecond",
                    "midday",
                    "midafternoon",
                    "attosecond",
                    "night",
                    "morning_time",
                    "high_noon",
                    "half-hour",
                    "twelve_noon",
                    "late-night_hour",
                    "early-morning_hour",
                    "lights-out",
                    "60_minutes",
                    "small_hours",
                    "30_minutes",
                    "15_minutes",
                    "quarter-hour"
                ],
                "predictions": [
                    {
                        "score": 0.2816487231643932,
                        "answer": "tail",
                        "hit": false
                    },
                    {
                        "score": 0.27491461432194153,
                        "answer": "instruction",
                        "hit": false
                    },
                    {
                        "score": 0.2740978587223586,
                        "answer": "feed",
                        "hit": false
                    },
                    {
                        "score": 0.2675141125771448,
                        "answer": "warren",
                        "hit": false
                    },
                    {
                        "score": 0.26586061221270735,
                        "answer": "refund",
                        "hit": false
                    },
                    {
                        "score": 0.26233165767371447,
                        "answer": "howe",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "day"
                ],
                "rank": 124,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6282339096069336
            },
            {
                "question verbose": "What is to deer ",
                "b": "deer",
                "expected answer": [
                    "antler",
                    "antlers",
                    "withers",
                    "flag",
                    "scut"
                ],
                "predictions": [
                    {
                        "score": 0.5023262822311685,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.315303157950636,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.27663020977891994,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.2727680780340136,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2684029379515209,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.26685081698940966,
                        "answer": "semi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "deer"
                ],
                "rank": 9450,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dollar ",
                "b": "dollar",
                "expected answer": [
                    "cent"
                ],
                "predictions": [
                    {
                        "score": 0.35875967165409833,
                        "answer": "deduction",
                        "hit": false
                    },
                    {
                        "score": 0.3447653457744926,
                        "answer": "resign",
                        "hit": false
                    },
                    {
                        "score": 0.34237257821021777,
                        "answer": "unit",
                        "hit": false
                    },
                    {
                        "score": 0.329479767672783,
                        "answer": "enrollment",
                        "hit": false
                    },
                    {
                        "score": 0.3249359454359212,
                        "answer": "revision",
                        "hit": false
                    },
                    {
                        "score": 0.32413607994199967,
                        "answer": "square",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dollar"
                ],
                "rank": 89,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7610735595226288
            },
            {
                "question verbose": "What is to door ",
                "b": "door",
                "expected answer": [
                    "hinge",
                    "keyhole",
                    "threshold",
                    "doorframe",
                    "bolt",
                    "deadbolt",
                    "doorjamb",
                    "tumbler",
                    "doorpost",
                    "doorstep",
                    "doorcase",
                    "case",
                    "lock",
                    "casing",
                    "doorsill"
                ],
                "predictions": [
                    {
                        "score": 0.40238472722815666,
                        "answer": "goose",
                        "hit": false
                    },
                    {
                        "score": 0.39745173371801645,
                        "answer": "curvature",
                        "hit": false
                    },
                    {
                        "score": 0.39343755448946477,
                        "answer": "resurrection",
                        "hit": false
                    },
                    {
                        "score": 0.3888478434134981,
                        "answer": "wally",
                        "hit": false
                    },
                    {
                        "score": 0.38447683974134744,
                        "answer": "kilogram",
                        "hit": false
                    },
                    {
                        "score": 0.3819112751338485,
                        "answer": "fastest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "door"
                ],
                "rank": 72,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7237509787082672
            },
            {
                "question verbose": "What is to dress ",
                "b": "dress",
                "expected answer": [
                    "sleeve",
                    "slide",
                    "plastron",
                    "zip",
                    "zipper",
                    "fastener",
                    "hemline",
                    "bodice",
                    "neckline",
                    "waistline",
                    "belt",
                    "slide_fastener",
                    "zip_fastener"
                ],
                "predictions": [
                    {
                        "score": 0.37351069955292404,
                        "answer": "constantly",
                        "hit": false
                    },
                    {
                        "score": 0.36980309448249177,
                        "answer": "breeder",
                        "hit": false
                    },
                    {
                        "score": 0.36464522508144215,
                        "answer": "segregation",
                        "hit": false
                    },
                    {
                        "score": 0.3617695610224597,
                        "answer": "spousal",
                        "hit": false
                    },
                    {
                        "score": 0.3588018618599758,
                        "answer": "persona",
                        "hit": false
                    },
                    {
                        "score": 0.3581899864568124,
                        "answer": "unpacking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dress"
                ],
                "rank": 7232,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6954869627952576
            },
            {
                "question verbose": "What is to filename ",
                "b": "filename",
                "expected answer": [
                    "extension",
                    "name"
                ],
                "predictions": [
                    {
                        "score": 0.5206275183045257,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3133087361178469,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2701574997230998,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.2694248578443919,
                        "answer": "cost",
                        "hit": false
                    },
                    {
                        "score": 0.2683137851937889,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2653136326135175,
                        "answer": "additional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "filename"
                ],
                "rank": 7042,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6043104231357574
            },
            {
                "question verbose": "What is to flower ",
                "b": "flower",
                "expected answer": [
                    "petal",
                    "leaf",
                    "anther",
                    "style",
                    "perianth",
                    "placenta",
                    "stamen",
                    "pistil",
                    "ovary",
                    "corolla",
                    "carpel",
                    "envelope",
                    "sepal",
                    "calyx",
                    "stigma",
                    "chlamys",
                    "petals",
                    "corona",
                    "perigonium",
                    "perigone",
                    "flower_petal",
                    "floral_leaf",
                    "floral_envelope",
                    "gynostegium"
                ],
                "predictions": [
                    {
                        "score": 0.48130589078197916,
                        "answer": "coincide",
                        "hit": false
                    },
                    {
                        "score": 0.47183373544763574,
                        "answer": "engenders",
                        "hit": false
                    },
                    {
                        "score": 0.46436810158107816,
                        "answer": "expanded",
                        "hit": false
                    },
                    {
                        "score": 0.4632477938285426,
                        "answer": "incorrect",
                        "hit": false
                    },
                    {
                        "score": 0.45915774990836145,
                        "answer": "antioxidant",
                        "hit": false
                    },
                    {
                        "score": 0.4530781491232228,
                        "answer": "bundesbank",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "flower"
                ],
                "rank": 1887,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8087905645370483
            },
            {
                "question verbose": "What is to gigabit ",
                "b": "gigabit",
                "expected answer": [
                    "megabit",
                    "kbit",
                    "kb",
                    "kilobit",
                    "mb",
                    "mbit"
                ],
                "predictions": [
                    {
                        "score": 0.5028839854770675,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31124506269021857,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.27298861179660094,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2702388458563366,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.26281641063710504,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.2625345917972378,
                        "answer": "additional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gigabit"
                ],
                "rank": 15098,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to gramm ",
                "b": "gramm",
                "expected answer": [
                    "milligram",
                    "mg",
                    "grain",
                    "milligram",
                    "mcg",
                    "carat",
                    "microgram",
                    "ng",
                    "dg",
                    "decigram",
                    "nanogram",
                    "metric_grain",
                    "obolus"
                ],
                "predictions": [
                    {
                        "score": 0.536510877697048,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.5166297411150905,
                        "answer": "ultrathin",
                        "hit": false
                    },
                    {
                        "score": 0.4909274593936599,
                        "answer": "harmony",
                        "hit": false
                    },
                    {
                        "score": 0.48710118490648,
                        "answer": "handheld",
                        "hit": false
                    },
                    {
                        "score": 0.48143259281016504,
                        "answer": "incidence",
                        "hit": false
                    },
                    {
                        "score": 0.47638097087931136,
                        "answer": "gradually",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gramm"
                ],
                "rank": 214,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8000160753726959
            },
            {
                "question verbose": "What is to guitar ",
                "b": "guitar",
                "expected answer": [
                    "string",
                    "deck",
                    "pegs",
                    "fret",
                    "strings",
                    "frets",
                    "peg",
                    "fingerboard"
                ],
                "predictions": [
                    {
                        "score": 0.3947732984065279,
                        "answer": "expanded",
                        "hit": false
                    },
                    {
                        "score": 0.38034644235542836,
                        "answer": "shout",
                        "hit": false
                    },
                    {
                        "score": 0.37478046738078213,
                        "answer": "unload",
                        "hit": false
                    },
                    {
                        "score": 0.37168539773916565,
                        "answer": "obgyn",
                        "hit": false
                    },
                    {
                        "score": 0.36317860353750625,
                        "answer": "geographical",
                        "hit": false
                    },
                    {
                        "score": 0.36060045626467063,
                        "answer": "fender",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guitar"
                ],
                "rank": 5897,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6398668140172958
            },
            {
                "question verbose": "What is to gun ",
                "b": "gun",
                "expected answer": [
                    "trigger",
                    "holder",
                    "safety",
                    "extractor",
                    "clip",
                    "magazine",
                    "barrel",
                    "hammer",
                    "viewfinder",
                    "gunsight",
                    "stock",
                    "cartridge",
                    "remover",
                    "chamber",
                    "ejector",
                    "cock",
                    "catch",
                    "finder",
                    "mechanism",
                    "gunstock",
                    "lock",
                    "action",
                    "muzzle",
                    "key",
                    "gunlock",
                    "cartridge_holder",
                    "gun_muzzle",
                    "gun-sight",
                    "action_mechanism",
                    "gun_chamber",
                    "gun_trigger",
                    "firing_mechanism",
                    "safety_lock",
                    "cartridge_extractor",
                    "firing_chamber",
                    "cartridge_ejector",
                    "view_finder",
                    "cartridge_clip",
                    "gun_barrel",
                    "safety_catch",
                    "cartridge_remover"
                ],
                "predictions": [
                    {
                        "score": 0.4399059600458426,
                        "answer": "column",
                        "hit": false
                    },
                    {
                        "score": 0.4069472259524103,
                        "answer": "rushed",
                        "hit": false
                    },
                    {
                        "score": 0.39570278478223225,
                        "answer": "injection",
                        "hit": false
                    },
                    {
                        "score": 0.3909811333075562,
                        "answer": "melatonin",
                        "hit": false
                    },
                    {
                        "score": 0.38568724269698657,
                        "answer": "suit",
                        "hit": false
                    },
                    {
                        "score": 0.37604016825025227,
                        "answer": "metal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gun"
                ],
                "rank": 209,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7587947845458984
            },
            {
                "question verbose": "What is to harbor ",
                "b": "harbor",
                "expected answer": [
                    "dock",
                    "facility",
                    "anchorage",
                    "landing",
                    "docking",
                    "dockage",
                    "anchorage_ground",
                    "docking_facility",
                    "landing_place",
                    "landing_stage"
                ],
                "predictions": [
                    {
                        "score": 0.4686581028058935,
                        "answer": "calculate",
                        "hit": false
                    },
                    {
                        "score": 0.46472370629220033,
                        "answer": "chestertown",
                        "hit": false
                    },
                    {
                        "score": 0.4643546287477747,
                        "answer": "antioxidant",
                        "hit": false
                    },
                    {
                        "score": 0.46091792855300684,
                        "answer": "actionscenes",
                        "hit": false
                    },
                    {
                        "score": 0.4546863496779956,
                        "answer": "curmudgeon",
                        "hit": false
                    },
                    {
                        "score": 0.4531554338978984,
                        "answer": "therewith",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "harbor"
                ],
                "rank": 5849,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7466290295124054
            },
            {
                "question verbose": "What is to jail ",
                "b": "jail",
                "expected answer": [
                    "cell",
                    "cellblock",
                    "guard",
                    "police",
                    "prison_cell",
                    "jail_cell"
                ],
                "predictions": [
                    {
                        "score": 0.484124129152491,
                        "answer": "generator",
                        "hit": false
                    },
                    {
                        "score": 0.46445322607174494,
                        "answer": "squeezing",
                        "hit": false
                    },
                    {
                        "score": 0.4561538887099495,
                        "answer": "upriver",
                        "hit": false
                    },
                    {
                        "score": 0.44514254550908233,
                        "answer": "armchair",
                        "hit": false
                    },
                    {
                        "score": 0.4442284416715004,
                        "answer": "code",
                        "hit": false
                    },
                    {
                        "score": 0.4391247854207961,
                        "answer": "porch",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jail"
                ],
                "rank": 491,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.760349839925766
            },
            {
                "question verbose": "What is to jewellery ",
                "b": "jewellery",
                "expected answer": [
                    "bead",
                    "stone",
                    "wire",
                    "beads",
                    "gemstone",
                    "gem",
                    "metal",
                    "gold",
                    "silver",
                    "platina",
                    "bead",
                    "beads",
                    "jewel"
                ],
                "predictions": [
                    {
                        "score": 0.5006977689249593,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3066493352193639,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.279974897642573,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2645791840617852,
                        "answer": "additional",
                        "hit": false
                    },
                    {
                        "score": 0.25953023475023446,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.25895719173333626,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jewellery"
                ],
                "rank": 2021,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to litre ",
                "b": "litre",
                "expected answer": [
                    "millilitre",
                    "cc",
                    "ml",
                    "milliliter",
                    "cl",
                    "dl",
                    "deciliter",
                    "mil",
                    "cubic_centimetre",
                    "centiliter",
                    "decilitre",
                    "cubic_centimeter",
                    "centilitre",
                    "cubic_millimeter",
                    "cubic_millimetre"
                ],
                "predictions": [
                    {
                        "score": 0.49767642563993225,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3105271090527204,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2803346834678907,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.271234168367123,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.27081476042137304,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.2689065604546436,
                        "answer": "camp",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "litre"
                ],
                "rank": 179,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to movie ",
                "b": "movie",
                "expected answer": [
                    "scene",
                    "subtitles",
                    "footage",
                    "credit",
                    "episode",
                    "shot",
                    "sequence",
                    "credits",
                    "caption"
                ],
                "predictions": [
                    {
                        "score": 0.3010253576047686,
                        "answer": "tdkr",
                        "hit": false
                    },
                    {
                        "score": 0.2845039283721462,
                        "answer": "hd",
                        "hit": false
                    },
                    {
                        "score": 0.27868329075687087,
                        "answer": "digestible",
                        "hit": false
                    },
                    {
                        "score": 0.2763527785101982,
                        "answer": "behtoven",
                        "hit": false
                    },
                    {
                        "score": 0.27267130963073255,
                        "answer": "expanding",
                        "hit": false
                    },
                    {
                        "score": 0.2722960647639292,
                        "answer": "brett",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "movie"
                ],
                "rank": 14,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6782160997390747
            },
            {
                "question verbose": "What is to orthography ",
                "b": "orthography",
                "expected answer": [
                    "hyphenation",
                    "punctuation",
                    "punctuation_mark",
                    "word_division",
                    "spelling"
                ],
                "predictions": [
                    {
                        "score": 0.49873819158574734,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29783942620786347,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2736230791056672,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.26589874766004185,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2630522600383422,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.2570241310636292,
                        "answer": "rush",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "orthography"
                ],
                "rank": 2420,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to piano ",
                "b": "piano",
                "expected answer": [
                    "keyboard",
                    "clavier",
                    "loud",
                    "hammer",
                    "sounding",
                    "pedal",
                    "sustaining",
                    "soft",
                    "fingerboard",
                    "soundboard",
                    "action",
                    "board",
                    "key",
                    "fallboard",
                    "piano_action",
                    "action_mechanism",
                    "damper_block",
                    "fall-board",
                    "sounding_board",
                    "sustaining_pedal",
                    "loud_pedal",
                    "piano_damper",
                    "piano_keyboard",
                    "sound_hole",
                    "soft_pedal"
                ],
                "predictions": [
                    {
                        "score": 0.4675213854513956,
                        "answer": "vaughn",
                        "hit": false
                    },
                    {
                        "score": 0.4630860200100117,
                        "answer": "hinge",
                        "hit": false
                    },
                    {
                        "score": 0.4627002754709066,
                        "answer": "longitudinal",
                        "hit": false
                    },
                    {
                        "score": 0.46119437089660853,
                        "answer": "lois",
                        "hit": false
                    },
                    {
                        "score": 0.45917442066714514,
                        "answer": "mmos",
                        "hit": false
                    },
                    {
                        "score": 0.4581087842850716,
                        "answer": "waxing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "piano"
                ],
                "rank": 435,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6932790875434875
            },
            {
                "question verbose": "What is to pie ",
                "b": "pie",
                "expected answer": [
                    "crust",
                    "cheese",
                    "filling",
                    "fruit",
                    "apple",
                    "berries",
                    "glazing",
                    "pastry",
                    "icing"
                ],
                "predictions": [
                    {
                        "score": 0.5452486344209575,
                        "answer": "cent",
                        "hit": false
                    },
                    {
                        "score": 0.5167907204218929,
                        "answer": "passive",
                        "hit": false
                    },
                    {
                        "score": 0.4991398398654729,
                        "answer": "visibility",
                        "hit": false
                    },
                    {
                        "score": 0.48300418750978663,
                        "answer": "hinge",
                        "hit": false
                    },
                    {
                        "score": 0.4714622191240485,
                        "answer": "administrative",
                        "hit": false
                    },
                    {
                        "score": 0.45042680617057596,
                        "answer": "upriver",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pie"
                ],
                "rank": 2528,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7361550629138947
            },
            {
                "question verbose": "What is to poem ",
                "b": "poem",
                "expected answer": [
                    "stanza",
                    "canto",
                    "prosody",
                    "rime",
                    "line",
                    "rhyme",
                    "verse",
                    "poetic_rhythm",
                    "rhythmic_pattern",
                    "verse_line",
                    "line_of_verse",
                    "line_of_poetry"
                ],
                "predictions": [
                    {
                        "score": 0.4808839359331762,
                        "answer": "torrent",
                        "hit": false
                    },
                    {
                        "score": 0.46570962102078883,
                        "answer": "dome",
                        "hit": false
                    },
                    {
                        "score": 0.4630122415313621,
                        "answer": "upriver",
                        "hit": false
                    },
                    {
                        "score": 0.45033511054708003,
                        "answer": "container",
                        "hit": false
                    },
                    {
                        "score": 0.447197897525195,
                        "answer": "code",
                        "hit": false
                    },
                    {
                        "score": 0.44298631934987565,
                        "answer": "persona",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "poem"
                ],
                "rank": 9315,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6062076836824417
            },
            {
                "question verbose": "What is to pub ",
                "b": "pub",
                "expected answer": [
                    "bar",
                    "barroom",
                    "taproom",
                    "saloon",
                    "ginmill"
                ],
                "predictions": [
                    {
                        "score": 0.5148603678861462,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3216126398053293,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.27540502816027934,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2720906833389658,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.2680452339642329,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.26146677241916344,
                        "answer": "careened",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pub"
                ],
                "rank": 10066,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6152636632323265
            },
            {
                "question verbose": "What is to radio ",
                "b": "radio",
                "expected answer": [
                    "receiver",
                    "tuner",
                    "radio",
                    "demodulator",
                    "transmitter",
                    "detector",
                    "receiving",
                    "set",
                    "amplifier",
                    "wireless",
                    "radio_transmitter",
                    "receiving_set",
                    "radio_receiver",
                    "radio_set"
                ],
                "predictions": [
                    {
                        "score": 0.33537902258256264,
                        "answer": "random",
                        "hit": false
                    },
                    {
                        "score": 0.33466402362219183,
                        "answer": "ideologue",
                        "hit": false
                    },
                    {
                        "score": 0.33230907907124857,
                        "answer": "tv",
                        "hit": false
                    },
                    {
                        "score": 0.3292581189074744,
                        "answer": "scribble",
                        "hit": false
                    },
                    {
                        "score": 0.3292172600841963,
                        "answer": "friday",
                        "hit": false
                    },
                    {
                        "score": 0.32461990338385505,
                        "answer": "preparing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "radio"
                ],
                "rank": 2382,
                "landing_b": true,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6529379040002823
            },
            {
                "question verbose": "What is to railcar ",
                "b": "railcar",
                "expected answer": [
                    "suspension",
                    "cushion",
                    "shock",
                    "shock_absorber",
                    "suspension_system"
                ],
                "predictions": [
                    {
                        "score": 0.5180780940918942,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3201234120966935,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.273955446869036,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.270891808071062,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.27028968400318587,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2628394692156903,
                        "answer": "semi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "railcar"
                ],
                "rank": 1740,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6358587294816971
            },
            {
                "question verbose": "What is to seafront ",
                "b": "seafront",
                "expected answer": [
                    "harbor",
                    "seaport",
                    "dock",
                    "haven",
                    "anchorage",
                    "landing",
                    "harbour",
                    "dockage",
                    "anchorage_ground",
                    "docking_facility",
                    "landing_place",
                    "landing_stage"
                ],
                "predictions": [
                    {
                        "score": 0.5155632669303525,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3146851216165498,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28320849364840006,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.27757097599805314,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2747006178953746,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.27320711489349586,
                        "answer": "additional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "seafront"
                ],
                "rank": 6345,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6535837650299072
            },
            {
                "question verbose": "What is to shilling ",
                "b": "shilling",
                "expected answer": [
                    "pence"
                ],
                "predictions": [
                    {
                        "score": 0.5027346013333286,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3116923029224396,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2917268030665258,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.28092590089560515,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.27583560477865743,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2727888986539573,
                        "answer": "additional",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shilling"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to shirt ",
                "b": "shirt",
                "expected answer": [
                    "button",
                    "dickie",
                    "collar",
                    "dickey",
                    "sleeve",
                    "buttons",
                    "dicky",
                    "pocket",
                    "shirttail",
                    "shirtsleeve",
                    "shirtfront",
                    "shirt_button"
                ],
                "predictions": [
                    {
                        "score": 0.4746826485134938,
                        "answer": "interior",
                        "hit": false
                    },
                    {
                        "score": 0.4724264961558284,
                        "answer": "cameo",
                        "hit": false
                    },
                    {
                        "score": 0.4446332680268979,
                        "answer": "tactical",
                        "hit": false
                    },
                    {
                        "score": 0.4417255590907487,
                        "answer": "grey",
                        "hit": false
                    },
                    {
                        "score": 0.43732669296547777,
                        "answer": "mode",
                        "hit": false
                    },
                    {
                        "score": 0.428625186486503,
                        "answer": "faded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shirt"
                ],
                "rank": 179,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6508532017469406
            },
            {
                "question verbose": "What is to sonata ",
                "b": "sonata",
                "expected answer": [
                    "movement",
                    "psrt"
                ],
                "predictions": [
                    {
                        "score": 0.5207840715171427,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30442183724480404,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2798734010662446,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.27939737552052807,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2714282861213295,
                        "answer": "additional",
                        "hit": false
                    },
                    {
                        "score": 0.2704110569395192,
                        "answer": "proposed",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sonata"
                ],
                "rank": 13686,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.571526937186718
            },
            {
                "question verbose": "What is to staircase ",
                "b": "staircase",
                "expected answer": [
                    "step",
                    "riser",
                    "landing",
                    "tread",
                    "stair",
                    "stairhead"
                ],
                "predictions": [
                    {
                        "score": 0.5158681046717403,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31432722293983417,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2771767526323064,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.2725977343710041,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.26756777732067605,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.26557677325149537,
                        "answer": "rush",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "staircase"
                ],
                "rank": 2086,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6208184435963631
            },
            {
                "question verbose": "What is to sword ",
                "b": "sword",
                "expected answer": [
                    "blade",
                    "forte",
                    "hilt",
                    "peak",
                    "foible",
                    "point",
                    "pommel",
                    "haft",
                    "tip",
                    "knob",
                    "helve",
                    "knife_edge",
                    "cutting_edge"
                ],
                "predictions": [
                    {
                        "score": 0.4917660877798345,
                        "answer": "expanded",
                        "hit": false
                    },
                    {
                        "score": 0.4818203229642898,
                        "answer": "calculate",
                        "hit": false
                    },
                    {
                        "score": 0.4762780307348466,
                        "answer": "melissa",
                        "hit": false
                    },
                    {
                        "score": 0.4629724808562604,
                        "answer": "happiest",
                        "hit": false
                    },
                    {
                        "score": 0.45817215122776667,
                        "answer": "gymnasium",
                        "hit": false
                    },
                    {
                        "score": 0.45194824055333,
                        "answer": "telegraph",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sword"
                ],
                "rank": 3176,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.624293752014637
            },
            {
                "question verbose": "What is to table ",
                "b": "table",
                "expected answer": [
                    "tabletop",
                    "tableware",
                    "leg",
                    "legs"
                ],
                "predictions": [
                    {
                        "score": 0.3404231112668723,
                        "answer": "build",
                        "hit": false
                    },
                    {
                        "score": 0.3393880179476964,
                        "answer": "leaving",
                        "hit": false
                    },
                    {
                        "score": 0.33752259517756034,
                        "answer": "timing",
                        "hit": false
                    },
                    {
                        "score": 0.33616000388314426,
                        "answer": "beautifully",
                        "hit": false
                    },
                    {
                        "score": 0.3339241509689105,
                        "answer": "mgkg",
                        "hit": false
                    },
                    {
                        "score": 0.33197297908000584,
                        "answer": "adapt",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "table"
                ],
                "rank": 2392,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7573549449443817
            },
            {
                "question verbose": "What is to teapot ",
                "b": "teapot",
                "expected answer": [
                    "spout",
                    "handle",
                    "knob",
                    "lid",
                    "body",
                    "belly",
                    "base",
                    "filter",
                    "tea_filter"
                ],
                "predictions": [
                    {
                        "score": 0.5157125412785323,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31477337370378705,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28135789195769156,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2734064047386271,
                        "answer": "additional",
                        "hit": false
                    },
                    {
                        "score": 0.27265954805987913,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.2660071093608447,
                        "answer": "send",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "teapot"
                ],
                "rank": 3122,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6494853049516678
            },
            {
                "question verbose": "What is to telephone ",
                "b": "telephone",
                "expected answer": [
                    "receiver",
                    "mouthpiece",
                    "dial",
                    "wire",
                    "screen",
                    "display",
                    "telephone_receiver"
                ],
                "predictions": [
                    {
                        "score": 0.5115467037195621,
                        "answer": "hippy",
                        "hit": false
                    },
                    {
                        "score": 0.4988253423153217,
                        "answer": "rushed",
                        "hit": false
                    },
                    {
                        "score": 0.49639393552768096,
                        "answer": "kilogram",
                        "hit": false
                    },
                    {
                        "score": 0.4845331104890937,
                        "answer": "mg",
                        "hit": false
                    },
                    {
                        "score": 0.4832653646034503,
                        "answer": "mgkg",
                        "hit": false
                    },
                    {
                        "score": 0.4788329179898486,
                        "answer": "medictions",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "telephone"
                ],
                "rank": 192,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8137326538562775
            },
            {
                "question verbose": "What is to tonne ",
                "b": "tonne",
                "expected answer": [
                    "kilogram",
                    "mg",
                    "gm",
                    "hg",
                    "grain",
                    "milligram",
                    "hundredweight",
                    "kilo",
                    "mcg",
                    "dag",
                    "carat",
                    "microgram",
                    "gramme",
                    "quintal",
                    "ng",
                    "gram",
                    "dg",
                    "g",
                    "kg",
                    "decigram",
                    "nanogram",
                    "myriagram",
                    "centner",
                    "metric_grain",
                    "myg",
                    "dekagram",
                    "decagram",
                    "metric_hundredweight",
                    "obolus",
                    "doppelzentner",
                    "hectogram",
                    "dkg"
                ],
                "predictions": [
                    {
                        "score": 0.5182001481208923,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3029067210816424,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2800100611223706,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.2771338929301371,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.26903127769994983,
                        "answer": "improvement",
                        "hit": false
                    },
                    {
                        "score": 0.26375982068953263,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tonne"
                ],
                "rank": 448,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6071102246642113
            },
            {
                "question verbose": "What is to torso ",
                "b": "torso",
                "expected answer": [
                    "chest",
                    "arse",
                    "shoulder",
                    "bum",
                    "midriff",
                    "groin",
                    "deltoid",
                    "diaphragm",
                    "lat",
                    "pectoralis",
                    "behind",
                    "gluteus",
                    "middle",
                    "stomach",
                    "teres",
                    "thorax",
                    "tush",
                    "ab",
                    "rump",
                    "breastbone",
                    "buns",
                    "gladiolus",
                    "haunch",
                    "backside",
                    "buttocks",
                    "serratus",
                    "pectoral",
                    "stern",
                    "navel",
                    "paunch",
                    "seat",
                    "mediastinum",
                    "ass",
                    "axilla",
                    "back",
                    "belly",
                    "bottom",
                    "omphalos",
                    "fundament",
                    "acromion",
                    "derriere",
                    "hindgut",
                    "gut",
                    "gallbladder",
                    "saddle",
                    "cheek",
                    "waist",
                    "butt",
                    "bowel",
                    "keister",
                    "bellybutton",
                    "pectus",
                    "prat",
                    "loins",
                    "abdominal",
                    "rear",
                    "dorsum",
                    "posterior",
                    "venter",
                    "small",
                    "abdomen",
                    "umbilicus",
                    "armpit",
                    "waistline",
                    "sternum",
                    "scapula",
                    "glute",
                    "manubrium",
                    "fanny",
                    "side",
                    "midsection",
                    "pecs",
                    "hip",
                    "buttock",
                    "intestine",
                    "breast",
                    "can",
                    "tail",
                    "hindquarters",
                    "xiphoid_process",
                    "thoracic_vertebra",
                    "abdominal_wall",
                    "musculus_deltoideus",
                    "abdominal_cavity",
                    "arteria_circumflexa_scapulae",
                    "arteria_glutes",
                    "arteria_axillaris",
                    "axillary_fossa",
                    "arteria_colica",
                    "articulatio_humeri",
                    "shoulder_joint",
                    "gall_bladder",
                    "spare_tire",
                    "arteria_circumflexa_humeri",
                    "serratus_muscles",
                    "axillary_artery",
                    "gluteus_muscle",
                    "love_handle",
                    "latissimus_dorsi",
                    "pectoral_muscle",
                    "tooshie",
                    "abdominal_muscle",
                    "circumflex_humeral_artery",
                    "abdominal_aorta",
                    "vena_thoracica",
                    "musculus_pectoralis",
                    "hypochondrium",
                    "area_of_cardiac_dullness",
                    "glenoid_cavity",
                    "rib_cage",
                    "thoracic_aorta",
                    "omphalus",
                    "circumflex_scapular_artery",
                    "acromial_process",
                    "shoulder_bone",
                    "tail_end",
                    "rear_end",
                    "gluteal_muscle",
                    "inguen",
                    "shoulder_blade",
                    "rotator_cuff",
                    "corpus_sternum",
                    "dorsal_vertebra",
                    "teres_muscle",
                    "thoracic_cavity",
                    "axillary_cavity",
                    "chest_cavity",
                    "deltoid_muscle",
                    "belly_button",
                    "gluteal_artery",
                    "thoracic_vein",
                    "glenoid_fossa",
                    "nates",
                    "colic_artery",
                    "lumbar_vertebra",
                    "hind_end"
                ],
                "predictions": [
                    {
                        "score": 0.5155853899507713,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.316003171894054,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.27170885952309126,
                        "answer": "careened",
                        "hit": false
                    },
                    {
                        "score": 0.2653563125737032,
                        "answer": "semi",
                        "hit": false
                    },
                    {
                        "score": 0.2619459998663122,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.25833859248855773,
                        "answer": "improvement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "torso"
                ],
                "rank": 35,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5969709381461143
            },
            {
                "question verbose": "What is to tripod ",
                "b": "tripod",
                "expected answer": [
                    "leg",
                    "stand"
                ],
                "predictions": [
                    {
                        "score": 0.4917647160120154,
                        "answer": "dashboard",
                        "hit": false
                    },
                    {
                        "score": 0.46803535817915737,
                        "answer": "armchair",
                        "hit": false
                    },
                    {
                        "score": 0.46266014566508734,
                        "answer": "persona",
                        "hit": false
                    },
                    {
                        "score": 0.45803609528926875,
                        "answer": "compose",
                        "hit": false
                    },
                    {
                        "score": 0.45479706059462305,
                        "answer": "engenders",
                        "hit": false
                    },
                    {
                        "score": 0.4540272840504003,
                        "answer": "unbroken",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tripod"
                ],
                "rank": 12307,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7076147198677063
            },
            {
                "question verbose": "What is to typewriter ",
                "b": "typewriter",
                "expected answer": [
                    "keyboard",
                    "typewriter",
                    "carriage",
                    "tab",
                    "ribbon",
                    "shift",
                    "backspace",
                    "action",
                    "key",
                    "typewriter_carriage",
                    "backspace_key",
                    "action_mechanism",
                    "typewriter_keyboard",
                    "shift_key",
                    "space_bar",
                    "tab_key",
                    "typewriter_ribbon",
                    "backspacer"
                ],
                "predictions": [
                    {
                        "score": 0.5181568350340213,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30371257333377005,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28871754633740154,
                        "answer": "camp",
                        "hit": false
                    },
                    {
                        "score": 0.26611237649861064,
                        "answer": "additional",
                        "hit": false
                    },
                    {
                        "score": 0.2658878859102796,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.26560543568099215,
                        "answer": "improvement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "typewriter"
                ],
                "rank": 4974,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5585223063826561
            },
            {
                "question verbose": "What is to window ",
                "b": "window",
                "expected answer": [
                    "pane",
                    "sash",
                    "windowpane",
                    "window",
                    "fastener",
                    "glass",
                    "frame",
                    "case",
                    "windowsill",
                    "mullion",
                    "jamb",
                    "lock",
                    "casing",
                    "sash_lock",
                    "window_glass",
                    "sash_fastener",
                    "pane_of_glass",
                    "window_lock",
                    "window_sash",
                    "window_frame"
                ],
                "predictions": [
                    {
                        "score": 0.5733943592042335,
                        "answer": "rt",
                        "hit": false
                    },
                    {
                        "score": 0.4625948584983809,
                        "answer": "interface",
                        "hit": false
                    },
                    {
                        "score": 0.4613706274552917,
                        "answer": "coded",
                        "hit": false
                    },
                    {
                        "score": 0.44243082971593173,
                        "answer": "mobile",
                        "hit": false
                    },
                    {
                        "score": 0.43438893423931896,
                        "answer": "button",
                        "hit": false
                    },
                    {
                        "score": 0.42486735422514427,
                        "answer": "apps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "window"
                ],
                "rank": 2104,
                "landing_b": true,
                "landing_b_prime": true,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6103275716304779
            },
            {
                "question verbose": "What is to womb ",
                "b": "womb",
                "expected answer": [
                    "cervix",
                    "caul",
                    "placenta",
                    "oviduct",
                    "veil",
                    "myometrium",
                    "endometrium",
                    "uterine_artery",
                    "embryonic_membrane",
                    "uterine_cervix",
                    "uterine_cavity",
                    "fallopian_tube",
                    "uterine_tube",
                    "cervix_uteri",
                    "arteria_uterina"
                ],
                "predictions": [
                    {
                        "score": 0.5444006977776037,
                        "answer": "coincide",
                        "hit": false
                    },
                    {
                        "score": 0.5405935550903671,
                        "answer": "engenders",
                        "hit": false
                    },
                    {
                        "score": 0.5297262727928134,
                        "answer": "calculate",
                        "hit": false
                    },
                    {
                        "score": 0.5272136009690858,
                        "answer": "erroneous",
                        "hit": false
                    },
                    {
                        "score": 0.5230933553502259,
                        "answer": "bicycleped",
                        "hit": false
                    },
                    {
                        "score": 0.5211929573112849,
                        "answer": "waived",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "womb"
                ],
                "rank": 3904,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6629311293363571
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L06 [meronyms - part].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "48132582-d5de-4a93-bd09-3510643c892a",
            "timestamp": "2020-10-22T15:57:52.246605"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to afraid ",
                "b": "afraid",
                "expected answer": [
                    "terrified",
                    "horrified",
                    "scared",
                    "stiff",
                    "petrified",
                    "fearful",
                    "panicky"
                ],
                "predictions": [
                    {
                        "score": 0.4590277008479451,
                        "answer": "nextone",
                        "hit": false
                    },
                    {
                        "score": 0.45458875211810235,
                        "answer": "demolished",
                        "hit": false
                    },
                    {
                        "score": 0.452008431656149,
                        "answer": "bleed",
                        "hit": false
                    },
                    {
                        "score": 0.45198363880525494,
                        "answer": "learner",
                        "hit": false
                    },
                    {
                        "score": 0.4370969688053003,
                        "answer": "reevaluate",
                        "hit": false
                    },
                    {
                        "score": 0.43022315030748354,
                        "answer": "musical",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "afraid"
                ],
                "rank": 574,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7467291802167892
            },
            {
                "question verbose": "What is to angry ",
                "b": "angry",
                "expected answer": [
                    "furious",
                    "enraged",
                    "outraged",
                    "aggrivated",
                    "irate",
                    "seething"
                ],
                "predictions": [
                    {
                        "score": 0.32283921111603264,
                        "answer": "reflecting",
                        "hit": false
                    },
                    {
                        "score": 0.321593289399968,
                        "answer": "portrayed",
                        "hit": false
                    },
                    {
                        "score": 0.3151085918264833,
                        "answer": "fok",
                        "hit": false
                    },
                    {
                        "score": 0.3093037278811826,
                        "answer": "hot",
                        "hit": false
                    },
                    {
                        "score": 0.30704656410845826,
                        "answer": "adores",
                        "hit": false
                    },
                    {
                        "score": 0.3027234185077281,
                        "answer": "alien",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "angry"
                ],
                "rank": 1397,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7946479618549347
            },
            {
                "question verbose": "What is to ask ",
                "b": "ask",
                "expected answer": [
                    "beg",
                    "implore",
                    "pray",
                    "entreat",
                    "supplicate",
                    "insist"
                ],
                "predictions": [
                    {
                        "score": 0.33000354590322867,
                        "answer": "embark",
                        "hit": false
                    },
                    {
                        "score": 0.3295294260386611,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.3253405102373535,
                        "answer": "newly",
                        "hit": false
                    },
                    {
                        "score": 0.32327442837566744,
                        "answer": "sacrifice",
                        "hit": false
                    },
                    {
                        "score": 0.31743207758462466,
                        "answer": "pray",
                        "hit": true
                    },
                    {
                        "score": 0.3159364860010327,
                        "answer": "answering",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ask"
                ],
                "rank": 4,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6511749625205994
            },
            {
                "question verbose": "What is to bad ",
                "b": "bad",
                "expected answer": [
                    "awful",
                    "atrocious",
                    "abominable",
                    "dreadful",
                    "painful",
                    "terrible",
                    "unspeakable"
                ],
                "predictions": [
                    {
                        "score": 0.28326548473758945,
                        "answer": "one",
                        "hit": false
                    },
                    {
                        "score": 0.2794336691724352,
                        "answer": "imagination",
                        "hit": false
                    },
                    {
                        "score": 0.2790912944181242,
                        "answer": "concern",
                        "hit": false
                    },
                    {
                        "score": 0.27826260947749015,
                        "answer": "burst",
                        "hit": false
                    },
                    {
                        "score": 0.27794473084606997,
                        "answer": "vagary",
                        "hit": false
                    },
                    {
                        "score": 0.2778146521608708,
                        "answer": "putting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bad"
                ],
                "rank": 637,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6837617754936218
            },
            {
                "question verbose": "What is to boring ",
                "b": "boring",
                "expected answer": [
                    "tedious",
                    "deadening",
                    "dull",
                    "ho-hum",
                    "irksome",
                    "tiresome",
                    "wearisome"
                ],
                "predictions": [
                    {
                        "score": 0.41894255857467283,
                        "answer": "brett",
                        "hit": false
                    },
                    {
                        "score": 0.39988384038259234,
                        "answer": "demille",
                        "hit": false
                    },
                    {
                        "score": 0.3995588632857186,
                        "answer": "listing",
                        "hit": false
                    },
                    {
                        "score": 0.39749547741012536,
                        "answer": "ungrateful",
                        "hit": false
                    },
                    {
                        "score": 0.392268338277618,
                        "answer": "fluidly",
                        "hit": false
                    },
                    {
                        "score": 0.39209969427018837,
                        "answer": "discusion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "boring"
                ],
                "rank": 2662,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5992209911346436
            },
            {
                "question verbose": "What is to cat ",
                "b": "cat",
                "expected answer": [
                    "lion",
                    "tiger",
                    "leopard",
                    "panther",
                    "jaguar"
                ],
                "predictions": [
                    {
                        "score": 0.370083629216982,
                        "answer": "permanently",
                        "hit": false
                    },
                    {
                        "score": 0.3656947585257197,
                        "answer": "czar",
                        "hit": false
                    },
                    {
                        "score": 0.3647923742151028,
                        "answer": "breed",
                        "hit": false
                    },
                    {
                        "score": 0.3589583145457024,
                        "answer": "albany",
                        "hit": false
                    },
                    {
                        "score": 0.357706130197494,
                        "answer": "foodservice",
                        "hit": false
                    },
                    {
                        "score": 0.3557665248829769,
                        "answer": "chum",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cat"
                ],
                "rank": 11651,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6910800933837891
            },
            {
                "question verbose": "What is to chuckle ",
                "b": "chuckle",
                "expected answer": [
                    "laugh",
                    "guffaw",
                    "chortle",
                    "guffaw",
                    "snicker",
                    "snigger",
                    "titter",
                    "roar"
                ],
                "predictions": [
                    {
                        "score": 0.6843677065732973,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2751412536861173,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.27496984740479347,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2721792368831494,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.26926255018056544,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2632019098283967,
                        "answer": "reminded",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "chuckle"
                ],
                "rank": 3902,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5832807719707489
            },
            {
                "question verbose": "What is to confused ",
                "b": "confused",
                "expected answer": [
                    "lost",
                    "bewildered",
                    "trapped",
                    "desperate"
                ],
                "predictions": [
                    {
                        "score": 0.40194334044428087,
                        "answer": "gamers",
                        "hit": false
                    },
                    {
                        "score": 0.3953985055213075,
                        "answer": "atleast",
                        "hit": false
                    },
                    {
                        "score": 0.38730185793141136,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.3865451787483,
                        "answer": "understood",
                        "hit": false
                    },
                    {
                        "score": 0.3812598026532161,
                        "answer": "intimating",
                        "hit": false
                    },
                    {
                        "score": 0.37370101048789683,
                        "answer": "farrel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "confused"
                ],
                "rank": 2967,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6813793778419495
            },
            {
                "question verbose": "What is to creative ",
                "b": "creative",
                "expected answer": [
                    "ingenious",
                    "inventive",
                    "demiurgic",
                    "deviceful",
                    "innovational",
                    "innovative",
                    "innovatory",
                    "original",
                    "originative"
                ],
                "predictions": [
                    {
                        "score": 0.3671544765908726,
                        "answer": "vary",
                        "hit": false
                    },
                    {
                        "score": 0.36231693586389424,
                        "answer": "creativity",
                        "hit": false
                    },
                    {
                        "score": 0.36035778609945085,
                        "answer": "accessible",
                        "hit": false
                    },
                    {
                        "score": 0.3495368969627248,
                        "answer": "ssi",
                        "hit": false
                    },
                    {
                        "score": 0.3481619915033689,
                        "answer": "throne",
                        "hit": false
                    },
                    {
                        "score": 0.34604699395769883,
                        "answer": "mercy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "creative"
                ],
                "rank": 18,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.48650870379060507
            },
            {
                "question verbose": "What is to cry ",
                "b": "cry",
                "expected answer": [
                    "scream",
                    "shriek",
                    "screech",
                    "screeching"
                ],
                "predictions": [
                    {
                        "score": 0.4097858988733533,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.39108235873728314,
                        "answer": "apr",
                        "hit": false
                    },
                    {
                        "score": 0.39008119343173014,
                        "answer": "attain",
                        "hit": false
                    },
                    {
                        "score": 0.37911751705055274,
                        "answer": "staggering",
                        "hit": false
                    },
                    {
                        "score": 0.37591045590344296,
                        "answer": "dalai",
                        "hit": false
                    },
                    {
                        "score": 0.3751863539375995,
                        "answer": "antibiotic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cry"
                ],
                "rank": 9373,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7111594378948212
            },
            {
                "question verbose": "What is to damp ",
                "b": "damp",
                "expected answer": [
                    "drenched",
                    "dripping",
                    "saturated",
                    "soaked",
                    "soaking",
                    "sodden",
                    "sopping",
                    "soppy",
                    "soused",
                    "wringing-wet"
                ],
                "predictions": [
                    {
                        "score": 0.6712689153456088,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28501276291796523,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.27439590411825143,
                        "answer": "observer",
                        "hit": false
                    },
                    {
                        "score": 0.2728366995122338,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2709505369222332,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.26880166149108115,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "damp"
                ],
                "rank": 1169,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to dinner ",
                "b": "dinner",
                "expected answer": [
                    "feast",
                    "banquet",
                    "fiesta"
                ],
                "predictions": [
                    {
                        "score": 0.37979730768262315,
                        "answer": "crock",
                        "hit": false
                    },
                    {
                        "score": 0.37548372149645043,
                        "answer": "staying",
                        "hit": false
                    },
                    {
                        "score": 0.37323617291604283,
                        "answer": "deadline",
                        "hit": false
                    },
                    {
                        "score": 0.3579950584647912,
                        "answer": "teamster",
                        "hit": false
                    },
                    {
                        "score": 0.35698344152433953,
                        "answer": "constant",
                        "hit": false
                    },
                    {
                        "score": 0.35544882773890735,
                        "answer": "humbly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dinner"
                ],
                "rank": 319,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7278842628002167
            },
            {
                "question verbose": "What is to dislike ",
                "b": "dislike",
                "expected answer": [
                    "hate",
                    "abhor",
                    "detest",
                    "loathe",
                    "abominate",
                    "execrate",
                    "contemn",
                    "despise",
                    "scorn",
                    "disdain"
                ],
                "predictions": [
                    {
                        "score": 0.4219682030975072,
                        "answer": "gamers",
                        "hit": false
                    },
                    {
                        "score": 0.40249443308214033,
                        "answer": "laughter",
                        "hit": false
                    },
                    {
                        "score": 0.39660212234723924,
                        "answer": "cheering",
                        "hit": false
                    },
                    {
                        "score": 0.3927087529037361,
                        "answer": "reflecting",
                        "hit": false
                    },
                    {
                        "score": 0.3924030118616661,
                        "answer": "practical",
                        "hit": false
                    },
                    {
                        "score": 0.39073891331347216,
                        "answer": "fining",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dislike"
                ],
                "rank": 8852,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6467266827821732
            },
            {
                "question verbose": "What is to doze ",
                "b": "doze",
                "expected answer": [
                    "sleep",
                    "slumber"
                ],
                "predictions": [
                    {
                        "score": 0.49317562891537964,
                        "answer": "donkey",
                        "hit": false
                    },
                    {
                        "score": 0.4855361515739649,
                        "answer": "mpx",
                        "hit": false
                    },
                    {
                        "score": 0.4754630146695579,
                        "answer": "optical",
                        "hit": false
                    },
                    {
                        "score": 0.46892677521146187,
                        "answer": "coalition",
                        "hit": false
                    },
                    {
                        "score": 0.46795808396902655,
                        "answer": "blu",
                        "hit": false
                    },
                    {
                        "score": 0.4660870917691083,
                        "answer": "recovers",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "doze"
                ],
                "rank": 7979,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7391324192285538
            },
            {
                "question verbose": "What is to drizzle ",
                "b": "drizzle",
                "expected answer": [
                    "rain",
                    "shower",
                    "raifall",
                    "deluge"
                ],
                "predictions": [
                    {
                        "score": 0.6817157206605333,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.29721382675315955,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28818371792192804,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2834665767481714,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2794694275002366,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.27570575515897605,
                        "answer": "persuasion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "drizzle"
                ],
                "rank": 6925,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6399523615837097
            },
            {
                "question verbose": "What is to excited ",
                "b": "excited",
                "expected answer": [
                    "agitated",
                    "nervous"
                ],
                "predictions": [
                    {
                        "score": 0.40989630768542634,
                        "answer": "programming",
                        "hit": false
                    },
                    {
                        "score": 0.3970606610213861,
                        "answer": "solves",
                        "hit": false
                    },
                    {
                        "score": 0.3858515627096004,
                        "answer": "raimi",
                        "hit": false
                    },
                    {
                        "score": 0.38479933166461067,
                        "answer": "notification",
                        "hit": false
                    },
                    {
                        "score": 0.3765698797851046,
                        "answer": "arrogated",
                        "hit": false
                    },
                    {
                        "score": 0.3741735904662336,
                        "answer": "humbly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "excited"
                ],
                "rank": 12653,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6195364147424698
            },
            {
                "question verbose": "What is to faith ",
                "b": "faith",
                "expected answer": [
                    "fanatism",
                    "fanaticism",
                    "zealotry"
                ],
                "predictions": [
                    {
                        "score": 0.4790221308582966,
                        "answer": "thy",
                        "hit": false
                    },
                    {
                        "score": 0.4055005742054545,
                        "answer": "shew",
                        "hit": false
                    },
                    {
                        "score": 0.3575977095145111,
                        "answer": "devoid",
                        "hit": false
                    },
                    {
                        "score": 0.3548006948561826,
                        "answer": "thee",
                        "hit": false
                    },
                    {
                        "score": 0.3536460039770172,
                        "answer": "sinful",
                        "hit": false
                    },
                    {
                        "score": 0.3490924649109361,
                        "answer": "god",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "faith"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5532718636095524
            },
            {
                "question verbose": "What is to giggle ",
                "b": "giggle",
                "expected answer": [
                    "laugh",
                    "guffaw",
                    "chortle",
                    "guffaw",
                    "snicker",
                    "snigger",
                    "titter",
                    "roar"
                ],
                "predictions": [
                    {
                        "score": 0.6841805977193282,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2852628446772149,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2842773468928148,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2758071588257598,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.27508360079792626,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.27356324808350724,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "giggle"
                ],
                "rank": 4114,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5832807719707489
            },
            {
                "question verbose": "What is to guilty ",
                "b": "guilty",
                "expected answer": [
                    "remorseful",
                    "sorrowful",
                    "unworthy"
                ],
                "predictions": [
                    {
                        "score": 0.4009951340389091,
                        "answer": "traveller",
                        "hit": false
                    },
                    {
                        "score": 0.38445105673217334,
                        "answer": "searched",
                        "hit": false
                    },
                    {
                        "score": 0.37942429320107257,
                        "answer": "prosecution",
                        "hit": false
                    },
                    {
                        "score": 0.3742230846821019,
                        "answer": "accept",
                        "hit": false
                    },
                    {
                        "score": 0.3719613574061294,
                        "answer": "therefore",
                        "hit": false
                    },
                    {
                        "score": 0.36983626001475944,
                        "answer": "covert",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "guilty"
                ],
                "rank": 12644,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7164680510759354
            },
            {
                "question verbose": "What is to happy ",
                "b": "happy",
                "expected answer": [
                    "ecstatic",
                    "enraptured",
                    "rapturous",
                    "rapt",
                    "rhapsodic"
                ],
                "predictions": [
                    {
                        "score": 0.32633755286913724,
                        "answer": "demo",
                        "hit": false
                    },
                    {
                        "score": 0.32067664801778445,
                        "answer": "learner",
                        "hit": false
                    },
                    {
                        "score": 0.31679614276559626,
                        "answer": "antibiotic",
                        "hit": false
                    },
                    {
                        "score": 0.3084228656788651,
                        "answer": "equip",
                        "hit": false
                    },
                    {
                        "score": 0.3076366261318421,
                        "answer": "accessible",
                        "hit": false
                    },
                    {
                        "score": 0.30395236427963535,
                        "answer": "alternate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happy"
                ],
                "rank": 9508,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5431520976126194
            },
            {
                "question verbose": "What is to house ",
                "b": "house",
                "expected answer": [
                    "palace",
                    "castle"
                ],
                "predictions": [
                    {
                        "score": 0.2562176853789031,
                        "answer": "pooling",
                        "hit": false
                    },
                    {
                        "score": 0.24557419092536587,
                        "answer": "regained",
                        "hit": false
                    },
                    {
                        "score": 0.24360118561580185,
                        "answer": "washing",
                        "hit": false
                    },
                    {
                        "score": 0.23884429072817376,
                        "answer": "utter",
                        "hit": false
                    },
                    {
                        "score": 0.23688504187963355,
                        "answer": "npc",
                        "hit": false
                    },
                    {
                        "score": 0.23420750599123732,
                        "answer": "rediscover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "house"
                ],
                "rank": 307,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6924178302288055
            },
            {
                "question verbose": "What is to hungry ",
                "b": "hungry",
                "expected answer": [
                    "starving",
                    "famished",
                    "peckish",
                    "ravenous",
                    "starved"
                ],
                "predictions": [
                    {
                        "score": 0.4693504929041706,
                        "answer": "greek",
                        "hit": false
                    },
                    {
                        "score": 0.46643850130743003,
                        "answer": "trauma",
                        "hit": false
                    },
                    {
                        "score": 0.46600061047507757,
                        "answer": "accord",
                        "hit": false
                    },
                    {
                        "score": 0.46518924825320934,
                        "answer": "odark",
                        "hit": false
                    },
                    {
                        "score": 0.4562177161561686,
                        "answer": "simplified",
                        "hit": false
                    },
                    {
                        "score": 0.4488997440180945,
                        "answer": "arrangement",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hungry"
                ],
                "rank": 1407,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7684657275676727
            },
            {
                "question verbose": "What is to indulge ",
                "b": "indulge",
                "expected answer": [
                    "pamper",
                    "spoil",
                    "coddle"
                ],
                "predictions": [
                    {
                        "score": 0.6765718587409546,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.28895123722817445,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2865490099654073,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.27765949413750135,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.27514517162319907,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2746030472755846,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "indulge"
                ],
                "rank": 4036,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to interesting ",
                "b": "interesting",
                "expected answer": [
                    "exciting",
                    "exhilarant",
                    "exhilarating",
                    "exhilarative",
                    "eye-popping",
                    "inspiring",
                    "intoxicating",
                    "rousing",
                    "stimulating",
                    "stirring",
                    "arresting",
                    "interesting",
                    "intriguing",
                    "moving",
                    "provocative",
                    "heady",
                    "thrilling"
                ],
                "predictions": [
                    {
                        "score": 0.3286696608161782,
                        "answer": "examine",
                        "hit": false
                    },
                    {
                        "score": 0.31957301617657163,
                        "answer": "tragic",
                        "hit": false
                    },
                    {
                        "score": 0.31551558109216893,
                        "answer": "indulgent",
                        "hit": false
                    },
                    {
                        "score": 0.31506664160742237,
                        "answer": "incidentally",
                        "hit": false
                    },
                    {
                        "score": 0.3061181552970008,
                        "answer": "view",
                        "hit": false
                    },
                    {
                        "score": 0.3019753332282859,
                        "answer": "prettier",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interesting"
                ],
                "rank": 3008,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6936904639005661
            },
            {
                "question verbose": "What is to irritate ",
                "b": "irritate",
                "expected answer": [
                    "enrage",
                    "incense",
                    "infuriate",
                    "ire",
                    "mad",
                    "madden",
                    "steam",
                    "umbrage"
                ],
                "predictions": [
                    {
                        "score": 0.6703768533225001,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2808545183822321,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2805125029102871,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2736804374532277,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2728063097052036,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2727242386488412,
                        "answer": "odark",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "irritate"
                ],
                "rank": 288,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to jog ",
                "b": "jog",
                "expected answer": [
                    "run",
                    "scarper",
                    "flee",
                    "fly"
                ],
                "predictions": [
                    {
                        "score": 0.6814045590385532,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2958576664822208,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28759496124447337,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2843102864249687,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2835407251080249,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.28121877040581195,
                        "answer": "observer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jog"
                ],
                "rank": 3189,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6364458352327347
            },
            {
                "question verbose": "What is to lake ",
                "b": "lake",
                "expected answer": [
                    "sea",
                    "ocean"
                ],
                "predictions": [
                    {
                        "score": 0.37888906099426173,
                        "answer": "sediment",
                        "hit": false
                    },
                    {
                        "score": 0.36253585630581026,
                        "answer": "muscle",
                        "hit": false
                    },
                    {
                        "score": 0.3557161065971623,
                        "answer": "gold",
                        "hit": false
                    },
                    {
                        "score": 0.35203225373582997,
                        "answer": "mediafire",
                        "hit": false
                    },
                    {
                        "score": 0.34881075280410934,
                        "answer": "transit",
                        "hit": false
                    },
                    {
                        "score": 0.3476491936017428,
                        "answer": "creek",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lake"
                ],
                "rank": 8157,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5537759810686111
            },
            {
                "question verbose": "What is to like ",
                "b": "like",
                "expected answer": [
                    "love",
                    "care",
                    "fond",
                    "crush",
                    "infatuate"
                ],
                "predictions": [
                    {
                        "score": 0.3138994795297576,
                        "answer": "nick",
                        "hit": false
                    },
                    {
                        "score": 0.3037458834246106,
                        "answer": "porno",
                        "hit": false
                    },
                    {
                        "score": 0.2755747716251115,
                        "answer": "general",
                        "hit": false
                    },
                    {
                        "score": 0.2652421952345883,
                        "answer": "turd",
                        "hit": false
                    },
                    {
                        "score": 0.2626370421137456,
                        "answer": "unwatchable",
                        "hit": false
                    },
                    {
                        "score": 0.25770703127158084,
                        "answer": "tampa",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "like"
                ],
                "rank": 3449,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5702951774001122
            },
            {
                "question verbose": "What is to love ",
                "b": "love",
                "expected answer": [
                    "adore",
                    "fetishize",
                    "idolize",
                    "idolise",
                    "worship",
                    "hero-worship",
                    "revere"
                ],
                "predictions": [
                    {
                        "score": 0.3729168486427558,
                        "answer": "humbly",
                        "hit": false
                    },
                    {
                        "score": 0.34880187001856017,
                        "answer": "ghandi",
                        "hit": false
                    },
                    {
                        "score": 0.3352776464437525,
                        "answer": "gadget",
                        "hit": false
                    },
                    {
                        "score": 0.33074216551960983,
                        "answer": "madly",
                        "hit": false
                    },
                    {
                        "score": 0.3296002237722555,
                        "answer": "hottest",
                        "hit": false
                    },
                    {
                        "score": 0.32830639700570785,
                        "answer": "mercy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "love"
                ],
                "rank": 1405,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6550970822572708
            },
            {
                "question verbose": "What is to monkey ",
                "b": "monkey",
                "expected answer": [
                    "gorilla"
                ],
                "predictions": [
                    {
                        "score": 0.44644222370712255,
                        "answer": "farrel",
                        "hit": false
                    },
                    {
                        "score": 0.4427771637544185,
                        "answer": "corine",
                        "hit": false
                    },
                    {
                        "score": 0.43727286515385927,
                        "answer": "mavs",
                        "hit": false
                    },
                    {
                        "score": 0.43303347439808976,
                        "answer": "opt",
                        "hit": false
                    },
                    {
                        "score": 0.43193341838938865,
                        "answer": "rizzoli",
                        "hit": false
                    },
                    {
                        "score": 0.4312608475155883,
                        "answer": "humbly",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "monkey"
                ],
                "rank": 4953,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8001566529273987
            },
            {
                "question verbose": "What is to nap ",
                "b": "nap",
                "expected answer": [
                    "sleep",
                    "slumber"
                ],
                "predictions": [
                    {
                        "score": 0.42875964128933974,
                        "answer": "gamers",
                        "hit": false
                    },
                    {
                        "score": 0.423728968781762,
                        "answer": "staggering",
                        "hit": false
                    },
                    {
                        "score": 0.42197399360101423,
                        "answer": "muddy",
                        "hit": false
                    },
                    {
                        "score": 0.42194885923273073,
                        "answer": "humbly",
                        "hit": false
                    },
                    {
                        "score": 0.414923463192198,
                        "answer": "foul",
                        "hit": false
                    },
                    {
                        "score": 0.41412368841754343,
                        "answer": "htpc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "nap"
                ],
                "rank": 8716,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7131522446870804
            },
            {
                "question verbose": "What is to necessary ",
                "b": "necessary",
                "expected answer": [
                    "essential",
                    "indispensable",
                    "vital",
                    "required"
                ],
                "predictions": [
                    {
                        "score": 0.36288542358622095,
                        "answer": "learner",
                        "hit": false
                    },
                    {
                        "score": 0.3564406847576686,
                        "answer": "breeding",
                        "hit": false
                    },
                    {
                        "score": 0.34696742196682256,
                        "answer": "altered",
                        "hit": false
                    },
                    {
                        "score": 0.33435254701077055,
                        "answer": "vary",
                        "hit": false
                    },
                    {
                        "score": 0.33257647047738037,
                        "answer": "unimportant",
                        "hit": false
                    },
                    {
                        "score": 0.33024750828943766,
                        "answer": "conditioning",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "necessary"
                ],
                "rank": 2112,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6818189173936844
            },
            {
                "question verbose": "What is to opposed ",
                "b": "opposed",
                "expected answer": [
                    "averse",
                    "antipathetic",
                    "antipathetical",
                    "indisposed",
                    "loath",
                    "loth"
                ],
                "predictions": [
                    {
                        "score": 0.4304674630049691,
                        "answer": "sacrifice",
                        "hit": false
                    },
                    {
                        "score": 0.42484193720973557,
                        "answer": "disguised",
                        "hit": false
                    },
                    {
                        "score": 0.4135880028798685,
                        "answer": "cognitively",
                        "hit": false
                    },
                    {
                        "score": 0.4125939437745902,
                        "answer": "cecile",
                        "hit": false
                    },
                    {
                        "score": 0.4115292723129415,
                        "answer": "evernote",
                        "hit": false
                    },
                    {
                        "score": 0.4087480766713383,
                        "answer": "conditioning",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "opposed"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5904299691319466
            },
            {
                "question verbose": "What is to pain ",
                "b": "pain",
                "expected answer": [
                    "torment",
                    "torture",
                    "agony"
                ],
                "predictions": [
                    {
                        "score": 0.347098254898487,
                        "answer": "proactive",
                        "hit": false
                    },
                    {
                        "score": 0.32791242552858374,
                        "answer": "resend",
                        "hit": false
                    },
                    {
                        "score": 0.3268430806789855,
                        "answer": "determine",
                        "hit": false
                    },
                    {
                        "score": 0.3231225219808876,
                        "answer": "reach",
                        "hit": false
                    },
                    {
                        "score": 0.32067229818592496,
                        "answer": "clunky",
                        "hit": false
                    },
                    {
                        "score": 0.3114462100884864,
                        "answer": "trauma",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pain"
                ],
                "rank": 6746,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5986275374889374
            },
            {
                "question verbose": "What is to pony ",
                "b": "pony",
                "expected answer": [
                    "horse"
                ],
                "predictions": [
                    {
                        "score": 0.4808048813627978,
                        "answer": "surging",
                        "hit": false
                    },
                    {
                        "score": 0.4759643758837035,
                        "answer": "seemsto",
                        "hit": false
                    },
                    {
                        "score": 0.4720371377771894,
                        "answer": "invention",
                        "hit": false
                    },
                    {
                        "score": 0.46804255526896277,
                        "answer": "utter",
                        "hit": false
                    },
                    {
                        "score": 0.46173608599148314,
                        "answer": "masshealth",
                        "hit": false
                    },
                    {
                        "score": 0.4609258155338547,
                        "answer": "trauma",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pony"
                ],
                "rank": 9824,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6890925019979477
            },
            {
                "question verbose": "What is to poorly ",
                "b": "poorly",
                "expected answer": [
                    "afflicted",
                    "sick",
                    "ill",
                    "stricken"
                ],
                "predictions": [
                    {
                        "score": 0.45805639034295037,
                        "answer": "officiate",
                        "hit": false
                    },
                    {
                        "score": 0.42300452956742,
                        "answer": "equip",
                        "hit": false
                    },
                    {
                        "score": 0.4161036451055596,
                        "answer": "oslo",
                        "hit": false
                    },
                    {
                        "score": 0.4121539996250477,
                        "answer": "replacing",
                        "hit": false
                    },
                    {
                        "score": 0.41183437260813494,
                        "answer": "perked",
                        "hit": false
                    },
                    {
                        "score": 0.4088151383799939,
                        "answer": "spacey",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "poorly"
                ],
                "rank": 5208,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6527057141065598
            },
            {
                "question verbose": "What is to rain ",
                "b": "rain",
                "expected answer": [
                    "deluge",
                    "shower"
                ],
                "predictions": [
                    {
                        "score": 0.4742420346744429,
                        "answer": "monitoring",
                        "hit": false
                    },
                    {
                        "score": 0.4524311915775399,
                        "answer": "nova",
                        "hit": false
                    },
                    {
                        "score": 0.45101876728286033,
                        "answer": "advertises",
                        "hit": false
                    },
                    {
                        "score": 0.4463144139274664,
                        "answer": "eaves",
                        "hit": false
                    },
                    {
                        "score": 0.4461320812436792,
                        "answer": "playback",
                        "hit": false
                    },
                    {
                        "score": 0.4460062928402357,
                        "answer": "implementation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rain"
                ],
                "rank": 1338,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6399523615837097
            },
            {
                "question verbose": "What is to sad ",
                "b": "sad",
                "expected answer": [
                    "desparate",
                    "despondent",
                    "despairing",
                    "desponding",
                    "forlorn",
                    "hopeless",
                    "melancholy"
                ],
                "predictions": [
                    {
                        "score": 0.3756854024127327,
                        "answer": "truly",
                        "hit": false
                    },
                    {
                        "score": 0.3621609489581391,
                        "answer": "searched",
                        "hit": false
                    },
                    {
                        "score": 0.3575579864740602,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.35344039470766175,
                        "answer": "learner",
                        "hit": false
                    },
                    {
                        "score": 0.35120021495092635,
                        "answer": "invention",
                        "hit": false
                    },
                    {
                        "score": 0.3458084548848135,
                        "answer": "inspired",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sad"
                ],
                "rank": 10437,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5860569924116135
            },
            {
                "question verbose": "What is to sea ",
                "b": "sea",
                "expected answer": [
                    "ocean"
                ],
                "predictions": [
                    {
                        "score": 0.685237322201858,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.294744942470084,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2915059510209245,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28339944016838264,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.2828852758062944,
                        "answer": "supporttime",
                        "hit": false
                    },
                    {
                        "score": 0.2753502337919939,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sea"
                ],
                "rank": 15327,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4473397359251976
            },
            {
                "question verbose": "What is to snack ",
                "b": "snack",
                "expected answer": [
                    "meal",
                    "eat"
                ],
                "predictions": [
                    {
                        "score": 0.5371673266625295,
                        "answer": "friendswill",
                        "hit": false
                    },
                    {
                        "score": 0.5111463130284334,
                        "answer": "farrel",
                        "hit": false
                    },
                    {
                        "score": 0.5066725612118768,
                        "answer": "aikido",
                        "hit": false
                    },
                    {
                        "score": 0.50042463462628,
                        "answer": "mapping",
                        "hit": false
                    },
                    {
                        "score": 0.4925556366303336,
                        "answer": "woth",
                        "hit": false
                    },
                    {
                        "score": 0.4884008848088876,
                        "answer": "milquetoast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "snack"
                ],
                "rank": 11969,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6653570979833603
            },
            {
                "question verbose": "What is to sniffles ",
                "b": "sniffles",
                "expected answer": [
                    "pneumonia"
                ],
                "predictions": [
                    {
                        "score": 0.6753878725431852,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2963075726482365,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.28568260527133293,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.283110010171907,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2747089093982804,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2686808325250064,
                        "answer": "melodrama",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sniffles"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to soon ",
                "b": "soon",
                "expected answer": [
                    "immediately",
                    "promptly",
                    "straightaway"
                ],
                "predictions": [
                    {
                        "score": 0.3260110561156656,
                        "answer": "infinite",
                        "hit": false
                    },
                    {
                        "score": 0.3221984344072597,
                        "answer": "focusing",
                        "hit": false
                    },
                    {
                        "score": 0.3173279076158656,
                        "answer": "jersey",
                        "hit": false
                    },
                    {
                        "score": 0.29606456635962874,
                        "answer": "altered",
                        "hit": false
                    },
                    {
                        "score": 0.29242845699187886,
                        "answer": "inc",
                        "hit": false
                    },
                    {
                        "score": 0.29087569033489186,
                        "answer": "darkness",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "soon"
                ],
                "rank": 13853,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6245329454541206
            },
            {
                "question verbose": "What is to strong ",
                "b": "strong",
                "expected answer": [
                    "powerful",
                    "forceful",
                    "super",
                    "potent"
                ],
                "predictions": [
                    {
                        "score": 0.2818064843335914,
                        "answer": "punter",
                        "hit": false
                    },
                    {
                        "score": 0.27634968966177265,
                        "answer": "dramatic",
                        "hit": false
                    },
                    {
                        "score": 0.271790349024516,
                        "answer": "moral",
                        "hit": false
                    },
                    {
                        "score": 0.2702962065671604,
                        "answer": "reevaluate",
                        "hit": false
                    },
                    {
                        "score": 0.26749482925582385,
                        "answer": "overload",
                        "hit": false
                    },
                    {
                        "score": 0.2663015231608932,
                        "answer": "notification",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "strong"
                ],
                "rank": 2952,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7728921175003052
            },
            {
                "question verbose": "What is to tasty ",
                "b": "tasty",
                "expected answer": [
                    "delicious",
                    "delectable",
                    "luscious",
                    "pleasant-tasting",
                    "scrumptious",
                    "toothsome",
                    "yummy",
                    "mouth-watering",
                    "ambrosial",
                    "heavenly"
                ],
                "predictions": [
                    {
                        "score": 0.6835808376630311,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2855340051973978,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.2822776036840862,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.26923777775605123,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.26876380907167646,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.26218163749486945,
                        "answer": "observer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tasty"
                ],
                "rank": 3017,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.665719211101532
            },
            {
                "question verbose": "What is to tired ",
                "b": "tired",
                "expected answer": [
                    "exhausted",
                    "drained"
                ],
                "predictions": [
                    {
                        "score": 0.3311146714725798,
                        "answer": "wandering",
                        "hit": false
                    },
                    {
                        "score": 0.3199917121052082,
                        "answer": "hipster",
                        "hit": false
                    },
                    {
                        "score": 0.31207822431982796,
                        "answer": "invention",
                        "hit": false
                    },
                    {
                        "score": 0.30841937601744057,
                        "answer": "whatsoever",
                        "hit": false
                    },
                    {
                        "score": 0.3051369990364389,
                        "answer": "humbly",
                        "hit": false
                    },
                    {
                        "score": 0.3043540681380749,
                        "answer": "sick",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tired"
                ],
                "rank": 13484,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7111750692129135
            },
            {
                "question verbose": "What is to unfortunate ",
                "b": "unfortunate",
                "expected answer": [
                    "tragic",
                    "woeful",
                    "grievous",
                    "wretched",
                    "miserable",
                    "awful",
                    "lamentable",
                    "regrettable",
                    "desperate",
                    "hopeless",
                    "disastrous"
                ],
                "predictions": [
                    {
                        "score": 0.5549172895336266,
                        "answer": "implementation",
                        "hit": false
                    },
                    {
                        "score": 0.5360048090409807,
                        "answer": "advertises",
                        "hit": false
                    },
                    {
                        "score": 0.5277230514129558,
                        "answer": "furiously",
                        "hit": false
                    },
                    {
                        "score": 0.5261689677793959,
                        "answer": "flaw",
                        "hit": false
                    },
                    {
                        "score": 0.5195019067115166,
                        "answer": "legally",
                        "hit": false
                    },
                    {
                        "score": 0.5066870148276964,
                        "answer": "timing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "unfortunate"
                ],
                "rank": 1984,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7881136238574982
            },
            {
                "question verbose": "What is to unhappy ",
                "b": "unhappy",
                "expected answer": [
                    "miserable",
                    "suffering",
                    "wretched"
                ],
                "predictions": [
                    {
                        "score": 0.6713631852149096,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.2911108398245625,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.28085137004945476,
                        "answer": "tempted",
                        "hit": false
                    },
                    {
                        "score": 0.27902665297899804,
                        "answer": "reminded",
                        "hit": false
                    },
                    {
                        "score": 0.2787496875552694,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2757869790797999,
                        "answer": "observer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "unhappy"
                ],
                "rank": 4251,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to want ",
                "b": "want",
                "expected answer": [
                    "crave",
                    "hunger",
                    "thirst",
                    "starve",
                    "lust"
                ],
                "predictions": [
                    {
                        "score": 0.2861773456204803,
                        "answer": "accepting",
                        "hit": false
                    },
                    {
                        "score": 0.2794169875295582,
                        "answer": "hearing",
                        "hit": false
                    },
                    {
                        "score": 0.27717072667948417,
                        "answer": "leon",
                        "hit": false
                    },
                    {
                        "score": 0.2691821137988022,
                        "answer": "capability",
                        "hit": false
                    },
                    {
                        "score": 0.26478164358194733,
                        "answer": "except",
                        "hit": false
                    },
                    {
                        "score": 0.26176774473380854,
                        "answer": "apartment",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "want"
                ],
                "rank": 10092,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.628312885761261
            },
            {
                "question verbose": "What is to warm ",
                "b": "warm",
                "expected answer": [
                    "hot",
                    "fiery",
                    "flaming",
                    "heated",
                    "red-hot",
                    "sizzling",
                    "sensual",
                    "sultry",
                    "torrid",
                    "white-hot"
                ],
                "predictions": [
                    {
                        "score": 0.3630138504119681,
                        "answer": "frightening",
                        "hit": false
                    },
                    {
                        "score": 0.3586014967151237,
                        "answer": "wrested",
                        "hit": false
                    },
                    {
                        "score": 0.3582927395885536,
                        "answer": "stimulant",
                        "hit": false
                    },
                    {
                        "score": 0.3562209106219484,
                        "answer": "gripe",
                        "hit": false
                    },
                    {
                        "score": 0.3523095938353754,
                        "answer": "advertises",
                        "hit": false
                    },
                    {
                        "score": 0.3478518929009143,
                        "answer": "nextone",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "warm"
                ],
                "rank": 453,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7502314746379852
            },
            {
                "question verbose": "What is to well ",
                "b": "well",
                "expected answer": [
                    "flourishing",
                    "robust",
                    "booming",
                    "prospering",
                    "prosperous",
                    "thriving"
                ],
                "predictions": [
                    {
                        "score": 0.3149756044491338,
                        "answer": "comp",
                        "hit": false
                    },
                    {
                        "score": 0.2899819390661062,
                        "answer": "lousy",
                        "hit": false
                    },
                    {
                        "score": 0.27715510181288716,
                        "answer": "totally",
                        "hit": false
                    },
                    {
                        "score": 0.2758374120677548,
                        "answer": "delaware",
                        "hit": false
                    },
                    {
                        "score": 0.2729715278636449,
                        "answer": "performing",
                        "hit": false
                    },
                    {
                        "score": 0.2714981084127171,
                        "answer": "indian",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "well"
                ],
                "rank": 763,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6115647330880165
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L07 [synonyms - intensity].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "1940975e-51d4-4035-9e5d-32fa42d6debc",
            "timestamp": "2020-10-22T15:57:53.355997"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to airplane ",
                "b": "airplane",
                "expected answer": [
                    "aeroplane",
                    "plane"
                ],
                "predictions": [
                    {
                        "score": 0.4814726980285074,
                        "answer": "wire",
                        "hit": false
                    },
                    {
                        "score": 0.47694938687061306,
                        "answer": "gregory",
                        "hit": false
                    },
                    {
                        "score": 0.45264041408280203,
                        "answer": "contingent",
                        "hit": false
                    },
                    {
                        "score": 0.45010130896426637,
                        "answer": "delhi",
                        "hit": false
                    },
                    {
                        "score": 0.44953035708650657,
                        "answer": "remoulade",
                        "hit": false
                    },
                    {
                        "score": 0.44312860261497544,
                        "answer": "intrusion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "airplane"
                ],
                "rank": 13971,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.61434605717659
            },
            {
                "question verbose": "What is to auto ",
                "b": "auto",
                "expected answer": [
                    "car",
                    "automobile",
                    "motorcar"
                ],
                "predictions": [
                    {
                        "score": 0.34155573914327514,
                        "answer": "rubber",
                        "hit": false
                    },
                    {
                        "score": 0.3358402920195944,
                        "answer": "fyi",
                        "hit": false
                    },
                    {
                        "score": 0.3346769497512157,
                        "answer": "scream",
                        "hit": false
                    },
                    {
                        "score": 0.331606489706672,
                        "answer": "fitted",
                        "hit": false
                    },
                    {
                        "score": 0.30626085831404526,
                        "answer": "fawning",
                        "hit": false
                    },
                    {
                        "score": 0.30274619242222384,
                        "answer": "examines",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "auto"
                ],
                "rank": 1178,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7190519124269485
            },
            {
                "question verbose": "What is to baby ",
                "b": "baby",
                "expected answer": [
                    "infant",
                    "babe"
                ],
                "predictions": [
                    {
                        "score": 0.2940573513107048,
                        "answer": "inspection",
                        "hit": false
                    },
                    {
                        "score": 0.27439086511982635,
                        "answer": "wire",
                        "hit": false
                    },
                    {
                        "score": 0.273017047045989,
                        "answer": "kill",
                        "hit": false
                    },
                    {
                        "score": 0.2684306224090161,
                        "answer": "iridescent",
                        "hit": false
                    },
                    {
                        "score": 0.265703243768062,
                        "answer": "flashlight",
                        "hit": false
                    },
                    {
                        "score": 0.2579365689488766,
                        "answer": "paranorman",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "baby"
                ],
                "rank": 7029,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6972582340240479
            },
            {
                "question verbose": "What is to bicycle ",
                "b": "bicycle",
                "expected answer": [
                    "bike",
                    "wheel",
                    "cycle"
                ],
                "predictions": [
                    {
                        "score": 0.49929748293088394,
                        "answer": "misleads",
                        "hit": false
                    },
                    {
                        "score": 0.4991183430764044,
                        "answer": "hub",
                        "hit": false
                    },
                    {
                        "score": 0.4719578498461208,
                        "answer": "prominence",
                        "hit": false
                    },
                    {
                        "score": 0.46636210595748523,
                        "answer": "extollo",
                        "hit": false
                    },
                    {
                        "score": 0.4648055514073459,
                        "answer": "lasting",
                        "hit": false
                    },
                    {
                        "score": 0.46175455290962203,
                        "answer": "intrusion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bicycle"
                ],
                "rank": 5021,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7801286578178406
            },
            {
                "question verbose": "What is to child ",
                "b": "child",
                "expected answer": [
                    "kid",
                    "youngster",
                    "minor",
                    "shaver",
                    "nipper",
                    "small_fry",
                    "tiddler",
                    "tike",
                    "tyke",
                    "fry",
                    "nestling"
                ],
                "predictions": [
                    {
                        "score": 0.3381674176121932,
                        "answer": "babysit",
                        "hit": false
                    },
                    {
                        "score": 0.30750184151283055,
                        "answer": "convicted",
                        "hit": false
                    },
                    {
                        "score": 0.2947031949757925,
                        "answer": "pretending",
                        "hit": false
                    },
                    {
                        "score": 0.29376668843164117,
                        "answer": "proactive",
                        "hit": false
                    },
                    {
                        "score": 0.283311118443831,
                        "answer": "whiny",
                        "hit": false
                    },
                    {
                        "score": 0.2825891218268331,
                        "answer": "basket",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "child"
                ],
                "rank": 17,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7116130590438843
            },
            {
                "question verbose": "What is to cloth ",
                "b": "cloth",
                "expected answer": [
                    "fabric",
                    "material",
                    "textile"
                ],
                "predictions": [
                    {
                        "score": 0.41879549368854846,
                        "answer": "rigidly",
                        "hit": false
                    },
                    {
                        "score": 0.4110900947585739,
                        "answer": "hub",
                        "hit": false
                    },
                    {
                        "score": 0.40872500452455507,
                        "answer": "downturn",
                        "hit": false
                    },
                    {
                        "score": 0.40394746387427055,
                        "answer": "shoring",
                        "hit": false
                    },
                    {
                        "score": 0.4034316463339905,
                        "answer": "performing",
                        "hit": false
                    },
                    {
                        "score": 0.40288358851566264,
                        "answer": "oily",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cloth"
                ],
                "rank": 12831,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7840331494808197
            },
            {
                "question verbose": "What is to clothes ",
                "b": "clothes",
                "expected answer": [
                    "clothing",
                    "apparel",
                    "dress"
                ],
                "predictions": [
                    {
                        "score": 0.4437489691176486,
                        "answer": "intrusion",
                        "hit": false
                    },
                    {
                        "score": 0.43864045260950263,
                        "answer": "lasting",
                        "hit": false
                    },
                    {
                        "score": 0.4381900581112805,
                        "answer": "passion",
                        "hit": false
                    },
                    {
                        "score": 0.4378194164140556,
                        "answer": "seriousness",
                        "hit": false
                    },
                    {
                        "score": 0.4311603540490106,
                        "answer": "goodness",
                        "hit": false
                    },
                    {
                        "score": 0.4266425074723186,
                        "answer": "urging",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clothes"
                ],
                "rank": 12458,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6989176124334335
            },
            {
                "question verbose": "What is to confused ",
                "b": "confused",
                "expected answer": [
                    "baffled",
                    "befuddled",
                    "bemused",
                    "bewildered",
                    "confounded",
                    "lost",
                    "mazed",
                    "mixed-up"
                ],
                "predictions": [
                    {
                        "score": 0.3607831176394187,
                        "answer": "hanh",
                        "hit": false
                    },
                    {
                        "score": 0.355788592651295,
                        "answer": "credible",
                        "hit": false
                    },
                    {
                        "score": 0.35412215437373457,
                        "answer": "gridlock",
                        "hit": false
                    },
                    {
                        "score": 0.35373758891878254,
                        "answer": "tanked",
                        "hit": false
                    },
                    {
                        "score": 0.34231085941175793,
                        "answer": "zone",
                        "hit": false
                    },
                    {
                        "score": 0.34217041897992684,
                        "answer": "glean",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "confused"
                ],
                "rank": 11212,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6346025168895721
            },
            {
                "question verbose": "What is to dollars ",
                "b": "dollars",
                "expected answer": [
                    "bucks"
                ],
                "predictions": [
                    {
                        "score": 0.7171106289852306,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3307265450988663,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3096226881860568,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.29263448717825324,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2923710349345621,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2856081450537825,
                        "answer": "skype",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dollars"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to emphasis ",
                "b": "emphasis",
                "expected answer": [
                    "accent",
                    "accentuaion",
                    "importance",
                    "stress",
                    "significance"
                ],
                "predictions": [
                    {
                        "score": 0.4813006329595992,
                        "answer": "loosie",
                        "hit": false
                    },
                    {
                        "score": 0.4672002340829069,
                        "answer": "placing",
                        "hit": false
                    },
                    {
                        "score": 0.46302854361603296,
                        "answer": "scream",
                        "hit": false
                    },
                    {
                        "score": 0.46118794826607507,
                        "answer": "inglis",
                        "hit": false
                    },
                    {
                        "score": 0.4601369168038132,
                        "answer": "seriousness",
                        "hit": false
                    },
                    {
                        "score": 0.4585465318156527,
                        "answer": "hub",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "emphasis"
                ],
                "rank": 9811,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6306777447462082
            },
            {
                "question verbose": "What is to father ",
                "b": "father",
                "expected answer": [
                    "dad",
                    "daddy"
                ],
                "predictions": [
                    {
                        "score": 0.2741921672346742,
                        "answer": "linkedin",
                        "hit": false
                    },
                    {
                        "score": 0.27297271828083464,
                        "answer": "rightly",
                        "hit": false
                    },
                    {
                        "score": 0.2656635027841324,
                        "answer": "ferry",
                        "hit": false
                    },
                    {
                        "score": 0.2655918607499532,
                        "answer": "childhood",
                        "hit": false
                    },
                    {
                        "score": 0.2613792630093842,
                        "answer": "ak",
                        "hit": false
                    },
                    {
                        "score": 0.25826842514831316,
                        "answer": "reorganized",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "father"
                ],
                "rank": 4950,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6232402622699738
            },
            {
                "question verbose": "What is to flower ",
                "b": "flower",
                "expected answer": [
                    "blossom",
                    "bloom"
                ],
                "predictions": [
                    {
                        "score": 0.4568525626129339,
                        "answer": "hethey",
                        "hit": false
                    },
                    {
                        "score": 0.446524895186571,
                        "answer": "capitalize",
                        "hit": false
                    },
                    {
                        "score": 0.4460505561838538,
                        "answer": "homeland",
                        "hit": false
                    },
                    {
                        "score": 0.44016349848441216,
                        "answer": "bachmann",
                        "hit": false
                    },
                    {
                        "score": 0.42663619592977203,
                        "answer": "occupier",
                        "hit": false
                    },
                    {
                        "score": 0.422096362346001,
                        "answer": "dotty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "flower"
                ],
                "rank": 9224,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.671718180179596
            },
            {
                "question verbose": "What is to harbor ",
                "b": "harbor",
                "expected answer": [
                    "seaport",
                    "haven",
                    "harbour"
                ],
                "predictions": [
                    {
                        "score": 0.4703947287016351,
                        "answer": "hethey",
                        "hit": false
                    },
                    {
                        "score": 0.45201110347063356,
                        "answer": "testament",
                        "hit": false
                    },
                    {
                        "score": 0.44626183570566164,
                        "answer": "rutger",
                        "hit": false
                    },
                    {
                        "score": 0.44219357902088424,
                        "answer": "chestertown",
                        "hit": false
                    },
                    {
                        "score": 0.44197382661518275,
                        "answer": "schnauzer",
                        "hit": false
                    },
                    {
                        "score": 0.4384851510228177,
                        "answer": "homeland",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "harbor"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6535837650299072
            },
            {
                "question verbose": "What is to help ",
                "b": "help",
                "expected answer": [
                    "aid",
                    "assist"
                ],
                "predictions": [
                    {
                        "score": 0.3331071118689638,
                        "answer": "referral",
                        "hit": false
                    },
                    {
                        "score": 0.32801826151843766,
                        "answer": "healing",
                        "hit": false
                    },
                    {
                        "score": 0.3261463655068893,
                        "answer": "verbally",
                        "hit": false
                    },
                    {
                        "score": 0.32381804131230335,
                        "answer": "friction",
                        "hit": false
                    },
                    {
                        "score": 0.3221060028045242,
                        "answer": "pasturing",
                        "hit": false
                    },
                    {
                        "score": 0.31522889638322144,
                        "answer": "assemble",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "help"
                ],
                "rank": 11886,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.589594803750515
            },
            {
                "question verbose": "What is to hieroglyph ",
                "b": "hieroglyph",
                "expected answer": [
                    "hieroglyphic",
                    "pictogram"
                ],
                "predictions": [
                    {
                        "score": 0.715265912241432,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30762522507190837,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2880022822264811,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28632392979144883,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2847142302397535,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2805017745781633,
                        "answer": "symbol",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hieroglyph"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to homogeneous ",
                "b": "homogeneous",
                "expected answer": [
                    "uniform",
                    "unvarying"
                ],
                "predictions": [
                    {
                        "score": 0.7271297233127221,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30802475089020925,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3049113054611332,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.29044643780605056,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.27746417427511955,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2630045631386437,
                        "answer": "symbol",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "homogeneous"
                ],
                "rank": 2083,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6507170498371124
            },
            {
                "question verbose": "What is to honest ",
                "b": "honest",
                "expected answer": [
                    "sincere",
                    "ingenuous",
                    "true",
                    "direct",
                    "truthful"
                ],
                "predictions": [
                    {
                        "score": 0.35795394853862766,
                        "answer": "ideology",
                        "hit": false
                    },
                    {
                        "score": 0.35260427143924694,
                        "answer": "automatically",
                        "hit": false
                    },
                    {
                        "score": 0.3518244480689885,
                        "answer": "factual",
                        "hit": false
                    },
                    {
                        "score": 0.34639616975649323,
                        "answer": "principled",
                        "hit": false
                    },
                    {
                        "score": 0.3457256537097914,
                        "answer": "latent",
                        "hit": false
                    },
                    {
                        "score": 0.343392846240419,
                        "answer": "generosity",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "honest"
                ],
                "rank": 1250,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6790069937705994
            },
            {
                "question verbose": "What is to identical ",
                "b": "identical",
                "expected answer": [
                    "same",
                    "indistinguishable"
                ],
                "predictions": [
                    {
                        "score": 0.5416308549437285,
                        "answer": "request",
                        "hit": false
                    },
                    {
                        "score": 0.5247663613630341,
                        "answer": "commercialization",
                        "hit": false
                    },
                    {
                        "score": 0.5208383381895748,
                        "answer": "everett",
                        "hit": false
                    },
                    {
                        "score": 0.5178025669043378,
                        "answer": "emerging",
                        "hit": false
                    },
                    {
                        "score": 0.5134410341935932,
                        "answer": "bonanza",
                        "hit": false
                    },
                    {
                        "score": 0.5090120328882813,
                        "answer": "freddie",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "identical"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6400229036808014
            },
            {
                "question verbose": "What is to incorrect ",
                "b": "incorrect",
                "expected answer": [
                    "wrong",
                    "counterfactual",
                    "erroneous",
                    "inaccurate",
                    "specious",
                    "unsound",
                    "untrue",
                    "false"
                ],
                "predictions": [
                    {
                        "score": 0.4703578718126022,
                        "answer": "partially",
                        "hit": false
                    },
                    {
                        "score": 0.444264776815629,
                        "answer": "consensus",
                        "hit": false
                    },
                    {
                        "score": 0.4309652517750959,
                        "answer": "consideration",
                        "hit": false
                    },
                    {
                        "score": 0.41739364320775135,
                        "answer": "advancing",
                        "hit": false
                    },
                    {
                        "score": 0.4160775099427949,
                        "answer": "salicylate",
                        "hit": false
                    },
                    {
                        "score": 0.4124903619717206,
                        "answer": "establish",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "incorrect"
                ],
                "rank": 1247,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6163651719689369
            },
            {
                "question verbose": "What is to intelligent ",
                "b": "intelligent",
                "expected answer": [
                    "clever",
                    "smart"
                ],
                "predictions": [
                    {
                        "score": 0.3511443138340474,
                        "answer": "assure",
                        "hit": false
                    },
                    {
                        "score": 0.3394549705925695,
                        "answer": "futuristic",
                        "hit": false
                    },
                    {
                        "score": 0.3384703500851656,
                        "answer": "poland",
                        "hit": false
                    },
                    {
                        "score": 0.334710941316076,
                        "answer": "occurs",
                        "hit": false
                    },
                    {
                        "score": 0.3339267796945939,
                        "answer": "telephone",
                        "hit": false
                    },
                    {
                        "score": 0.331542429387122,
                        "answer": "identifies",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "intelligent"
                ],
                "rank": 4057,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7304646074771881
            },
            {
                "question verbose": "What is to jewel ",
                "b": "jewel",
                "expected answer": [
                    "gem",
                    "stone"
                ],
                "predictions": [
                    {
                        "score": 0.7193235656421754,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3270261941849637,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3015039027173097,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2922765160205483,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2862598163911673,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.2844286274737701,
                        "answer": "perhaps",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "jewel"
                ],
                "rank": 9234,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lad ",
                "b": "lad",
                "expected answer": [
                    "chap",
                    "fellow",
                    "feller",
                    "fella",
                    "gent",
                    "blighter",
                    "cuss",
                    "bloke"
                ],
                "predictions": [
                    {
                        "score": 0.7166321039095794,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3215291699164056,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3075930121707888,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.29918919937120486,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2862672180549534,
                        "answer": "restrict",
                        "hit": false
                    },
                    {
                        "score": 0.28420421944492036,
                        "answer": "skype",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lad"
                ],
                "rank": 6996,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to lady ",
                "b": "lady",
                "expected answer": [
                    "madam",
                    "dame",
                    "ma'am",
                    "gentlewoman",
                    "madame",
                    "woman"
                ],
                "predictions": [
                    {
                        "score": 0.43848900931157986,
                        "answer": "brutally",
                        "hit": false
                    },
                    {
                        "score": 0.39845905243308033,
                        "answer": "honorary",
                        "hit": false
                    },
                    {
                        "score": 0.3820833067354893,
                        "answer": "shakespeare",
                        "hit": false
                    },
                    {
                        "score": 0.3726996452995127,
                        "answer": "diego",
                        "hit": false
                    },
                    {
                        "score": 0.3710500585018418,
                        "answer": "rutger",
                        "hit": false
                    },
                    {
                        "score": 0.3687374918213037,
                        "answer": "carey",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lady"
                ],
                "rank": 12260,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6031798869371414
            },
            {
                "question verbose": "What is to lazy ",
                "b": "lazy",
                "expected answer": [
                    "indolent",
                    "faineant",
                    "otiose",
                    "slothful",
                    "work-shy"
                ],
                "predictions": [
                    {
                        "score": 0.3529985785897573,
                        "answer": "arrogant",
                        "hit": false
                    },
                    {
                        "score": 0.3411515611865106,
                        "answer": "schnauzer",
                        "hit": false
                    },
                    {
                        "score": 0.33783044720823424,
                        "answer": "hoard",
                        "hit": false
                    },
                    {
                        "score": 0.3345812820384512,
                        "answer": "plastic",
                        "hit": false
                    },
                    {
                        "score": 0.324861970784097,
                        "answer": "pithy",
                        "hit": false
                    },
                    {
                        "score": 0.324126312609947,
                        "answer": "stumbled",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lazy"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6205447390675545
            },
            {
                "question verbose": "What is to list ",
                "b": "list",
                "expected answer": [
                    "listing",
                    "enumeration"
                ],
                "predictions": [
                    {
                        "score": 0.30149302140238515,
                        "answer": "blazing",
                        "hit": false
                    },
                    {
                        "score": 0.2991371685833955,
                        "answer": "contacted",
                        "hit": false
                    },
                    {
                        "score": 0.29678245045746093,
                        "answer": "shared",
                        "hit": false
                    },
                    {
                        "score": 0.288931994310595,
                        "answer": "crashed",
                        "hit": false
                    },
                    {
                        "score": 0.27849646996960864,
                        "answer": "write",
                        "hit": false
                    },
                    {
                        "score": 0.2753199705537341,
                        "answer": "ceremony",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "list"
                ],
                "rank": 4686,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6389162689447403
            },
            {
                "question verbose": "What is to loyal ",
                "b": "loyal",
                "expected answer": [
                    "faithful",
                    "true",
                    "dependable",
                    "devoted",
                    "reliable"
                ],
                "predictions": [
                    {
                        "score": 0.47200270229096547,
                        "answer": "intrusion",
                        "hit": false
                    },
                    {
                        "score": 0.4595163539106795,
                        "answer": "disgrace",
                        "hit": false
                    },
                    {
                        "score": 0.45631054930845805,
                        "answer": "touchstone",
                        "hit": false
                    },
                    {
                        "score": 0.4473911542638396,
                        "answer": "tanked",
                        "hit": false
                    },
                    {
                        "score": 0.44108274615868576,
                        "answer": "scream",
                        "hit": false
                    },
                    {
                        "score": 0.4402413319071048,
                        "answer": "hub",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "loyal"
                ],
                "rank": 5472,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8850874900817871
            },
            {
                "question verbose": "What is to market ",
                "b": "market",
                "expected answer": [
                    "marketplace",
                    "mart",
                    "bazaar"
                ],
                "predictions": [
                    {
                        "score": 0.29278951088919003,
                        "answer": "bn",
                        "hit": false
                    },
                    {
                        "score": 0.2882465207315748,
                        "answer": "accumulated",
                        "hit": false
                    },
                    {
                        "score": 0.2822863449043718,
                        "answer": "obeys",
                        "hit": false
                    },
                    {
                        "score": 0.2784130218229412,
                        "answer": "cog",
                        "hit": false
                    },
                    {
                        "score": 0.27731662060278045,
                        "answer": "cash",
                        "hit": false
                    },
                    {
                        "score": 0.2715993900386754,
                        "answer": "utilizing",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "market"
                ],
                "rank": 4540,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6988775134086609
            },
            {
                "question verbose": "What is to mend ",
                "b": "mend",
                "expected answer": [
                    "repair",
                    "fix",
                    "fixing",
                    "fixture",
                    "mending",
                    "reparation",
                    "patch",
                    "darn",
                    "darning"
                ],
                "predictions": [
                    {
                        "score": 0.7312815869211787,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31687592457826036,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.2941170457186612,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2867773154435161,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28621984655886146,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.28612030412155465,
                        "answer": "skype",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mend"
                ],
                "rank": 2872,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.4877019599080086
            },
            {
                "question verbose": "What is to mesh ",
                "b": "mesh",
                "expected answer": [
                    "gauze",
                    "netting",
                    "veiling",
                    "hairnet",
                    "reseau",
                    "net",
                    "save-all",
                    "snood",
                    "sparker",
                    "tulle",
                    "wirework",
                    "grillwork"
                ],
                "predictions": [
                    {
                        "score": 0.545277023554128,
                        "answer": "loosie",
                        "hit": false
                    },
                    {
                        "score": 0.5315938248729226,
                        "answer": "contingent",
                        "hit": false
                    },
                    {
                        "score": 0.5259499750658778,
                        "answer": "disabled",
                        "hit": false
                    },
                    {
                        "score": 0.5231720448251714,
                        "answer": "capitalize",
                        "hit": false
                    },
                    {
                        "score": 0.5229891199969495,
                        "answer": "creationism",
                        "hit": false
                    },
                    {
                        "score": 0.5215305703523196,
                        "answer": "celtic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mesh"
                ],
                "rank": 9913,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6283604055643082
            },
            {
                "question verbose": "What is to monument ",
                "b": "monument",
                "expected answer": [
                    "memorial"
                ],
                "predictions": [
                    {
                        "score": 0.42466446908685024,
                        "answer": "privately",
                        "hit": false
                    },
                    {
                        "score": 0.4183762845902539,
                        "answer": "originated",
                        "hit": false
                    },
                    {
                        "score": 0.38750064929869404,
                        "answer": "eagle",
                        "hit": false
                    },
                    {
                        "score": 0.3691285261556991,
                        "answer": "disagreed",
                        "hit": false
                    },
                    {
                        "score": 0.36709129398741003,
                        "answer": "symbol",
                        "hit": false
                    },
                    {
                        "score": 0.3519939193603836,
                        "answer": "religious",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "monument"
                ],
                "rank": 154,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7877764403820038
            },
            {
                "question verbose": "What is to mother ",
                "b": "mother",
                "expected answer": [
                    "mom",
                    "mommy",
                    "mum"
                ],
                "predictions": [
                    {
                        "score": 0.28056128244744766,
                        "answer": "altar",
                        "hit": false
                    },
                    {
                        "score": 0.2704198880674832,
                        "answer": "defendant",
                        "hit": false
                    },
                    {
                        "score": 0.26415858491115063,
                        "answer": "anonymously",
                        "hit": false
                    },
                    {
                        "score": 0.2633253576380878,
                        "answer": "butter",
                        "hit": false
                    },
                    {
                        "score": 0.2606372759146411,
                        "answer": "helpless",
                        "hit": false
                    },
                    {
                        "score": 0.25925689043432787,
                        "answer": "impressive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mother"
                ],
                "rank": 4522,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6315865963697433
            },
            {
                "question verbose": "What is to murder ",
                "b": "murder",
                "expected answer": [
                    "slaying",
                    "slay",
                    "execution"
                ],
                "predictions": [
                    {
                        "score": 0.31727843193674454,
                        "answer": "tr",
                        "hit": false
                    },
                    {
                        "score": 0.3131537414095956,
                        "answer": "converted",
                        "hit": false
                    },
                    {
                        "score": 0.3124080153373719,
                        "answer": "touchstone",
                        "hit": false
                    },
                    {
                        "score": 0.30946693672936265,
                        "answer": "civility",
                        "hit": false
                    },
                    {
                        "score": 0.3061043540128757,
                        "answer": "undeniable",
                        "hit": false
                    },
                    {
                        "score": 0.30381674957025717,
                        "answer": "inconsistency",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "murder"
                ],
                "rank": 1162,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5786032974720001
            },
            {
                "question verbose": "What is to new ",
                "b": "new",
                "expected answer": [
                    "modern",
                    "recent"
                ],
                "predictions": [
                    {
                        "score": 0.3860512810563732,
                        "answer": "zealand",
                        "hit": false
                    },
                    {
                        "score": 0.37022382708842977,
                        "answer": "gras",
                        "hit": false
                    },
                    {
                        "score": 0.3688894004880558,
                        "answer": "york",
                        "hit": false
                    },
                    {
                        "score": 0.34827745538978594,
                        "answer": "orleans",
                        "hit": false
                    },
                    {
                        "score": 0.3279415668552551,
                        "answer": "haigh",
                        "hit": false
                    },
                    {
                        "score": 0.3212346100642732,
                        "answer": "mardi",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "new"
                ],
                "rank": 9451,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5755161717534065
            },
            {
                "question verbose": "What is to obsolete ",
                "b": "obsolete",
                "expected answer": [
                    "outdated",
                    "out-of-date",
                    "superannuated"
                ],
                "predictions": [
                    {
                        "score": 0.47764982769167297,
                        "answer": "striving",
                        "hit": false
                    },
                    {
                        "score": 0.45716107102256964,
                        "answer": "contingent",
                        "hit": false
                    },
                    {
                        "score": 0.45593532419954264,
                        "answer": "counselor",
                        "hit": false
                    },
                    {
                        "score": 0.4467245213850234,
                        "answer": "enrollment",
                        "hit": false
                    },
                    {
                        "score": 0.44096565196227966,
                        "answer": "gridlock",
                        "hit": false
                    },
                    {
                        "score": 0.4324189164698806,
                        "answer": "graduation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "obsolete"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6318370550870895
            },
            {
                "question verbose": "What is to organized ",
                "b": "organized",
                "expected answer": [
                    "arranged",
                    "configured",
                    "corporate",
                    "incorporated",
                    "re-formed",
                    "reorganized",
                    "reorganised"
                ],
                "predictions": [
                    {
                        "score": 0.5783519493249882,
                        "answer": "presbyterian",
                        "hit": false
                    },
                    {
                        "score": 0.5485695094612012,
                        "answer": "fayetteville",
                        "hit": false
                    },
                    {
                        "score": 0.542487580699947,
                        "answer": "bennett",
                        "hit": false
                    },
                    {
                        "score": 0.5359652643719408,
                        "answer": "jordan",
                        "hit": false
                    },
                    {
                        "score": 0.5254868696564086,
                        "answer": "unveiled",
                        "hit": false
                    },
                    {
                        "score": 0.5077552320694334,
                        "answer": "proclamation",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "organized"
                ],
                "rank": 159,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7404337674379349
            },
            {
                "question verbose": "What is to package ",
                "b": "package",
                "expected answer": [
                    "parcel",
                    "pack",
                    "packet",
                    "bundle"
                ],
                "predictions": [
                    {
                        "score": 0.37640869013834327,
                        "answer": "sank",
                        "hit": false
                    },
                    {
                        "score": 0.35394065540800773,
                        "answer": "insert",
                        "hit": false
                    },
                    {
                        "score": 0.34754975175804975,
                        "answer": "operandi",
                        "hit": false
                    },
                    {
                        "score": 0.34644859334273054,
                        "answer": "exploding",
                        "hit": false
                    },
                    {
                        "score": 0.33931439351247855,
                        "answer": "gesture",
                        "hit": false
                    },
                    {
                        "score": 0.3370289474703558,
                        "answer": "motion",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "package"
                ],
                "rank": 200,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7214062213897705
            },
            {
                "question verbose": "What is to phone ",
                "b": "phone",
                "expected answer": [
                    "telephone",
                    "cell",
                    "cellphone",
                    "smartphone"
                ],
                "predictions": [
                    {
                        "score": 0.4015056087750032,
                        "answer": "usb",
                        "hit": false
                    },
                    {
                        "score": 0.4003791828894318,
                        "answer": "app",
                        "hit": false
                    },
                    {
                        "score": 0.3675426397340901,
                        "answer": "lavish",
                        "hit": false
                    },
                    {
                        "score": 0.3508907860502745,
                        "answer": "updated",
                        "hit": false
                    },
                    {
                        "score": 0.34593303068486536,
                        "answer": "hathaway",
                        "hit": false
                    },
                    {
                        "score": 0.342602746800645,
                        "answer": "playback",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "phone"
                ],
                "rank": 12,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7232188135385513
            },
            {
                "question verbose": "What is to portion ",
                "b": "portion",
                "expected answer": [
                    "part",
                    "component_part",
                    "component",
                    "constituent"
                ],
                "predictions": [
                    {
                        "score": 0.3642651477541773,
                        "answer": "verbally",
                        "hit": false
                    },
                    {
                        "score": 0.35887557180662033,
                        "answer": "bakken",
                        "hit": false
                    },
                    {
                        "score": 0.3567586763152453,
                        "answer": "monitor",
                        "hit": false
                    },
                    {
                        "score": 0.34491591747875694,
                        "answer": "honour",
                        "hit": false
                    },
                    {
                        "score": 0.3415113204969027,
                        "answer": "manitoba",
                        "hit": false
                    },
                    {
                        "score": 0.33986625087780165,
                        "answer": "backwards",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "portion"
                ],
                "rank": 1866,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6416252851486206
            },
            {
                "question verbose": "What is to railway ",
                "b": "railway",
                "expected answer": [
                    "railroad"
                ],
                "predictions": [
                    {
                        "score": 0.466363393127673,
                        "answer": "hethey",
                        "hit": false
                    },
                    {
                        "score": 0.465575521574576,
                        "answer": "stone",
                        "hit": false
                    },
                    {
                        "score": 0.461787513541349,
                        "answer": "karyle",
                        "hit": false
                    },
                    {
                        "score": 0.4520356884191812,
                        "answer": "courtyard",
                        "hit": false
                    },
                    {
                        "score": 0.4516160033798248,
                        "answer": "slurpy",
                        "hit": false
                    },
                    {
                        "score": 0.4503807925806872,
                        "answer": "louisiana",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "railway"
                ],
                "rank": 2694,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.761741578578949
            },
            {
                "question verbose": "What is to rational ",
                "b": "rational",
                "expected answer": [
                    "logical",
                    "coherent",
                    "reasonable",
                    "sane"
                ],
                "predictions": [
                    {
                        "score": 0.3316383626311491,
                        "answer": "suspense",
                        "hit": false
                    },
                    {
                        "score": 0.32837919236571417,
                        "answer": "equipping",
                        "hit": false
                    },
                    {
                        "score": 0.3270528589147992,
                        "answer": "assessment",
                        "hit": false
                    },
                    {
                        "score": 0.32395556353635635,
                        "answer": "lens",
                        "hit": false
                    },
                    {
                        "score": 0.3231713941489664,
                        "answer": "kaminska",
                        "hit": false
                    },
                    {
                        "score": 0.3213915860149892,
                        "answer": "fried",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rational"
                ],
                "rank": 641,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7764697670936584
            },
            {
                "question verbose": "What is to reasonable ",
                "b": "reasonable",
                "expected answer": [
                    "sensible"
                ],
                "predictions": [
                    {
                        "score": 0.3785816910487551,
                        "answer": "breakdown",
                        "hit": false
                    },
                    {
                        "score": 0.36761268123063756,
                        "answer": "schnauzer",
                        "hit": false
                    },
                    {
                        "score": 0.36249577145437295,
                        "answer": "spendy",
                        "hit": false
                    },
                    {
                        "score": 0.35614439020548366,
                        "answer": "utilizing",
                        "hit": false
                    },
                    {
                        "score": 0.35564460560733574,
                        "answer": "photoshopping",
                        "hit": false
                    },
                    {
                        "score": 0.3551318944287578,
                        "answer": "bankrupting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "reasonable"
                ],
                "rank": 2539,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7654047906398773
            },
            {
                "question verbose": "What is to rock ",
                "b": "rock",
                "expected answer": [
                    "stone"
                ],
                "predictions": [
                    {
                        "score": 0.3657061366169571,
                        "answer": "rutger",
                        "hit": false
                    },
                    {
                        "score": 0.3416906351835251,
                        "answer": "hurry",
                        "hit": false
                    },
                    {
                        "score": 0.33241869042343114,
                        "answer": "railroad",
                        "hit": false
                    },
                    {
                        "score": 0.31722730418224915,
                        "answer": "cotillard",
                        "hit": false
                    },
                    {
                        "score": 0.3144277620758438,
                        "answer": "concert",
                        "hit": false
                    },
                    {
                        "score": 0.31430308928034284,
                        "answer": "resembles",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rock"
                ],
                "rank": 1028,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6696389019489288
            },
            {
                "question verbose": "What is to shore ",
                "b": "shore",
                "expected answer": [
                    "coast",
                    "strand",
                    "bank",
                    "beach",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.43390993191855326,
                        "answer": "mesh",
                        "hit": false
                    },
                    {
                        "score": 0.42628959635871105,
                        "answer": "externality",
                        "hit": false
                    },
                    {
                        "score": 0.4262220659099377,
                        "answer": "contingent",
                        "hit": false
                    },
                    {
                        "score": 0.42409345944991,
                        "answer": "advancing",
                        "hit": false
                    },
                    {
                        "score": 0.4225007083077174,
                        "answer": "shearlings",
                        "hit": false
                    },
                    {
                        "score": 0.4163688663226554,
                        "answer": "scream",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "shore"
                ],
                "rank": 7352,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6963171362876892
            },
            {
                "question verbose": "What is to snake ",
                "b": "snake",
                "expected answer": [
                    "serpent",
                    "ophidian"
                ],
                "predictions": [
                    {
                        "score": 0.49213963567019786,
                        "answer": "advancing",
                        "hit": false
                    },
                    {
                        "score": 0.4839834163249944,
                        "answer": "throrough",
                        "hit": false
                    },
                    {
                        "score": 0.47895737544710154,
                        "answer": "nimble",
                        "hit": false
                    },
                    {
                        "score": 0.4744687542941122,
                        "answer": "obeys",
                        "hit": false
                    },
                    {
                        "score": 0.47389601119275787,
                        "answer": "intrusion",
                        "hit": false
                    },
                    {
                        "score": 0.47283707722214946,
                        "answer": "downturn",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "snake"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6700489372014999
            },
            {
                "question verbose": "What is to sofa ",
                "b": "sofa",
                "expected answer": [
                    "couch",
                    "lounge"
                ],
                "predictions": [
                    {
                        "score": 0.7308693537823095,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3232184203442924,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.3192203822341194,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.29138239888587086,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.2837552035716859,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.2823711315142587,
                        "answer": "chinese",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sofa"
                ],
                "rank": 7652,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6081694439053535
            },
            {
                "question verbose": "What is to spouse ",
                "b": "spouse",
                "expected answer": [
                    "partner",
                    "mate",
                    "better_half"
                ],
                "predictions": [
                    {
                        "score": 0.4294938029986823,
                        "answer": "civility",
                        "hit": false
                    },
                    {
                        "score": 0.42596367375746946,
                        "answer": "schnauzer",
                        "hit": false
                    },
                    {
                        "score": 0.40450434901635884,
                        "answer": "disgrace",
                        "hit": false
                    },
                    {
                        "score": 0.3983083750882174,
                        "answer": "enthused",
                        "hit": false
                    },
                    {
                        "score": 0.3978571743157653,
                        "answer": "latent",
                        "hit": false
                    },
                    {
                        "score": 0.3953805151183929,
                        "answer": "scream",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "spouse"
                ],
                "rank": 7212,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.612902544438839
            },
            {
                "question verbose": "What is to style ",
                "b": "style",
                "expected answer": [
                    "manner",
                    "mode",
                    "fashion",
                    "way"
                ],
                "predictions": [
                    {
                        "score": 0.2979496461036374,
                        "answer": "verbally",
                        "hit": false
                    },
                    {
                        "score": 0.2944933228327585,
                        "answer": "grazier",
                        "hit": false
                    },
                    {
                        "score": 0.28822018018860845,
                        "answer": "donation",
                        "hit": false
                    },
                    {
                        "score": 0.2868362782491323,
                        "answer": "brainless",
                        "hit": false
                    },
                    {
                        "score": 0.2867717512315657,
                        "answer": "modern",
                        "hit": false
                    },
                    {
                        "score": 0.2857935292480915,
                        "answer": "arena",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "style"
                ],
                "rank": 432,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6605008244514465
            },
            {
                "question verbose": "What is to sweets ",
                "b": "sweets",
                "expected answer": [
                    "confectionery",
                    "dessert",
                    "confection"
                ],
                "predictions": [
                    {
                        "score": 0.7168653889113042,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3080256657378166,
                        "answer": "harm",
                        "hit": false
                    },
                    {
                        "score": 0.29507822933861166,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.28500046651415695,
                        "answer": "perhaps",
                        "hit": false
                    },
                    {
                        "score": 0.28244344829849943,
                        "answer": "suppression",
                        "hit": false
                    },
                    {
                        "score": 0.27514984629952666,
                        "answer": "restrict",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sweets"
                ],
                "rank": 12265,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to villain ",
                "b": "villain",
                "expected answer": [
                    "scoundrel",
                    "rascal"
                ],
                "predictions": [
                    {
                        "score": 0.4089882624523011,
                        "answer": "hathaway",
                        "hit": false
                    },
                    {
                        "score": 0.3958387495994369,
                        "answer": "gruden",
                        "hit": false
                    },
                    {
                        "score": 0.3791831812374893,
                        "answer": "bedwell",
                        "hit": false
                    },
                    {
                        "score": 0.3750156666905902,
                        "answer": "ol",
                        "hit": false
                    },
                    {
                        "score": 0.37417118617017275,
                        "answer": "doom",
                        "hit": false
                    },
                    {
                        "score": 0.3715554974100298,
                        "answer": "lovely",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "villain"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6376533210277557
            },
            {
                "question verbose": "What is to vocabulary ",
                "b": "vocabulary",
                "expected answer": [
                    "lexicon",
                    "wordbook",
                    "dictionary",
                    "glossary"
                ],
                "predictions": [
                    {
                        "score": 0.4140985502573949,
                        "answer": "visualization",
                        "hit": false
                    },
                    {
                        "score": 0.41121587620624955,
                        "answer": "skype",
                        "hit": false
                    },
                    {
                        "score": 0.40725966150494725,
                        "answer": "wasson",
                        "hit": false
                    },
                    {
                        "score": 0.4071210676198694,
                        "answer": "lasting",
                        "hit": false
                    },
                    {
                        "score": 0.4069165068077873,
                        "answer": "operandi",
                        "hit": false
                    },
                    {
                        "score": 0.4058492941354054,
                        "answer": "polar",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "vocabulary"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.596232570707798
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L08 [synonyms - exact].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "2e046c48-2b76-4c62-ac05-32ebe666fcee",
            "timestamp": "2020-10-22T15:57:54.558884"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to able ",
                "b": "able",
                "expected answer": [
                    "unable",
                    "incapable",
                    "incompetent",
                    "unequal"
                ],
                "predictions": [
                    {
                        "score": 0.3670532824387007,
                        "answer": "steam",
                        "hit": false
                    },
                    {
                        "score": 0.3443831685028382,
                        "answer": "poppa",
                        "hit": false
                    },
                    {
                        "score": 0.33204478985418556,
                        "answer": "accommodate",
                        "hit": false
                    },
                    {
                        "score": 0.3271172260581819,
                        "answer": "dependency",
                        "hit": false
                    },
                    {
                        "score": 0.3248878151124843,
                        "answer": "heft",
                        "hit": false
                    },
                    {
                        "score": 0.3224468580882048,
                        "answer": "sc",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "able"
                ],
                "rank": 269,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7326332479715347
            },
            {
                "question verbose": "What is to abundant ",
                "b": "abundant",
                "expected answer": [
                    "scarce",
                    "rare",
                    "tight",
                    "meager",
                    "meagre",
                    "meagerly",
                    "stingy",
                    "scrimpy",
                    "insufficient",
                    "deficient"
                ],
                "predictions": [
                    {
                        "score": 0.639698293735249,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3367459233504517,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.31401351348731316,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.30960819862285793,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.30705141743676845,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.3020059628056946,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "abundant"
                ],
                "rank": 387,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5990625321865082
            },
            {
                "question verbose": "What is to aware ",
                "b": "aware",
                "expected answer": [
                    "unaware",
                    "oblivious",
                    "unmindful",
                    "unconscious",
                    "unsuspecting",
                    "asleep",
                    "insensible",
                    "unconscious",
                    "unwitting",
                    "ignorant",
                    "indifferent",
                    "oblivious"
                ],
                "predictions": [
                    {
                        "score": 0.3876936185160966,
                        "answer": "optic",
                        "hit": false
                    },
                    {
                        "score": 0.37470817996620337,
                        "answer": "accommodate",
                        "hit": false
                    },
                    {
                        "score": 0.370298955817161,
                        "answer": "exercise",
                        "hit": false
                    },
                    {
                        "score": 0.36949308131399217,
                        "answer": "nap",
                        "hit": false
                    },
                    {
                        "score": 0.3669461279547845,
                        "answer": "admire",
                        "hit": false
                    },
                    {
                        "score": 0.36609126174249895,
                        "answer": "sophistication",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "aware"
                ],
                "rank": 424,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7151608467102051
            },
            {
                "question verbose": "What is to beautiful ",
                "b": "beautiful",
                "expected answer": [
                    "ugly",
                    "disfigured",
                    "evil-looking",
                    "fugly",
                    "grotesque",
                    "monstrous",
                    "hideous",
                    "repulsive",
                    "ill-favored",
                    "ill-favoured",
                    "scrofulous",
                    "unlovely",
                    "unpicturesque",
                    "unsightly",
                    "displeasing",
                    "unattractive",
                    "awkward"
                ],
                "predictions": [
                    {
                        "score": 0.3593968480053934,
                        "answer": "lately",
                        "hit": false
                    },
                    {
                        "score": 0.35610916865604203,
                        "answer": "fantasize",
                        "hit": false
                    },
                    {
                        "score": 0.35510380985819645,
                        "answer": "lisa",
                        "hit": false
                    },
                    {
                        "score": 0.3525268942651144,
                        "answer": "visage",
                        "hit": false
                    },
                    {
                        "score": 0.3523113367922781,
                        "answer": "springpad",
                        "hit": false
                    },
                    {
                        "score": 0.3521985589417758,
                        "answer": "dazzle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beautiful"
                ],
                "rank": 480,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7186542451381683
            },
            {
                "question verbose": "What is to big ",
                "b": "big",
                "expected answer": [
                    "small",
                    "atomic",
                    "subatomic",
                    "bantam",
                    "diminutive",
                    "lilliputian",
                    "midget",
                    "petite",
                    "tiny",
                    "flyspeck",
                    "bitty",
                    "bittie",
                    "teensy",
                    "teentsy",
                    "teeny",
                    "wee",
                    "weeny",
                    "weensy",
                    "teensy-weensy",
                    "teeny-weeny",
                    "itty-bitty",
                    "itsy-bitsy",
                    "dinky",
                    "dwarfish",
                    "elfin",
                    "elflike",
                    "gnomish",
                    "half-size",
                    "infinitesimal",
                    "minute",
                    "lesser",
                    "microscopic",
                    "microscopical",
                    "micro",
                    "miniature",
                    "minuscule",
                    "miniscule",
                    "olive-sized",
                    "pocket-size",
                    "pocket-sized",
                    "pocketable",
                    "puny",
                    "runty",
                    "shrimpy",
                    "slender",
                    "slim",
                    "smaller",
                    "littler",
                    "smallish",
                    "small-scale",
                    "undersize",
                    "undersized"
                ],
                "predictions": [
                    {
                        "score": 0.33769329108101465,
                        "answer": "lebowski",
                        "hit": false
                    },
                    {
                        "score": 0.3329581871078363,
                        "answer": "ahhh",
                        "hit": false
                    },
                    {
                        "score": 0.32833098169550096,
                        "answer": "enables",
                        "hit": false
                    },
                    {
                        "score": 0.3241152442106838,
                        "answer": "imaginary",
                        "hit": false
                    },
                    {
                        "score": 0.3203974121034431,
                        "answer": "boob",
                        "hit": false
                    },
                    {
                        "score": 0.31653165477932954,
                        "answer": "splashy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "big"
                ],
                "rank": 433,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5820504650473595
            },
            {
                "question verbose": "What is to bright ",
                "b": "bright",
                "expected answer": [
                    "pale",
                    "colorless",
                    "colourless",
                    "dull",
                    "neutral",
                    "pale",
                    "white",
                    "bleached",
                    "faded",
                    "washed-out",
                    "washy",
                    "drab",
                    "somber",
                    "sombre",
                    "dulled",
                    "greyed",
                    "etiolate",
                    "etiolated",
                    "lurid",
                    "waxen",
                    "waxlike",
                    "waxy",
                    "whitened"
                ],
                "predictions": [
                    {
                        "score": 0.43322100207962105,
                        "answer": "squeaked",
                        "hit": false
                    },
                    {
                        "score": 0.4149225827649325,
                        "answer": "undertake",
                        "hit": false
                    },
                    {
                        "score": 0.4147933476062424,
                        "answer": "fatty",
                        "hit": false
                    },
                    {
                        "score": 0.40885706847372494,
                        "answer": "dependency",
                        "hit": false
                    },
                    {
                        "score": 0.40381338448052667,
                        "answer": "armchair",
                        "hit": false
                    },
                    {
                        "score": 0.3996019246737127,
                        "answer": "petty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "bright"
                ],
                "rank": 182,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8134347498416901
            },
            {
                "question verbose": "What is to cheap ",
                "b": "cheap",
                "expected answer": [
                    "expensive",
                    "big-ticket",
                    "high-ticket",
                    "costly",
                    "dear",
                    "high-priced",
                    "pricey",
                    "pricy",
                    "dearly-won",
                    "overpriced",
                    "valuable"
                ],
                "predictions": [
                    {
                        "score": 0.4399534400330921,
                        "answer": "remarried",
                        "hit": false
                    },
                    {
                        "score": 0.43214848758947116,
                        "answer": "zapit",
                        "hit": false
                    },
                    {
                        "score": 0.4200751258724971,
                        "answer": "couldnt",
                        "hit": false
                    },
                    {
                        "score": 0.41808369511161303,
                        "answer": "continuity",
                        "hit": false
                    },
                    {
                        "score": 0.4067868987013632,
                        "answer": "slug",
                        "hit": false
                    },
                    {
                        "score": 0.40656062153638245,
                        "answer": "directx",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cheap"
                ],
                "rank": 2262,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7632831037044525
            },
            {
                "question verbose": "What is to clean ",
                "b": "clean",
                "expected answer": [
                    "dirty",
                    "soiled",
                    "unclean",
                    "augean",
                    "bedraggled",
                    "draggled",
                    "befouled",
                    "fouled",
                    "begrimed",
                    "dingy",
                    "grimy",
                    "grubby",
                    "grungy",
                    "raunchy",
                    "black",
                    "smutty",
                    "buggy",
                    "cobwebby",
                    "dirty-faced",
                    "feculent",
                    "filthy",
                    "foul",
                    "nasty",
                    "flyblown",
                    "squalid",
                    "sordid",
                    "greasy",
                    "oily",
                    "lousy",
                    "maculate",
                    "mucky",
                    "muddy",
                    "ratty",
                    "scummy",
                    "smudgy",
                    "snotty",
                    "snot-nosed",
                    "sooty",
                    "travel-soiled",
                    "travel-stained",
                    "uncleanly",
                    "unswept",
                    "unwashed"
                ],
                "predictions": [
                    {
                        "score": 0.361653531167094,
                        "answer": "kaufman",
                        "hit": false
                    },
                    {
                        "score": 0.34788307702126625,
                        "answer": "purchasing",
                        "hit": false
                    },
                    {
                        "score": 0.3394760909796843,
                        "answer": "softest",
                        "hit": false
                    },
                    {
                        "score": 0.33906655015308323,
                        "answer": "shoe",
                        "hit": false
                    },
                    {
                        "score": 0.33536274228417823,
                        "answer": "leather",
                        "hit": false
                    },
                    {
                        "score": 0.3291719735144752,
                        "answer": "ginger",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clean"
                ],
                "rank": 96,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6487742811441422
            },
            {
                "question verbose": "What is to clear ",
                "b": "clear",
                "expected answer": [
                    "vague",
                    "obscure",
                    "unclear",
                    "undefined",
                    "indefinable",
                    "undefinable"
                ],
                "predictions": [
                    {
                        "score": 0.3850513243743475,
                        "answer": "innocence",
                        "hit": false
                    },
                    {
                        "score": 0.377022575947261,
                        "answer": "logical",
                        "hit": false
                    },
                    {
                        "score": 0.3725931441113451,
                        "answer": "wrap",
                        "hit": false
                    },
                    {
                        "score": 0.36649393519343065,
                        "answer": "useless",
                        "hit": false
                    },
                    {
                        "score": 0.3576806762119304,
                        "answer": "generally",
                        "hit": false
                    },
                    {
                        "score": 0.353803969790963,
                        "answer": "definition",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "clear"
                ],
                "rank": 2295,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6948942691087723
            },
            {
                "question verbose": "What is to close ",
                "b": "close",
                "expected answer": [
                    "distant",
                    "remote",
                    "removed",
                    "far",
                    "faraway",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.38410030568692366,
                        "answer": "unaware",
                        "hit": false
                    },
                    {
                        "score": 0.3593005027914369,
                        "answer": "cant",
                        "hit": false
                    },
                    {
                        "score": 0.3460292231227615,
                        "answer": "mcdonald",
                        "hit": false
                    },
                    {
                        "score": 0.3398138717536632,
                        "answer": "mull",
                        "hit": false
                    },
                    {
                        "score": 0.33814193578541074,
                        "answer": "pitch",
                        "hit": false
                    },
                    {
                        "score": 0.3347610130672105,
                        "answer": "ku",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "close"
                ],
                "rank": 354,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7439108490943909
            },
            {
                "question verbose": "What is to colorful ",
                "b": "colorful",
                "expected answer": [
                    "colorless",
                    "colourless",
                    "dull",
                    "neutral",
                    "pale",
                    "white",
                    "bleached",
                    "faded",
                    "washed-out",
                    "washy",
                    "drab",
                    "somber",
                    "sombre",
                    "dulled",
                    "greyed",
                    "etiolate",
                    "etiolated",
                    "lurid",
                    "waxen",
                    "waxlike",
                    "waxy",
                    "whitened"
                ],
                "predictions": [
                    {
                        "score": 0.5013673505530869,
                        "answer": "unrelated",
                        "hit": false
                    },
                    {
                        "score": 0.4960680616273985,
                        "answer": "bereft",
                        "hit": false
                    },
                    {
                        "score": 0.49211711331380076,
                        "answer": "tail",
                        "hit": false
                    },
                    {
                        "score": 0.4704092704635384,
                        "answer": "installment",
                        "hit": false
                    },
                    {
                        "score": 0.4668605490719612,
                        "answer": "stormtrooper",
                        "hit": false
                    },
                    {
                        "score": 0.4648410127836301,
                        "answer": "doings",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "colorful"
                ],
                "rank": 925,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6120443344116211
            },
            {
                "question verbose": "What is to common ",
                "b": "common",
                "expected answer": [
                    "rare",
                    "infrequent",
                    "scarce",
                    "uncommon",
                    "extraordinary"
                ],
                "predictions": [
                    {
                        "score": 0.36280694161570315,
                        "answer": "proof",
                        "hit": false
                    },
                    {
                        "score": 0.3575684275611785,
                        "answer": "exercising",
                        "hit": false
                    },
                    {
                        "score": 0.34817963948320846,
                        "answer": "ungrateful",
                        "hit": false
                    },
                    {
                        "score": 0.34754615522004334,
                        "answer": "intended",
                        "hit": false
                    },
                    {
                        "score": 0.3469979932226788,
                        "answer": "sarkar",
                        "hit": false
                    },
                    {
                        "score": 0.3462748115782364,
                        "answer": "frantic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "common"
                ],
                "rank": 683,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6758504956960678
            },
            {
                "question verbose": "What is to competent ",
                "b": "competent",
                "expected answer": [
                    "incompetent",
                    "unskilled",
                    "unqualified",
                    "inefficient",
                    "unqualified",
                    "unskilled"
                ],
                "predictions": [
                    {
                        "score": 0.5375899500008738,
                        "answer": "frantic",
                        "hit": false
                    },
                    {
                        "score": 0.4920829905596776,
                        "answer": "predisposition",
                        "hit": false
                    },
                    {
                        "score": 0.47008313218743486,
                        "answer": "instilled",
                        "hit": false
                    },
                    {
                        "score": 0.4685712427927494,
                        "answer": "biblical",
                        "hit": false
                    },
                    {
                        "score": 0.4680812669213339,
                        "answer": "clicked",
                        "hit": false
                    },
                    {
                        "score": 0.46576081227777977,
                        "answer": "dumbest",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "competent"
                ],
                "rank": 147,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7736694514751434
            },
            {
                "question verbose": "What is to concerned ",
                "b": "concerned",
                "expected answer": [
                    "unconcerned",
                    "blase",
                    "blithe",
                    "casual",
                    "insouciant",
                    "nonchalant",
                    "degage",
                    "detached",
                    "uninvolved",
                    "indifferent",
                    "careless",
                    "uninvolved",
                    "untroubled"
                ],
                "predictions": [
                    {
                        "score": 0.42313152185148667,
                        "answer": "remarried",
                        "hit": false
                    },
                    {
                        "score": 0.4019578545085465,
                        "answer": "biblical",
                        "hit": false
                    },
                    {
                        "score": 0.39081877826120276,
                        "answer": "preindustrial",
                        "hit": false
                    },
                    {
                        "score": 0.38547486324943325,
                        "answer": "hellishly",
                        "hit": false
                    },
                    {
                        "score": 0.385263359705587,
                        "answer": "empathetic",
                        "hit": false
                    },
                    {
                        "score": 0.3808825540662749,
                        "answer": "slug",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "concerned"
                ],
                "rank": 8283,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5645842626690865
            },
            {
                "question verbose": "What is to cooked ",
                "b": "cooked",
                "expected answer": [
                    "raw",
                    "half-baked",
                    "fresh",
                    "natural",
                    "underdone",
                    "rare",
                    "uncooked",
                    "untoasted"
                ],
                "predictions": [
                    {
                        "score": 0.6447606239087158,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3241513684308142,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.3235113112884261,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.32345046808306666,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.32318128226687726,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.31244652011620466,
                        "answer": "naive",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "cooked"
                ],
                "rank": 515,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5993308201432228
            },
            {
                "question verbose": "What is to dangerous ",
                "b": "dangerous",
                "expected answer": [
                    "safe",
                    "fail-safe",
                    "risk-free",
                    "riskless",
                    "unhazardous",
                    "safe-and-sound",
                    "unhurt",
                    "harmless",
                    "invulnerable",
                    "secure",
                    "uninjured"
                ],
                "predictions": [
                    {
                        "score": 0.4354987789500827,
                        "answer": "slight",
                        "hit": false
                    },
                    {
                        "score": 0.39696529286731824,
                        "answer": "tagged",
                        "hit": false
                    },
                    {
                        "score": 0.38642458274885766,
                        "answer": "untested",
                        "hit": false
                    },
                    {
                        "score": 0.37428585648277923,
                        "answer": "racial",
                        "hit": false
                    },
                    {
                        "score": 0.3683979123850088,
                        "answer": "aimless",
                        "hit": false
                    },
                    {
                        "score": 0.3682180319341192,
                        "answer": "rumor",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dangerous"
                ],
                "rank": 11604,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5486175641417503
            },
            {
                "question verbose": "What is to decisive ",
                "b": "decisive",
                "expected answer": [
                    "hesitating",
                    "indecisive",
                    "hesitant",
                    "inconclusive",
                    "irresolute"
                ],
                "predictions": [
                    {
                        "score": 0.5542630058869574,
                        "answer": "debater",
                        "hit": false
                    },
                    {
                        "score": 0.5487268107151917,
                        "answer": "ragged",
                        "hit": false
                    },
                    {
                        "score": 0.5401709051253804,
                        "answer": "recovering",
                        "hit": false
                    },
                    {
                        "score": 0.5306907854940125,
                        "answer": "remarried",
                        "hit": false
                    },
                    {
                        "score": 0.5276661808158964,
                        "answer": "roleplaying",
                        "hit": false
                    },
                    {
                        "score": 0.5205376608471821,
                        "answer": "stormtrooper",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "decisive"
                ],
                "rank": 2533,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.647268995642662
            },
            {
                "question verbose": "What is to dry ",
                "b": "dry",
                "expected answer": [
                    "wet",
                    "bedewed",
                    "dewy",
                    "besprent",
                    "boggy",
                    "marshy",
                    "miry",
                    "mucky",
                    "muddy",
                    "quaggy",
                    "sloppy",
                    "sloughy",
                    "soggy",
                    "squashy",
                    "swampy",
                    "waterlogged",
                    "clammy",
                    "dank",
                    "damp",
                    "dampish",
                    "moist",
                    "sodden",
                    "soppy",
                    "drippy",
                    "drizzly",
                    "humid",
                    "misty",
                    "muggy",
                    "steamy",
                    "sticky",
                    "reeking",
                    "watery",
                    "rheumy",
                    "showery",
                    "rainy",
                    "steaming",
                    "tacky",
                    "undried",
                    "washed"
                ],
                "predictions": [
                    {
                        "score": 0.44204787102530324,
                        "answer": "remarried",
                        "hit": false
                    },
                    {
                        "score": 0.43115044350819587,
                        "answer": "beef",
                        "hit": false
                    },
                    {
                        "score": 0.4269696687978973,
                        "answer": "suitability",
                        "hit": false
                    },
                    {
                        "score": 0.42339972064375314,
                        "answer": "recite",
                        "hit": false
                    },
                    {
                        "score": 0.42170109383912935,
                        "answer": "orphan",
                        "hit": false
                    },
                    {
                        "score": 0.420776075584996,
                        "answer": "footprint",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dry"
                ],
                "rank": 363,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5945135727524757
            },
            {
                "question verbose": "What is to energetic ",
                "b": "energetic",
                "expected answer": [
                    "lethargic",
                    "unergetic",
                    "dazed",
                    "foggy",
                    "groggy",
                    "logy",
                    "stuporous",
                    "dreamy",
                    "lackadaisical",
                    "languid",
                    "languorous",
                    "listless",
                    "inactive"
                ],
                "predictions": [
                    {
                        "score": 0.5578777824116722,
                        "answer": "booed",
                        "hit": false
                    },
                    {
                        "score": 0.5412724765727813,
                        "answer": "debater",
                        "hit": false
                    },
                    {
                        "score": 0.5235281874940607,
                        "answer": "tabletop",
                        "hit": false
                    },
                    {
                        "score": 0.5228210710774974,
                        "answer": "throwaway",
                        "hit": false
                    },
                    {
                        "score": 0.5222143135800766,
                        "answer": "recovering",
                        "hit": false
                    },
                    {
                        "score": 0.520465897924913,
                        "answer": "judiau",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "energetic"
                ],
                "rank": 3250,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8463212251663208
            },
            {
                "question verbose": "What is to familiar ",
                "b": "familiar",
                "expected answer": [
                    "unfamiliar",
                    "strange",
                    "unknown",
                    "unacquainted",
                    "unacquainted",
                    "foreign",
                    "strange",
                    "unknown"
                ],
                "predictions": [
                    {
                        "score": 0.4513445318295364,
                        "answer": "circular",
                        "hit": false
                    },
                    {
                        "score": 0.4402982573530949,
                        "answer": "logical",
                        "hit": false
                    },
                    {
                        "score": 0.43721462284105217,
                        "answer": "clicked",
                        "hit": false
                    },
                    {
                        "score": 0.43524338303661503,
                        "answer": "visual",
                        "hit": false
                    },
                    {
                        "score": 0.4328151013556012,
                        "answer": "slimier",
                        "hit": false
                    },
                    {
                        "score": 0.42784781929621707,
                        "answer": "discredited",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "familiar"
                ],
                "rank": 2276,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7481729388237
            },
            {
                "question verbose": "What is to fat ",
                "b": "fat",
                "expected answer": [
                    "thin",
                    "lean",
                    "anorexic",
                    "anorectic",
                    "bony",
                    "cadaverous",
                    "emaciated",
                    "gaunt",
                    "haggard",
                    "pinched",
                    "skeletal",
                    "wasted",
                    "deep-eyed",
                    "hollow-eyed",
                    "sunken-eyed",
                    "gangling",
                    "gangly",
                    "lanky",
                    "lank",
                    "spindly",
                    "rawboned",
                    "reedy",
                    "reedlike",
                    "twiggy",
                    "twiglike",
                    "scarecrowish",
                    "scraggy",
                    "boney",
                    "scrawny",
                    "skinny",
                    "underweight",
                    "weedy",
                    "shriveled",
                    "shrivelled",
                    "shrunken",
                    "withered",
                    "wizen",
                    "wizened",
                    "slender",
                    "slight",
                    "slim",
                    "svelte",
                    "slender-waisted",
                    "slim-waisted",
                    "wasp-waisted",
                    "spare",
                    "trim",
                    "spindle-legged",
                    "spindle-shanked",
                    "stringy",
                    "wiry",
                    "wisplike",
                    "wispy"
                ],
                "predictions": [
                    {
                        "score": 0.4734475530195522,
                        "answer": "fantasize",
                        "hit": false
                    },
                    {
                        "score": 0.4528664058746026,
                        "answer": "realistic",
                        "hit": false
                    },
                    {
                        "score": 0.45023680784324327,
                        "answer": "lazy",
                        "hit": false
                    },
                    {
                        "score": 0.4269860127865858,
                        "answer": "delighted",
                        "hit": false
                    },
                    {
                        "score": 0.42622767137411227,
                        "answer": "elmer",
                        "hit": false
                    },
                    {
                        "score": 0.4181849044523707,
                        "answer": "nephew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fat"
                ],
                "rank": 1298,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6477892398834229
            },
            {
                "question verbose": "What is to full ",
                "b": "full",
                "expected answer": [
                    "empty",
                    "bare",
                    "stripped",
                    "blank",
                    "clean",
                    "white",
                    "empty-handed",
                    "glassy",
                    "glazed",
                    "lifeless",
                    "looted",
                    "pillaged",
                    "plundered",
                    "ransacked",
                    "vacant",
                    "vacuous",
                    "void"
                ],
                "predictions": [
                    {
                        "score": 0.3427980755720278,
                        "answer": "bold",
                        "hit": false
                    },
                    {
                        "score": 0.3380812406296762,
                        "answer": "color",
                        "hit": false
                    },
                    {
                        "score": 0.3296832465393609,
                        "answer": "impossible",
                        "hit": false
                    },
                    {
                        "score": 0.32037218291151864,
                        "answer": "rare",
                        "hit": false
                    },
                    {
                        "score": 0.3156937184847985,
                        "answer": "hisself",
                        "hit": false
                    },
                    {
                        "score": 0.3128214480465739,
                        "answer": "dirty",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "full"
                ],
                "rank": 255,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6597699373960495
            },
            {
                "question verbose": "What is to gaseous ",
                "b": "gaseous",
                "expected answer": [
                    "solid",
                    "hard",
                    "coagulated",
                    "solidified",
                    "concrete",
                    "congealed",
                    "jelled",
                    "jellied",
                    "dry",
                    "semisolid",
                    "solid-state"
                ],
                "predictions": [
                    {
                        "score": 0.6389709030378581,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32916221583062505,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.3243442414526175,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.3177784890271118,
                        "answer": "motorcycle",
                        "hit": false
                    },
                    {
                        "score": 0.31712574157977746,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.3136921913176775,
                        "answer": "roleplaying",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "gaseous"
                ],
                "rank": 12682,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5552620887756348
            },
            {
                "question verbose": "What is to generous ",
                "b": "generous",
                "expected answer": [
                    "stingy",
                    "beggarly",
                    "mean",
                    "cheap",
                    "chinchy",
                    "chintzy",
                    "cheeseparing",
                    "close",
                    "near",
                    "penny-pinching",
                    "skinny",
                    "closefisted",
                    "hardfisted",
                    "tightfisted",
                    "grudging",
                    "niggardly",
                    "scrimy",
                    "mingy",
                    "miserly",
                    "tight",
                    "parsimonious",
                    "penurious",
                    "selfish",
                    "uncharitable",
                    "ungenerous",
                    "meanspirited"
                ],
                "predictions": [
                    {
                        "score": 0.5140760832077589,
                        "answer": "doings",
                        "hit": false
                    },
                    {
                        "score": 0.5008297413092115,
                        "answer": "ltbrgt",
                        "hit": false
                    },
                    {
                        "score": 0.49596949441228005,
                        "answer": "snickering",
                        "hit": false
                    },
                    {
                        "score": 0.4887384135897858,
                        "answer": "busking",
                        "hit": false
                    },
                    {
                        "score": 0.4857200591839496,
                        "answer": "obrian",
                        "hit": false
                    },
                    {
                        "score": 0.4855300237406385,
                        "answer": "fame",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "generous"
                ],
                "rank": 6611,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6378095746040344
            },
            {
                "question verbose": "What is to happy ",
                "b": "happy",
                "expected answer": [
                    "sad",
                    "bittersweet",
                    "doleful",
                    "mournful",
                    "heavyhearted",
                    "melancholy",
                    "melancholic",
                    "pensive",
                    "wistful",
                    "tragic",
                    "tragical",
                    "tragicomic",
                    "tragicomical",
                    "deplorable",
                    "distressing",
                    "lamentable",
                    "pitiful",
                    "sorry"
                ],
                "predictions": [
                    {
                        "score": 0.38659260753595565,
                        "answer": "goth",
                        "hit": false
                    },
                    {
                        "score": 0.3648831540959045,
                        "answer": "innocence",
                        "hit": false
                    },
                    {
                        "score": 0.356069095795096,
                        "answer": "idiot",
                        "hit": false
                    },
                    {
                        "score": 0.3541496474991387,
                        "answer": "plug",
                        "hit": false
                    },
                    {
                        "score": 0.35030864025937275,
                        "answer": "partycandidatepundits",
                        "hit": false
                    },
                    {
                        "score": 0.3499840905039104,
                        "answer": "beef",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "happy"
                ],
                "rank": 4781,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6998877823352814
            },
            {
                "question verbose": "What is to hard ",
                "b": "hard",
                "expected answer": [
                    "soft",
                    "mellow",
                    "brushed",
                    "fleecy",
                    "napped",
                    "cheeselike",
                    "compressible",
                    "squeezable",
                    "cottony",
                    "cushioned",
                    "cushiony",
                    "padded",
                    "demulcent",
                    "emollient",
                    "salving",
                    "softening",
                    "downy",
                    "downlike",
                    "flossy",
                    "fluffy",
                    "flaccid",
                    "flocculent",
                    "woolly",
                    "wooly",
                    "yielding",
                    "mushy",
                    "overstuffed",
                    "softish",
                    "semisoft",
                    "spongy",
                    "squashy",
                    "squishy",
                    "spongelike",
                    "velvet",
                    "velvety"
                ],
                "predictions": [
                    {
                        "score": 0.32805613911578974,
                        "answer": "accommodate",
                        "hit": false
                    },
                    {
                        "score": 0.3143304189673446,
                        "answer": "sore",
                        "hit": false
                    },
                    {
                        "score": 0.31233814050386793,
                        "answer": "harden",
                        "hit": false
                    },
                    {
                        "score": 0.3098851907104974,
                        "answer": "disagreeing",
                        "hit": false
                    },
                    {
                        "score": 0.30474211170912757,
                        "answer": "jaded",
                        "hit": false
                    },
                    {
                        "score": 0.3016674767629532,
                        "answer": "inclination",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hard"
                ],
                "rank": 2732,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.747327521443367
            },
            {
                "question verbose": "What is to hot ",
                "b": "hot",
                "expected answer": [
                    "cold",
                    "chilly",
                    "frosty",
                    "cool",
                    "frigid"
                ],
                "predictions": [
                    {
                        "score": 0.43920638779661736,
                        "answer": "rnc",
                        "hit": false
                    },
                    {
                        "score": 0.3592396316832859,
                        "answer": "stove",
                        "hit": false
                    },
                    {
                        "score": 0.3538810910313021,
                        "answer": "incarnation",
                        "hit": false
                    },
                    {
                        "score": 0.35247012479817463,
                        "answer": "continuity",
                        "hit": false
                    },
                    {
                        "score": 0.3509753803988678,
                        "answer": "heater",
                        "hit": false
                    },
                    {
                        "score": 0.3507563026310405,
                        "answer": "emulates",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "hot"
                ],
                "rank": 535,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7063677459955215
            },
            {
                "question verbose": "What is to interesting ",
                "b": "interesting",
                "expected answer": [
                    "uninteresting",
                    "dull",
                    "boring",
                    "deadening",
                    "ho-hum",
                    "irksome",
                    "slow",
                    "tedious",
                    "tiresome",
                    "wearisome",
                    "insipid",
                    "jejune",
                    "narcotic",
                    "soporiferous",
                    "soporific",
                    "prosaic",
                    "prosy",
                    "earthbound",
                    "ponderous",
                    "putdownable",
                    "unexciting",
                    "unstimulating"
                ],
                "predictions": [
                    {
                        "score": 0.4050146493039485,
                        "answer": "lately",
                        "hit": false
                    },
                    {
                        "score": 0.3976750297010488,
                        "answer": "insightful",
                        "hit": false
                    },
                    {
                        "score": 0.38132319790779917,
                        "answer": "frantic",
                        "hit": false
                    },
                    {
                        "score": 0.38064096072598763,
                        "answer": "lib",
                        "hit": false
                    },
                    {
                        "score": 0.37818440751487975,
                        "answer": "antique",
                        "hit": false
                    },
                    {
                        "score": 0.3745569661212196,
                        "answer": "gilgamesh",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interesting"
                ],
                "rank": 3582,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5910343676805496
            },
            {
                "question verbose": "What is to introvert ",
                "b": "introvert",
                "expected answer": [
                    "extravert",
                    "extrovert",
                    "outgoing",
                    "extroverted",
                    "forthcoming",
                    "sociable"
                ],
                "predictions": [
                    {
                        "score": 0.6172487667882909,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33483197938663245,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.33094724414656107,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.3229759595654214,
                        "answer": "naive",
                        "hit": false
                    },
                    {
                        "score": 0.31921621480423196,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.317596240387523,
                        "answer": "motorcycle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "introvert"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to large ",
                "b": "large",
                "expected answer": [
                    "small",
                    "little",
                    "atomic",
                    "subatomic",
                    "bantam",
                    "diminutive",
                    "lilliputian",
                    "midget",
                    "petite",
                    "tiny",
                    "flyspeck",
                    "bitty",
                    "bittie",
                    "teensy",
                    "teentsy",
                    "teeny",
                    "wee",
                    "weeny",
                    "weensy",
                    "teensy-weensy",
                    "teeny-weeny",
                    "itty-bitty",
                    "itsy-bitsy",
                    "dinky",
                    "dwarfish",
                    "elfin",
                    "elflike",
                    "gnomish",
                    "half-size",
                    "infinitesimal",
                    "minute",
                    "lesser",
                    "microscopic",
                    "microscopical",
                    "micro",
                    "miniature",
                    "minuscule",
                    "miniscule",
                    "olive-sized",
                    "pocket-size",
                    "pocket-sized",
                    "pocketable",
                    "puny",
                    "runty",
                    "shrimpy",
                    "slender",
                    "slim",
                    "smaller",
                    "littler",
                    "smallish",
                    "small-scale",
                    "undersize",
                    "undersized"
                ],
                "predictions": [
                    {
                        "score": 0.2846626087595077,
                        "answer": "hypothesis",
                        "hit": false
                    },
                    {
                        "score": 0.28451616163813914,
                        "answer": "desk",
                        "hit": false
                    },
                    {
                        "score": 0.28014528310413506,
                        "answer": "subconscious",
                        "hit": false
                    },
                    {
                        "score": 0.28010858355383866,
                        "answer": "tagged",
                        "hit": false
                    },
                    {
                        "score": 0.27807313706361847,
                        "answer": "survivor",
                        "hit": false
                    },
                    {
                        "score": 0.27802031729396043,
                        "answer": "coasting",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "large"
                ],
                "rank": 505,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6513851433992386
            },
            {
                "question verbose": "What is to long ",
                "b": "long",
                "expected answer": [
                    "short",
                    "abbreviated",
                    "shortened",
                    "truncated",
                    "brief",
                    "clipped",
                    "fleeting",
                    "fugitive",
                    "momentaneous",
                    "momentary",
                    "short_and_sweet",
                    "short-dated",
                    "short-range",
                    "short-run",
                    "short-term",
                    "abbreviated",
                    "brief",
                    "close",
                    "curtal",
                    "sawed-off",
                    "sawn-off",
                    "shortened",
                    "shortish",
                    "short-range",
                    "short-snouted",
                    "snub",
                    "stubby",
                    "telescoped",
                    "truncate",
                    "truncated",
                    "chunky",
                    "dumpy",
                    "low-set",
                    "squat",
                    "squatty",
                    "stumpy",
                    "compact",
                    "heavyset",
                    "stocky",
                    "thick",
                    "thickset",
                    "half-length",
                    "pint-size",
                    "pint-sized",
                    "runty",
                    "sawed-off",
                    "sawn-off",
                    "short-stalked",
                    "squab",
                    "squabby"
                ],
                "predictions": [
                    {
                        "score": 0.3353051979633103,
                        "answer": "anaysis",
                        "hit": false
                    },
                    {
                        "score": 0.3106426878792029,
                        "answer": "winded",
                        "hit": false
                    },
                    {
                        "score": 0.3070186501281066,
                        "answer": "influential",
                        "hit": false
                    },
                    {
                        "score": 0.2996139953448529,
                        "answer": "capability",
                        "hit": false
                    },
                    {
                        "score": 0.2993952794813009,
                        "answer": "cancel",
                        "hit": false
                    },
                    {
                        "score": 0.2903015402934154,
                        "answer": "scot",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "long"
                ],
                "rank": 1996,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6767927259206772
            },
            {
                "question verbose": "What is to lumpy ",
                "b": "lumpy",
                "expected answer": [
                    "flat",
                    "level",
                    "plane",
                    "even",
                    ""
                ],
                "predictions": [
                    {
                        "score": 0.6457413361454757,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.336686142044595,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.3316249438002608,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.3237071830926436,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.3179357465126744,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.31489332639512774,
                        "answer": "roleplaying",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "lumpy"
                ],
                "rank": 1310,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5323086492717266
            },
            {
                "question verbose": "What is to noisy ",
                "b": "noisy",
                "expected answer": [
                    "silent",
                    "uncommunicative",
                    "dumb",
                    "mute",
                    "inarticulate",
                    "unarticulate"
                ],
                "predictions": [
                    {
                        "score": 0.5760671246657393,
                        "answer": "fantasize",
                        "hit": false
                    },
                    {
                        "score": 0.5678462334067075,
                        "answer": "meandering",
                        "hit": false
                    },
                    {
                        "score": 0.5581057537280022,
                        "answer": "doings",
                        "hit": false
                    },
                    {
                        "score": 0.5516917506481408,
                        "answer": "frantic",
                        "hit": false
                    },
                    {
                        "score": 0.5389216137575565,
                        "answer": "ant",
                        "hit": false
                    },
                    {
                        "score": 0.5238971227305123,
                        "answer": "spat",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "noisy"
                ],
                "rank": 8696,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8185543417930603
            },
            {
                "question verbose": "What is to normal ",
                "b": "normal",
                "expected answer": [
                    "abnormal",
                    "aberrant",
                    "deviant",
                    "deviate",
                    "anomalous",
                    "antidromic",
                    "atypical",
                    "irregular",
                    "brachydactylic",
                    "brachydactylous",
                    "defective",
                    "freakish",
                    "kinky",
                    "perverted",
                    "subnormal",
                    "supernormal",
                    "vicarious",
                    "unusual",
                    "exceptional",
                    "insane",
                    "unnatural"
                ],
                "predictions": [
                    {
                        "score": 0.4330575140659292,
                        "answer": "trap",
                        "hit": false
                    },
                    {
                        "score": 0.39030071578701436,
                        "answer": "sanitizer",
                        "hit": false
                    },
                    {
                        "score": 0.37770416491928677,
                        "answer": "foolish",
                        "hit": false
                    },
                    {
                        "score": 0.3743858397555211,
                        "answer": "hairy",
                        "hit": false
                    },
                    {
                        "score": 0.3685327701011082,
                        "answer": "leash",
                        "hit": false
                    },
                    {
                        "score": 0.3640565992139486,
                        "answer": "fist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "normal"
                ],
                "rank": 1762,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6509604752063751
            },
            {
                "question verbose": "What is to organized ",
                "b": "organized",
                "expected answer": [
                    "unorganized",
                    "unstructured",
                    "uncoordinated",
                    "unformed",
                    "unincorporated"
                ],
                "predictions": [
                    {
                        "score": 0.42404220323566627,
                        "answer": "unluckily",
                        "hit": false
                    },
                    {
                        "score": 0.4192420388108689,
                        "answer": "doings",
                        "hit": false
                    },
                    {
                        "score": 0.3814079645013882,
                        "answer": "brighten",
                        "hit": false
                    },
                    {
                        "score": 0.37728033841949943,
                        "answer": "marijuana",
                        "hit": false
                    },
                    {
                        "score": 0.3698139459791808,
                        "answer": "averaged",
                        "hit": false
                    },
                    {
                        "score": 0.3691564346858649,
                        "answer": "forecast",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "organized"
                ],
                "rank": 15339,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5405449829995632
            },
            {
                "question verbose": "What is to pale ",
                "b": "pale",
                "expected answer": [
                    "tanned",
                    "bronzed",
                    "suntanned",
                    "brunet",
                    "brunette"
                ],
                "predictions": [
                    {
                        "score": 0.5993159680445187,
                        "answer": "fantasize",
                        "hit": false
                    },
                    {
                        "score": 0.5903562664341608,
                        "answer": "stormtrooper",
                        "hit": false
                    },
                    {
                        "score": 0.5871260534297353,
                        "answer": "bereft",
                        "hit": false
                    },
                    {
                        "score": 0.582858323269407,
                        "answer": "beef",
                        "hit": false
                    },
                    {
                        "score": 0.5701473960963749,
                        "answer": "masking",
                        "hit": false
                    },
                    {
                        "score": 0.5587488031512983,
                        "answer": "outcoached",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "pale"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6285474598407745
            },
            {
                "question verbose": "What is to rich ",
                "b": "rich",
                "expected answer": [
                    "poor",
                    "broke",
                    "bust",
                    "skint",
                    "stone-broke",
                    "stony-broke",
                    "destitute",
                    "impoverished",
                    "indigent",
                    "necessitous",
                    "needy",
                    "poverty-stricken",
                    "hard_up",
                    "impecunious",
                    "penniless",
                    "penurious",
                    "pinched",
                    "moneyless",
                    "unprovided"
                ],
                "predictions": [
                    {
                        "score": 0.3921300369869188,
                        "answer": "likeable",
                        "hit": false
                    },
                    {
                        "score": 0.3860334179472514,
                        "answer": "fist",
                        "hit": false
                    },
                    {
                        "score": 0.3836814929690773,
                        "answer": "beef",
                        "hit": false
                    },
                    {
                        "score": 0.37660962485966276,
                        "answer": "suddenly",
                        "hit": false
                    },
                    {
                        "score": 0.36661935108512306,
                        "answer": "intention",
                        "hit": false
                    },
                    {
                        "score": 0.3652409260050654,
                        "answer": "doubtless",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rich"
                ],
                "rank": 2407,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6211961209774017
            },
            {
                "question verbose": "What is to sane ",
                "b": "sane",
                "expected answer": [
                    "crazy",
                    "loony",
                    "looney",
                    "nutcase",
                    "weirdo",
                    "mad",
                    "insane"
                ],
                "predictions": [
                    {
                        "score": 0.5485354630579266,
                        "answer": "researching",
                        "hit": false
                    },
                    {
                        "score": 0.5111269455495707,
                        "answer": "bereft",
                        "hit": false
                    },
                    {
                        "score": 0.5044180030941089,
                        "answer": "tabletop",
                        "hit": false
                    },
                    {
                        "score": 0.49997155489179596,
                        "answer": "dlc",
                        "hit": false
                    },
                    {
                        "score": 0.494813829789642,
                        "answer": "replying",
                        "hit": false
                    },
                    {
                        "score": 0.49404290332862705,
                        "answer": "hypothesis",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sane"
                ],
                "rank": 1004,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6123333349823952
            },
            {
                "question verbose": "What is to short ",
                "b": "short",
                "expected answer": [
                    "tall",
                    "gangling",
                    "gangly",
                    "lanky",
                    "rangy",
                    "height",
                    "leggy",
                    "long-legged",
                    "long-shanked",
                    "tall-growing",
                    "long",
                    "long-stalked",
                    "tall-stalked",
                    "stately",
                    "statuesque",
                    "tallish"
                ],
                "predictions": [
                    {
                        "score": 0.3555386712573143,
                        "answer": "span",
                        "hit": false
                    },
                    {
                        "score": 0.3362490905262118,
                        "answer": "disputings",
                        "hit": false
                    },
                    {
                        "score": 0.3334831462974657,
                        "answer": "panorama",
                        "hit": false
                    },
                    {
                        "score": 0.3320395137624402,
                        "answer": "cant",
                        "hit": false
                    },
                    {
                        "score": 0.32921125108595045,
                        "answer": "likeable",
                        "hit": false
                    },
                    {
                        "score": 0.3262610710621534,
                        "answer": "truthfully",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "short"
                ],
                "rank": 6770,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6145892664790154
            },
            {
                "question verbose": "What is to simple ",
                "b": "simple",
                "expected answer": [
                    "difficult",
                    "challenging",
                    "hard",
                    "complicated",
                    "demanding",
                    "daunting",
                    "taxing"
                ],
                "predictions": [
                    {
                        "score": 0.40143489954803924,
                        "answer": "explain",
                        "hit": false
                    },
                    {
                        "score": 0.3527206933562491,
                        "answer": "necessarily",
                        "hit": false
                    },
                    {
                        "score": 0.3481937979858306,
                        "answer": "doubtless",
                        "hit": false
                    },
                    {
                        "score": 0.3480973094143025,
                        "answer": "ferrici",
                        "hit": false
                    },
                    {
                        "score": 0.34803449948050247,
                        "answer": "objection",
                        "hit": false
                    },
                    {
                        "score": 0.34718401467798216,
                        "answer": "precisely",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "simple"
                ],
                "rank": 227,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6652008146047592
            },
            {
                "question verbose": "What is to sincere ",
                "b": "sincere",
                "expected answer": [
                    "insincere",
                    "bootlicking",
                    "fawning",
                    "obsequious",
                    "sycophantic",
                    "toadyish",
                    "buttery",
                    "fulsome",
                    "oily",
                    "oleaginous",
                    "smarmy",
                    "soapy",
                    "unctuous",
                    "dissimulative",
                    "false",
                    "feigned",
                    "gilded",
                    "meretricious",
                    "specious",
                    "hypocritical",
                    "plausible",
                    "counterfeit",
                    "imitative",
                    "dishonest",
                    "dishonorable",
                    "disingenuous",
                    "artful",
                    "false",
                    "unreal"
                ],
                "predictions": [
                    {
                        "score": 0.49855419047375543,
                        "answer": "outcoach",
                        "hit": false
                    },
                    {
                        "score": 0.48836865778723315,
                        "answer": "tagged",
                        "hit": false
                    },
                    {
                        "score": 0.4764842579594411,
                        "answer": "racial",
                        "hit": false
                    },
                    {
                        "score": 0.4751466796993791,
                        "answer": "instill",
                        "hit": false
                    },
                    {
                        "score": 0.47276725304120476,
                        "answer": "vale",
                        "hit": false
                    },
                    {
                        "score": 0.4720359338572989,
                        "answer": "activated",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sincere"
                ],
                "rank": 1796,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8068929016590118
            },
            {
                "question verbose": "What is to slow ",
                "b": "slow",
                "expected answer": [
                    "fast",
                    "quick",
                    "accelerated",
                    "alacritous",
                    "blistering",
                    "hot",
                    "red-hot",
                    "double-quick",
                    "express",
                    "fast-breaking",
                    "fast-paced",
                    "fleet",
                    "swift",
                    "high-speed",
                    "high-velocity",
                    "hurrying",
                    "scurrying",
                    "immediate",
                    "prompt",
                    "quick",
                    "straightaway",
                    "instantaneous",
                    "instant",
                    "meteoric",
                    "speedy",
                    "rapid",
                    "winged",
                    "windy"
                ],
                "predictions": [
                    {
                        "score": 0.4520976781872995,
                        "answer": "explorer",
                        "hit": false
                    },
                    {
                        "score": 0.4229054239982338,
                        "answer": "clunky",
                        "hit": false
                    },
                    {
                        "score": 0.41757748065564587,
                        "answer": "cobra",
                        "hit": false
                    },
                    {
                        "score": 0.3970081647610041,
                        "answer": "fantasize",
                        "hit": false
                    },
                    {
                        "score": 0.3949160051221229,
                        "answer": "circular",
                        "hit": false
                    },
                    {
                        "score": 0.39136723806321566,
                        "answer": "submission",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "slow"
                ],
                "rank": 1684,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6682691127061844
            },
            {
                "question verbose": "What is to sweet ",
                "b": "sweet",
                "expected answer": [
                    "sour",
                    "acerb",
                    "acerbic",
                    "astringent",
                    "acetose",
                    "acetous",
                    "vinegary",
                    "vinegarish",
                    "acidic",
                    "acid",
                    "acidulent",
                    "acidulous",
                    "lemony",
                    "lemonlike",
                    "sourish",
                    "tangy",
                    "tart",
                    "subacid",
                    "bitter"
                ],
                "predictions": [
                    {
                        "score": 0.46842932785402874,
                        "answer": "remarried",
                        "hit": false
                    },
                    {
                        "score": 0.4584115015449524,
                        "answer": "racial",
                        "hit": false
                    },
                    {
                        "score": 0.4511702085935638,
                        "answer": "functionality",
                        "hit": false
                    },
                    {
                        "score": 0.4499670980683834,
                        "answer": "fantasize",
                        "hit": false
                    },
                    {
                        "score": 0.44829805689973456,
                        "answer": "blabs",
                        "hit": false
                    },
                    {
                        "score": 0.4474327929247048,
                        "answer": "behaved",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "sweet"
                ],
                "rank": 2032,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7597781717777252
            },
            {
                "question verbose": "What is to tasty ",
                "b": "tasty",
                "expected answer": [
                    "tasteless",
                    "bland",
                    "flat",
                    "flavorless",
                    "flavourless",
                    "insipid",
                    "savorless",
                    "savourless",
                    "vapid",
                    "unflavored",
                    "unflavoured",
                    "nonflavored",
                    "nonflavoured",
                    "unsalted",
                    "unseasoned",
                    "unappetizing",
                    "unappetising",
                    "unpalatable"
                ],
                "predictions": [
                    {
                        "score": 0.6145744751983832,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.33046773470067814,
                        "answer": "definitely",
                        "hit": false
                    },
                    {
                        "score": 0.32464447321272,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.32458147843052626,
                        "answer": "ideological",
                        "hit": false
                    },
                    {
                        "score": 0.32017299842517216,
                        "answer": "noticeable",
                        "hit": false
                    },
                    {
                        "score": 0.31939523012085735,
                        "answer": "motorcycle",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tasty"
                ],
                "rank": 4869,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to tight ",
                "b": "tight",
                "expected answer": [
                    "loose",
                    "lax",
                    "baggy",
                    "loose-fitting",
                    "sloppy",
                    "flyaway",
                    "free",
                    "liberal",
                    "informal",
                    "unofficial"
                ],
                "predictions": [
                    {
                        "score": 0.40964640889937387,
                        "answer": "ragone",
                        "hit": false
                    },
                    {
                        "score": 0.3749263932518501,
                        "answer": "lenghts",
                        "hit": false
                    },
                    {
                        "score": 0.3748515665636552,
                        "answer": "wheelchair",
                        "hit": false
                    },
                    {
                        "score": 0.3664041373068464,
                        "answer": "buta",
                        "hit": false
                    },
                    {
                        "score": 0.3658482930274894,
                        "answer": "extinguisher",
                        "hit": false
                    },
                    {
                        "score": 0.3614367367960489,
                        "answer": "unluckily",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "tight"
                ],
                "rank": 1686,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7385206520557404
            },
            {
                "question verbose": "What is to warm ",
                "b": "warm",
                "expected answer": [
                    "cool",
                    "cold",
                    "chilly",
                    "frosty",
                    "cool",
                    "frigid"
                ],
                "predictions": [
                    {
                        "score": 0.39000236561966517,
                        "answer": "masking",
                        "hit": false
                    },
                    {
                        "score": 0.38619994704730376,
                        "answer": "biblical",
                        "hit": false
                    },
                    {
                        "score": 0.3811315697851589,
                        "answer": "razorblade",
                        "hit": false
                    },
                    {
                        "score": 0.3782435529218702,
                        "answer": "inclination",
                        "hit": false
                    },
                    {
                        "score": 0.3770984857375509,
                        "answer": "leisurely",
                        "hit": false
                    },
                    {
                        "score": 0.37700459915121665,
                        "answer": "practiced",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "warm"
                ],
                "rank": 254,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6783798784017563
            },
            {
                "question verbose": "What is to white ",
                "b": "white",
                "expected answer": [
                    "black",
                    "dark",
                    "lightless"
                ],
                "predictions": [
                    {
                        "score": 0.32732795074807686,
                        "answer": "incamera",
                        "hit": false
                    },
                    {
                        "score": 0.3148497181084843,
                        "answer": "latino",
                        "hit": false
                    },
                    {
                        "score": 0.3067265671859501,
                        "answer": "romneyryan",
                        "hit": false
                    },
                    {
                        "score": 0.3027422131706986,
                        "answer": "bc",
                        "hit": false
                    },
                    {
                        "score": 0.29601289082347615,
                        "answer": "ruindestroy",
                        "hit": false
                    },
                    {
                        "score": 0.29319110711435714,
                        "answer": "hispanic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "white"
                ],
                "rank": 1864,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7993013262748718
            },
            {
                "question verbose": "What is to wide ",
                "b": "wide",
                "expected answer": [
                    "narrow",
                    "constricting",
                    "constrictive",
                    "narrowing",
                    "narrowed",
                    "narrow-mouthed",
                    "slender",
                    "thin",
                    "strait",
                    "straplike",
                    "tapered",
                    "tapering",
                    "limited"
                ],
                "predictions": [
                    {
                        "score": 0.4128989067145104,
                        "answer": "doings",
                        "hit": false
                    },
                    {
                        "score": 0.4084348907905737,
                        "answer": "tagged",
                        "hit": false
                    },
                    {
                        "score": 0.4045813643727349,
                        "answer": "tweaked",
                        "hit": false
                    },
                    {
                        "score": 0.40359923579438856,
                        "answer": "wowww",
                        "hit": false
                    },
                    {
                        "score": 0.3940985372577194,
                        "answer": "endorsing",
                        "hit": false
                    },
                    {
                        "score": 0.39158217284125474,
                        "answer": "anaysis",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "wide"
                ],
                "rank": 2232,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6846197545528412
            },
            {
                "question verbose": "What is to willing ",
                "b": "willing",
                "expected answer": [
                    "unwilling",
                    "defiantnoncompliant",
                    "involuntary",
                    "nonvoluntary",
                    "unvoluntary",
                    "disinclined",
                    "averse",
                    "backward",
                    "hesitant",
                    "indisposed",
                    "loath",
                    "reluctant",
                    "uneager",
                    "unwishful"
                ],
                "predictions": [
                    {
                        "score": 0.454278360373178,
                        "answer": "heft",
                        "hit": false
                    },
                    {
                        "score": 0.4446530593688226,
                        "answer": "procedural",
                        "hit": false
                    },
                    {
                        "score": 0.42138120160359227,
                        "answer": "panorama",
                        "hit": false
                    },
                    {
                        "score": 0.4154146710113829,
                        "answer": "nephew",
                        "hit": false
                    },
                    {
                        "score": 0.4149199249025587,
                        "answer": "introspection",
                        "hit": false
                    },
                    {
                        "score": 0.4147455363925923,
                        "answer": "cant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "willing"
                ],
                "rank": 775,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7177295684814453
            },
            {
                "question verbose": "What is to young ",
                "b": "young",
                "expected answer": [
                    "old",
                    "aged",
                    "elderly",
                    "older",
                    "senior",
                    "age",
                    "aging",
                    "ageing",
                    "senescent",
                    "ancient",
                    "anile",
                    "centenarian",
                    "darkened",
                    "doddering",
                    "doddery",
                    "gaga",
                    "senile",
                    "emeritus",
                    "grey",
                    "gray",
                    "grey-haired",
                    "gray-haired",
                    "grey-headed",
                    "gray-headed",
                    "grizzly",
                    "hoar",
                    "hoary",
                    "white-haired",
                    "middle-aged",
                    "nonagenarian",
                    "octogenarian",
                    "oldish",
                    "overage",
                    "overaged",
                    "superannuated",
                    "over-the-hill",
                    "sexagenarian",
                    "venerable",
                    "experienced",
                    "experient",
                    "mature",
                    "senior"
                ],
                "predictions": [
                    {
                        "score": 0.34378225770185855,
                        "answer": "researching",
                        "hit": false
                    },
                    {
                        "score": 0.33317042933875785,
                        "answer": "gravely",
                        "hit": false
                    },
                    {
                        "score": 0.31153797922522675,
                        "answer": "goth",
                        "hit": false
                    },
                    {
                        "score": 0.30332885612723,
                        "answer": "aligned",
                        "hit": false
                    },
                    {
                        "score": 0.3020743245889013,
                        "answer": "lebowski",
                        "hit": false
                    },
                    {
                        "score": 0.29927149687978877,
                        "answer": "hispanic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "young"
                ],
                "rank": 5399,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6900036484003067
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L09 [antonyms - gradable].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "85a5c2cf-9940-4051-a49e-fa08394ae4ac",
            "timestamp": "2020-10-22T15:57:55.882387"
        }
    },
    {
        "details": [
            {
                "question verbose": "What is to after ",
                "b": "after",
                "expected answer": [
                    "before",
                    "earlier",
                    "previously"
                ],
                "predictions": [
                    {
                        "score": 0.5108408761583678,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.316689724913051,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.301094892783821,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.30073778114550764,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.29917592973687707,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.29870890753628654,
                        "answer": "hungry",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "after"
                ],
                "rank": 12011,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to ahead ",
                "b": "ahead",
                "expected answer": [
                    "behind",
                    "rear",
                    "after",
                    "tail",
                    "beforehand"
                ],
                "predictions": [
                    {
                        "score": 0.3713511153160852,
                        "answer": "pony",
                        "hit": false
                    },
                    {
                        "score": 0.36402822021073405,
                        "answer": "brag",
                        "hit": false
                    },
                    {
                        "score": 0.3616166042513883,
                        "answer": "indiegogo",
                        "hit": false
                    },
                    {
                        "score": 0.35834852226521985,
                        "answer": "salem",
                        "hit": false
                    },
                    {
                        "score": 0.3540616622396646,
                        "answer": "trimmed",
                        "hit": false
                    },
                    {
                        "score": 0.35399853375728146,
                        "answer": "angelinos",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "ahead"
                ],
                "rank": 719,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6623029410839081
            },
            {
                "question verbose": "What is to anterior ",
                "b": "anterior",
                "expected answer": [
                    "posterior"
                ],
                "predictions": [
                    {
                        "score": 0.5107839584594271,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3188257737285059,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3082029539973494,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.30530388916443923,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.30448802768135563,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.302252625707629,
                        "answer": "rush",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "anterior"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to backward ",
                "b": "backward",
                "expected answer": [
                    "forward",
                    "forwards",
                    "frontward",
                    "frontwards",
                    "forrad",
                    "forrard",
                    "forth",
                    "onward"
                ],
                "predictions": [
                    {
                        "score": 0.5236891537875954,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3100680069874709,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.3091548569515206,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3059024119935468,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.30502366270937226,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.3046632448913991,
                        "answer": "eschew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "backward"
                ],
                "rank": 8068,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.570742167532444
            },
            {
                "question verbose": "What is to before ",
                "b": "before",
                "expected answer": [
                    "after",
                    "subsequently",
                    "later",
                    "afterwards",
                    "afterward",
                    "ahead"
                ],
                "predictions": [
                    {
                        "score": 0.5094986625067051,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31147578547007254,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3058316351746882,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.30322592256521785,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.29932319990612066,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.2980736619302399,
                        "answer": "eschew",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "before"
                ],
                "rank": 6273,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to beginning ",
                "b": "beginning",
                "expected answer": [
                    "end",
                    "terminal",
                    "ending",
                    "last",
                    "final",
                    "finish",
                    "finale",
                    "conclusion"
                ],
                "predictions": [
                    {
                        "score": 0.35637948663957547,
                        "answer": "distributed",
                        "hit": false
                    },
                    {
                        "score": 0.3282203953956206,
                        "answer": "behave",
                        "hit": false
                    },
                    {
                        "score": 0.32589407049439506,
                        "answer": "tyler",
                        "hit": false
                    },
                    {
                        "score": 0.32547183960818904,
                        "answer": "amer",
                        "hit": false
                    },
                    {
                        "score": 0.3234079487895984,
                        "answer": "provision",
                        "hit": false
                    },
                    {
                        "score": 0.3229423843841988,
                        "answer": "tango",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "beginning"
                ],
                "rank": 2705,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.605964370071888
            },
            {
                "question verbose": "What is to below ",
                "b": "below",
                "expected answer": [
                    "above",
                    "higher",
                    "up"
                ],
                "predictions": [
                    {
                        "score": 0.5081164934538932,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31426771148814847,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3116424495227591,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.3098494518952321,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.30858030534727887,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3074378093542961,
                        "answer": "badassery",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "below"
                ],
                "rank": 3691,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to climb ",
                "b": "climb",
                "expected answer": [
                    "descend",
                    "declivitous",
                    "downhill",
                    "downward-sloping",
                    "degressive",
                    "descendant",
                    "descendent",
                    "down",
                    "downward",
                    "downward-arching",
                    "drizzling",
                    "dropping",
                    "falling",
                    "raining"
                ],
                "predictions": [
                    {
                        "score": 0.4460334320805922,
                        "answer": "infidelity",
                        "hit": false
                    },
                    {
                        "score": 0.44178108710504405,
                        "answer": "buta",
                        "hit": false
                    },
                    {
                        "score": 0.44121718613077493,
                        "answer": "intercourse",
                        "hit": false
                    },
                    {
                        "score": 0.4359841921114941,
                        "answer": "nuture",
                        "hit": false
                    },
                    {
                        "score": 0.4355940450761071,
                        "answer": "discarded",
                        "hit": false
                    },
                    {
                        "score": 0.4354352410330002,
                        "answer": "rebuff",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "climb"
                ],
                "rank": 418,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.8008139133453369
            },
            {
                "question verbose": "What is to dead ",
                "b": "dead",
                "expected answer": [
                    "alive",
                    "living",
                    "live"
                ],
                "predictions": [
                    {
                        "score": 0.34554330961072144,
                        "answer": "cardboard",
                        "hit": false
                    },
                    {
                        "score": 0.33355783970697456,
                        "answer": "constantly",
                        "hit": false
                    },
                    {
                        "score": 0.3311517081760995,
                        "answer": "penalized",
                        "hit": false
                    },
                    {
                        "score": 0.3234193295699954,
                        "answer": "rodger",
                        "hit": false
                    },
                    {
                        "score": 0.3224448615417626,
                        "answer": "unsure",
                        "hit": false
                    },
                    {
                        "score": 0.31914714329759614,
                        "answer": "kneel",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dead"
                ],
                "rank": 2502,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6705560833215714
            },
            {
                "question verbose": "What is to decrement ",
                "b": "decrement",
                "expected answer": [
                    "increment"
                ],
                "predictions": [
                    {
                        "score": 0.5229101110977065,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30385669390030295,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.3031611371708212,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.2947563398956996,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.29260659274872663,
                        "answer": "hungry",
                        "hit": false
                    },
                    {
                        "score": 0.2915933008973039,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "decrement"
                ],
                "rank": 582,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7019960135221481
            },
            {
                "question verbose": "What is to descend ",
                "b": "descend",
                "expected answer": [
                    "ascend",
                    "climb",
                    "rise",
                    "upward",
                    "ascendent",
                    "ascendant"
                ],
                "predictions": [
                    {
                        "score": 0.5402267984209234,
                        "answer": "swf",
                        "hit": false
                    },
                    {
                        "score": 0.5284650314386574,
                        "answer": "brag",
                        "hit": false
                    },
                    {
                        "score": 0.5235880561990877,
                        "answer": "brisk",
                        "hit": false
                    },
                    {
                        "score": 0.5213082100144893,
                        "answer": "tacky",
                        "hit": false
                    },
                    {
                        "score": 0.5206582258203138,
                        "answer": "bloodhorse",
                        "hit": false
                    },
                    {
                        "score": 0.5155541075816024,
                        "answer": "immature",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "descend"
                ],
                "rank": 6610,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6479316204786301
            },
            {
                "question verbose": "What is to dive ",
                "b": "dive",
                "expected answer": [
                    "emerge"
                ],
                "predictions": [
                    {
                        "score": 0.43519525472439036,
                        "answer": "racked",
                        "hit": false
                    },
                    {
                        "score": 0.4335464559608462,
                        "answer": "screwdriver",
                        "hit": false
                    },
                    {
                        "score": 0.4168342656383835,
                        "answer": "roy",
                        "hit": false
                    },
                    {
                        "score": 0.4162414481756466,
                        "answer": "coldly",
                        "hit": false
                    },
                    {
                        "score": 0.41515670923916254,
                        "answer": "innocence",
                        "hit": false
                    },
                    {
                        "score": 0.4146212797630281,
                        "answer": "creepy",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dive"
                ],
                "rank": 7858,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6555524319410324
            },
            {
                "question verbose": "What is to down ",
                "b": "down",
                "expected answer": [
                    "up",
                    "above",
                    "ahead",
                    "lead",
                    "leading",
                    "aweigh",
                    "dormie",
                    "dormy",
                    "heavenward",
                    "skyward",
                    "risen",
                    "sprouted",
                    "upbound",
                    "upfield",
                    "upward"
                ],
                "predictions": [
                    {
                        "score": 0.4497542120444469,
                        "answer": "ups",
                        "hit": false
                    },
                    {
                        "score": 0.4458296264835468,
                        "answer": "nzpc",
                        "hit": false
                    },
                    {
                        "score": 0.432684748700646,
                        "answer": "biblical",
                        "hit": false
                    },
                    {
                        "score": 0.4285419441831728,
                        "answer": "huffington",
                        "hit": false
                    },
                    {
                        "score": 0.4276276274929132,
                        "answer": "counterterrorist",
                        "hit": false
                    },
                    {
                        "score": 0.4227900797828439,
                        "answer": "bruised",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "down"
                ],
                "rank": 11175,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6039124801754951
            },
            {
                "question verbose": "What is to downslope ",
                "b": "downslope",
                "expected answer": [
                    "upslope",
                    "ascent"
                ],
                "predictions": [
                    {
                        "score": 0.5088980755334713,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3016043493569883,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.2987837243930861,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.2981678973274217,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.29686779826312043,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.2957011400246355,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "downslope"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to drop ",
                "b": "drop",
                "expected answer": [
                    "lift",
                    "pick_up"
                ],
                "predictions": [
                    {
                        "score": 0.40515790532151036,
                        "answer": "barking",
                        "hit": false
                    },
                    {
                        "score": 0.40177561438274384,
                        "answer": "individually",
                        "hit": false
                    },
                    {
                        "score": 0.39938283054741236,
                        "answer": "hostage",
                        "hit": false
                    },
                    {
                        "score": 0.3971723354550694,
                        "answer": "eve",
                        "hit": false
                    },
                    {
                        "score": 0.39575006916383837,
                        "answer": "nut",
                        "hit": false
                    },
                    {
                        "score": 0.3931026387558255,
                        "answer": "turkey",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "drop"
                ],
                "rank": 15339,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6204886585474014
            },
            {
                "question verbose": "What is to dynamic ",
                "b": "dynamic",
                "expected answer": [
                    "static",
                    "still",
                    "unmoving",
                    "unchanging"
                ],
                "predictions": [
                    {
                        "score": 0.4105533077392842,
                        "answer": "shew",
                        "hit": false
                    },
                    {
                        "score": 0.40586925810862623,
                        "answer": "blank",
                        "hit": false
                    },
                    {
                        "score": 0.4004997819321319,
                        "answer": "hop",
                        "hit": false
                    },
                    {
                        "score": 0.39877737991397416,
                        "answer": "evaluatory",
                        "hit": false
                    },
                    {
                        "score": 0.38859101708188304,
                        "answer": "seated",
                        "hit": false
                    },
                    {
                        "score": 0.38372259215482574,
                        "answer": "evolutionist",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "dynamic"
                ],
                "rank": 11978,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7350664734840393
            },
            {
                "question verbose": "What is to employ ",
                "b": "employ",
                "expected answer": [
                    "dismiss",
                    "fire",
                    "sack",
                    "displace",
                    "terminate"
                ],
                "predictions": [
                    {
                        "score": 0.503627367491268,
                        "answer": "heaped",
                        "hit": false
                    },
                    {
                        "score": 0.5015375515851679,
                        "answer": "unethical",
                        "hit": false
                    },
                    {
                        "score": 0.4925721582860596,
                        "answer": "pulling",
                        "hit": false
                    },
                    {
                        "score": 0.485641684318204,
                        "answer": "curtain",
                        "hit": false
                    },
                    {
                        "score": 0.4849169673542245,
                        "answer": "authorizing",
                        "hit": false
                    },
                    {
                        "score": 0.4795053561757598,
                        "answer": "exotic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "employ"
                ],
                "rank": 937,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7539868652820587
            },
            {
                "question verbose": "What is to exit ",
                "b": "exit",
                "expected answer": [
                    "entrance",
                    "entranceway",
                    "entryway",
                    "entry",
                    "entree"
                ],
                "predictions": [
                    {
                        "score": 0.5230542298697358,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31807751766304226,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.3162689630652597,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3157749382047169,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.31328970448029203,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.31203153992656063,
                        "answer": "cinematic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "exit"
                ],
                "rank": 2229,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6355628222227097
            },
            {
                "question verbose": "What is to fall ",
                "b": "fall",
                "expected answer": [
                    "rise",
                    "upward",
                    "climb"
                ],
                "predictions": [
                    {
                        "score": 0.2987888428004317,
                        "answer": "parlor",
                        "hit": false
                    },
                    {
                        "score": 0.29565737843754847,
                        "answer": "prodigal",
                        "hit": false
                    },
                    {
                        "score": 0.29543886529800073,
                        "answer": "snickering",
                        "hit": false
                    },
                    {
                        "score": 0.28948827747089123,
                        "answer": "forever",
                        "hit": false
                    },
                    {
                        "score": 0.28929476447782376,
                        "answer": "washed",
                        "hit": false
                    },
                    {
                        "score": 0.28795452509113645,
                        "answer": "creates",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "fall"
                ],
                "rank": 756,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6487670689821243
            },
            {
                "question verbose": "What is to first ",
                "b": "first",
                "expected answer": [
                    "last",
                    "end",
                    "terminal",
                    "ending",
                    "final",
                    "finish",
                    "finale",
                    "conclusion"
                ],
                "predictions": [
                    {
                        "score": 0.315090712386667,
                        "answer": "amendment",
                        "hit": false
                    },
                    {
                        "score": 0.3139960038487948,
                        "answer": "violation",
                        "hit": false
                    },
                    {
                        "score": 0.3020766947266758,
                        "answer": "repealed",
                        "hit": false
                    },
                    {
                        "score": 0.28992286301563025,
                        "answer": "supplant",
                        "hit": false
                    },
                    {
                        "score": 0.2863571501946717,
                        "answer": "trap",
                        "hit": false
                    },
                    {
                        "score": 0.28186077451514696,
                        "answer": "wera",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "first"
                ],
                "rank": 7134,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6022719666361809
            },
            {
                "question verbose": "What is to forget ",
                "b": "forget",
                "expected answer": [
                    "remember",
                    "retrieve",
                    "recall",
                    "call_back",
                    "call_up",
                    "recollect"
                ],
                "predictions": [
                    {
                        "score": 0.3829420853303684,
                        "answer": "cnn",
                        "hit": false
                    },
                    {
                        "score": 0.3689988574971639,
                        "answer": "expounding",
                        "hit": false
                    },
                    {
                        "score": 0.3677836940090296,
                        "answer": "cancel",
                        "hit": false
                    },
                    {
                        "score": 0.36101396177264117,
                        "answer": "dream",
                        "hit": false
                    },
                    {
                        "score": 0.3605051048556084,
                        "answer": "panic",
                        "hit": false
                    },
                    {
                        "score": 0.36042357274964876,
                        "answer": "waited",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "forget"
                ],
                "rank": 1544,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6965588182210922
            },
            {
                "question verbose": "What is to forward ",
                "b": "forward",
                "expected answer": [
                    "backward",
                    "back",
                    "backwards",
                    "rearward",
                    "rearwards"
                ],
                "predictions": [
                    {
                        "score": 0.4112577488369554,
                        "answer": "undoing",
                        "hit": false
                    },
                    {
                        "score": 0.3894035730080701,
                        "answer": "threatens",
                        "hit": false
                    },
                    {
                        "score": 0.37681117728293234,
                        "answer": "admittedly",
                        "hit": false
                    },
                    {
                        "score": 0.3689473258590031,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.36843617718921456,
                        "answer": "originator",
                        "hit": false
                    },
                    {
                        "score": 0.3653635245461036,
                        "answer": "bloodhorse",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "forward"
                ],
                "rank": 326,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.570742167532444
            },
            {
                "question verbose": "What is to front ",
                "b": "front",
                "expected answer": [
                    "back",
                    "rear",
                    "forepart"
                ],
                "predictions": [
                    {
                        "score": 0.34421870866984394,
                        "answer": "lung",
                        "hit": false
                    },
                    {
                        "score": 0.3257900246840596,
                        "answer": "downing",
                        "hit": false
                    },
                    {
                        "score": 0.31633640558303167,
                        "answer": "plaza",
                        "hit": false
                    },
                    {
                        "score": 0.307982158774365,
                        "answer": "exodus",
                        "hit": false
                    },
                    {
                        "score": 0.30567536656433375,
                        "answer": "downed",
                        "hit": false
                    },
                    {
                        "score": 0.30227960034457163,
                        "answer": "smoking",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "front"
                ],
                "rank": 557,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6540588587522507
            },
            {
                "question verbose": "What is to in ",
                "b": "in",
                "expected answer": [
                    "out",
                    "outer",
                    "exterior",
                    "outside"
                ],
                "predictions": [
                    {
                        "score": 0.5228242932320404,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31489347537928264,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.31324842167062367,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3126306920302654,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3095972497456238,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3084121879703809,
                        "answer": "cinematic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "in"
                ],
                "rank": 3288,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6113622263073921
            },
            {
                "question verbose": "What is to inbound ",
                "b": "inbound",
                "expected answer": [
                    "outbound"
                ],
                "predictions": [
                    {
                        "score": 0.5098634578933641,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.323930956772018,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.31516670123455554,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3126530294939273,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.3098295065605423,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3090565851312186,
                        "answer": "flame",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inbound"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to inhale ",
                "b": "inhale",
                "expected answer": [
                    "exhale"
                ],
                "predictions": [
                    {
                        "score": 0.5084754842601295,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31305067926357044,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3101180468086036,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.30294311682890274,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.301767413682439,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.29961873983386744,
                        "answer": "resonate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inhale"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to input ",
                "b": "input",
                "expected answer": [
                    "output"
                ],
                "predictions": [
                    {
                        "score": 0.4536174025880431,
                        "answer": "dismiss",
                        "hit": false
                    },
                    {
                        "score": 0.44707629387970266,
                        "answer": "normally",
                        "hit": false
                    },
                    {
                        "score": 0.4394472750069574,
                        "answer": "completion",
                        "hit": false
                    },
                    {
                        "score": 0.43452056646971615,
                        "answer": "alleging",
                        "hit": false
                    },
                    {
                        "score": 0.4333272749436541,
                        "answer": "confuses",
                        "hit": false
                    },
                    {
                        "score": 0.4326089598430078,
                        "answer": "pregnant",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "input"
                ],
                "rank": 250,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7619598805904388
            },
            {
                "question verbose": "What is to inside ",
                "b": "inside",
                "expected answer": [
                    "outside",
                    "exterior",
                    "out"
                ],
                "predictions": [
                    {
                        "score": 0.40712165883372475,
                        "answer": "douchebags",
                        "hit": false
                    },
                    {
                        "score": 0.39258280564160386,
                        "answer": "shipload",
                        "hit": false
                    },
                    {
                        "score": 0.3850700373409879,
                        "answer": "penalized",
                        "hit": false
                    },
                    {
                        "score": 0.3803668276648602,
                        "answer": "tree",
                        "hit": false
                    },
                    {
                        "score": 0.3794257802005664,
                        "answer": "wax",
                        "hit": false
                    },
                    {
                        "score": 0.366714322878837,
                        "answer": "versus",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inside"
                ],
                "rank": 5723,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7341013997793198
            },
            {
                "question verbose": "What is to interior ",
                "b": "interior",
                "expected answer": [
                    "exterior",
                    "outdoor",
                    "out-of-door",
                    "outside"
                ],
                "predictions": [
                    {
                        "score": 0.416291048716532,
                        "answer": "section",
                        "hit": false
                    },
                    {
                        "score": 0.3994982854237712,
                        "answer": "lars",
                        "hit": false
                    },
                    {
                        "score": 0.3991181913972068,
                        "answer": "kg",
                        "hit": false
                    },
                    {
                        "score": 0.3917896432898006,
                        "answer": "nurture",
                        "hit": false
                    },
                    {
                        "score": 0.3910263742923031,
                        "answer": "killer",
                        "hit": false
                    },
                    {
                        "score": 0.3905271459008042,
                        "answer": "allocating",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "interior"
                ],
                "rank": 3740,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.787669450044632
            },
            {
                "question verbose": "What is to internal ",
                "b": "internal",
                "expected answer": [
                    "external",
                    "outer",
                    "outside"
                ],
                "predictions": [
                    {
                        "score": 0.3928735343524016,
                        "answer": "tweeted",
                        "hit": false
                    },
                    {
                        "score": 0.3785399173865289,
                        "answer": "assailant",
                        "hit": false
                    },
                    {
                        "score": 0.3778350079420389,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3709734689471456,
                        "answer": "muntants",
                        "hit": false
                    },
                    {
                        "score": 0.3658111226624726,
                        "answer": "pregnant",
                        "hit": false
                    },
                    {
                        "score": 0.35824602597192146,
                        "answer": "elegance",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "internal"
                ],
                "rank": 272,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7634604275226593
            },
            {
                "question verbose": "What is to inverse ",
                "b": "inverse",
                "expected answer": [
                    "reverse",
                    "reversion",
                    "reversal",
                    "turnabout",
                    "turnaround"
                ],
                "predictions": [
                    {
                        "score": 0.522379795377567,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31940044470450807,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.31313862786013746,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.31187682769980596,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.30821770247573427,
                        "answer": "gene",
                        "hit": false
                    },
                    {
                        "score": 0.30527089109028277,
                        "answer": "badassery",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "inverse"
                ],
                "rank": 1554,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6859570741653442
            },
            {
                "question verbose": "What is to mortal ",
                "b": "mortal",
                "expected answer": [
                    "immortal"
                ],
                "predictions": [
                    {
                        "score": 0.5223295906276536,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.320956678005105,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3144612425707787,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.3108140122324829,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.31079466478418966,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.3051375318119985,
                        "answer": "prevented",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "mortal"
                ],
                "rank": 2319,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.65712670981884
            },
            {
                "question verbose": "What is to occupied ",
                "b": "occupied",
                "expected answer": [
                    "vacant",
                    "free"
                ],
                "predictions": [
                    {
                        "score": 0.5240138239529799,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3081581095487606,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3072165083119483,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.2976202003435276,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.2956729312851259,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.2934386583624113,
                        "answer": "gene",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "occupied"
                ],
                "rank": 3033,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6648062765598297
            },
            {
                "question verbose": "What is to off ",
                "b": "off",
                "expected answer": [
                    "on"
                ],
                "predictions": [
                    {
                        "score": 0.5107864657676742,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.32450102186917024,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.31813045464757056,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.31521150121466396,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3105609668771738,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.30948541902354215,
                        "answer": "rush",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "off"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to out ",
                "b": "out",
                "expected answer": [
                    "in",
                    "inside"
                ],
                "predictions": [
                    {
                        "score": 0.49628263168439873,
                        "answer": "excessive",
                        "hit": false
                    },
                    {
                        "score": 0.4911545366078986,
                        "answer": "failing",
                        "hit": false
                    },
                    {
                        "score": 0.4910144023829901,
                        "answer": "tacky",
                        "hit": false
                    },
                    {
                        "score": 0.48965021980466183,
                        "answer": "allot",
                        "hit": false
                    },
                    {
                        "score": 0.48951173895731254,
                        "answer": "roy",
                        "hit": false
                    },
                    {
                        "score": 0.48835826346845457,
                        "answer": "brisk",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "out"
                ],
                "rank": 14462,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6113622263073921
            },
            {
                "question verbose": "What is to outward ",
                "b": "outward",
                "expected answer": [
                    "upward",
                    "up",
                    "upwards"
                ],
                "predictions": [
                    {
                        "score": 0.5219367941235147,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3165086081704539,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.30773038930726515,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3060610877643789,
                        "answer": "send",
                        "hit": false
                    },
                    {
                        "score": 0.3051248664616401,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.3043796413196335,
                        "answer": "cinematic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "outward"
                ],
                "rank": 4038,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6520956158638
            },
            {
                "question verbose": "What is to over ",
                "b": "over",
                "expected answer": [
                    "under",
                    "below",
                    "beneath"
                ],
                "predictions": [
                    {
                        "score": 0.5090578809962177,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3107860627792736,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.30333215348196524,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.30225006514538844,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3021501235183917,
                        "answer": "hungry",
                        "hit": false
                    },
                    {
                        "score": 0.3017041673016047,
                        "answer": "cinematic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "over"
                ],
                "rank": 11913,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to previously ",
                "b": "previously",
                "expected answer": [
                    "subsequently",
                    "later",
                    "afterwards",
                    "afterward",
                    "after",
                    "subsequent"
                ],
                "predictions": [
                    {
                        "score": 0.4964773147936911,
                        "answer": "wsj",
                        "hit": false
                    },
                    {
                        "score": 0.47997323796480873,
                        "answer": "champlain",
                        "hit": false
                    },
                    {
                        "score": 0.4591809346554035,
                        "answer": "hr",
                        "hit": false
                    },
                    {
                        "score": 0.4439139805404169,
                        "answer": "sprout",
                        "hit": false
                    },
                    {
                        "score": 0.4371990405071438,
                        "answer": "forecast",
                        "hit": false
                    },
                    {
                        "score": 0.4258692063201225,
                        "answer": "peripheal",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "previously"
                ],
                "rank": 6441,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5660735070705414
            },
            {
                "question verbose": "What is to proceed ",
                "b": "proceed",
                "expected answer": [
                    "retreat",
                    "return"
                ],
                "predictions": [
                    {
                        "score": 0.5125162095502274,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31562268334987104,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.3137545328780225,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.30749598026173497,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.30560278754728964,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.3031271022332206,
                        "answer": "flame",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "proceed"
                ],
                "rank": 5671,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to rise ",
                "b": "rise",
                "expected answer": [
                    "sink",
                    "drop",
                    "fall"
                ],
                "predictions": [
                    {
                        "score": 0.4742448270527362,
                        "answer": "knight",
                        "hit": false
                    },
                    {
                        "score": 0.38019201824847915,
                        "answer": "mount",
                        "hit": false
                    },
                    {
                        "score": 0.37503778309323166,
                        "answer": "dark",
                        "hit": false
                    },
                    {
                        "score": 0.3697541111840342,
                        "answer": "midday",
                        "hit": false
                    },
                    {
                        "score": 0.3655034932612379,
                        "answer": "stupidly",
                        "hit": false
                    },
                    {
                        "score": 0.35802745393214835,
                        "answer": "leftover",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "rise"
                ],
                "rank": 6545,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7144113183021545
            },
            {
                "question verbose": "What is to south ",
                "b": "south",
                "expected answer": [
                    "north"
                ],
                "predictions": [
                    {
                        "score": 0.359322967719051,
                        "answer": "april",
                        "hit": false
                    },
                    {
                        "score": 0.34979485480935535,
                        "answer": "attu",
                        "hit": false
                    },
                    {
                        "score": 0.3467743536368265,
                        "answer": "chemist",
                        "hit": false
                    },
                    {
                        "score": 0.3420992042391718,
                        "answer": "march",
                        "hit": false
                    },
                    {
                        "score": 0.3390019563259208,
                        "answer": "riot",
                        "hit": false
                    },
                    {
                        "score": 0.3323786578085164,
                        "answer": "milions",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "south"
                ],
                "rank": 1593,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7743901014328003
            },
            {
                "question verbose": "What is to southeast ",
                "b": "southeast",
                "expected answer": [
                    "southwest",
                    "northeast"
                ],
                "predictions": [
                    {
                        "score": 0.5100076385633122,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.30727338466428467,
                        "answer": "gene",
                        "hit": false
                    },
                    {
                        "score": 0.3071219459037369,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.304580712416483,
                        "answer": "proposed",
                        "hit": false
                    },
                    {
                        "score": 0.30269883554870763,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3016331091388753,
                        "answer": "badassery",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "southeast"
                ],
                "rank": 14374,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to submerge ",
                "b": "submerge",
                "expected answer": [
                    "emerge"
                ],
                "predictions": [
                    {
                        "score": 0.5561344980798097,
                        "answer": "buta",
                        "hit": false
                    },
                    {
                        "score": 0.5500482113726587,
                        "answer": "tweaked",
                        "hit": false
                    },
                    {
                        "score": 0.542790293811433,
                        "answer": "partycandidatepundits",
                        "hit": false
                    },
                    {
                        "score": 0.5410255807739016,
                        "answer": "stimulous",
                        "hit": false
                    },
                    {
                        "score": 0.5405425792116221,
                        "answer": "escalating",
                        "hit": false
                    },
                    {
                        "score": 0.5394663082686997,
                        "answer": "resonate",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "submerge"
                ],
                "rank": 4243,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7690723538398743
            },
            {
                "question verbose": "What is to top ",
                "b": "top",
                "expected answer": [
                    "bottom",
                    "underside",
                    "undersurface"
                ],
                "predictions": [
                    {
                        "score": 0.3357307723835025,
                        "answer": "rec",
                        "hit": false
                    },
                    {
                        "score": 0.3047897680059253,
                        "answer": "reflective",
                        "hit": false
                    },
                    {
                        "score": 0.2939866501007959,
                        "answer": "sassy",
                        "hit": false
                    },
                    {
                        "score": 0.29202492273171615,
                        "answer": "instrument",
                        "hit": false
                    },
                    {
                        "score": 0.2892654618948334,
                        "answer": "nucleus",
                        "hit": false
                    },
                    {
                        "score": 0.2872030582952977,
                        "answer": "earned",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "top"
                ],
                "rank": 5518,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7060615569353104
            },
            {
                "question verbose": "What is to toward ",
                "b": "toward",
                "expected answer": [
                    "away",
                    "off",
                    "forth",
                    "aside"
                ],
                "predictions": [
                    {
                        "score": 0.3697031880134643,
                        "answer": "capitalism",
                        "hit": false
                    },
                    {
                        "score": 0.35686028294889643,
                        "answer": "lounge",
                        "hit": false
                    },
                    {
                        "score": 0.3536251276904716,
                        "answer": "incresae",
                        "hit": false
                    },
                    {
                        "score": 0.34326237228058015,
                        "answer": "confront",
                        "hit": false
                    },
                    {
                        "score": 0.33946698140495873,
                        "answer": "debtor",
                        "hit": false
                    },
                    {
                        "score": 0.33677530892805924,
                        "answer": "undead",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "toward"
                ],
                "rank": 5803,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5391580238938332
            },
            {
                "question verbose": "What is to true ",
                "b": "true",
                "expected answer": [
                    "false",
                    "incorrect",
                    "wrong",
                    "mistaken"
                ],
                "predictions": [
                    {
                        "score": 0.33744072315451185,
                        "answer": "strange",
                        "hit": false
                    },
                    {
                        "score": 0.32875463876603456,
                        "answer": "frequently",
                        "hit": false
                    },
                    {
                        "score": 0.32153024680861086,
                        "answer": "allegedly",
                        "hit": false
                    },
                    {
                        "score": 0.318866295653801,
                        "answer": "incest",
                        "hit": false
                    },
                    {
                        "score": 0.3158943733852954,
                        "answer": "sage",
                        "hit": false
                    },
                    {
                        "score": 0.3105747215265757,
                        "answer": "fetus",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "true"
                ],
                "rank": 199,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6424547731876373
            },
            {
                "question verbose": "What is to under ",
                "b": "under",
                "expected answer": [
                    "over",
                    "above",
                    "up"
                ],
                "predictions": [
                    {
                        "score": 0.5127705335536398,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3246647375846574,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.3150431602382637,
                        "answer": "hungry",
                        "hit": false
                    },
                    {
                        "score": 0.3145466160606414,
                        "answer": "gene",
                        "hit": false
                    },
                    {
                        "score": 0.31402053696154464,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.30721341680062897,
                        "answer": "badassery",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "under"
                ],
                "rank": 15340,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 1.0
            },
            {
                "question verbose": "What is to up ",
                "b": "up",
                "expected answer": [
                    "down",
                    "downwards",
                    "downward",
                    "downwardly"
                ],
                "predictions": [
                    {
                        "score": 0.522115163545005,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.31097901189001337,
                        "answer": "prevented",
                        "hit": false
                    },
                    {
                        "score": 0.3108817691734624,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.3063375929446548,
                        "answer": "eschew",
                        "hit": false
                    },
                    {
                        "score": 0.3049601870469822,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.30423708797263854,
                        "answer": "insured",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "up"
                ],
                "rank": 706,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.6039124801754951
            },
            {
                "question verbose": "What is to uphill ",
                "b": "uphill",
                "expected answer": [
                    "downhill",
                    "decline",
                    "fall",
                    "declivitous",
                    "downward-sloping"
                ],
                "predictions": [
                    {
                        "score": 0.5232020323855886,
                        "answer": "would",
                        "hit": false
                    },
                    {
                        "score": 0.3230386763385485,
                        "answer": "flame",
                        "hit": false
                    },
                    {
                        "score": 0.3064449493269808,
                        "answer": "insured",
                        "hit": false
                    },
                    {
                        "score": 0.30637149086878857,
                        "answer": "rush",
                        "hit": false
                    },
                    {
                        "score": 0.3027093313022783,
                        "answer": "badassery",
                        "hit": false
                    },
                    {
                        "score": 0.29885487740203104,
                        "answer": "cinematic",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "uphill"
                ],
                "rank": 11717,
                "landing_b": false,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.5914892703294754
            },
            {
                "question verbose": "What is to west ",
                "b": "west",
                "expected answer": [
                    "east"
                ],
                "predictions": [
                    {
                        "score": 0.38602419109560826,
                        "answer": "informing",
                        "hit": false
                    },
                    {
                        "score": 0.38558573463131124,
                        "answer": "assembly",
                        "hit": false
                    },
                    {
                        "score": 0.3823898352540563,
                        "answer": "wrapped",
                        "hit": false
                    },
                    {
                        "score": 0.3727334378928467,
                        "answer": "wally",
                        "hit": false
                    },
                    {
                        "score": 0.3725319564606776,
                        "answer": "rebuff",
                        "hit": false
                    },
                    {
                        "score": 0.3695086437641975,
                        "answer": "amer",
                        "hit": false
                    }
                ],
                "set_exclude": [
                    "west"
                ],
                "rank": 4921,
                "landing_b": true,
                "landing_b_prime": false,
                "landing_a": false,
                "landing_a_prime": false,
                "similarity b to b_prime cosine": 0.7201949059963226
            }
        ],
        "result": {
            "cnt_questions_correct": 0,
            "cnt_questions_total": 50,
            "accuracy": 0.0
        },
        "experiment_setup": {
            "dataset": {
                "_base_path": "/home/genvekt/Dev/NLP/HW1/BATS_3.0/",
                "class": "dataset",
                "task": "analogy",
                "language": [
                    "English"
                ],
                "name": "BATS",
                "description": "Bigger Analogy Test Set",
                "domain": "general",
                "date": "2016",
                "source": "original",
                "project_page": "http://vecto.space/projects/BATS",
                "version": "3.0",
                "size": "98,200 analogy questions (2,000 word pairs)",
                "cite": [
                    {
                        "contribution": "the original dataset",
                        "bibtex": {
                            "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
                            "author": [
                                {
                                    "name": "Anna Gladkova"
                                },
                                {
                                    "name": "Aleksandr Drozd"
                                },
                                {
                                    "name": "Satoshi Matsuoka"
                                }
                            ],
                            "doi": "10.18653/v1/N16-2002",
                            "url": "https://www.aclweb.org/anthology/N/N16/N16-2002.pdf",
                            "booktitle": "Proceedings of the NAACL-HLT SRW",
                            "publisher": "ACL",
                            "year": 2016,
                            "pages": "47-54",
                            "type": "inproceedings",
                            "id": "GladkovaDrozdEtAl_2016"
                        }
                    }
                ],
                "_class": "vecto.data.base.Dataset"
            },
            "embeddings": {
                "_class": "vecto.embeddings.dense.WordEmbeddingsDense",
                "normalized": true
            },
            "category": "4_Lexicographic_semantics",
            "subcategory": "L10 [antonyms - binary].txt",
            "task": "word_analogy",
            "default_measurement": "accuracy",
            "method": "LRCos",
            "uuid": "f4f46037-da9d-4cef-8f0c-31f80e3af46e",
            "timestamp": "2020-10-22T15:57:57.032668"
        }
    }
]